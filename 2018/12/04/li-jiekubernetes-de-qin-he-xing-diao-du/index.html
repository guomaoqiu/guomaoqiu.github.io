<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>理解Kubernetes的亲和性调度 | OpsThoughts</title><meta name="author" content="NoardGuo-Ops"><meta name="copyright" content="NoardGuo-Ops"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="NodeName、nodeSelector、nodeAffinity、podAffinity、Taints以及Tolerations用法 1. NodeNamePod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是强制匹配。例子：我的预期">
<meta property="og:type" content="article">
<meta property="og:title" content="理解Kubernetes的亲和性调度">
<meta property="og:url" content="https://blog.sctux.cc/2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/index.html">
<meta property="og:site_name" content="OpsThoughts">
<meta property="og:description" content="NodeName、nodeSelector、nodeAffinity、podAffinity、Taints以及Tolerations用法 1. NodeNamePod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是强制匹配。例子：我的预期">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.unsplash.com/photo-1563207153-f403bf289096?w=800&h=400">
<meta property="article:published_time" content="2018-12-04T15:53:16.000Z">
<meta property="article:modified_time" content="2025-09-01T01:59:08.920Z">
<meta property="article:author" content="NoardGuo-Ops">
<meta property="article:tag" content="k8s">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1563207153-f403bf289096?w=800&h=400"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "理解Kubernetes的亲和性调度",
  "url": "https://blog.sctux.cc/2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/",
  "image": "https://images.unsplash.com/photo-1563207153-f403bf289096?w=800&h=400",
  "datePublished": "2018-12-04T15:53:16.000Z",
  "dateModified": "2025-09-01T01:59:08.920Z",
  "author": [
    {
      "@type": "Person",
      "name": "NoardGuo-Ops",
      "url": "https://blog.sctux.cc"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://blog.sctux.cc/2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '理解Kubernetes的亲和性调度',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="OpsThoughts" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      if ($loadingBox.classList.contains('loaded')) return
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()

  if (document.readyState === 'complete') {
    preloader.endLoading()
  } else {
    window.addEventListener('load', preloader.endLoading)
    document.addEventListener('DOMContentLoaded', preloader.endLoading)
    // Add timeout protection: force end after 7 seconds
    setTimeout(preloader.endLoading, 7000)
  }

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">99</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">93</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">30</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/works/"><i class="fa-fw fas fa-folder-open"></i><span> 作品</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://images.unsplash.com/photo-1563207153-f403bf289096?w=800&amp;h=400);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">OpsThoughts</span></a><a class="nav-page-title" href="/"><span class="site-name">理解Kubernetes的亲和性调度</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/works/"><i class="fa-fw fas fa-folder-open"></i><span> 作品</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">理解Kubernetes的亲和性调度</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2018-12-04T15:53:16.000Z" title="发表于 2018-12-04 23:53:16">2018-12-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-01T01:59:08.920Z" title="更新于 2025-09-01 09:59:08">2025-09-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Docker/">Docker</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><code>NodeName、nodeSelector、nodeAffinity、podAffinity、Taints以及Tolerations用法</code></p>
<h2 id="1-NodeName"><a href="#1-NodeName" class="headerlink" title="1. NodeName"></a>1. NodeName</h2><p>Pod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是<code>强制匹配</code>。<br>例子：<br>我的预期是将该pod运行于节点名称为 192.168.56.11 这个节点：</p>
<h6 id="1-1-例如-test-nodename-yaml"><a href="#1-1-例如-test-nodename-yaml" class="headerlink" title="1.1 例如(test-nodename.yaml)"></a>1.1 例如(test-nodename.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    app: with-nodename-busybox-pod
  name: with-nodename
spec:
  nodeName: 192.168.56.11 #通过这里指定
  containers:
  - command:
    - sleep
    - "3600"
    image: busybox
    imagePullPolicy: IfNotPresent
    name: test-busybox
</code></pre>
<h6 id="1-2-查看创建事件、与预期相符"><a href="#1-2-查看创建事件、与预期相符" class="headerlink" title="1.2 查看创建事件、与预期相符:"></a>1.2 查看创建事件、与预期相符:</h6><pre><code>Events:
  Type    Reason                 Age   From                    Message
  ----    ------                 ----  ----                    -------
  Normal  Scheduled              3m    default-scheduler       Successfully assigned with-pod-affinity to 192.168.56.11
  Normal  SuccessfulMountVolume  3m    kubelet, 192.168.56.11  MountVolume.SetUp succeeded for volume "default-token-5htws"
  Normal  Pulling                3m    kubelet, 192.168.56.11  pulling image "nginx"
  Normal  Pulled                 1m    kubelet, 192.168.56.11  Successfully pulled image "nginx"
  Normal  Created                1m    kubelet, 192.168.56.11  Created container
  Normal  Started                1m    kubelet, 192.168.56.11  Started container
</code></pre>
<h2 id="2-NodeSelector"><a href="#2-NodeSelector" class="headerlink" title="2. NodeSelector"></a>2. NodeSelector</h2><p>Pod.spec.nodeSelector是通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是<code>强制约束</code>。使用节点选择器的步骤为：</p>
<h6 id="2-1-Node添加label标记-标记规则："><a href="#2-1-Node添加label标记-标记规则：" class="headerlink" title="2.1 Node添加label标记,标记规则："></a>2.1 Node添加label标记,标记规则：</h6><pre><code>kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;
kubectl label nodes 192.168.56.14 server_type=game_server
</code></pre>
<h6 id="2-2-确认标记"><a href="#2-2-确认标记" class="headerlink" title="2.2 确认标记"></a>2.2 确认标记</h6><pre><code>[root@linux-node1 ~]# kubectl get nodes 192.168.56.14 --show-labels
NAME            STATUS    ROLES     AGE       VERSION   LABELS
192.168.56.14   Ready     node      82d       v1.10.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.56.14,node-role.kubernetes.io/node=true,server_type=game_server
</code></pre>
<h6 id="2-3-例如-test-nodeselector-yaml"><a href="#2-3-例如-test-nodeselector-yaml" class="headerlink" title="2.3 例如(test-nodeselector.yaml)"></a>2.3 例如(test-nodeselector.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    app: with-nodeselector-busybox-pod
  name: with-node-selector
spec:
  containers:
  - command:
    - sleep
    - "3600"
    image: busybox
    imagePullPolicy: IfNotPresent
    name: test-busybox
  # 通过节点标签进行预设该pod将会运行于有此标签的节点上面，
  # 如果多个节点拥有此标签，调度器将会择优进行调度
  nodeSelector:
    server_type: game_server
</code></pre>
<h5 id="2-4-查看创建事件"><a href="#2-4-查看创建事件" class="headerlink" title="2.4 查看创建事件:"></a>2.4 查看创建事件:</h5><pre><code>Events:
  Type    Reason                 Age   From                    Message
  ----    ------                 ----  ----                    -------
  Normal  Scheduled              16s   default-scheduler       Successfully assigned with-node-selector to 192.168.56.14
  Normal  SuccessfulMountVolume  16s   kubelet, 192.168.56.14  MountVolume.SetUp succeeded for volume "default-token-5htws"
  Normal  Pulling                11s   kubelet, 192.168.56.14  pulling image "busybox"
</code></pre>
<p>通过上面的例子我们可以感受到nodeSelector的方式比较直观，但是还够灵活，控制粒度偏大，下面我们再看另外一种更加灵活的方式：<code>nodeAffinity</code>。</p>
<h2 id="3-NodeAffinity"><a href="#3-NodeAffinity" class="headerlink" title="3. NodeAffinity"></a>3. NodeAffinity</h2><p><code>nodeAffinity</code>就是节点亲和性，相对应的是<code>Anti-Affinity</code>，就是反亲和性，这种方法比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。<br>调度可以分成软策略和硬策略两种方式，软策略就是如果你没有满足调度要求的节点的话，POD 就会忽略这条规则，继续完成调度过程。 <code>nodeAffinity</code>就有两上面两种策略：</p>
<pre><code># 软策略:(满足条件最好了，没有的话也无所谓了的策略)
preferredDuringSchedulingIgnoredDuringExecution
# 硬策略:(你必须满足我的要求，不然我就不干)
requiredDuringSchedulingIgnoredDuringExecution
</code></pre>
<h6 id="3-1-例如-test-node-affinity-yaml"><a href="#3-1-例如-test-node-affinity-yaml" class="headerlink" title="3.1 例如(test-node-affinity.yaml)"></a>3.1 例如(test-node-affinity.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
  labels:
    app: node-affinity-pod
spec:
  containers:
  - name: with-node-affinity
    image: nginx
    imagePullPolicy: IfNotPresent
  affinity:
    nodeAffinity:
      # 硬性策略
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: NotIn
            values:
            - 192.168.56.11
            - 192.168.56.12
      # 软性策略
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: server_type
            operator: In
            values:
              - game_server
</code></pre>
<p>上面的Pod我们进行一下解读:<br>1、该pod 通过硬性策略约束后将禁止调度到节点11，12上面去<br>2、排除节点11，12后，选择其他节点含有label为<code>server_type:game_server</code>的节点运行<br>3、如果其他节点也没有含有label <code>server_type:game_server</code>, 那么将会调度到任意节点运行<br>同样的我们可以使用descirbe命令查看具体的调度情况是否满足我们的要求。这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种：</p>
<ul>
<li>In：label 的值在某个列表中</li>
<li>NotIn：label 的值不在某个列表中</li>
<li>Gt：label 的值大于某个值</li>
<li>Lt：label 的值小于某个值</li>
<li>Exists：某个 label 存在</li>
<li>DoesNotExist：某个 label 不存在</li>
</ul>
<p>如果<code>nodeSelectorTerms</code>下面有多个选项的话，满足任何一个条件就可以了；如果<code>matchExpressions</code>有多个选项的话，则必须同时满足这些条件才能正常调度 POD。</p>
<h2 id="4-PodAffinity"><a href="#4-PodAffinity" class="headerlink" title="4. PodAffinity"></a>4. PodAffinity</h2><p>上面三种方式都是让POD去选择节点的，有的时候我们也希望能够根据 POD 之间的关系进行调度，Kubernetes在1.4版本引入的podAffinity概念就可以实现我们这个需求。</p>
<p>和<code>nodeAffinity</code>类似，<code>podAffinity</code>也有<code>requiredDuringSchedulingIgnoredDuringExecution(硬性策略)</code>和 <code>preferredDuringSchedulingIgnoredDuringExecution(软性策略)</code>两种调度策略，唯一不同的是如果要使用互斥性，我们需要使用<code>podAntiAffinity</code>字段。 如下例子，我们希望【with-pod-affinity】和【with-nodename-busybox-pod】能够就近部署，而不希望和【node-affinity-pod】部署在同一个拓扑域下面：</p>
<h6 id="4-1-例如-test-pod-affinity-yaml"><a href="#4-1-例如-test-pod-affinity-yaml" class="headerlink" title="4.1 例如(test-pod-affinity.yaml)"></a>4.1 例如(test-pod-affinity.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
  labels:
    app: pod-affinity-pod
spec:
  containers:
  - name: with-pod-affinity
    image: nginx
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - with-nodename-busybox-pod
        topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - node-affinity-pod
          topologyKey: kubernetes.io/hostname
</code></pre>
<p>上面的pod我们解读一下:<br>1、POD 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 POD：这个 POD 有一个<code>app=busybox-pod</code>的 label。<br>2、<code>podAntiAffinity</code>则是希望最好不要调度到这样的节点：这个节点上运行了某个 POD，而这个 POD 有app=node-affinity-pod的 label。<br>3、根据前面两个 POD 的定义，我们可以预见上面这个 POD 应该会被调度到192.168.56.11的节点上，因为前面实践的时候【with-nodename-busybox-pod】被调度到了192.168.56.11节点，而【node-affinity-pod】被调度到了192.168.56.11以为的节点，正好满足上面的需求。<br>通过describe查看：</p>
<pre><code>Events:
  Type    Reason                 Age   From                    Message
  ----    ------                 ----  ----                    -------
  Normal  Scheduled              53m   default-scheduler       Successfully assigned with-pod-affinity to 192.168.56.11
  Normal  SuccessfulMountVolume  52m   kubelet, 192.168.56.11  MountVolume.SetUp succeeded for volume "default-token-5htws"
  Normal  Pulling                52m   kubelet, 192.168.56.11  pulling image "nginx"
  Normal  Pulled                 51m   kubelet, 192.168.56.11  Successfully pulled image "nginx"
  Normal  Created                51m   kubelet, 192.168.56.11  Created container
  Normal  Started                51m   kubelet, 192.168.56.11  Started container
</code></pre>
<p>在labelSelector和 topologyKey的同级，还可以定义 namespaces 列表，表示匹配哪些 namespace 里面的 pod，默认情况下，会匹配定义的 pod 所在的 namespace；如果定义了这个字段，但是它的值为空，则匹配所有的 namespaces。</p>
<p>so，以上我们创建的四个例子结果为下:</p>
<pre><code>[root@linux-node1 affinity_study]# kubectl get pods -o wide
NAME                          READY     STATUS    RESTARTS   AGE       IP           NODE
with-node-affinity            1/1       Running   2          10h       10.2.70.16   192.168.56.14
with-node-selector            1/1       Running   0          8m        10.2.70.15   192.168.56.14
with-nodename                 1/1       Running   1          11h       10.2.88.9    192.168.56.11
with-pod-affinity             1/1       Running   0          10h       10.2.88.10   192.168.56.11
</code></pre>
<h2 id="5-污点（Taints）与容忍（tolerations）"><a href="#5-污点（Taints）与容忍（tolerations）" class="headerlink" title="5. 污点（Taints）与容忍（tolerations）"></a>5. 污点（Taints）与容忍（tolerations）</h2><p>对于nodeAffinity无论是硬策略还是软策略方式，都是调度 POD 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 POD 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度pod。</p>
<p>比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 POD，则污点就很有用了，POD 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：</p>
<h6 id="5-1-设置污点"><a href="#5-1-设置污点" class="headerlink" title="5.1 设置污点"></a>5.1 设置污点</h6><pre><code> kubectl taint node [node] key=value[effect]   
      其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ]
       NoSchedule ：一定不能被调度。POD 不会被调度到标记为 taints 节点。
       PreferNoSchedule：尽量不要调度。NoSchedule 的软策略版本。
       NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。
  示例：kubectl taint node 192.168.56.11 key1=value1:NoSchedule
</code></pre>
<h6 id="5-2-去除污点"><a href="#5-2-去除污点" class="headerlink" title="5.2 去除污点"></a>5.2 去除污点</h6><pre><code>#比如设置污点：
    kubectl taint nodes 192.168.56.11 key1=value1:NoSchedule
    kubectl taint nodes 192.168.56.11 key2=value2:NoExecute
    kubectl taint nodes 192.168.56.11 key3=value3:PreferNoSchedule
 #去除指定key及其effect：
    kubectl taint nodes 192.168.56.11 key:[effect]-    #(这里的key不用指定value)
                
 #去除指定key所有的effect: 
     kubectl taint nodes 192.168.56.11 key-
 
 #示例：
     kubectl taint node 192.168.56.11 key1:NoSchedule-
     kubectl taint node 192.168.56.11 key2:NoExecute-
     kubectl taint node 192.168.56.11 key3:PreferNoSchedule-
     kubectl taint node 192.168.56.11 key3- (去除指定key的所有effct)
</code></pre>
<h6 id="5-3-实践"><a href="#5-3-实践" class="headerlink" title="5.3 实践"></a>5.3 实践</h6><h6 id="5-3-1-给节点设置污点"><a href="#5-3-1-给节点设置污点" class="headerlink" title="# 5.3.1 给节点设置污点"></a># 5.3.1 给节点设置污点</h6><p>首先我的环境中还没有设置污点，我们执行以下例子(nginx-deployment.yaml):</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-pod
  labels:
    app: nginx-pod
spec:
  replicas: 10 # 这里为了实践效果特意设定了10个副本
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        
# 部署结果:
[root@linux-node1 ~]# kubectl get pods -o wide | grep nginx
nginx-pod-7d9f9876cc-95fj2   1/1       Running   0          1m        10.2.70.26   192.168.56.14
nginx-pod-7d9f9876cc-jclvf   1/1       Running   0          1m        10.2.44.28   192.168.56.13
nginx-pod-7d9f9876cc-k2m6x   1/1       Running   0          1m        10.2.44.31   192.168.56.13
nginx-pod-7d9f9876cc-l74hf   1/1       Running   0          1m        10.2.88.15   192.168.56.11
nginx-pod-7d9f9876cc-n8g6c   1/1       Running   0          1m        10.2.44.29   192.168.56.13
nginx-pod-7d9f9876cc-p2cft   1/1       Running   0          1m        10.2.88.14   192.168.56.11
nginx-pod-7d9f9876cc-p7hvt   1/1       Running   0          1m        10.2.69.25   192.168.56.12
nginx-pod-7d9f9876cc-rjj97   1/1       Running   0          1m        10.2.70.27   192.168.56.14
nginx-pod-7d9f9876cc-t2wlw   1/1       Running   0          1m        10.2.69.26   192.168.56.12
nginx-pod-7d9f9876cc-whkx9   1/1       Running   0          1m        10.2.44.30   192.168.56.13
</code></pre>
<p>以上例子可以看出在我的环境中已经分配到了每个节点包括我们接下来准备设置污点的节点192.168.56.11。</p>
<p>下面我把master节点192.168.56.11 设置污点(这里我用的effect是NoSchedule)</p>
<pre><code>[root@linux-node1 affinity_study]# kubectl taint nodes 192.168.56.11 server_type=k8s_system:NoSchedule
node "192.168.56.11" tainted

# 然后部署下面这个设置例子:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-pod-taints
  labels:
    app: nginx-pod-taints
spec:
  replicas: 10 # 这里为了实践效果特意设定了10个副本
  selector:
    matchLabels:
      app: nginx-taints
  template:
    metadata:
      labels:
        app: nginx-taints
    spec:
      containers:
      - name: nginx-taints
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
# 部署结果:
[root@linux-node1 ~]# kubectl get pods -o wide | grep nginx-pod-taints
nginx-pod-taints-57757d7677-5h6nj   1/1       Running   0          2m        10.2.69.31   192.168.56.12
nginx-pod-taints-57757d7677-dkd8h   1/1       Running   0          2m        10.2.70.34   192.168.56.14
nginx-pod-taints-57757d7677-f2vdn   1/1       Running   0          2m        10.2.44.38   192.168.56.13
nginx-pod-taints-57757d7677-fnx2n   1/1       Running   0          2m        10.2.44.36   192.168.56.13
nginx-pod-taints-57757d7677-gsc5g   1/1       Running   0          2m        10.2.70.32   192.168.56.14
nginx-pod-taints-57757d7677-kjl89   1/1       Running   0          2m        10.2.44.35   192.168.56.13
nginx-pod-taints-57757d7677-mqx27   1/1       Running   0          2m        10.2.70.33   192.168.56.14
nginx-pod-taints-57757d7677-skdd4   1/1       Running   0          2m        10.2.69.32   192.168.56.12
nginx-pod-taints-57757d7677-spwfh   1/1       Running   0          2m        10.2.69.30   192.168.56.12
nginx-pod-taints-57757d7677-tpcm8   1/1       Running   0          2m        10.2.44.37   192.168.56.13
</code></pre>
<p>以上例子可以看出我们将master节点设置污点之后调度器并没有把pod调度到master节点上。<br>这里我选择的effect是NoSchedule，也就是一定不要调度到该节点。</p>
<h6 id="5-3-2-给部署配置设置容忍性"><a href="#5-3-2-给部署配置设置容忍性" class="headerlink" title="# 5.3.2 给部署配置设置容忍性"></a># 5.3.2 给部署配置设置容忍性</h6><p>首先 我们上面设置了污点<br>即:</p>
<pre><code>  taints:
  - effect: NoSchedule
    key: server_type
    value: k8s_system
</code></pre>
<p>然后我们部署以下例子(test-tolertations.yaml)</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-pod-taints-tolerations
  labels:
    app: nginx-pod-taints-tolerations
spec:
  replicas: 10 # 这里为了实践效果特意设定了10个副本
  selector:
    matchLabels:
      app: nginx-taints-tolerations
  template:
    metadata:
      labels:
        app: nginx-taints-tolerations
    spec:
      # 设置容忍性
      tolerations:
      - key: "server_type"
        operator: "Equal"
        value: "k8s_system"
        effect: "NoSchedule"
      containers:
      - name: nginx-taints-tolerations
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80

# 部署结果:
[root@linux-node1 affinity_study]# kubectl get pods -o wide  | grep nginx-pod-taints-tolerations
nginx-pod-taints-tolerations-5b4988bd4-2pk46   1/1       Running   0          29m       10.2.69.33   192.168.56.12
nginx-pod-taints-tolerations-5b4988bd4-854xx   1/1       Running   0          29m       10.2.70.35   192.168.56.14
nginx-pod-taints-tolerations-5b4988bd4-88xrt   1/1       Running   0          29m       10.2.69.34   192.168.56.12
nginx-pod-taints-tolerations-5b4988bd4-8czpg   1/1       Running   0          29m       10.2.88.17   192.168.56.11
nginx-pod-taints-tolerations-5b4988bd4-czdss   1/1       Running   0          21m       10.2.44.41   192.168.56.13
nginx-pod-taints-tolerations-5b4988bd4-dj6mw   1/1       Running   0          29m       10.2.88.16   192.168.56.11
nginx-pod-taints-tolerations-5b4988bd4-g4ltd   1/1       Running   0          29m       10.2.44.39   192.168.56.13
nginx-pod-taints-tolerations-5b4988bd4-npthj   1/1       Running   0          29m       10.2.44.40   192.168.56.13
nginx-pod-taints-tolerations-5b4988bd4-ptlqg   1/1       Running   0          29m       10.2.88.18   192.168.56.11
nginx-pod-taints-tolerations-5b4988bd4-t9gs6   1/1       Running   0          29m       10.2.69.35   192.168.56.12
</code></pre>
<p>以上实践我们解读一下:<br>1. 在节点master 192.168.56.11上面设置了污点；但期望该节点能容忍调度;<br>2. 于是通过设置tolerations来实现，其中Pod要容忍的有污点的Node的key是server_type Equal k8s_system,效果是NoSchedule.<br>3. 通过设置容忍机制结果与预期相符。</p>
<p>对于tolerations属性的写法：</p>
<pre><code>其中的key、value、effect 与Node的Taint设置需保持一致， 还有以下几点说明：
     1、如果operator的值是Exists，则value属性可省略。
     2、如果operator的值是Equal，则表示其key与value之间的关系是equal(等于)。
     3、如果不指定operator属性，则默认值为Equal。
另外，还有两个特殊值：
     1、空的key 如果再配合Exists 就能匹配所有的key与value ，也是是能容忍所有node的所有Taints。
     2、空的effect 匹配所有的effect。
</code></pre>
<h6 id="5-3-3-其他"><a href="#5-3-3-其他" class="headerlink" title="# 5.3.3 其他"></a># 5.3.3 其他</h6><p>一个node上可以有多个污点：</p>
<pre><code>[root@linux-node1 affinity_study]# kubectl describe node/192.168.56.11
........
........
Taints:             server_type=k8s_system:NoSchedule
                    test=wahaha:PreferNoSchedule
........
........
</code></pre>
<p>如果按照我上面的例子来的话结果将不会有任何pod调度到该节点，因为上述我只容忍了一个污点；所以可以新加一个污点容忍:</p>
<pre><code>[root@linux-node1 affinity_study]# more test-tolertations.yaml
........
........      
    tolerations:
      - key: "server_type"
        operator: "Equal"
        value: "k8s_system"
        effect: "NoSchedule"
      - key: "test"
        operator: "Equal"
        value: "wahaha"
        effect: "PreferNoSchedule"
........
........
</code></pre>
<p>重新部署后结果也可以预见 只要两个容忍满足就可以调度过去啦~</p>
<p>设置容忍的效果：</p>
<ul>
<li>如果在设置node的Taints(污点)之前，就已经运行了一些Pod，那么这些Pod是否还能继续在此Node上运行？ 这就要看设置Taints污点时的effect(效果)了。</li>
<li>如果effect的值是NoSchedule或PreferNoSchedule，那么已运行的Pod仍然可以运行，只是新Pod(如果没有容忍)不会再往上调度。</li>
<li>如果 pod 不能忍受effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐</li>
<li>如果 pod 能够忍受effect 值为 NoExecute 的 taint，但是在 toleration 定义中没有指定 tolerationSeconds，则 pod 还会一直在这个节点上运行。</li>
<li>如果 pod 能够忍受effect 值为 NoExecute 的 taint，而且指定了 tolerationSeconds，则 pod 还能在这个节点上继续运行这个指定的时间长度。 虽然是立刻被驱逐，但是K8S为了彰显人性化，又给具有NoExecute效果的污点， 在容忍属性中有一个可选的tolerationSeconds字段，用来设置这些Pod还可以在这个Node之上运行多久，给它们一点宽限的时间，到时间才驱逐。</li>
</ul>
<p>不同的部署启动方式：</p>
<ul>
<li><p>如果是以Pod来启动的，那么Pod被驱逐后， 将不会再被运行，就等于把它删除了。</p>
</li>
<li><p>如果是deployment/rc，那么删除的pod会再其它节点运行。</p>
</li>
<li><p>如果是DaemonSet在此Node上启动的Pod，那么也不会再被运行，直到Node上的NoExecute污被去除或者Pod容忍。</p>
<p>#设置Pod的宽限时间<br>spec:<br>  tolerations: #设置容忍性</p>
<ul>
<li>key: “test”<br>operator: “Equal” #如果操作符为Exists，那么value属性可省略<br>value: “16”<br>effect: “wahaha”<br>tolerationSeconds: 180 <h1 id="如果运行此Pod的Node，被设置了具有NoExecute效果的Taint-污点-，这个Pod将在存活180s后才被驱逐。"><a href="#如果运行此Pod的Node，被设置了具有NoExecute效果的Taint-污点-，这个Pod将在存活180s后才被驱逐。" class="headerlink" title="如果运行此Pod的Node，被设置了具有NoExecute效果的Taint(污点)，这个Pod将在存活180s后才被驱逐。"></a>如果运行此Pod的Node，被设置了具有NoExecute效果的Taint(污点)，这个Pod将在存活180s后才被驱逐。</h1><h1 id="如果没有设置tolerationSeconds字段，将永久运行。"><a href="#如果没有设置tolerationSeconds字段，将永久运行。" class="headerlink" title="如果没有设置tolerationSeconds字段，将永久运行。"></a>如果没有设置tolerationSeconds字段，将永久运行。</h1></li>
</ul>
</li>
</ul>
<p>通过对Taints和Tolerations的了解，可以知道，通过它们可以让某些特定应用，独占一个Node，给特定的Node设置一个Taint，只让某些特定的应用来容忍这些污点，容忍后就有可能会被调度到此特定Node，但是也不一定会调度给此特定Node，设置容忍并不阻止调度器调度给其它Node，那么如何让特定应用的Node，只能被调度到此特定的Node呢，这就要结合NodeAffinity节点亲和性，给Node打个标签，然后在Pod属性里设置NodeAffinity到Node。如此就能达到要求了。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.sctux.cc">NoardGuo-Ops</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.sctux.cc/2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/">https://blog.sctux.cc/2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.sctux.cc" target="_blank">OpsThoughts</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/k8s/">k8s</a></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/zhifubao.png" target="_blank"><img class="post-qr-code-img" src="/img/zhifubao.png" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2018/12/07/k8s-ji-qun-zhongpause-rong-qi-shi-gan-ma-de/" title="K8s集群中pause容器是干嘛的~"><img class="cover" src="https://images.unsplash.com/photo-1504639725590-34d0984388bd?w=800&amp;h=400" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">K8s集群中pause容器是干嘛的~</div></div><div class="info-2"><div class="info-item-1">当我们在检查k8s集群状态的时候会发现有很多 pause 容器运行于服务器上面，然后每次启动一个容器，都会伴随一个pause容器的启动。那它究竟是干啥子的？ Pause容器，又叫Infra容器，下面通过实验来理解它。 我们知道在搭建k8s集群的时候，kubelet的配置中有这样一个参数： [root@linux-node1 cfg]# more /usr/lib/systemd/system/kubelet.service ······ ······   --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \ ······ ······  我这里是直接将这些配置参数通过启动脚本来补充进去的。Pause容器，是可以自己来定义，官方使用的gcr.io/google_containers/pause-amd64:3.0容器的代码见Github，使用C语言编写。 Pause容器的作用我们检查nod节点的时候会发现每个node上都运行了很多的pause...</div></div></div></a><a class="pagination-related" href="/2018/12/03/kubernetes-diao-du-zhinodeselector-trashed/" title="kubernetes调度之NodeSelector"><img class="cover" src="https://images.unsplash.com/photo-1535223289827-42f1e9919769?w=800&amp;h=400" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">kubernetes调度之NodeSelector</div></div><div class="info-2"><div class="info-item-1">1 NodeNamePod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是强制匹配。例子： apiVersion: apps/v1 kind: Deployment metadata:   name: nginx-test   labels:     app: nginx spec:   replicas: 3   selector:     matchLabels:       app: nginx   template:     metadata:       labels:         app: nginx     spec:       nodeName: 192.168.56.13 # 指定pod调度到该节点       containers:       - name: nginx         image: nginx:latest         imagePullPolicy: IfNotPres...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2018/12/07/k8s-ji-qun-zhongpause-rong-qi-shi-gan-ma-de/" title="K8s集群中pause容器是干嘛的~"><img class="cover" src="https://images.unsplash.com/photo-1504639725590-34d0984388bd?w=800&h=400" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-07</div><div class="info-item-2">K8s集群中pause容器是干嘛的~</div></div><div class="info-2"><div class="info-item-1">当我们在检查k8s集群状态的时候会发现有很多 pause 容器运行于服务器上面，然后每次启动一个容器，都会伴随一个pause容器的启动。那它究竟是干啥子的？ Pause容器，又叫Infra容器，下面通过实验来理解它。 我们知道在搭建k8s集群的时候，kubelet的配置中有这样一个参数： [root@linux-node1 cfg]# more /usr/lib/systemd/system/kubelet.service ······ ······   --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \ ······ ······  我这里是直接将这些配置参数通过启动脚本来补充进去的。Pause容器，是可以自己来定义，官方使用的gcr.io/google_containers/pause-amd64:3.0容器的代码见Github，使用C语言编写。 Pause容器的作用我们检查nod节点的时候会发现每个node上都运行了很多的pause...</div></div></div></a><a class="pagination-related" href="/2018/12/09/k8s-yaml-bian-xie-xiao-ji-qiao/" title="K8s Yaml编写小技巧"><img class="cover" src="https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800&h=400" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-09</div><div class="info-item-2">K8s Yaml编写小技巧</div></div><div class="info-2"><div class="info-item-1">  学习使用k8s的童鞋都知道我们在部署pod的时候有时候需要手动去编写一些yaml文件；比如我需要编写deployment,那除了在其他地方粘贴拷贝外有没有其他方法呢？答案是有的 1.用run命令生成，然后作为模板进行编辑。12345678910kubectl run --image=nginx my-deploy -o yaml --dry-run &gt; my-deploy.yaml ```   ### 2.用get命令导出，然后作为模板进行编辑。```  # 注意: --export 是为了去除当前正在运行的这个deployment生成的一些状态，我们用不到就过滤掉kubectl get deployment/nginx -o=yaml --export  &gt; new.yaml```      ### 3.Pod亲和性下面字段的拼写忘记了 kubectl explain pod.spec.affinity.podAffinity  示例: ---  我想生成一个有三个副本的redis pod的yaml，然后我想把这三个pod 通过node亲和性调度到同一个node...</div></div></div></a><a class="pagination-related" href="/2018/07/15/kubernetes-session-bao-chi-deng-she-zhi/" title="Kubernetes Session亲和性设置"><img class="cover" src="https://images.unsplash.com/photo-1639762681057-408e52192e55?w=800&h=400" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-07-15</div><div class="info-item-2">Kubernetes Session亲和性设置</div></div><div class="info-2"><div class="info-item-1">当我们在部署了多个pod，以及一个Service后，就可以在集群内部通过ServiceIP访问pod提供的服务了；￼￼ 当不设置session保持时，service向后台pod转发规则是轮询:￼￼￼以上我通过点击页面请求，可以就看出service将我的请求分发到了后面的三个pod; k8s会根据访问的ip来把请求转发给他以前访问过的pod，这样session就保持住了。查看创建service时的yaml文件内容，如果没有设置的话 该项是为None的￼ </div></div></div></a><a class="pagination-related" href="/2018/07/14/ru-he-jiangpod-zhong-decontainer-shi-qu-geng-gai-w/" title="如何将pod中的container时区更改为同一时区的城市或UTC时区偏移"><img class="cover" src="https://images.unsplash.com/photo-1677442136019-21780ecad995?w=800&h=400" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2018-07-14</div><div class="info-item-2">如何将pod中的container时区更改为同一时区的城市或UTC时区偏移</div></div><div class="info-2"><div class="info-item-1">问题:在创建pod container后发现里面的时区是UTC,对于国内习惯还是CST时区比较易读；那如何解决这种问题嘛？暂时想到的两种办法: [root@linux-node1 ~]# kubectl exec flask-app-nginx-66b56f556c-zb84s date -n flask-app-extions-stage Mon Jul 14 07:32:52 UTC 2018 [root@linux-node1 ~]# date Mon Jul 14 15:32:52 CST 2018   直接修改镜像的时间设置，好处是应用部署时无需特殊设置，但是需要手动从新构建Docker镜像 部署应用时，单独读取主机的”/etc/localtime”,无需修改镜像，但是每个应用都要单独设置。  解决:为了快速，简单的解决此问题，先使用第二种方法；yaml文件中映射主机的”/etc/localtime”文件, 添加yaml配置: ...... ......     spec:       containers:       - name: nginx         im...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">NoardGuo-Ops</div><div class="author-info-description">Happiness is not something ready-made. It comes from your own actions💪🏻.</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">99</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">93</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">30</div></a></div><a id="card-info-btn" href="https://github.com/guomaoqiu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/guomaoqiu" target="_blank" title="Github"><i class="fab fa-github" style="color: #b9410e;"></i></a><a class="social-icon" href="mailto:guomaoqiu@icloud.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #663399;"></i></a><a class="social-icon" href="https://t.me/noardguo_devops" target="_blank" title="TG"><i class="fa-brands fa-telegram" style="color: #00ff99;"></i></a><a class="social-icon" href="weixin://NoardGuo-Ops" target="_blank" title="WeChat"><i class="fa-brands fa-weixin" style="color: #b197fc;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-NodeName"><span class="toc-number">1.</span> <span class="toc-text">1. NodeName</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-1-%E4%BE%8B%E5%A6%82-test-nodename-yaml"><span class="toc-number">1.0.0.0.1.</span> <span class="toc-text">1.1 例如(test-nodename.yaml)</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#1-2-%E6%9F%A5%E7%9C%8B%E5%88%9B%E5%BB%BA%E4%BA%8B%E4%BB%B6%E3%80%81%E4%B8%8E%E9%A2%84%E6%9C%9F%E7%9B%B8%E7%AC%A6"><span class="toc-number">1.0.0.0.2.</span> <span class="toc-text">1.2 查看创建事件、与预期相符:</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-NodeSelector"><span class="toc-number">2.</span> <span class="toc-text">2. NodeSelector</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2-1-Node%E6%B7%BB%E5%8A%A0label%E6%A0%87%E8%AE%B0-%E6%A0%87%E8%AE%B0%E8%A7%84%E5%88%99%EF%BC%9A"><span class="toc-number">2.0.0.0.1.</span> <span class="toc-text">2.1 Node添加label标记,标记规则：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-2-%E7%A1%AE%E8%AE%A4%E6%A0%87%E8%AE%B0"><span class="toc-number">2.0.0.0.2.</span> <span class="toc-text">2.2 确认标记</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-3-%E4%BE%8B%E5%A6%82-test-nodeselector-yaml"><span class="toc-number">2.0.0.0.3.</span> <span class="toc-text">2.3 例如(test-nodeselector.yaml)</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-%E6%9F%A5%E7%9C%8B%E5%88%9B%E5%BB%BA%E4%BA%8B%E4%BB%B6"><span class="toc-number">2.0.0.1.</span> <span class="toc-text">2.4 查看创建事件:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-NodeAffinity"><span class="toc-number">3.</span> <span class="toc-text">3. NodeAffinity</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#3-1-%E4%BE%8B%E5%A6%82-test-node-affinity-yaml"><span class="toc-number">3.0.0.0.1.</span> <span class="toc-text">3.1 例如(test-node-affinity.yaml)</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-PodAffinity"><span class="toc-number">4.</span> <span class="toc-text">4. PodAffinity</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4-1-%E4%BE%8B%E5%A6%82-test-pod-affinity-yaml"><span class="toc-number">4.0.0.0.1.</span> <span class="toc-text">4.1 例如(test-pod-affinity.yaml)</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%B1%A1%E7%82%B9%EF%BC%88Taints%EF%BC%89%E4%B8%8E%E5%AE%B9%E5%BF%8D%EF%BC%88tolerations%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">5. 污点（Taints）与容忍（tolerations）</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#5-1-%E8%AE%BE%E7%BD%AE%E6%B1%A1%E7%82%B9"><span class="toc-number">5.0.0.0.1.</span> <span class="toc-text">5.1 设置污点</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5-2-%E5%8E%BB%E9%99%A4%E6%B1%A1%E7%82%B9"><span class="toc-number">5.0.0.0.2.</span> <span class="toc-text">5.2 去除污点</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5-3-%E5%AE%9E%E8%B7%B5"><span class="toc-number">5.0.0.0.3.</span> <span class="toc-text">5.3 实践</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5-3-1-%E7%BB%99%E8%8A%82%E7%82%B9%E8%AE%BE%E7%BD%AE%E6%B1%A1%E7%82%B9"><span class="toc-number">5.0.0.0.4.</span> <span class="toc-text"># 5.3.1 给节点设置污点</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5-3-2-%E7%BB%99%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E8%AE%BE%E7%BD%AE%E5%AE%B9%E5%BF%8D%E6%80%A7"><span class="toc-number">5.0.0.0.5.</span> <span class="toc-text"># 5.3.2 给部署配置设置容忍性</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#5-3-3-%E5%85%B6%E4%BB%96"><span class="toc-number">5.0.0.0.6.</span> <span class="toc-text"># 5.3.3 其他</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E8%BF%90%E8%A1%8C%E6%AD%A4Pod%E7%9A%84Node%EF%BC%8C%E8%A2%AB%E8%AE%BE%E7%BD%AE%E4%BA%86%E5%85%B7%E6%9C%89NoExecute%E6%95%88%E6%9E%9C%E7%9A%84Taint-%E6%B1%A1%E7%82%B9-%EF%BC%8C%E8%BF%99%E4%B8%AAPod%E5%B0%86%E5%9C%A8%E5%AD%98%E6%B4%BB180s%E5%90%8E%E6%89%8D%E8%A2%AB%E9%A9%B1%E9%80%90%E3%80%82"><span class="toc-number"></span> <span class="toc-text">如果运行此Pod的Node，被设置了具有NoExecute效果的Taint(污点)，这个Pod将在存活180s后才被驱逐。</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E6%B2%A1%E6%9C%89%E8%AE%BE%E7%BD%AEtolerationSeconds%E5%AD%97%E6%AE%B5%EF%BC%8C%E5%B0%86%E6%B0%B8%E4%B9%85%E8%BF%90%E8%A1%8C%E3%80%82"><span class="toc-number"></span> <span class="toc-text">如果没有设置tolerationSeconds字段，将永久运行。</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/04/%E6%B1%82%E8%81%8C%E5%B8%96/" title="求职-系统运维/SRE方向"><img src="https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=800&amp;h=400" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="求职-系统运维/SRE方向"/></a><div class="content"><a class="title" href="/2025/09/04/%E6%B1%82%E8%81%8C%E5%B8%96/" title="求职-系统运维/SRE方向">求职-系统运维/SRE方向</a><time datetime="2025-09-03T16:01:01.000Z" title="发表于 2025-09-04 00:01:01">2025-09-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/" title="Docker构建日志收集平台EFK(TLS)"><img src="https://images.unsplash.com/photo-1579389083078-4e7018379f7e?w=800&amp;h=400" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker构建日志收集平台EFK(TLS)"/></a><div class="content"><a class="title" href="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/" title="Docker构建日志收集平台EFK(TLS)">Docker构建日志收集平台EFK(TLS)</a><time datetime="2024-07-23T01:56:27.000Z" title="发表于 2024-07-23 09:56:27">2024-07-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/" title="如何利用ElastAlert2对ES中的日志创建告警"><img src="https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&amp;h=400" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="如何利用ElastAlert2对ES中的日志创建告警"/></a><div class="content"><a class="title" href="/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/" title="如何利用ElastAlert2对ES中的日志创建告警">如何利用ElastAlert2对ES中的日志创建告警</a><time datetime="2024-07-16T09:56:39.000Z" title="发表于 2024-07-16 17:56:39">2024-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/09/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Python%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89pod%E5%AE%B9%E5%99%A8%E7%8A%B6%E6%80%81/" title="如何利用Python获取所有pod容器状态"><img src="https://images.unsplash.com/photo-1581094794329-c8112a89af12?w=800&amp;h=400" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="如何利用Python获取所有pod容器状态"/></a><div class="content"><a class="title" href="/2024/07/09/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Python%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89pod%E5%AE%B9%E5%99%A8%E7%8A%B6%E6%80%81/" title="如何利用Python获取所有pod容器状态">如何利用Python获取所有pod容器状态</a><time datetime="2024-07-09T09:38:00.000Z" title="发表于 2024-07-09 17:38:00">2024-07-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/" title="K8S中MySQL使用NFS挂载的异常问题处理"><img src="https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800&amp;h=400" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="K8S中MySQL使用NFS挂载的异常问题处理"/></a><div class="content"><a class="title" href="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/" title="K8S中MySQL使用NFS挂载的异常问题处理">K8S中MySQL使用NFS挂载的异常问题处理</a><time datetime="2024-05-10T12:58:02.000Z" title="发表于 2024-05-10 20:58:02">2024-05-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/" title="Prometheus+Consul实现企业级宿主机+容器监控告警"><img src="https://images.unsplash.com/photo-1639762681057-408e52192e55?w=800&amp;h=400" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prometheus+Consul实现企业级宿主机+容器监控告警"/></a><div class="content"><a class="title" href="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/" title="Prometheus+Consul实现企业级宿主机+容器监控告警">Prometheus+Consul实现企业级宿主机+容器监控告警</a><time datetime="2022-09-12T06:45:26.000Z" title="发表于 2022-09-12 14:45:26">2022-09-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://images.unsplash.com/photo-1563207153-f403bf289096?w=800&amp;h=400);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2013 - 2025 By NoardGuo-Ops</span><span class="framework-info"><span>框架 </span><a href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div><div class="footer_custom_text">Hi, welcome to my <a href="https://blog.sctux.cc/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>