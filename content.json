{"meta":{"title":"OpsThoughts","subtitle":"Devops、Linux、Kubernetes、Docker、Flask、Python、Shell、SRE","description":"Happiness is not something ready-made. It comes from your own actions💪🏻.","author":"NoardGuo-Ops","url":"https://blog.sctux.cc"},"pages":[{"title":"🌟 关于我","date":"2018-12-12T03:26:52.000Z","updated":"2025-09-02T12:17:06.987Z","comments":true,"path":"about/index.html","permalink":"https://blog.sctux.cc/about/index.html","excerpt":"","text":"好记性，不如烂笔头 ✍️ 🚀 博客历程51cto 2013——2014 自建Wordpress 2014——2015 Hexo + Github 2015——至今 👨‍💻 个人简介📍 定居成都自2010年踏入计算机领域，独身北上求学，奠定技术根基。2012年正式投身行业，长期专注于服务器运维与技术架构工作。在运维领域拥有扎实的实践经验，擅长高并发系统保障、自动化运维体系构建与性能深度优化，始终坚持通过技术创新驱动业务稳定与效率提升。 🎨 个人兴趣书法 吉他 摄影 编程 旅游 烹饪 🔧 技术专长追崇 DevOps理念，致力于实现： 流程化 ⚡ 工具化 🛠️ 平台化 🌐 自动化 🤖 🛠 技术栈系统与运维：Linux Shell Zabbix ELK SaltStack Jenkins 开发与编程：Python Flask BlueKing Platform LNMP 云平台：AliCloud Aws HuaweiCloud 容器与编排：Docker Kubernetes 版本控制：Git Github Gitlab Gogs 🔮 未来展望 对于未来不敢想太多，做好当下吧~I am too lazy, So, I want make everything automation! 🍺 保持热爱，持续学习，自动化一切可自动化的事情！ 🚀 📞 联系交流欢迎通过以下方式与我交流： 📧 邮箱：guomaoqiu@icloud.com 🍓 TG: @noardguo_devops 💻 GitHub：https://github.com/guomaoqiu 不忘初心，方得始终 🌈"},{"title":"我的简历","date":"2025-09-03T19:00:00.000Z","updated":"2025-09-03T16:51:37.343Z","comments":true,"path":"resume/index.html","permalink":"https://blog.sctux.cc/resume/index.html","excerpt":"请输入密码查看并下载我的简历。","text":"ff81b43b7e43c5a636aafda9fb25250358d091e1582be1ce2bc4a2d658f70b882ccdddf85a772efcfa8160d7f2d35a36829692f7085e62afef59f113de75c9c75ee2c7e4a968286d55f62fbac971b8e96c0e75e923fad7db7752b3d9012e507a964af6d85963917eb0c53a2cd3dbd1c4e1eee38714546ace9c358a3e68ca14a0c7dda7b499c30c060b9afca3d34b061155f5a8014c72d10a4adaf58781e123946ec153b562dc14b8c9d949b4e50e4f6d51810d731446dc6703df121c22f3fcc28240f08a61961d9e2c6462c07b8ef0fe34c42b41fa50254ca28daa541e96a45104ccee1e9a461fa1b8f581d9fd5b211d322df4876267aafc02324dec2ff9cba704276e2b41091f1a904b56b2ffe14b9eb5c4a7ba7d684e33237e220696eb3f8ffca01c20f0ee9a111ea4520e4193ace3a39ddcc82f9c7c8d9b0414037fe850b63b1ed0e3225400ac5ad2fb49da22f6a7e37ab9e459577bfd964f36c7512d21ab9ae7cd176754c4fbb25d01922f806fa6230a22924a273212ec0e4d1f1e3ac5adef14eac05ec37ad955e511c888f41eac2afaf63612ee27182e5850ab7144f121 请联系我本人索取密码，谢谢。"},{"title":"Categories","date":"2025-09-01T01:59:09.360Z","updated":"2025-09-01T01:59:09.360Z","comments":false,"path":"categories/index.html","permalink":"https://blog.sctux.cc/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2025-09-01T01:59:09.358Z","updated":"2025-09-01T01:59:09.358Z","comments":false,"path":"tags/index.html","permalink":"https://blog.sctux.cc/tags/index.html","excerpt":"","text":""},{"title":"🚀 个人开源作品集","date":"2018-12-12T03:26:52.000Z","updated":"2025-09-01T01:59:09.358Z","comments":true,"path":"works/index.html","permalink":"https://blog.sctux.cc/works/index.html","excerpt":"","text":"🔧 运维开发平台1. 基于腾讯蓝鲸集成平台开发的运维后台 技术栈: 腾讯蓝鲸平台 | Python | Django 简介: 企业级运维自动化解决方案，集成CMDB、作业平台等核心功能 2. 基于腾讯蓝鲸集成平台开发的 App 权限管控平台 技术栈: 腾讯蓝鲸平台 | 权限管理 | RBAC 简介: 细粒度的应用权限管理系统，保障平台安全访问 3. 基于 Flask+Bootstrap 开发的运维管理后台 🌐 Demo: http://demo.sctux.cc 技术栈: Flask | Bootstrap | MySQL 注: Demo环境部署在海外服务器，访问缓慢时可尝试使用网络加速工具 ⏰ 任务调度平台4. 定时作业任务平台 🌐 Demo: http://jobcenter.sctux.cc 功能: 分布式任务调度 | 执行历史 | 实时日志 5. Supervisor 多节点管理平台 🌐 Demo: http://super.sctux.cc 功能: 多节点监控 | 进程管理 | 状态可视化 ☁️ 云平台工具6. 专属云巡检平台 📖 详细介绍: 自动化巡检的必要性以及重要性 功能: 自动化巡检 | 健康检查 | 报表生成 🌳 技能图谱7. 我的技能树 内容: 技术栈全景 | 熟练程度 | 学习路径 8. 技术证书CKA 您的浏览器不支持 iframe。 点击这里下载 PDF。 RHCE 您的浏览器不支持 iframe。 点击这里下载 PDF。 📫 联系我如有任何问题或合作意向，欢迎通过 GitHub 提交 Issue 或通过博客留言联系！ 保持开源精神，共同进步！ 🎯"},{"title":"","date":"2025-09-01T01:59:09.360Z","updated":"2025-09-01T01:59:09.360Z","comments":true,"path":"kubernetes/yaml/static-pod.json","permalink":"https://blog.sctux.cc/kubernetes/yaml/static-pod.json","excerpt":"","text":"{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"name\":\"static-web-from-url\",\"labels\":{\"role\":\"myrole\"}},\"spec\":{\"containers\":[{\"name\":\"web\",\"image\":\"nginx\",\"ports\":[{\"name\":\"web\",\"containerPort\":80,\"protocol\":\"TCP\"}]}]}}"}],"posts":[{"title":"求职-系统运维/SRE方向","slug":"求职帖","date":"2025-09-03T16:01:01.000Z","updated":"2025-09-04T04:53:51.963Z","comments":true,"path":"2025/09/04/求职帖/","permalink":"https://blog.sctux.cc/2025/09/04/%E6%B1%82%E8%81%8C%E5%B8%96/","excerpt":"各位朋友，大家好！ 我于今年7月底正式离职，结束了上一段工作旅程。目前正在寻找下一份系统运维或SRE相关的工作，希望能借助大家的力量。 我有10年左右的运维经验，在这期间从事过IDC、游戏、物联网、区块链行业。熟练使用Linux操作系统、Kubernetes集群部署维护、监控系统部署维护以及二次开发以及一些自动化脚本工具开发的能力。在过去的工作中，我负责维护系统的稳定运行，处理过一些故障，也参与从传统架构到云原生架构的迁移和转型。我不是最顶尖的大牛，但我相信自己有很强的责任心、学习能力和团队协作精神，能脚踏实地地完成任务。 我个人非常热爱运维这个事业，作为运维/SRE在为公司创造价值的同时能满足自己的成就感是非常完美的一件事情。非常渴望能加入一个优秀的团队，与大家共同成长，为公司贡献出自己的全部能力。 如果您所在团队有招聘需求，或者有相关的信息，恳请您不吝推荐或联系我。感激不尽！ 📞 我的联系方式欢迎通过以下方式与我交流： 📧 邮箱：guomaoqiu@icloud.com 🍓 TG: @noardguo_devops 💻 GitHub：https://github.com/guomaoqiu","text":"各位朋友，大家好！ 我于今年7月底正式离职，结束了上一段工作旅程。目前正在寻找下一份系统运维或SRE相关的工作，希望能借助大家的力量。 我有10年左右的运维经验，在这期间从事过IDC、游戏、物联网、区块链行业。熟练使用Linux操作系统、Kubernetes集群部署维护、监控系统部署维护以及二次开发以及一些自动化脚本工具开发的能力。在过去的工作中，我负责维护系统的稳定运行，处理过一些故障，也参与从传统架构到云原生架构的迁移和转型。我不是最顶尖的大牛，但我相信自己有很强的责任心、学习能力和团队协作精神，能脚踏实地地完成任务。 我个人非常热爱运维这个事业，作为运维/SRE在为公司创造价值的同时能满足自己的成就感是非常完美的一件事情。非常渴望能加入一个优秀的团队，与大家共同成长，为公司贡献出自己的全部能力。 如果您所在团队有招聘需求，或者有相关的信息，恳请您不吝推荐或联系我。感激不尽！ 📞 我的联系方式欢迎通过以下方式与我交流： 📧 邮箱：guomaoqiu@icloud.com 🍓 TG: @noardguo_devops 💻 GitHub：https://github.com/guomaoqiu","categories":[],"tags":[],"keywords":[]},{"title":"Docker构建日志收集平台EFK(TLS)","slug":"Docker构建日志收集平台EFK-TLS","date":"2024-07-23T01:56:27.000Z","updated":"2025-09-01T01:59:08.871Z","comments":true,"path":"2024/07/23/Docker构建日志收集平台EFK-TLS/","permalink":"https://blog.sctux.cc/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/","excerpt":"前言如题， 主要记录一下之前搭建部署的一套日志收集系统。这里采用docker-compose的方式运行，还是那句话主要是思路。 方式: 一台服务器上面利用docker-compose运行三个ES节点跟Kibana,并启用SSL,再使用Filebeat来收集服务器上面的应用日志到ES， 最后Kibana来做展示 配置目录创建1234mkdir -p /data/{es/node-1/{data,certs,logs,config},plugins}mkdir -p /data/{es/node-2/{data,certs,logs,config},plugins}mkdir -p /data/{es/node-3/{data,certs,logs,config},plugins}mkdir -p /data/kibana/{certs,config} -p 部署文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131version: \"3\"services: node-1: image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 networks: bitdata: ipv4_address: 172.20.0.3 container_name: node-1 hostname: node-1 environment: - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" - \"TZ=Asia/Shanghai\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 ports: - \"9200:9200\" logging: driver: \"json-file\" options: max-size: \"50m\" volumes: - ./es/node-1/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./es/plugins:/usr/share/elasticsearch/plugins - ./es/node-1/data:/usr/share/elasticsearch/data - ./es/node-1/certs:/usr/share/elasticsearch/config/certs - ./es/node-1/log:/usr/share/elasticsearch/log healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:9200 || exit 1\"] interval: 10s timeout: 10s retries: 5 node-2: image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 networks: bitdata: ipv4_address: 172.20.0.4 container_name: node-2 hostname: node-2 environment: - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" - \"TZ=Asia/Shanghai\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 ports: - \"9201:9200\" logging: driver: \"json-file\" options: max-size: \"50m\" volumes: - ./es/node-2/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./es/plugins:/usr/share/elasticsearch/plugins - ./es/node-2/data:/usr/share/elasticsearch/data - ./es/node-2/certs:/usr/share/elasticsearch/config/certs - ./es/node-2/log:/usr/share/elasticsearch/log healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:9200 || exit 1\"] interval: 10s timeout: 10s retries: 5 node-3: image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 networks: bitdata: ipv4_address: 172.20.0.5 container_name: node-3 hostname: node-3 environment: - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" - \"TZ=Asia/Shanghai\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 ports: - \"9202:9200\" logging: driver: \"json-file\" options: max-size: \"50m\" volumes: - ./es/node-3/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./es/plugins:/usr/share/elasticsearch/plugins - ./es/node-3/data:/usr/share/elasticsearch/data - ./es/node-3/certs:/usr/share/elasticsearch/config/certs - ./es/node-3/log:/usr/share/elasticsearch/log healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:9200 || exit 1\"] interval: 10s timeout: 10s retries: 5 kibana: networks: bitdata: ipv4_address: 172.20.0.6 container_name: kibana hostname: kibana image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/kibana:7.17.5 environment: TZ: 'Asia/Shanghai' volumes: - ./kibana/cert:/etc/kibana/cert - ./kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml ports: - 5601:5601 healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:5601 || exit 1\"] interval: 10s timeout: 10s retries: 5# 连接外部网络networks: bitdata: driver: bridge ipam: config: - subnet: 172.20.0.0/24 非SSLelasticsearch.yaml配置文件","text":"前言如题， 主要记录一下之前搭建部署的一套日志收集系统。这里采用docker-compose的方式运行，还是那句话主要是思路。 方式: 一台服务器上面利用docker-compose运行三个ES节点跟Kibana,并启用SSL,再使用Filebeat来收集服务器上面的应用日志到ES， 最后Kibana来做展示 配置目录创建1234mkdir -p /data/{es/node-1/{data,certs,logs,config},plugins}mkdir -p /data/{es/node-2/{data,certs,logs,config},plugins}mkdir -p /data/{es/node-3/{data,certs,logs,config},plugins}mkdir -p /data/kibana/{certs,config} -p 部署文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131version: \"3\"services: node-1: image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 networks: bitdata: ipv4_address: 172.20.0.3 container_name: node-1 hostname: node-1 environment: - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" - \"TZ=Asia/Shanghai\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 ports: - \"9200:9200\" logging: driver: \"json-file\" options: max-size: \"50m\" volumes: - ./es/node-1/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./es/plugins:/usr/share/elasticsearch/plugins - ./es/node-1/data:/usr/share/elasticsearch/data - ./es/node-1/certs:/usr/share/elasticsearch/config/certs - ./es/node-1/log:/usr/share/elasticsearch/log healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:9200 || exit 1\"] interval: 10s timeout: 10s retries: 5 node-2: image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 networks: bitdata: ipv4_address: 172.20.0.4 container_name: node-2 hostname: node-2 environment: - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" - \"TZ=Asia/Shanghai\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 ports: - \"9201:9200\" logging: driver: \"json-file\" options: max-size: \"50m\" volumes: - ./es/node-2/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./es/plugins:/usr/share/elasticsearch/plugins - ./es/node-2/data:/usr/share/elasticsearch/data - ./es/node-2/certs:/usr/share/elasticsearch/config/certs - ./es/node-2/log:/usr/share/elasticsearch/log healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:9200 || exit 1\"] interval: 10s timeout: 10s retries: 5 node-3: image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 networks: bitdata: ipv4_address: 172.20.0.5 container_name: node-3 hostname: node-3 environment: - \"ES_JAVA_OPTS=-Xms1024m -Xmx1024m\" - \"TZ=Asia/Shanghai\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 ports: - \"9202:9200\" logging: driver: \"json-file\" options: max-size: \"50m\" volumes: - ./es/node-3/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml - ./es/plugins:/usr/share/elasticsearch/plugins - ./es/node-3/data:/usr/share/elasticsearch/data - ./es/node-3/certs:/usr/share/elasticsearch/config/certs - ./es/node-3/log:/usr/share/elasticsearch/log healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:9200 || exit 1\"] interval: 10s timeout: 10s retries: 5 kibana: networks: bitdata: ipv4_address: 172.20.0.6 container_name: kibana hostname: kibana image: registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/kibana:7.17.5 environment: TZ: 'Asia/Shanghai' volumes: - ./kibana/cert:/etc/kibana/cert - ./kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml ports: - 5601:5601 healthcheck: test: [\"CMD-SHELL\", \"curl -I http://localhost:5601 || exit 1\"] interval: 10s timeout: 10s retries: 5# 连接外部网络networks: bitdata: driver: bridge ipam: config: - subnet: 172.20.0.0/24 非SSLelasticsearch.yaml配置文件这里以node-1为例，其他不同的地方就是 node.name 12345678910111213141516171819202122232425262728293031323334353637383940vim /data/es/node-1/config/elasticsearch.yml#集群名称cluster.name: elastic#当前该节点的名称node.name: node-1#是不是有资格竞选主节点node.master: true#是否存储数据node.data: true#最大集群节点数node.max_local_storage_nodes: 3#给当前节点自定义属性（可以省略）#node.attr.rack: r1#数据存档位置path.data: /usr/share/elasticsearch/data#日志存放位置path.logs: /usr/share/elasticsearch/log#是否开启时锁定内存（默认为是）#bootstrap.memory_lock: true#设置网关地址，我是被这个坑死了，这个地址我原先填写了自己的实际物理IP地址，#然后启动一直报无效的IP地址，无法注入9300端口，这里只需要填写0.0.0.0network.host: 0.0.0.0#设置映射端口http.port: 9200#内部节点之间沟通端口transport.tcp.port: 9300#集群发现默认值为127.0.0.1:9300,如果要在其他主机上形成包含节点的群集,如果搭建集群则需要填写#es7.x 之后新增的配置，写入候选主节点的设备地址，在开启服务后可以被选为主节点，也就是说把所有的节点都写上discovery.seed_hosts: [\"172.20.0.3\",\"172.20.0.4\",\"172.17.0.5\"]#当你在搭建集群的时候，选出合格的节点集群，有些人说的太官方了，#其实就是，让你选择比较好的几个节点，在你节点启动时，在这些节点中选一个做领导者，#如果你不设置呢，elasticsearch就会自己选举，这里我们把三个节点都写上cluster.initial_master_nodes: [\"node-1\",\"node-2\",\"node-3\"]#在群集完全重新启动后阻止初始恢复，直到启动N个节点#简单点说在集群启动后，至少复活多少个节点以上，那么这个服务才可以被使用，否则不可以被使用，gateway.recover_after_nodes: 2#删除索引是是否需要显示其名称，默认为显示#action.destructive_requires_name: true# 禁用安全配置xpack.security.enabled: false kibana.yaml1234567891011121314vim /data/kibana/config/kibana.ymlserver.host: 0.0.0.0# 监听端口server.port: 5601server.name: \"kibana\"# kibana访问es服务器的URL,就可以有多个，以逗号\",\"隔开elasticsearch.hosts: [\"http://172.20.0.3:9200\",\"http://172.20.0.4:9200\",\"http://172.20.0.5:9200\"]monitoring.ui.container.elasticsearch.enabled: true# kibana访问Elasticsearch的账号与密码(如果ElasticSearch设置了的话)elasticsearch.username: \"\"elasticsearch.password: \"\"# kibana web语言i18n.locale: \"zh-CN\" 启动12345678910111213root@ip-10-10-10-29:/data# docker-compose up -d[+] Running 5/5 ⠿ Network data_bitdata Created 0.0s ⠿ Container node-2 Started 0.6s ⠿ Container node-3 Started 0.8s ⠿ Container node-1 Started 0.8s ⠿ Container kibana Started 0.5sCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4d5dae9021be registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 \"/bin/tini -- /usr/l…\" About a minute ago Up About a minute (healthy) 0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp, 9300/tcp node-184e191d6f875 registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 \"/bin/tini -- /usr/l…\" About a minute ago Up About a minute (healthy) 9300/tcp, 0.0.0.0:9202-&gt;9200/tcp, :::9202-&gt;9200/tcp node-3758a4bff2a73 registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:7.17.5 \"/bin/tini -- /usr/l…\" About a minute ago Up About a minute (healthy) 9300/tcp, 0.0.0.0:9201-&gt;9200/tcp, :::9201-&gt;9200/tcp node-2fe55f0446902 registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/kibana:7.17.5 \"/bin/tini -- /usr/l…\" About a minute ago Up About a minute (healthy) 0.0.0.0:5601-&gt;5601/tcp, :::5601-&gt;5601/tcp kibana 上面已经启动，如果有报错查看容器日志就行了，之前部署的时候报错内容还是很清晰 进入容器查看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091root@node-1:/usr/share/elasticsearch# curl localhost:9200{ \"name\" : \"node-1\", \"cluster_name\" : \"elastic\", \"cluster_uuid\" : \"tcKUxyWVQb-7LV4zmomsgg\", \"version\" : { \"number\" : \"7.17.5\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84\", \"build_date\" : \"2022-06-23T21:57:28.736740635Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.11.1\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\"}root@node-1:/usr/share/elasticsearch# curl 172.20.3:9200{ \"name\" : \"node-1\", \"cluster_name\" : \"elastic\", \"cluster_uuid\" : \"tcKUxyWVQb-7LV4zmomsgg\", \"version\" : { \"number\" : \"7.17.5\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84\", \"build_date\" : \"2022-06-23T21:57:28.736740635Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.11.1\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\"}root@node-1:/usr/share/elasticsearch# curl 172.20.4:9200{ \"name\" : \"node-2\", \"cluster_name\" : \"elastic\", \"cluster_uuid\" : \"tcKUxyWVQb-7LV4zmomsgg\", \"version\" : { \"number\" : \"7.17.5\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84\", \"build_date\" : \"2022-06-23T21:57:28.736740635Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.11.1\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\"}root@node-1:/usr/share/elasticsearch# curl 172.20.5:9200{ \"name\" : \"node-3\", \"cluster_name\" : \"elastic\", \"cluster_uuid\" : \"tcKUxyWVQb-7LV4zmomsgg\", \"version\" : { \"number\" : \"7.17.5\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84\", \"build_date\" : \"2022-06-23T21:57:28.736740635Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.11.1\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\"}root@ip-10-10-10-29:/data# curl -s 172.20.0.3:9200/_cluster/health | python3 -m json.tool{ \"cluster_name\": \"elastic\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 3, \"number_of_data_nodes\": 3, \"active_primary_shards\": 3, \"active_shards\": 6, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100.0} 以上说明ES集群已经正常工作，看一下通过nginx代理转发到kibana的结果 kibana也能正常工作，只是这里在访问的时候直接就进入了kibana，如果不加一个身份验证功能，这就等于裸奔，即便在放在内网也是极其不安全的，所以需要加一个身份验证。 SSL方式利用自带的工具生成证书，也可以自行生成证书，但注意要限制域名和IP，否则在进行https通讯时会校验失败 进入node-1容器 ，操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394root@ip-10-10-10-29:~# docker exec -it node-1 bashroot@node-1:/usr/share/elasticsearch## 根据实际情况配置生成证书的yamlcat &lt;&lt;EOF &gt; /usr/share/elasticsearch/node.ymlinstances: - name: \"node\" ip: - \"172.20.0.3\" - \"172.20.0.4\" - \"172.20.0.5\" - \"127.0.0.1\" - \"172.20.0.6\" - \"10.10.10.29\" # 这个IP是服务器的IP地址，因为我这里使用的是docker运行es集群，这个必须加上去 dns: - \"node-1\" - \"node-2\" - \"node-3 \" - \"localhost\" - \"kibana\" - \"others\"EOF# 生成ca证书，默认文件名为elastic-stack-ca.p12，可添加密码bin/elasticsearch-certutil ca # 一路回车即可# 生成证书，默认文件名为certificate-bundle.zipbin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --silent --in node.yml # 一路回车即可# 将证书复制到node-1配置目录下root@node-1:/usr/share/elasticsearch# cp certificate-bundle.zip elastic-stack-ca.p12 config/certs/root@node-1:/usr/share/elasticsearch# cd config/certs/root@node-1:/usr/share/elasticsearch/config/certs# lscertificate-bundle.zip elastic-stack-ca.p12# 解压证书，目录结构为 实例名/实例名.p12 ,此处为node/node.p12root@node-1:/usr/share/elasticsearch/config/certs# unzip certificate-bundle.zip Archive: certificate-bundle.zip creating: node/ inflating: node/node.p12 root@node-1:/usr/share/elasticsearch/config/certs# lscertificate-bundle.zip elastic-stack-ca.p12 noderoot@node-1:/usr/share/elasticsearch/config/certs# ls node/node.p12root@node-1:/usr/share/elasticsearch/config/certs# rm -rf node certificate-bundle.ziproot@node-1:/usr/share/elasticsearch/config/certs# lscertificate-bundle.zip elastic-stack-ca.p12 node.p12# 修改证书访问权限root@node-1:/usr/share/elasticsearch/config# chown -R root:elasticsearch certs/ root@node-1:/usr/share/elasticsearch/config# chmod -R a+rx certs/# 此时我们需要的证书已经生成了，同样的方式复制到node-2,node3后修改权限，修改属主等操作# 退出容器,进入node-1的挂在目录：root@ip-10-10-10-29:/data/es/node-1/certs# pwd/data/es/node-1/certsroot@ip-10-10-10-29:/data/es/node-1/certs# cd ..root@ip-10-10-10-29:/data/es/node-1# lscerts config data logroot@ip-10-10-10-29:/data/es/node-1# cp -rf certs ../node-2/root@ip-10-10-10-29:/data/es/node-1# cp -rf certs ../node-3/root@ip-10-10-10-29:/data/es/node-1# # 证书已经准备完毕es/├── node-1│&nbsp;&nbsp; ├── certs│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elastic-stack-ca.p12│&nbsp;&nbsp; │&nbsp;&nbsp; └── node.p12│&nbsp;&nbsp; ├── config│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elasticsearch.yml│&nbsp;&nbsp; ├── data│&nbsp;&nbsp; │&nbsp;&nbsp; └── nodes│&nbsp;&nbsp; └── log├── node-2│&nbsp;&nbsp; ├── certs│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elastic-stack-ca.p12│&nbsp;&nbsp; │&nbsp;&nbsp; └── node.p12│&nbsp;&nbsp; ├── config│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elasticsearch.yml│&nbsp;&nbsp; ├── data│&nbsp;&nbsp; │&nbsp;&nbsp; └── nodes│&nbsp;&nbsp; └── log├── node-3│&nbsp;&nbsp; ├── certs│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elastic-stack-ca.p12│&nbsp;&nbsp; │&nbsp;&nbsp; └── node.p12│&nbsp;&nbsp; ├── config│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elasticsearch.yml│&nbsp;&nbsp; ├── data│&nbsp;&nbsp; │&nbsp;&nbsp; └── nodes│&nbsp;&nbsp; └── log└── plugins 修改配置文件/data/es/node-1/config/elasticsearch.yml. 以节点node-1为例 12345678910111213141516# 这里的内容同非ssl部署一样..............# 开启xpack安全特性xpack.security.enabled: truexpack.security.authc.api_key.enabled: true# 开启https并配置证书xpack.security.http.ssl.enabled: truexpack.security.http.ssl.keystore.path: certs/node.p12xpack.security.http.ssl.truststore.path: certs/node.p12# 开启节点间通讯ssl并配置证书xpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificatexpack.security.transport.ssl.client_authentication: requiredxpack.security.transport.ssl.keystore.path: certs/node.p12xpack.security.transport.ssl.truststore.path: certs/node.p12 每个节点都需修改后，重启一下es集群 重启完毕后，查看状态和日志，检查是否有异常 若无异常，可以开始设置密码， 进入容器 node-1 1234567891011121314root@ip-10-10-10-29:/data# docker exec -it node-1 bashroot@node-1:/usr/share/elasticsearch# ./bin/elasticsearch-setup-passwords auto Initiating the setup of passwords for reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user.The passwords will be randomly generated and printed to the console.Please confirm that you would like to continue [y/N]y# 若集群没问题的话这里就会自动生成系统的一些密码：............................# 也可以手动自定义密码./bin/elasticsearch-setup-passwords interactive 使用 ES API 检查各个节点是否正常 123curl -k --user elastic:密码 -X GET \"https://172.20.0.3:9200\" --cert-type P12 --cert /usr/share/elasticsearch/config/certs/node.p12curl -k --user elastic:密码 -X GET \"https://172.20.0.4:9200\" --cert-type P12 --cert /usr/share/elasticsearch/config/certs/node.p12curl -k --user elastic:密码 -X GET \"https://172.20.0.5:9200\" --cert-type P12 --cert /usr/share/elasticsearch/config/certs/node.p12 此时ssl已配置完毕，配置kibana 123456# 把之前放在node-1下面的证书复制到kibana/cert下面：# 提取公钥openssl pkcs12 -in elastic-stack-ca.p12 -out elastic-stack-ca.pem -nokeys -clcertsopenssl pkcs12 -in node.p12 -out node.pem -nokeys -clcerts# 提取私钥openssl pkcs12 -in node.p12 -out node.key -nocerts -nodes 修改配置文件kibana/config/kibana.yml 12345678910111213141516171819server.host: 0.0.0.0# 监听端口server.port: 5601server.name: \"kibana\"# kibana访问es服务器的URL,就可以有多个，以逗号\",\"隔开# es 开启了 ssl，所以这里的url 必须使用https协议elasticsearch.hosts: [\"https://172.20.0.3:9200\",\"https://172.20.0.4:9200\",\"https://172.20.0.5:9200\"]monitoring.ui.container.elasticsearch.enabled: true# kibana访问Elasticsearch的账号与密码(如果ElasticSearch设置了的话)elasticsearch.username: \"kibana_system\"elasticsearch.password: \"xxxxxxxxxxxx\"# kibana web语言i18n.locale: \"zh-CN\"# es证书配置, 这里是在docker-compose中已经挂载进容器了elasticsearch.ssl.certificate: /etc/kibana/cert/node.pemelasticsearch.ssl.key: /etc/kibana/cert/node.key# es的ca证书elasticsearch.ssl.certificateAuthorities: [ \"/etc/kibana/cert/elastic-stack-ca.pem\" ] 修改完毕后 ，重新部署一下 kibana服务即可 以上只要登录就可以使用了。 Filebeat 日志采集这里我才用的方式是 docker运行 filebeat服务，然后将网关服务器的Nginx日志挂载进filebeat 1234567891011121314vim /opt/filbeat/docker-compose.yamlversion: \"3\"services: filebeat: container_name: filebeat_test image: elastic/filebeat:7.9.0 user: root volumes: - /data/wwwlogs/:/var/log/nginx:ro - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro - ./ca.crt:/usr/share/filebeat/certs/ca.crt # 这个证书在接入kibana的时候已经提取出来了，即文件：elastic-stack-ca.pem # 他们内容是一样的只是文件名、格式不一样 command: filebeat -e filebeat的配置文件在 /opt/filebeat/filebeat.yml 1234567891011121314151617181920212223242526272829303132333435363738vim /opt/filebeat/filebeat.ymlfilebeat.inputs: ##################################################################################- type: log paths: - /var/log/nginx/testnet-xxxxxxxxxxxx.com.log tags: [\"proxy-testnet-xxxxxxxxxxxx.com.log\"] fields: index: \"proxy-testnet-xxxxxxxxxxxx.com.log\" json.keys_under_root: true json.overwrite_keys: true processors: - add_kubernetes_metadata: matchers: - logs_path: logs_path: \"/var/log/nginx/\" - decode_json_fields: when: regexp: message: \"{*}\" fields: [\"message\"] overwrite_keys: true ##################################################################################output.elasticsearch: hosts: [\"https://10.10.10.29:9200\",\"https://10.10.10.29:9201\",\"https://10.10.10.29:9201\"] username: elastic password: vDNnbd8FXqR2i13NYGfj ssl.certificate_authorities: - /usr/share/filebeat/certs/ca.crt indices: - index: \"proxy-testnet-xxxxxxxxxxxx.com.log-%{+yyyy.MM.dd}\" when.contains: fields: index: \"proxy-testnet-xxxxxxxxxxxx.com.log\" setup.kibana: hosts: \"http://10.10.10.29:5601\" username: \"elastic\" password: \"xxxxxxxxxxxx\" 配置完毕之后，查看日志是否异常，如果没有异常，到node-1上面通过ES API查询是否有index生成 12345678root@node-1:/usr/share/elasticsearch# curl -k --user elastic:xxxxxxxxxxxx -X GET \"https://172.20.0.3:9200/_cat/indices \" --cert-type P12 --cert /usr/share/elasticsearch/config/certs/node.p12..................green open proxy-testnet-xxxxxxxxxxxx.com.log-2024.07.23 Su-T62zGTheqbDsDZLzH_A 1 1 2911 0 1.7mb 914.2kb.................. Kibana配置上面可以看到filebeat已经正常往ES写入数据，在kibana中配置一下： Stack Manager - Index Patterns - Create index pattern 直接创建后菜单栏： Discover 选择刚刚创建的这个Index Pattern 最后直接就可以看到收集到的日志kibana已经做好了字段分隔，只需要选择需要的字段，或者匹配某些字段就可以了。 比如 我想看状态码大于等于500的： status &gt;= 500，都可以在上面的搜索框中去定义筛选 当然也可以把这个日志做成可视化，但选择某个指标的时候，其他指标也可以联动起来，例如： 最后再结合前面写过的 如何利用ElastAlert2对ES中的日志创建告警 就可以轻松的对日志进行监控告警啦~ 写在最后 上面过程仅仅是记录，主要还是思路。 k8s version123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189--- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: ops labels: k8s-app: filebeat data: filebeat.yml: |- #====================== input ================= filebeat.inputs: # nginx-ingress - type: container paths: - /var/log/containers/*goerli-testnet-collector*.log tags: [\"goerli-testnet-collector\"] fields: index: \"goerli-testnet-collector\" processors: - add_kubernetes_metadata: host: ${NODE_NAME} matchers: - logs_path: logs_path: \"/var/log/containers/\" - decode_json_fields: when: regexp: message: \"{*}\" fields: [\"message\"] overwrite_keys: true target: \"\" # testnet-bsc-collector - type: container paths: - /var/log/containers/*testnet-bsc-collector*.log tags: [\"testnet-bsc-collector\"] fields: index: \"testnet-bsc-collector\" processors: - add_kubernetes_metadata: host: ${NODE_NAME} matchers: - logs_path: logs_path: \"/var/log/containers/\" - decode_json_fields: when: regexp: message: \"{*}\" fields: [\"message\"] #overwrite_keys: true target: \"\" #================ output ===================== output.elasticsearch: hosts: [\"https://10.10.20.23:9200\", \"https://10.10.20.120:9200\", \"https://10.10.20.160:9200\"] username: elastic password: r40SgkMhmjwK8rIpClFM ssl.certificate_authorities: - /usr/share/filebeat/ca.crt indices: - index: \"INDEX-NAME-%{+yyyy.MM.dd}\" when.contains: fields: index: \"INDEX-NAME\" - index: \"testnet-bsc-collector-%{+yyyy.MM.dd}\" when.contains: fields: index: \"testnet-bsc-collector\" #============== Elasticsearch template setting ========== setup.ilm.enabled: false setup.template.name: 'k8s-logs' setup.template.pattern: 'k8s-logs-*' processors: - drop_fields: fields: [\"agent\",\"kubernetes.labels\",\"input.type\",\"log\",\"ecs.version\",\"host.name\",\"kubernetes.replicaset.name\",\"kubernetes.pod.uid\",\"kubernetes.pod.uid\",\"tags\",\"stream\",\"kubernetes.container.name\"] --- apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: ops labels: k8s-app: filebeat spec: selector: matchLabels: k8s-app: filebeat template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 hostNetwork: true dnsPolicy: ClusterFirstWithHostNet tolerations: - effect: NoSchedule operator: Exists containers: - name: filebeat image: elastic/filebeat:7.9.0 args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName securityContext: runAsUser: 0 # If using Red Hat OpenShift uncomment this: #privileged: true resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/containerd/ readOnly: true - name: varlog mountPath: /var/log readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/containerd/ - name: varlog hostPath: path: /var/log # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: opsroleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: ops labels: k8s-app: filebeat ---","categories":[],"tags":[],"keywords":[]},{"title":"如何利用ElastAlert2对ES中的日志创建告警","slug":"如何利用ElastAlert2对ES中的日志创建告警","date":"2024-07-16T09:56:39.000Z","updated":"2025-09-01T01:59:08.942Z","comments":true,"path":"2024/07/16/如何利用ElastAlert2对ES中的日志创建告警/","permalink":"https://blog.sctux.cc/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/","excerpt":"背景线上API服务出现了5xx的请求错误, 未能在第一时间发现导致用户主动向团队报告接口有异常， 由于API的接口比较多，也只是监控了部分，事后通过日志才发现的确用户侧在对某个接口访问时，频繁出现5xx告警，我这边也没有收到什么通知，目前先暂且不说出现5xx是什么造成的，而是在API接口响应状态的监控方面没有做到很到位， 所以需要对Nginx日志中的状态为5xx的请求监控告警，以便能快速响应，解决问题。 ElastAlert2ElastAlert 2是一个简单的框架，用于对来自 Elasticsearch 和 OpenSearch 的数据的异常、尖峰或其他感兴趣的模式发出警报。 官方文档：https://elastalert2.readthedocs.io/en/latest/ Github: https://github.com/jertel/elastalert2 配置部署当然还是选择容器化方式运行，官方也提供得有镜像： 1docker pull docker pull jertel/elastalert2 配置文件配置","text":"背景线上API服务出现了5xx的请求错误, 未能在第一时间发现导致用户主动向团队报告接口有异常， 由于API的接口比较多，也只是监控了部分，事后通过日志才发现的确用户侧在对某个接口访问时，频繁出现5xx告警，我这边也没有收到什么通知，目前先暂且不说出现5xx是什么造成的，而是在API接口响应状态的监控方面没有做到很到位， 所以需要对Nginx日志中的状态为5xx的请求监控告警，以便能快速响应，解决问题。 ElastAlert2ElastAlert 2是一个简单的框架，用于对来自 Elasticsearch 和 OpenSearch 的数据的异常、尖峰或其他感兴趣的模式发出警报。 官方文档：https://elastalert2.readthedocs.io/en/latest/ Github: https://github.com/jertel/elastalert2 配置部署当然还是选择容器化方式运行，官方也提供得有镜像： 1docker pull docker pull jertel/elastalert2 配置文件配置可参考 https://github.com/jertel/elastalert2 目录中的 examples/config.yaml.example 1234567891011121314151617181920mkdir -p /opt/elastalert2/{data,rules}vim /opt/elastalert2/config.yamlrules_folder: rules # 存放规则文件的目录，这个目录需要挂载到/opt/elastalert/下run_every: minutes: 1buffer_time: minutes: 15es_host: 10.10.20.23es_port: 9200use_ssl: Trueverify_certs: Trueca_certs: /opt/elastalert/ca.crt # 如果集群启用了ssl记得要把ca证书挂在到容器的这个位置ssl_show_warn: Truees_send_get_body_as: GETes_username: # YOUR ES USERNAMEes_password: # YOUR ES PASSWORDwriteback_index: elastalert_statusalert_time_limit: days: 2 Rule规则配置我们网关服务器的Nginx日志是通过filebeat写到Elasticsearch的，也做好了index，其实在ELK 界面也可以看到这些状态数据，但是仅仅只是展示，没有告警，所以才有了这篇blog。 创建一个针对 某个API域名的日志的状态监控文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960vim /opt/elastalert2/rules/mainnet-api.xxxxx.yaml#rule名字,必须唯一name: The number of times this request responds with a status code of 5xx in the log is greater than 5 times within 1 minute, please pay attention(mainnet-api.xxxxx)!#类型,官方提供多种类型type: frequency#ES索引,支持通配符index: proxy-mainnet-api.xxxxx.log-*#在timeframe时间内,匹配到多少个结果便告警num_events: 1#监控周期.默认是minutes: 1timeframe: seconds: 5#匹配模式.这里我匹配的是500—599的所有可能会出现的状态码filter:- range: status: from: 500 to: 599# 自定义发送格式(start)# 说明： 以下可以自定义发送的内容，如果什么都不指定，会发送ElastAlert2原生的一大堆json格式的内容，易读性很低，所以这里为了内容简介，我这里就只是定义了这些内容：alert_text_type: alert_text_onlyalert_text: \" 响应状态码: {} \\n 发生时间UTC: {} \\n ES Index: {} \\n num_hits: {} \\n 请求URL: https://mainnet-api.xxxxx{} \\n ClientIP: {} \\n num_matches: {} \\n http_user_agent: {}\"alert_text_args: - status - \"@timestamp\" - _index - num_hits - request_uri - http_x_forwarded_for - num_matches - http_user_agent# 自定义发送格式(end)# 这里选择使用的是Discord来接收通知，ElastAlert2其实很强大，支持很多渠道的告警# 这里发现并不能同时多个渠道发送告警消息# 如果需要多个告警渠道，可以写一个类似于AlertCenter的服务，通过这个服务整合后发送到不同的渠道，这里这是个思路，目前先满足需求alert:- \"discord\"discord_webhook_url: \"YOUR DISCORD WEBHOOK URL\"discord_emoji_title: \":lock:\"discord_embed_color: 0xE24D42discord_embed_footer: \"Message sent by from api status moniotor(for @noardguo)\"discord_embed_icon_url: \"https://humancoders-formations.s3.amazonaws.com/uploads/course/logo/38/thumb_bigger_formation-elasticsearch.png\" 部署运行上面已经准备好了所需要的文件： 123456789root@ip-10-10-10-11:/opt/elastalert2# tree -L 2.├── ca.crt├── config.yaml├── data└── rules ├── mainnet-api.xxxxx.yml2 directories, 3 files docker 方式运行： 1234567891011121314151617181920docker run -d --name=elastalert2 \\ --env=TZ=Asia/Shanghai \\ --volume=/opt/elastalert2/data:/opt/elastalert/data \\ --volume=/opt/elastalert2/config.yaml:/opt/elastalert/config.yaml \\ --volume=/opt/elastalert2/rules:/opt/elastalert/rules \\ --volume=/opt/elastalert2/ca.crt:/opt/elastalert/ca.crt \\ --restart=always \\ jertel/elastalert2 root@ip-10-10-10-11:/opt/elastalert2# docker logs -f wonderful_tharpReading Elastic 7 index mappings:Reading index mapping 'es_mappings/7/silence.json'Reading index mapping 'es_mappings/7/elastalert_status.json'Reading index mapping 'es_mappings/7/elastalert.json'Reading index mapping 'es_mappings/7/past_elastalert.json'Reading index mapping 'es_mappings/7/elastalert_error.json'Index elastalert_status already exists. Skipping index creation.WARNING:py.warnings:/usr/local/lib/python3.12/site-packages/elasticsearch/connection/base.py:193: ElasticsearchDeprecationWarning: Camel case format name dateOptionalTime is deprecated and will be removed in a future version. Use snake case name date_optional_time instead. warnings.warn(message, category=ElasticsearchDeprecationWarning) 如果没有报错就说明运行正常，此时从kibana面板也可以看到ElaltAlert2生成了监控告警所需要的index: 测试当运行起来后，此时就是是时间监控日志文件的一个状态 在网关服务器上面echo一段带有status为500的message到日志文件中： 1echo '''{\"@timestamp\":\"2024-07-16T20:28:10+08:00\",\"server_addr\":\"110.10.30.187\",\"remote_addr\":\"10.10.10.248\",\"http_x_forwarded_for\":\"172.104.86.126, 172.68.119.188\",\"scheme\":\"http\",\"request_method\":\"POST\",\"request_uri\": \"Alert_Test\",\"request_length\": \"953\",\"uri\": \"/api/v1/alerttest\", \"request_time\":0.002,\"body_bytes_sent\":0,\"bytes_sent\":335,\"status\":\"500\",\"upstream_time\":\"0.002\",\"upstream_host\":\"10.10.10.139:31333\",\"upstream_status\":\"200\",\"host\":\"mainnet-api.xxxxxx\",\"http_referer\":\"https://test.xyz/\",\"http_user_agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"}''' &gt;&gt; mainnet-api.xxxxxx.log 此时查看 index已经记录到这条告警消息, 通过查询索引也找到了这个请求 同时，Discord也收到了告警通知 最后，因为我们跑了k8s,下面直接把相关文件用configmap方式挂载进去，运行一个Deployment副本即可。 elastalert-config.yaml 123456789101112131415161718192021222324apiVersion: v1data: config.yaml: |- rules_folder: rules run_every: minutes: 1 buffer_time: minutes: 15 es_host: 10.10.20.23 es_port: 9200 use_ssl: True verify_certs: True ca_certs: /opt/elastalert/ca.crt ssl_show_warn: True es_send_get_body_as: GET es_username: elastic es_password: A2VdKUHHFoUmlyekVFgd writeback_index: elastalert_status alert_time_limit: days: 2metadata: name: elastalert-config namespace: monitor ealstalert-rules.yaml 123456789101112131415161718192021apiVersion: v1data: mainnet-api.xxxxxx.yml: \"#rule名字,必须唯一\\nname: The number of times this request responds with a status code of 5xx in the log is greater than 5 times within 1 minute, please pay attention(mainnet-api.xxxxx)!\\n\\n#类型,官方提供多种类型\\ntype: frequency\\n\\n#ES索引,支持通配符\\nindex: proxy-mainnet-api.xxxxx.log-*\\n\\n#在timeframe时间内,匹配到多少个结果便告警\\nnum_events: 1\\n\\n#监控周期.默认是minutes: 1\\ntimeframe:\\n seconds: 5 \\n \\n#匹配模式.\\nfilter:\\n- range:\\n \\ status:\\n from: 500\\n to: 599\\n \\nalert_text_type: alert_text_only\\nalert_text: \\\" \\n 响应状态码: {} \\\\n\\n 发生时间UTC: {} \\\\n\\n ES Index: {} \\\\n\\n num_hits: {} \\\\n\\n \\ 请求URL: https://mainnet-api.xxxxx.org{} \\\\n\\n ClientIP: {} \\\\n\\n num_matches: {} \\\\n\\n http_user_agent: {}\\n\\\"\\nalert_text_args:\\n - status\\n - \\\"@timestamp\\\"\\n \\ - _index\\n - num_hits\\n - request_uri\\n - http_x_forwarded_for\\n \\ - num_matches\\n - http_user_agent\\n \\n \\nalert:\\n- \\\"discord\\\"\\n#discord_webhook_url: \\\"DISCORD_WEBHOOK_URL\"\\ndiscord_webhook_url: \\\"https://https://discord.com/api/webhooks/1196752947493752933/VRQ01W1pT0PHpl55z0hayqsyjWzt3bzXUMSA4-_5W56fn9j5Nl1zDQT7ZtU_CQWnnlYH\\\"\\ndiscord_emoji_title: \\\":lock:\\\"\\ndiscord_embed_color: 0xE24D42\\ndiscord_embed_footer: \\\"Message sent by from api status moniotor(for @Ops-NoardGuo)\\\"\\ndiscord_embed_icon_url: \\\"https://humancoders-formations.s3.amazonaws.com/uploads/course/logo/38/thumb_bigger_formation-elasticsearch.png\\\"kind: ConfigMapmetadata: name: ealstalert-rules namespace: monitor ca.crt略 elastalert-deployment.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566---apiVersion: apps/v1kind: Deploymentmetadata: annotations: k8s.kuboard.cn/displayName: elastalert labels: k8s.kuboard.cn/name: elastalert name: elastalert namespace: monitorspec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s.kuboard.cn/name: elastalert strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: k8s.kuboard.cn/name: elastalert spec: containers: - args: - '--verbose' # 这里指定参数是默认镜像里面可以获取的参数，用于定义日志记录 排错或者运行状态查看 - image: jertel/elastalert2 imagePullPolicy: IfNotPresent name: elastalert resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /opt/elastalert/config.yaml name: volume-ps74r subPath: config.yaml - mountPath: /opt/elastalert/rules/ name: rules - mountPath: /opt/elastalert/ca.crt name: volume-zmdx7 subPath: ca.crt dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - configMap: defaultMode: 420 name: elastalert-config name: volume-ps74r - configMap: defaultMode: 420 items: - key: mainnet-api.xxxxx.yml path: mainnet-api.xxxxx.yml name: ealstalert-rules name: rules - configMap: defaultMode: 420 name: elasticsearch-ca.crt name: volume-zmdx7 至此告警功能已经实现，以上只是一个大体的思路，不仅仅监控的可以是状态码，应该是你能想到的，他能提供的都可以监控起来。 好啦，我要去看5xx的报错原因了😀","categories":[],"tags":[],"keywords":[]},{"title":"如何利用Python获取所有pod容器状态","slug":"如何利用Python获取所有pod容器状态","date":"2024-07-09T09:38:00.000Z","updated":"2025-09-01T07:57:25.123Z","comments":true,"path":"2024/07/09/如何利用Python获取所有pod容器状态/","permalink":"https://blog.sctux.cc/2024/07/09/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Python%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89pod%E5%AE%B9%E5%99%A8%E7%8A%B6%E6%80%81/","excerpt":"在日常巡检过程当中，不需要登录服务器去查看，通过调用k8s api的方式获取所有pod的状态 然后在每天9点执行本脚本即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101from kubernetes import client, configfrom kubernetes.client.rest import ApiExceptionfrom datetime import datetime, timezoneimport requestsimport jsonimport sysimport pytz# 加载 Kubernetes 配置#默认会在master节点去读取 ~/.kube/config 文件config.load_kube_config()# 创建 Kubernetes API 客户端实例api_instance = client.CoreV1Api()# 指定要获取的命名空间列表target_namespaces = [sys.argv[1]] # 替换为你的目标命名空间列表filtered_keywords = [\"mysql\", \"redis\", \"memcached\", \"postgres\", \"backend\"]# Discord Webhook URLdiscord_webhook_url = \"\" # FOR TEST# 替换为你的 Discord Webhook URLcontainers_without_restart = [] # 没有重启原因的容器列表containers_with_restart = [] # 具有重启原因的容器列表try: for target_namespace in target_namespaces: # 获取命名空间下的所有 Pod pods = api_instance.list_namespaced_pod(namespace=target_namespace).items for pod in pods: pod_name = pod.metadata.name pod_status = pod.status.phase pod_restart_reason = \"\" pod_start_time = pod.metadata.creation_timestamp if any(keyword in pod_name for keyword in filtered_keywords): continue # 获取当前时间 cst_timezone = pytz.timezone(\"Asia/Shanghai\") current_time = datetime.now(timezone.utc) # 计算运行时长 if pod_start_time is not None: pod_duration = current_time - pod_start_time pod_duration_str = str(pod_duration).split(\".\")[0] # 格式化为字符串，去掉小数部分 else: pod_duration_str = \"Unknown\" # 检查 Pod 是否有重启记录 if pod.status.container_statuses is not None: for container_status in pod.status.container_statuses: restart_count = container_status.restart_count # 如果重启次数大于 0，则获取重启原因 if restart_count &gt; 0: pod_restart_reason = container_status.last_state.terminated.reason if pod_restart_reason: containers_with_restart.append( (pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count) ) else: containers_without_restart.append( (pod_name, pod_status, pod_duration_str) ) # 生成消息 message = \"环境: {0} 获取时间：{1}\\n\\n\".format(target_namespace,datetime.now(cst_timezone).strftime(\"%Y-%m-%d %H:%M:%S\") + \" CST\") # 添加没有重启原因的容器信息 for container in containers_without_restart: pod_name, pod_status, pod_duration_str = container message += \"容器名称: {0} 当前状态: {1} 运行时长: {2}\\n\".format( pod_name, pod_status, pod_duration_str ) message += \"------------------------------------------------------\\n\" # 添加具有重启原因的容器信息 for container in containers_with_restart: pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count = container message += \"容器名称: {0} 当前状态: {1} 运行时长: {2} 重启原因: {3} 重启次数：{4}\\n\".format( pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count ) message = \"```{0}```\".format(message) # 发送消息到 Discord payload = {\"content\": message} headers = {\"Content-Type\": \"application/json\"} response = requests.post( discord_webhook_url, data=json.dumps(payload), headers=headers ) print(response.content) if response.status_code == 204: print(\"Message sent to Discord successfully\") else: print(f\"Failed to send message to Discord. Status code: {response.status_code}\")except ApiException as e: print(f\"Exception when calling CoreV1Api: {e}\\n\")","text":"在日常巡检过程当中，不需要登录服务器去查看，通过调用k8s api的方式获取所有pod的状态 然后在每天9点执行本脚本即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101from kubernetes import client, configfrom kubernetes.client.rest import ApiExceptionfrom datetime import datetime, timezoneimport requestsimport jsonimport sysimport pytz# 加载 Kubernetes 配置#默认会在master节点去读取 ~/.kube/config 文件config.load_kube_config()# 创建 Kubernetes API 客户端实例api_instance = client.CoreV1Api()# 指定要获取的命名空间列表target_namespaces = [sys.argv[1]] # 替换为你的目标命名空间列表filtered_keywords = [\"mysql\", \"redis\", \"memcached\", \"postgres\", \"backend\"]# Discord Webhook URLdiscord_webhook_url = \"\" # FOR TEST# 替换为你的 Discord Webhook URLcontainers_without_restart = [] # 没有重启原因的容器列表containers_with_restart = [] # 具有重启原因的容器列表try: for target_namespace in target_namespaces: # 获取命名空间下的所有 Pod pods = api_instance.list_namespaced_pod(namespace=target_namespace).items for pod in pods: pod_name = pod.metadata.name pod_status = pod.status.phase pod_restart_reason = \"\" pod_start_time = pod.metadata.creation_timestamp if any(keyword in pod_name for keyword in filtered_keywords): continue # 获取当前时间 cst_timezone = pytz.timezone(\"Asia/Shanghai\") current_time = datetime.now(timezone.utc) # 计算运行时长 if pod_start_time is not None: pod_duration = current_time - pod_start_time pod_duration_str = str(pod_duration).split(\".\")[0] # 格式化为字符串，去掉小数部分 else: pod_duration_str = \"Unknown\" # 检查 Pod 是否有重启记录 if pod.status.container_statuses is not None: for container_status in pod.status.container_statuses: restart_count = container_status.restart_count # 如果重启次数大于 0，则获取重启原因 if restart_count &gt; 0: pod_restart_reason = container_status.last_state.terminated.reason if pod_restart_reason: containers_with_restart.append( (pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count) ) else: containers_without_restart.append( (pod_name, pod_status, pod_duration_str) ) # 生成消息 message = \"环境: {0} 获取时间：{1}\\n\\n\".format(target_namespace,datetime.now(cst_timezone).strftime(\"%Y-%m-%d %H:%M:%S\") + \" CST\") # 添加没有重启原因的容器信息 for container in containers_without_restart: pod_name, pod_status, pod_duration_str = container message += \"容器名称: {0} 当前状态: {1} 运行时长: {2}\\n\".format( pod_name, pod_status, pod_duration_str ) message += \"------------------------------------------------------\\n\" # 添加具有重启原因的容器信息 for container in containers_with_restart: pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count = container message += \"容器名称: {0} 当前状态: {1} 运行时长: {2} 重启原因: {3} 重启次数：{4}\\n\".format( pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count ) message = \"```{0}```\".format(message) # 发送消息到 Discord payload = {\"content\": message} headers = {\"Content-Type\": \"application/json\"} response = requests.post( discord_webhook_url, data=json.dumps(payload), headers=headers ) print(response.content) if response.status_code == 204: print(\"Message sent to Discord successfully\") else: print(f\"Failed to send message to Discord. Status code: {response.status_code}\")except ApiException as e: print(f\"Exception when calling CoreV1Api: {e}\\n\")","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s,pod","slug":"k8s-pod","permalink":"https://blog.sctux.cc/tags/k8s-pod/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"K8S中MySQL使用NFS挂载的异常问题处理","slug":"K8S中MySQL使用NFS挂载的异常问题处理","date":"2024-05-10T12:58:02.000Z","updated":"2025-09-01T01:59:08.984Z","comments":true,"path":"2024/05/10/K8S中MySQL使用NFS挂载的异常问题处理/","permalink":"https://blog.sctux.cc/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/","excerpt":"问题原因在k8s中使用nfs作为持久存储方式，根据不同环境对应不同名称空间的pod使用的存储类/存储卷 相对应，发现mysql8.0.23 在启动时挂载NFS会卡住的情况： 问题复现这里在某台宿主机上面创建了一个本地目录，然后按照默认方式： 1mount -t nfs 10.10.10.142:/data/bbb /tmp/cc 将nfs挂载到本地，然后映射到容器中去，所以这里跟k8s环境没多大联系，主要是nfs挂载问题： 初始化一直卡着，而且在挂载目录发现只有这个文件： 问题排查","text":"问题原因在k8s中使用nfs作为持久存储方式，根据不同环境对应不同名称空间的pod使用的存储类/存储卷 相对应，发现mysql8.0.23 在启动时挂载NFS会卡住的情况： 问题复现这里在某台宿主机上面创建了一个本地目录，然后按照默认方式： 1mount -t nfs 10.10.10.142:/data/bbb /tmp/cc 将nfs挂载到本地，然后映射到容器中去，所以这里跟k8s环境没多大联系，主要是nfs挂载问题： 初始化一直卡着，而且在挂载目录发现只有这个文件： 问题排查检查一下nfs挂载的参数情况： 12345mount -v | grep nfs10.10.10.137:/data/nfs_share_137 on /tmp/fans type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.10.10.137,mountvers=3,mo=udp,local_lock=none,addr=10.10.10.137)10.10.10.142:/data/bbb on /tmp/cc type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.10.10.142,mountvers=3,mountport=569lock=none,addr=10.10.10.142) 如上可以看出我们如果使用默认的方式直接挂载，它会有一些默认参数 后面也是通过该容器运行的宿主机上的内核日志才发现，这个问题： 问题处理这个错误提示实际上是NFS客户端无法获得文件锁定，而不是无法与NFS服务器建立联系。当NFS客户端无法获得文件锁定时，它会尝试向NFS服务器发送锁定请求，并等待服务器响应。如果NFS服务器没有响应，则可能会出现类似的错误提示。可能造成这个错误的原因如下： 检查NFS服务器的负载情况，如果负载过高，则可能需要优化服务器配置或增加服务器数量。 检查NFS客户端与服务器之间的网络连接，如果存在网络故障，则可能需要修复网络问题或更改网络配置。 考虑使用NFS挂载选项来优化NFS客户端的行为，例如使用soft选项而不是hard选项，或者使用intr选项允许中断NFS操作。 如果NFS客户端和服务器在不同的时区中运行，则可能需要使用noac选项来禁用NFS客户端的属性缓存，以确保文件锁定请求和响应的时间戳是正确的。 如果您的NFS客户端和服务器运行的是不同的操作系统，则可能需要使用适当的挂载选项来确保文件锁定机制得到正确支持，例如使用nolock选项来禁用文件锁定机制。 因为我们使用了kuboard, 无法全局对这个存储类做暂时不能影响业务，不能所有变更： 这里只是单独这个mysql服务没有办法挂在使用，是由需要添加挂载参数： 找到我们在页面上新建的pv: 123456789101112131415161718192021222324252627282930313233343536373839 kubectl edit pv -n testnet pvc-b0de2c7f-49de-4e16-83cc-492de8292e5f# Please edit the object below. Lines beginning with a '#' will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1kind: PersistentVolumemetadata: annotations: pv.kubernetes.io/provisioned-by: nfs-testnet creationTimestamp: \"2023-07-20T16:29:30Z\" finalizers: - kubernetes.io/pv-protection name: pvc-b0de2c7f-49de-4e16-83cc-492de8292e5f resourceVersion: \"147600146\" uid: e9f111ac-2932-46a0-b6bc-876275dfa02fspec: accessModes: - ReadWriteOnce capacity: storage: 2G claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: dassssssssssssssss namespace: testnet resourceVersion: \"147600141\" uid: b0de2c7f-49de-4e16-83cc-492de8292e5f nfs: path: /data/testnet-dassssssssssssssss-pvc-b0de2c7f-49de-4e16-83cc-492de8292e5f server: 10.10.10.142 persistentVolumeReclaimPolicy: Retain storageClassName: testnet volumeMode: Filesystemstatus: phase: Bound 然后我们在volumeMode同级 就可以定义挂载参数(具体参数：https://poe.com/s/FyTCOkthr3VREHU3mByA)： 12345mountOptions:- nolock- timeo=120- intr- retrans=10 这样我们单独为MySQL服务创建的这个pvc就拥有了这些挂载参数，初始化，运行正常： 在这个pod运行的宿主机上 查看结果，可以看到我们修改pv后的参数已经生效，并且在nfs服务端可以看到我们的MySQL初始化文件已经生成： 最终该挂载问题完美解决。 对于特殊，不同的deployment 可能需要特殊处理，这里按照mysql的部署，记录一下顺序 创建pvc 修改默认pvc配置，增加nfs挂载参数 启动pod 检查挂载目录数据是否正常生成 检查挂载选项是否已经增加 By the way:谁说的K8S不能跑MySQL， 我嫩死TA 🔪🔪🔪","categories":[],"tags":[{"name":"NFS","slug":"NFS","permalink":"https://blog.sctux.cc/tags/NFS/"}],"keywords":[]},{"title":"Prometheus+Consul实现企业级宿主机+容器监控告警","slug":"Prometheus-Consul实现企业级宿主机-容器监控告警","date":"2022-09-12T06:45:26.000Z","updated":"2025-09-01T01:59:08.871Z","comments":true,"path":"2022/09/12/Prometheus-Consul实现企业级宿主机-容器监控告警/","permalink":"https://blog.sctux.cc/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/","excerpt":"一、背景因公司历史遗留原因有个别环境暂时没有使用kubernets, 现在需要将这批服务器的监控系统从zabbix替换到Prometheus, 于是乎这边有个问题就是需要将所有服务器上面的所有的exporter mertics（即 target）地址写到Prometheus配置文件中，这样一来，维护一个文件，似乎还算可以，但是这里我采用Prometheus+Consul的方式来管理服务器上的所有exporter， 那么这样做的好处是我们能更清晰的通过Consul来管理上面的每个exporter url , 以及通过consul自带的自定义元数据，再结合Promethues无疑是个很灵活的方式。 二、架构 promtheus配置数据源为consu_sd_configs 地址指向一个consul客户端地址； 通过脚本调用consulapi的方式将宿主机上面的cadvisor-exporter,node-exporter metrics的地址注册到consul中； promtheus 检测到了新增服务后，会通过这个http://xxxxxx:xxx/metrics url 抓取采集数据； 后续的数据采集就跟平常我们使用的方式一样了，采集到的数据时序保存在promtheus； Grafana作为采集数据的一个展示,通过各种label，更加方便的对面板进行配置； 添加node-exporter， cadvisor-exporter的指标报警规则，通过Alertmanager 发出主机或容器的告警； 三、实现Prometheus以及周边组件部署该编排文件中部署了 Prometheus: 监控系统主程序 Alertmanager: 告警发送 Grafana: 数据展示 Prometheus-dingding-webhook: 钉钉告警推送 alertmanager-dashboard: 主要用于告警条目展示，该软件是开源项目，随意下载部署 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104cat &gt; docker-compose.yaml &lt;&lt; EOFversion: '3.7'services: prometheus: image: prom/prometheus:latest volumes: - ./prometheus/:/etc/prometheus/ - /data/prometheus-data:/prometheus command: - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/usr/share/prometheus/console_libraries' - '--web.console.templates=/usr/share/prometheus/consoles' - '--web.enable-lifecycle' - '--web.external-url=http://192.168.18.178:9090' ports: - 9090:9090 links: - alertmanager:alertmanager restart: always deploy: resources: limits: cpus: \"1.0\" memory: 500M alertmanager: image: prom/alertmanager ports: - 9093:9093 volumes: - ./alertmanager/:/etc/alertmanager/ restart: always command: - '--config.file=/etc/alertmanager/config.yml' - '--storage.path=/alertmanager' deploy: resources: limits: cpus: \"1.0\" memory: 500M grafana: image: grafana/grafana:8.4.0 depends_on: - prometheus ports: - 3000:3000 volumes: - /data/grafana-data:/var/lib/grafana - ./grafana/provisioning/:/etc/grafana/provisioning/ env_file: - ./grafana/config.monitoring restart: always deploy: resources: limits: cpus: \"1.0\" memory: 500M webhook: image: timonwong/prometheus-webhook-dingtalk depends_on: - prometheus volumes: - ./prometheus-webhook-dingtalk/config.yml:/config/config.yml - ./prometheus-webhook-dingtalk/dingding.tmpl:/config/dingding.tmpl ports: - 9060:8060 command: - '--web.listen-address=:8060' - '--config.file=/config/config.yml' - '--web.enable-ui' - '--web.enable-lifecycle' restart: always deploy: resources: limits: cpus: \"1.0\" memory: 500M alertmanager-dashboard: image: ghcr.io/prymitive/karma:latest ports: - 9094:8080 restart: always environment: - ALERTMANAGER_URI=http://192.168.18.178:9093 deploy: resources: limits: cpus: \"1.0\" memory: 500MEOF# run起来docker-compose up -d","text":"一、背景因公司历史遗留原因有个别环境暂时没有使用kubernets, 现在需要将这批服务器的监控系统从zabbix替换到Prometheus, 于是乎这边有个问题就是需要将所有服务器上面的所有的exporter mertics（即 target）地址写到Prometheus配置文件中，这样一来，维护一个文件，似乎还算可以，但是这里我采用Prometheus+Consul的方式来管理服务器上的所有exporter， 那么这样做的好处是我们能更清晰的通过Consul来管理上面的每个exporter url , 以及通过consul自带的自定义元数据，再结合Promethues无疑是个很灵活的方式。 二、架构 promtheus配置数据源为consu_sd_configs 地址指向一个consul客户端地址； 通过脚本调用consulapi的方式将宿主机上面的cadvisor-exporter,node-exporter metrics的地址注册到consul中； promtheus 检测到了新增服务后，会通过这个http://xxxxxx:xxx/metrics url 抓取采集数据； 后续的数据采集就跟平常我们使用的方式一样了，采集到的数据时序保存在promtheus； Grafana作为采集数据的一个展示,通过各种label，更加方便的对面板进行配置； 添加node-exporter， cadvisor-exporter的指标报警规则，通过Alertmanager 发出主机或容器的告警； 三、实现Prometheus以及周边组件部署该编排文件中部署了 Prometheus: 监控系统主程序 Alertmanager: 告警发送 Grafana: 数据展示 Prometheus-dingding-webhook: 钉钉告警推送 alertmanager-dashboard: 主要用于告警条目展示，该软件是开源项目，随意下载部署 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104cat &gt; docker-compose.yaml &lt;&lt; EOFversion: '3.7'services: prometheus: image: prom/prometheus:latest volumes: - ./prometheus/:/etc/prometheus/ - /data/prometheus-data:/prometheus command: - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/usr/share/prometheus/console_libraries' - '--web.console.templates=/usr/share/prometheus/consoles' - '--web.enable-lifecycle' - '--web.external-url=http://192.168.18.178:9090' ports: - 9090:9090 links: - alertmanager:alertmanager restart: always deploy: resources: limits: cpus: \"1.0\" memory: 500M alertmanager: image: prom/alertmanager ports: - 9093:9093 volumes: - ./alertmanager/:/etc/alertmanager/ restart: always command: - '--config.file=/etc/alertmanager/config.yml' - '--storage.path=/alertmanager' deploy: resources: limits: cpus: \"1.0\" memory: 500M grafana: image: grafana/grafana:8.4.0 depends_on: - prometheus ports: - 3000:3000 volumes: - /data/grafana-data:/var/lib/grafana - ./grafana/provisioning/:/etc/grafana/provisioning/ env_file: - ./grafana/config.monitoring restart: always deploy: resources: limits: cpus: \"1.0\" memory: 500M webhook: image: timonwong/prometheus-webhook-dingtalk depends_on: - prometheus volumes: - ./prometheus-webhook-dingtalk/config.yml:/config/config.yml - ./prometheus-webhook-dingtalk/dingding.tmpl:/config/dingding.tmpl ports: - 9060:8060 command: - '--web.listen-address=:8060' - '--config.file=/config/config.yml' - '--web.enable-ui' - '--web.enable-lifecycle' restart: always deploy: resources: limits: cpus: \"1.0\" memory: 500M alertmanager-dashboard: image: ghcr.io/prymitive/karma:latest ports: - 9094:8080 restart: always environment: - ALERTMANAGER_URI=http://192.168.18.178:9093 deploy: resources: limits: cpus: \"1.0\" memory: 500MEOF# run起来docker-compose up -d 以上运行起来后，基本的功能已经完成，此时只需要将target添加到prometheus中就可以了 3.2 exporter(数据采集器)部署上面提到我们这部分环境没有迁移到k8s，所有微服务都是docker方式运行在宿主机上面的，所以，这里需要单独需要去部署对宿主机跟容器的eporter(数据采集器), 目前的需求就是监控宿主机以及容器，并将采集到的数据进行展示、告警等。 宿主机的监控：利用Prometheus搭建部署监控系统，那么对于服务器层面的数据采集我们首选的是node-exporter； 容器的监控：在调研容器监控这块儿的时候发现container-exporter监控指标虽然能满足我们的需求，但是发现采集的数据过于简洁且在运行过程中会造成内存积压最终导致服务不可用，除非是人为干涉重启一下，释放内存，但是这种事儿不应该发生。因此还是采用cadvisor exporter来进行容器的数据采集 需要在每台宿主机上面部署两个exporter 创建目录(一般放到普通用户app家目录即可)1mkdir exporter-deploy &amp;&amp; cd exporter-deploy 编排文件准备12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455cat &gt; docker-compose.yaml &lt;&lt; EOFversion: '3.7'services: # 宿主机数据采集 node-exporter: image: bitnami/node-exporter:latest volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro - /:/host:ro,rslave command: - '--path.rootfs=/host' - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - --web.disable-exporter-metrics - --collector.filesystem.ignored-mount-points - \"^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)\" ports: - 9100:9100 restart: always deploy: resources: limits: cpus: \"1.0\" memory: 500M # 容器数据采集 cadvisor-exporter: image: gcr.io/cadvisor/cadvisor/cadvisor:v0.45.0 volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro command: - \"-docker_only=true\" - \"-housekeeping_interval=10s\" - \"-docker_only=true\" - \"--allow_dynamic_housekeeping=false\" - \"--storage_duration=20s\" ports: - 9105:8080 restart: always deploy: mode: global resources: limits: cpus: \"2.0\" memory: 500MEOF # run起来docker-compose up -d 以上，只需要此配置文件就部署好了 “exporter客户端” 下面就是 将这台服务器以及连个exporter的url 注册到consul中，如果有批量安装需求，直接在jump上面通过ansible 推送即可 3.3 consul 集群部署手动创建一个docker 网络1docker network create --driver bridge --subnet 10.10.0.0/24 consul-network 创建consul的数据、配置目录12sudo mkdir -p /data/consul/consul-server{1..3}/{data,config}sudo mkdir -p /data/consul/consul-client{1..2}/{data,config} 编辑docker-compose配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889cat &gt; docker-compose-consul-cluster.yaml &lt;&lt; \\EOFversion: '3.7' services: consul-server1: image: consul:latest network_mode: consul-network container_name: consul-server1 restart: always command: agent -server -client=0.0.0.0 -bootstrap-expect=3 -node=consul-server1 -bind=0.0.0.0 -config-dir=/consul/config volumes: - /data/consul/consul-server1/data:/consul/data - /data/consul/consul-server1/config:/consul/config consul-server2: image: consul:latest network_mode: consul-network container_name: consul-server2 restart: always command: agent -server -client=0.0.0.0 -retry-join=consul-server1 -node=consul-server2 -bind=0.0.0.0 -config-dir=/consul/config volumes: - /data/consul/consul-server2/data:/consul/data - /data/consul/consul-server2/config:/consul/config depends_on: - consul-server1 consul-server3: image: consul:latest network_mode: consul-network container_name: consul-server3 restart: always command: agent -server -client=0.0.0.0 -retry-join=consul-server1 -node=consul-server3 -bind=0.0.0.0 -config-dir=/consul/config volumes: - /data/consul/consul-server3/data:/consul/data - /data/consul/consul-server3/config:/consul/config depends_on: - consul-server1 consul-client1: image: consul:latest network_mode: consul-network container_name: consul-client1 restart: always ports: - 8500:8500 command: agent -client=0.0.0.0 -retry-join=consul-server1 -ui -node=consul-client1 -bind=0.0.0.0 -config-dir=/consul/config volumes: - /data/consul/consul-client1/data:/consul/data - /data/consul/consul-client1/config:/consul/config depends_on: - consul-server2 - consul-server3 consul-client2: image: consul:latest network_mode: consul-network container_name: consul-client2 restart: always ports: - 8501:8500 command: agent -client=0.0.0.0 -retry-join=consul-server1 -ui -node=consul-client2 -bind=0.0.0.0 -config-dir=/consul/config volumes: - /data/consul/consul-client2/data:/consul/data - /data/consul/consul-client2/config:/consul/config depends_on: - consul-server2 - consul-server3EOF # 运行起来：docker-compose -f docker-compose-consul-cluster.yaml up -d docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES223826237a7b consul:latest \"docker-entrypoint.s…\" 3 seconds ago Up 2 seconds 8300-8302/tcp, 8301-8302/udp, 8600/tcp, 8600/udp, 0.0.0.0:8501-&gt;8500/tcp, :::8501-&gt;8500/tcp consul-client2337a3188fb75 consul:latest \"docker-entrypoint.s…\" 3 seconds ago Up 2 seconds 8300-8302/tcp, 8301-8302/udp, 8600/tcp, 8600/udp, 0.0.0.0:8500-&gt;8500/tcp, :::8500-&gt;8500/tcp consul-client1685440aefca3 consul:latest \"docker-entrypoint.s…\" 4 seconds ago Up 3 seconds 8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp consul-server25f9927f2ecac consul:latest \"docker-entrypoint.s…\" 4 seconds ago Up 3 seconds 8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp consul-server3612b93b95ad9 consul:latest \"docker-entrypoint.s…\" 5 seconds ago Up 4 seconds 8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp consul-server1 # 查看集群状态：docker exec -it consul-server1 consul membersNode Address Status Type Build Protocol DC Partition Segmentconsul-server1 10.10.0.2:8301 alive server 1.13.1 2 dc1 default &lt;all&gt;consul-server2 10.10.0.3:8301 alive server 1.13.1 2 dc1 default &lt;all&gt;consul-server3 10.10.0.4:8301 alive server 1.13.1 2 dc1 default &lt;all&gt;consul-client1 10.10.0.6:8301 alive client 1.13.1 2 dc1 default &lt;default&gt;consul-client2 10.10.0.5:8301 alive client 1.13.1 2 dc1 default &lt;default&gt; 访问consul的web页面http://192.168.18.179:8501/ui/dc1/overview/server-status 此时集群已经搭建完毕 将服务器的exporter注册到consul在consul ui中可以看到以下结果 以上就是将191.168.18.21主机上的cadvisor服务注册到了consu上面，将这类服务的名称统称为 cadvisor-exporter ,因为服务实例id必须唯一，所以这里需要特殊定义一下 exporter type - 主机名 - ip地址 最后就是将两个epxorter都将21这台主机的两个exporter都注册到了consul. 配置promeheus 使其支持consul1234567891011121314151617181920212223242526272829303132..................scrape_configs: - job_name: 'prometheus' scrape_interval: 15s static_configs: - targets: ['localhost:9090'] - job_name: \"cnosul-prometheus\" consul_sd_configs: - server: \"192.168.18.178:8501\" services: [] relabel_configs: # relabe 重写 - source_labels: [__meta_consul_service] # service源标签 regex: \"consul\" # service匹配 action: drop # 执行的动作, 这里不需要consul的metrics，所以drop掉 - source_labels: [__meta_consul_service_metadata_hostname] # 将此标签的值重写为instance target_label: instance action: replace - source_labels: [__meta_consul_service_metadata_group] target_label: group action: replace - source_labels: [__scheme__, __address__, __metrics_path__] regex: \"(http|https)(.*)\" # 两个分组 separator: \"\" target_label: \"endpoint\" replacement: \"${1}://${2}\" # 引用两个分组 action: replace.................. 新增加以上配置以后重新加载配置promtheus服务 curl -I -X POST http://127.0.0.1:9090/-/reload 注意：prometheus开启api热加载，需要加上启动参数： –web.enable-lifecycle consul注册服务/实例的时候就需要添加这两个元标签，再通过标签重新 为我们需要的label，便于后面prometheus/grafana使用 以上，通过配置 prometheus 为 consul_sd_configs，之后能够正确通过服务发现获取到两个服务得metircs url。 服务器众多的情况下，可以那么这里也可以通过脚本的方式去添加，因为consul有以下元标签，那么更容易使得我们的数据可控，例如上面我们在注册服务的时候加了一个 group ，hostname, consul传递到promtheus之后我们可以通过relable的方式去重写, 以上代表的就是这个主机属于哪个分组，后续可以作为grafan面板中，或者报警中的一个关键指标。 如何批量注册cadvisor/node exporter 到 consulconsul 自身提供了API， 就像上面一样做成脚本以后，批量注册服务/实例即可： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103cat &gt; register_exporter_to_consul.py &lt;&lt; \\EOF # -*- coding:utf-8 -*-# date: 2022-09-06# author: guomaoqiu# desc: 批量注册cadvisor-exporter, node-exporter服务到consul import jsonimport requestsimport json consul_url = \"http://192.168.18.179:8501/v1/agent/service/register\" # 注册node-exporterdef register_node_exporter(ip,hostname,group): nodedata = { \"id\": \"node-exporter-{0}-{1}\".format(hostname,ip), # \"id\": hostname, \"name\": \"node-exporter\", \"address\": ip, \"port\": 9100, \"tags\": [], \"meta\": { \"hostname\": hostname, \"group\": group }, \"checks\": [ { \"http\": \"http://{0}:9100/metrics\".format(ip), \"interval\": \"5s\" } ] } print(nodedata) try: r = requests.put(url=consul_url, data=json.dumps(nodedata)) if r.status_code == 200: print(ip, \"Node Exporter 注册成功\") else: print(ip, \"Node Exporter 注册失败\") except Exception as e: print(e) # 注册cadvisor-exporterdef register_container_exporter(ip,hostname,group): container_data = { \"id\": \"cadvisor-exporter-{0}-{1}\".format(hostname,ip), \"name\": \"cadvisor-exporter\", \"address\": ip, \"port\": 9105, \"tags\": [], \"meta\": { \"hostname\": hostname, \"group\": group }, \"checks\": [ { \"http\": \"http://{0}:9105/metrics\".format(ip), \"interval\": \"5s\" } ] } try: r = requests.put(url=consul_url, data=json.dumps(container_data)) print(r.status_code,r.content) if r.status_code == 200: print(ip, \"Container Exporter 注册成功\") else: print(ip, \"Container Exporter 注册失败\") except Exception as e: print(e) def register(): with open(\"server_list.txt\",\"r\") as f: lines = (f.readlines()) for data in lines: ip = (str(data).split(\"\\t\")[1].strip(\"\\n\")) group = (str(data).split(\"\\t\")[2].strip(\"\\n\")) hostname = (str(data).split(\"\\t\")[0]) register_node_exporter(ip=ip,hostname=hostname,group=group) register_container_exporter(ip=ip,hostname=hostname,group=group) register() EOF ### 以上需要提前准备好一个文本文件，每行一条数据里面格式如下： cat server_list.txthostname1 192.168.18.21 IDEhostname2 192.168.18.22 IDEhostname3 192.168.18.31 IDE..................... # 这里是线下环境的一个配置，所以单纯只是将环境作为分组，这里可以自定义，但是不能少了或多了字段数据# 执行脚本：python3 register_exporter_to_consul.py 执行以上脚本以后就可以在consul以及promtheus web页面进行查看检验了： 从上面可以看到已经注册了54个，但是这其实只添加了27台服务器而已，因为每台服务器上面运行了两个exporter,一个是node-exporter,一个是cadvisor-exporter， 他们有各自的metrics URL ,最后再到prometheus检查是否添加即可。 以上可以看到 ，服务数量、exporter数据获取正常。 至此，consul的部署，服务/实例的注册，promtheus的consul_sd_configs服务自动发现 ，数据采集已经完成，后续就是将prometheus 接入到 grafana： 上图中可以看到通过自定义的label进行分组 告警系统接入已经可以看到容器&amp;宿主机的数据已经正常采集并且已经能够在grafan面板上进行展示 报警系统使用的是Alertmanager 搭配 Webhook来实现： alertmanager配置1234567891011121314151617181920212223242526272829303132333435363738394041424344cat &gt; alertmanager/config.yml &lt;&lt; EOFglobal: # 全局配置 resolve_timeout: 10m # 超时时间 默认10minhibit_rules: - source_match: ## 源报警规则 alertname: 'critical' target_match: alertname: 'warning' equal: ['alertname'] # 通过alertname去抑制route: receiver: default-receiver group_wait: 30s group_interval: 5m repeat_interval: 3h group_by: ['alertname'] routes: - receiver: webhook-ide matchers: - group = IDE # 通过prometehus alert rule 查询出来的结果必须包含 group字段，否则该条报警不能发送出来 - receiver: webhook-stage matchers: - group = Stage - receiver: webhook-Pet matchers: - group = Petreceivers:- name: default-receiver # 这里不需要设置，需要精确匹配到每一条告规则 ，但是这里必须要存在，上面强调了必须要有一个默认的receiver- name: webhook-ide webhook_configs: # webhook告警配置 - url: 'http://192.168.18.178:9060/dingtalk/webhook-ide/send' send_resolved: true- name: webhook-stage webhook_configs: # webhook告警配置 - url: 'http://192.168.18.178:9060/dingtalk/webhook-stage/send' send_resolved: true- name: webhook-Pet webhook_configs: # webhook告警配置 - url: 'http://192.168.18.178:9060/dingtalk/webhook-Pet/send' send_resolved: true EOF# 重启服务加载 dingding-webhook配置这里按照环境进行了分组，将不同的告警信息通过环境发送到不同的钉钉群 1234567891011121314151617181920212223242526272829303132333435363738cat &gt; prometheus-dingding-webhook/config.yml &lt;&lt; EOF## Request timeout# timeout: 5s ## Uncomment following line in order to write template from scratch (be careful!)#no_builtin_template: false ## Customizable templates path#templates:# - /config/dingding.tmpltemplates: - /config/template.tmpl## You can also override default template using `default_message`## The following example to use the 'legacy' template from v0.3.0#default_message: # title: '{{ template \"legacy.title\" . }}'# text: '{{ template \"legacy.content\" . }}'## Targets, previously was known as \"profiles\"# text: '{{ template \"_dingtalk.link.content\" . }}'targets: webhook1: url: https://oapi.dingtalk.com/robot/send?access_token=92268cd2c48db0ec2a10a753213dda6a11e4d54ea8fdbce356f217fa44925a7f secret: 填写你的钉钉secret webhook-ide: url: https://oapi.dingtalk.com/robot/send?access_token=92268cd2c48db0ec2a10a753213dda6a11e4d54ea8fdbce356f217fa44925a7f secret: 填写你的钉钉secret webhook-stage: url: https://oapi.dingtalk.com/robot/send?access_token=9d7172785d72f50ab335367bd7db8cb013073f62bd0fd7bfc9605020a6a4b95c secret: 填写你的钉钉secret webhook-Pet: url: https://oapi.dingtalk.com/robot/send?access_token=89b881f5798ec7cdff1538ecf3a7001dd30d19c7e5281e5e8329cb1e243b813e secret: 你的钉钉secret EOF 这里还自定义了报警模板，上述文件中需要引入： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667cat &gt; dingding.tmpl &lt;&lt; EOF{{ define \"__subject\" }}[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}]{{ end }}{{ define \"__alert_list\" }}{{ range . }}---**告警名称**: {{ index .Annotations \"title\" }}**告警级别**: {{ .Labels.severity }}**告警主机组**: {{ .Labels.group }}**告警主机**: {{ .Labels.instance }}**图表数据**: [Click]({{ .GeneratorURL }})**告警信息**: {{ index .Annotations \"description\" }}**告警时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }}**预案链接**: [CONF文档]({{ index .Annotations \"plan_url\" }}){{ end }}{{ end }}{{ define \"__resolved_list\" }}{{ range . }}---**告警名称**: {{ index .Annotations \"title\" }}**告警级别**: {{ .Labels.severity }}**告警主机**: {{ .Labels.instance }}**图表数据**: [Click]({{ .GeneratorURL }})**告警信息**: {{ index .Annotations \"description\" }}**告警时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.StartsAt) \"Asia/Shanghai\" }}**恢复时间**: {{ dateInZone \"2006.01.02 15:04:05\" (.EndsAt) \"Asia/Shanghai\" }}{{ end }}{{ end }}{{ define \"default.title\" }}{{ template \"__subject\" . }}{{ end }}{{ define \"default.content\" }}{{ if gt (len .Alerts.Firing) 0 }}**💔💔💔侦测到{{ .Alerts.Firing | len }}个告警💔💔💔**{{ template \"__alert_list\" .Alerts.Firing }}---{{ end }}{{ if gt (len .Alerts.Resolved) 0 }}**💚💚💚恢复{{ .Alerts.Resolved | len }}个告警💚💚💚**{{ template \"__resolved_list\" .Alerts.Resolved }}{{ end }}{{ end }}{{ define \"ding.link.title\" }}{{ template \"default.title\" . }}{{ end }}{{ define \"ding.link.content\" }}{{ template \"default.content\" . }}{{ end }}{{ template \"default.title\" . }}{{ template \"default.content\" . }}EOF 这里需要注意的是，这个模板中我们加了自定义的一些自己想要的东西： 需要在告警的时候加上针对每个告警的日常处理方式的一个预案，那么这个就给出一个文档的链接就行了，日常运维人员看到告警之后，一般都会进行一个故障处理过程或记录吧，这样能更加友好。那如何定义？首先在告警规则中去定义字段,比如这里我添加了一个 plan_url:然后再模板中引用即可. Prometheus中需要开启 外部url 访问，即在启动的时候加入参数- '--web.external-url=http://192.168.18.178:9090', 否则告警模板中的 .GeneratorURL 这个值应用的是prometheus的主机名，即运行prometheus的那个容器的主机名，那样访问是访问不到的，所以需要修改 另外在部署prometheus 我们还部署了一个容器上是github上有人开源了一个接入alertmanager api 的方式获取报警内容并进行展示。比起alertmanager这个组件自身的那个ui更加好用。出处：(https://github.com/prymitive/karma） 至此整个监控系统就搭建完毕，还是有比较细节的地方需要处理比如告警规则、告警频率这些需要去优化。","categories":[{"name":"monitor","slug":"monitor","permalink":"https://blog.sctux.cc/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://blog.sctux.cc/tags/Prometheus/"},{"name":"Consul","slug":"Consul","permalink":"https://blog.sctux.cc/tags/Consul/"},{"name":"Alertmanager","slug":"Alertmanager","permalink":"https://blog.sctux.cc/tags/Alertmanager/"}],"keywords":[{"name":"monitor","slug":"monitor","permalink":"https://blog.sctux.cc/categories/monitor/"}]},{"title":"基于容器化部署Nacos集群","slug":"基于容器化部署Nacos集群","date":"2022-06-22T14:41:33.000Z","updated":"2025-09-01T01:59:08.862Z","comments":true,"path":"2022/06/22/基于容器化部署Nacos集群/","permalink":"https://blog.sctux.cc/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/","excerpt":"一、搭建架构● 3个或3个以上Nacos节点才能构成集群；● Nacos Nginx Proxy用于代理转发； 二、准备工作Nacos2.0版本相比1.X新增了gRPC的通信方式，因此需要增加2个端口。新增端口是在配置的主端口(server.port)基础上，进行一定偏移量自动生成。 端口 与主端口的偏移量 描述 8848 0 Nacos程序主配置端口 9848 +1000 客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求 9849 +1001 服务端gRPC请求服务端端口，用于服务间同步等 7848 -1000 Nacos 集群通信端口，用于Nacos 集群间进行选举，检测等 使用VIP/nginx请求时，需要配置成TCP转发，不能配置http2转发，否则连接会被nginx断开 按照上述官方的端口分配要求 ，此处部署的使用三台服务器上面创建的Nacos集群端口分配如下： 节点 IP 端口（所需暴露） 备注 版本 当前线下环境部署文件路径 Nacos_1 192168.18.73 宿主机：8858，9858，9859，7858 容器：8858，9858，9859，7858 Nacos 节点一 nacos/nacos-server:2.0.2 /root/nacos-deploy/ Nacos_2 192168.18.74 宿主机：8858，9858，9859，7858 容器：8858，9858，9859，7858 Nacos 节点二 nacos/nacos-server:2.0.2 /root/nacos-deploy/ Nacos_3 192168.18.75 宿主机：8858，9858，9859，7858 容器：8858，9858，9859，7858 Nacos 节点三 nacos/nacos-server:2.0.2 /root/nacos-deploy/ Nacos DB 192168.18.75 宿主机：3306 容器：3306 Nacos所需数据库 mysql:5.7.34 /root/nacos-mysql-deploy Nacos Nginx Proxy 192168.18.75 宿主机：80 容器：80 Nacos代理 nginx:stable-alpine /root/nacos-proxy-deploy 2.1 创建Nacos数据库","text":"一、搭建架构● 3个或3个以上Nacos节点才能构成集群；● Nacos Nginx Proxy用于代理转发； 二、准备工作Nacos2.0版本相比1.X新增了gRPC的通信方式，因此需要增加2个端口。新增端口是在配置的主端口(server.port)基础上，进行一定偏移量自动生成。 端口 与主端口的偏移量 描述 8848 0 Nacos程序主配置端口 9848 +1000 客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求 9849 +1001 服务端gRPC请求服务端端口，用于服务间同步等 7848 -1000 Nacos 集群通信端口，用于Nacos 集群间进行选举，检测等 使用VIP/nginx请求时，需要配置成TCP转发，不能配置http2转发，否则连接会被nginx断开 按照上述官方的端口分配要求 ，此处部署的使用三台服务器上面创建的Nacos集群端口分配如下： 节点 IP 端口（所需暴露） 备注 版本 当前线下环境部署文件路径 Nacos_1 192168.18.73 宿主机：8858，9858，9859，7858 容器：8858，9858，9859，7858 Nacos 节点一 nacos/nacos-server:2.0.2 /root/nacos-deploy/ Nacos_2 192168.18.74 宿主机：8858，9858，9859，7858 容器：8858，9858，9859，7858 Nacos 节点二 nacos/nacos-server:2.0.2 /root/nacos-deploy/ Nacos_3 192168.18.75 宿主机：8858，9858，9859，7858 容器：8858，9858，9859，7858 Nacos 节点三 nacos/nacos-server:2.0.2 /root/nacos-deploy/ Nacos DB 192168.18.75 宿主机：3306 容器：3306 Nacos所需数据库 mysql:5.7.34 /root/nacos-mysql-deploy Nacos Nginx Proxy 192168.18.75 宿主机：80 容器：80 Nacos代理 nginx:stable-alpine /root/nacos-proxy-deploy 2.1 创建Nacos数据库这里使用的是容器化运行nacos，需要创建一个数据库，并从官方对应的版本中去导入基本数据库结构的数据文件 2.1.1 获取nacos 2.0.2 包文件中的基础表结构数据123456789101112131415161718192021222324https://github.com/alibaba/nacos/releases/download/2.0.2/nacos-server-2.0.2.tar.gz# tar -xf nacos-server-2.0.2.tar.gz# cd nacos# tree -L 2.├── bin│ ├── shutdown.cmd│ ├── shutdown.sh│ ├── startup.cmd│ └── startup.sh├── conf│ ├── 1.4.0-ipv6_support-update.sql│ ├── application.properties│ ├── application.properties.example│ ├── cluster.conf.example│ ├── nacos-logback.xml│ ├── nacos-mysql.sql│ └── schema.sql├── LICENSE├── NOTICE└── target └── nacos-server.jar以上是nacos的二进制包文件 ，这里只用到了 nacos-mysql.sql 这个文件 2.1.2 创建mysql服务123456789101112131415161718192021222324252627282930313233# 192.168.18.75sudo -i mkdir /data/mysqlcat &lt;&lt; EOF &gt; /root/nacos-mysql-deploy/mysql.yamlversion: \"3\"services: mysql-db: container_name: nacos-mysql image: mysql:5.7.34 ports: - \"3306:3306\" environment: MYSQL_ROOT_PASSWORD: xxxxxxxxxxxx volumes: - \"/data/mysql:/var/lib/mysql\" restart: alwaysEOFdocker-compose -f /root/nacos-mysql-deploy/mysql.yaml up -d# 导入官方表数据docker cp /root/nacos/conf/nacos-mysql.sql nacos-mysql:/tmpdocker exec -it nacos-mysql shmysql -uroot -pxxxxxxxxxxxxcreate database nacos;use nacos;source /tmp/nacos-mysql.sql; 2.2 Naocs服务2.2.1 创建、编写yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104sudo -imkdir /data/nacos2.0.2_1/logs -pmkdir /root/nacos-deploy/cat &lt;&lt; EOF &gt; /root/nacos-deploy/nacos1.yamlversion: \"3\"services: nacos1: hostname: nacos2.0.2_1 container_name: nacos2.0.2_1 image: nacos/nacos-server:2.0.2 volumes: - /data/nacos2.0.2_1/logs:/home/nacos/logs - /data/nacos2.0.2_1/custom.properties:/home/nacos/init.d/custom.properties ports: - \"8858:8858\" - \"9858:9858\" - \"9859:9859\" - \"7858:7858\" environment: - MODE=cluster - PREFER_HOST_MODE=hostname - NACOS_SERVER_IP=192.168.18.73 - NACOS_APPLICATION_PORT=8858 - NACOS_SERVERS=192.168.18.73:8858 192.168.18.74:8858 192.168.18.75:8858 - SPRING_DATASOURCE_PLATFORM=mysql - MYSQL_SERVICE_HOST=192.168.18.75 - MYSQL_SERVICE_DB_NAME=nacos - MYSQL_SERVICE_PORT=3306 - MYSQL_SERVICE_USER=root - MYSQL_SERVICE_PASSWORD=xxxxxxxxxxx - NACOS_AUTH_ENABLE=true - MYSQL_DATABASE_NUM=1 restart: alwaysEOF######################sudo -imkdir /data/nacos2.0.2_2/logs -pmkdir /root/nacos-deploy/cat &lt;&lt; EOF &gt; /root/nacos-deploy/nacos2.yamlversion: \"3\"services: nacos2: hostname: nacos2.0.2_2 container_name: nacos2.0.2_2 image: nacos/nacos-server:2.0.2 volumes: - /data/nacos2.0.2_2/logs:/home/nacos/logs - /data/nacos2.0.2_2/custom.properties:/home/nacos/init.d/custom.properties ports: - \"8858:8858\" - \"9858:9858\" - \"9859:9859\" - \"7858:7858\" environment: - MODE=cluster - PREFER_HOST_MODE=hostname - NACOS_SERVER_IP=192.168.18.74 - NACOS_APPLICATION_PORT=8858 - NACOS_SERVERS=192.168.18.73:8858 192.168.18.74:8858 192.168.18.75:8858 - SPRING_DATASOURCE_PLATFORM=mysql - MYSQL_SERVICE_HOST=192.168.18.75 - MYSQL_SERVICE_DB_NAME=nacos - MYSQL_SERVICE_PORT=3306 - MYSQL_SERVICE_USER=root - MYSQL_SERVICE_PASSWORD=xxxxxxxxxxx - NACOS_AUTH_ENABLE=true - MYSQL_DATABASE_NUM=1 restart: alwaysEOF######################sudo -imkdir /data/nacos2.0.2_3/logs -pmkdir /root/nacos-deploy/cat &lt;&lt; EOF &gt; /root/nacos-deploy/nacos3.yamlversion: \"3\"services: nacos3: hostname: nacos2.0.2_3 container_name: nacos2.0.2_3 image: nacos/nacos-server:2.0.2 volumes: - /data/nacos2.0.2_3/logs:/home/nacos/logs - /data/nacos2.0.2_3/custom.properties:/home/nacos/init.d/custom.properties ports: - \"8858:8858\" - \"9858:9858\" - \"9859:9859\" - \"7858:7858\" environment: - MODE=cluster - PREFER_HOST_MODE=hostname - NACOS_SERVER_IP=192.168.18.75 - NACOS_APPLICATION_PORT=8858 - NACOS_SERVERS=192.168.18.73:8858 192.168.18.74:8858 192.168.18.75:8858 - SPRING_DATASOURCE_PLATFORM=mysql - MYSQL_SERVICE_HOST=192.168.18.75 - MYSQL_SERVICE_DB_NAME=nacos - MYSQL_SERVICE_PORT=3306 - MYSQL_SERVICE_USER=root - MYSQL_SERVICE_PASSWORD=xxxxxxxxxxx - NACOS_AUTH_ENABLE=true - MYSQL_DATABASE_NUM=1 restart: alwaysEOF 以上 各配置中的端口非标准端口，需要注意环境变量 NACOS_APPLICATION_PORT 如果不指定为非特定端口，那么缺省在配置中的环境变量则为 8848 ，在同一台机器上运行三个节点会出现问题！ 2.2.2 运行Nacos各节点运行即可 12345678# 192.168.18.73docker-compose -f /root/nacos-deploy/nacos1.yaml up -d# 192.168.18.74docker-compose -f /root/nacos-deploy/nacos2.yaml up -d# 192.168.18.75docker-compose -f /root/nacos-deploy/nacos3.yaml up -d 上图 三个节点单独访问页面正常；通过代理访问正常； 注意：web页面并没有使用到客户端，服务端gRPC调用服务，但是在程序中需要请求对应的端口，所以务必需要暴露出来，因为使用了非标准端口，那么在nginx做代理时就直接(伪装)映射成标准端口即可。 2.3 Nginx代理配置此处仍然使用容器化方式运行。配置nginx代理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374mkdir /root/nacos-proxy-deploycd /root/nacos-proxy-deploycat &lt;&lt;EOF &gt; nginx.conf user nginx;worker_processes auto; error_log /var/log/nginx/error.log notice;pid /var/run/nginx.pid; events { worker_connections 65535;} stream { upstream nacos-server-grpc9848 { server 192.168.18.73:9858 max_fails=1 fail_timeout=30s; server 192.168.18.74:9858 max_fails=1 fail_timeout=30s; server 192.168.18.75:9858 max_fails=1 fail_timeout=30s } server { listen 9848; proxy_pass nacos-server-grpc9848; } upstream nacos-server-grpc9849 { server 192.168.18.73:9859 max_fails=1 fail_timeout=30s; server 192.168.18.74:9859 max_fails=1 fail_timeout=30s; server 192.168.18.75:9859 max_fails=1 fail_timeout=30s; } server { listen 9849; proxy_pass nacos-server-grpc9849; }} http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; upstream NACOS { server 192.168.18.73:8858 max_fails=1 fail_timeout=30s; server 192.168.18.74:8858 max_fails=1 fail_timeout=30s; server 192.168.18.75:8858 max_fails=1 fail_timeout=30s; } server { listen 8848 default_server; server_name _; location / { proxy_pass http://NACOS; } } include /etc/nginx/conf.d/*.conf;}EOF 代理容器yaml文件编写 123456789101112131415161718cat &gt; nacos-nginx-proxy.yaml &lt;&lt; EOF version: \"3\"services: nacos_proxy: container_name: nacos_nginx_proxy image: \"nginx:stable-alpine\" ports: - 80: 8848 #这里定义一个80是为了前端访问，或客户端配置定义时无需再加端口，一个ip或者域名即可 - 8848:8848 - 9848:9848 - 9849:9849 volumes: - /root/nacos-proxy-deploy/nginx.conf:/etc/nginx/nginx.conf:ro restart: alwaysEOF# 容器运行docker-compose -f nacos-proxy.yaml up -d 至此搭建完毕，但是容器运行正常不一定服务就是正常，这时需要测试实例服务注册是否正常 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193cat &gt; nacos_check_status.py &lt;&lt; EOF# author: maoqiu.guo# desc: Nacos服务注册功能测试、状态监控# date：2022-06-11 import requestsimport jsonimport timeimport loggingfrom logging.handlers import RotatingFileHandlerimport sys logging.basicConfig(level=logging.DEBUG)# 创建日志记录器，指明日志保存的路径，每个日志文件的最大值，保存的日志文件个数上限log_handle = RotatingFileHandler('./log.txt', maxBytes=1024*1024, backupCount=5)# 创建日志记录的格式formatter = logging.Formatter(\"format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s-%(funcName)s',\")# 为创建的日志记录器设置日志记录格式log_handle.setFormatter(formatter)# 为全局的日志工具对象添加日志记录器logging.getLogger().addHandler(log_handle) # 进度条def progress_bar(timmer): for i in range(1, 101): print(\"\\r\", end=\"\") print(\"Waiting: {}%: \".format(i), \"▋\" * (i // 2), end=\"\") sys.stdout.flush() time.sleep(timmer) class Nacos(): def __init__(self): self.webhookurl=\"https://oapi.dingtalk.com/robot/send?access_token=1968dd11647dfd686c4e1107cf1ad3d0d21c3813a824ee632728327722d9fa82\" self.login_url = \"/nacos/v1/auth/users/login\" self.accessToken=None self.service_url = \"/nacos/v1/ns/catalog/services\" self.instance_url = \"/nacos/v1/ns/instance\" @staticmethod def dingding(self, content): data = { \"msgtype\": \"text\", \"text\": { \"content\": \"Nacos-\" + content, }, } headers = {'Content-Type': 'application/json;charset=utf-8'} response = requests.post(self.webhookurl, data=json.dumps(data), headers=headers) print(response.content) def get_access_token(self, nacos_server): logging.info(\"\\n\\n************** NacosServer: {0} **************\".format(nacos_server)) \"登录获取nacos accessToken\" params= \"username=nacos&amp;password=nacos\" try: r = json.loads(requests.post(params=params, url=\"http://\" + nacos_server + self.login_url, timeout=5).text) accessToken = str((r['accessToken'])) return {\"accessToken\": accessToken, \"result\": True, \"server\": nacos_server, \"msg\": \"Login Nacos Success...[{0}]\".format(nacos_server)} except Exception as e: return {\"accessToken\": None, \"result\": False, \"server\": nacos_server, \"msg\": \"Login Nacos Falied...[{0}] Error: {1}\".format(nacos_server, str(e))} def register_test(self, accessToken, nacos_server): ''' 1. 创建实例 2. 注册服务 3. 删除实例 4. 删除服务 ''' data={ \"serviceName\": \"test_service_instance\", \"namespaceId\": \"public\", \"accessToken\": accessToken, \"ip\": nacos_server.split(\":\")[0], \"port\": nacos_server.split(\":\")[1], \"ephemeral\": False } # 创建服务(注册实例) try: r = (requests.post(params=data, url=\"http://\" + nacos_server + self.instance_url, timeout=5).text) if r == \"ok\": msg = \"[注册实例&amp;创建服务成功] Service: {0} NameSpace: {1} {2} InstanceIP: {3}\".format(data['serviceName'], data['namespaceId'], nacos_server, data['ip']) logging.info(msg) else: msg = \"[注册实例&amp;创建服务失败] Service: {0} NameSpace: {1} {2} {3}\".format(data['serviceName'], data['namespaceId'], nacos_server, r) logging.error(msg) self.dingding(self, content=msg) except Exception as e: msg = \"[注册实例&amp;创建服务失败] Service: {0} NameSpace: {1} {2} {3}\".format(data['serviceName'], data['namespaceId'], nacos_server, r) logging.error(msg) self.dingding(self, content=msg) # 删除实例 def delete_instance_service(self, accessToken, nacos_server): data={ \"serviceName\": \"test_service_instance\", \"namespaceId\": \"public\", \"accessToken\": accessToken, \"ip\": nacos_server.split(\":\")[0], \"port\": nacos_server.split(\":\")[1], \"ephemeral\": False } # 注销实例 try: r = (requests.delete(params=data, url=\"http://\" + nacos_server + self.instance_url, timeout=5).text) if r == \"ok\": msg = \"[注销实例成功] Service: {0} NameSpace: {1} {2} \".format(data['serviceName'], data['namespaceId'], nacos_server) logging.info(msg) else: msg = \"[注销实例失败] Service: {0} NameSpace: {1} {2} {3}\".format(data['serviceName'], data['namespaceId'], nacos_server, r) logging.error(msg) self.dingding(self, content=msg) except Exception as e: msg = \"[注销实例失败] Service: {0} NameSpace: {1} {2} {3}\".format(data['serviceName'], data['namespaceId'], nacos_server, r) logging.error(msg) self.dingding(self, content=msg) progress_bar(timmer=0.08) # 删除服务 try: r = (requests.delete(params=data, url=\"http://\" + nacos_server + \"/nacos/v1/ns/service\", timeout=5).text) if r == \"ok\": msg = \"[删除服务成功] Service: {0} NameSpace: {1} {2} \".format(data['serviceName'], data['namespaceId'], nacos_server) logging.info(msg) else: msg = \"[删除服务失败] Service: {0} NameSpace: {1} {2} {3}\".format(data['serviceName'], data['namespaceId'], nacos_server, r) logging.error(msg) self.dingding(self, content=msg) except Exception as e: msg = \"[删除服务失败] Service: {0} NameSpace: {1} {2} {3}\".format(data['serviceName'], data['namespaceId'], nacos_server, r) logging.error(msg) self.dingding(self, content=msg) progress_bar(timmer=0.1) def get_service_list(self, accessToken, nacos_server): \"请求访问某个环境的某一个服务列表, 检查返回数据是否为空,为空则为不正常\" payload={ \"accessToken\": accessToken, \"pageNo\": 1, \"pageSize\": 10, \"namespaceId\": \"stage\" } try: r = json.loads(requests.get(params=payload, url=\"http://\" + nacos_server + self.service_url).text) if r['count'] == 0: msg = \"[获取注册服务数据失败] Nacos Server {0} 当前Stage服务列表为空!\".format(nacos_server) logging.info(msg) self.dingding(self, content=msg) else: # print(\"Nacos 注册服务数据正常...[{0}]\".format(nacos_server)) logging.info(\"[获取注册服务数据成功] ServiceTotal {1} on {0} - SercieName: stage \".format(nacos_server,r['count'])) except Exception as e: pass def letsgo(self, nacos_server): # 登录获取token login_nacos_res = (self.get_access_token(nacos_server)) if login_nacos_res[\"result\"]: logging.info(\"[登录获取accessToken 成功] Nacos Server {0} \".format((login_nacos_res['server']))) self.get_service_list(login_nacos_res['accessToken'], login_nacos_res['server'] ) # 创建实例&amp;服务 self.register_test(login_nacos_res['accessToken'], login_nacos_res['server'] ) #time.sleep(60 * 2) progress_bar(timmer=0.1) self.delete_instance_service(login_nacos_res['accessToken'], login_nacos_res['server'] ) else: msg = \"[登录获取accessToken 失败] {0} \".format((login_nacos_res['msg'])) logging.info(msg) self.dingding(self, content=msg) if __name__ == \"__main__\": while True: nacos_server_list=[ #\"nacos.axiba.com\", \"192.168.18.75:80\", \"192.168.18.75:8848\", \"192.168.18.73:8858\", \"192.168.18.74:8858\", \"192.168.18.75:8858\", ] for i in nacos_server_list: Nacos().letsgo(i)EOF 运行后： 以上脚本通过Nacos的OpenApi循环式通过从每个节点访问后发起以及通过Nginx代理发起服务实例的查询、注册、删除操作。 登录获取AccessToke(每个请求需要携带) 在 获取stage环境的服务列表数据，这是为了监测返回数据是否正常，因为这里刚搭建起来，如果里面服务数据不正常则会出现上面提示，并发送消息到钉钉； 上图是通过代理访问获取stage中的服务为0，后发送通知，说明此时可能(因为通过负载并不知道该请求是落在了哪一个Nacos节点)集群中某个节点出现故障数据不正常了，但是在检测脚本中会通过每个节点去注册，那么一定会在出现通知某个节点故障。4. 在public环境中通过调用OpenApi方式创建、注册一个实例 test_service_instance 跟服务，然后注销实例，并删除服务，这是为了监测集群是否正常。 三、小记 虽然文档给出的描述是只有当服务下实例数为0时允许删除，但是实际上并没有强制检查校验服务下的实例数是否为0。Nacos的服务创建有多种方式，可以主动创建可以在注册实例的时候创建，可以在实例发送心跳时创建，所以哪怕主动删除了还会自动创建回来。； 所有已注册服务在容器重启后会丢失，但是只要保证集群中有一个节点正常，那么重启后的服务数据会同步其他正常节点的，即需要客户端重新注册； 正常情况在某个节点注册了服务后会同步到其他节点。 各节点无法选举一个leader时 查看日志：/data/nacos2.0.2_1/logs/alipay-jraft.log 实例服务相关日志: /data/nacos2.0.2_1/naming-raft.log 通过日志发现，暴露8848端口是为了nacos客户端登陆获取Token，后续操作都会携带Token进行资源操作；然后暴露9848端口是客户端用于gRPC的请求，所以如果是设置了代理转发，务必将其暴露，否则连接失败失败。 四、吐槽Nacos 官方为了推商业版，社区版几乎毛病百出，不管不顾，文档也是写得一团糟~ 无力吐槽","categories":[{"name":"基础组件","slug":"基础组件","permalink":"https://blog.sctux.cc/categories/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/"},{"name":"部署","slug":"基础组件/部署","permalink":"https://blog.sctux.cc/categories/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"https://blog.sctux.cc/tags/nacos/"}],"keywords":[{"name":"基础组件","slug":"基础组件","permalink":"https://blog.sctux.cc/categories/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/"},{"name":"部署","slug":"基础组件/部署","permalink":"https://blog.sctux.cc/categories/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/%E9%83%A8%E7%BD%B2/"}]},{"title":"运维工作中自动化巡检的必要性以及重要性","slug":"自动化巡检的必要性以及重要性","date":"2022-05-12T02:42:13.000Z","updated":"2025-09-01T01:59:08.876Z","comments":true,"path":"2022/05/12/自动化巡检的必要性以及重要性/","permalink":"https://blog.sctux.cc/2022/05/12/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A1%E6%A3%80%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7%E4%BB%A5%E5%8F%8A%E9%87%8D%E8%A6%81%E6%80%A7/","excerpt":"一、为什么要进行巡检 当前平台架构复杂，中间件繁多，组件之间耦合度高，微服务还未达到故障自愈水平，所以需要通过告警或巡检等手段发现问题来保障平台持续稳定运行。 当前有客户对平台及上层应用使用频率低，例如三四天登录查看一次数据，但是设备是正常运转的，一旦平台出现问题，刚好客户发现问题，运维 才去解决就为时已晚。 定期巡检方案是模拟人工登录各业务页面，而非接口调用，更能真实地发现问题，并通过截图真实保留平台运行状态。 Prometheus平台的监控报警功能还未覆盖到整个业务系统，部分问题还未能实时监控到，导致平台出现异常后而无法感知。 当前并不能保证客户环境的Prometheus平台本身不存在问题，针对这种不确定性，定期巡检是一个保障平台稳定性的方案，实现平台双保障。 部分客户环境不能够连接外网，Prometheus的监控告警信息无法同步到微信、飞书等，但可以通过定期巡检方案来保障平台稳定运行。 由于以上原因，为了保证SLA，必须进行定期巡检。 二、巡检检查项2.1 服务器基础信息 cpu 利用率 磁盘利用率 内存利用率 服务器时间同步 日常数据备份文件检查 2.2 k8s集群状态 证书过期检查 API通信是否正常 各名称空间下的pod运行状态 ceph共享存储是否正常 2.3 业务状态 业务平台登录是否正常 kafka是否积压 kafka消费速率","text":"一、为什么要进行巡检 当前平台架构复杂，中间件繁多，组件之间耦合度高，微服务还未达到故障自愈水平，所以需要通过告警或巡检等手段发现问题来保障平台持续稳定运行。 当前有客户对平台及上层应用使用频率低，例如三四天登录查看一次数据，但是设备是正常运转的，一旦平台出现问题，刚好客户发现问题，运维 才去解决就为时已晚。 定期巡检方案是模拟人工登录各业务页面，而非接口调用，更能真实地发现问题，并通过截图真实保留平台运行状态。 Prometheus平台的监控报警功能还未覆盖到整个业务系统，部分问题还未能实时监控到，导致平台出现异常后而无法感知。 当前并不能保证客户环境的Prometheus平台本身不存在问题，针对这种不确定性，定期巡检是一个保障平台稳定性的方案，实现平台双保障。 部分客户环境不能够连接外网，Prometheus的监控告警信息无法同步到微信、飞书等，但可以通过定期巡检方案来保障平台稳定运行。 由于以上原因，为了保证SLA，必须进行定期巡检。 二、巡检检查项2.1 服务器基础信息 cpu 利用率 磁盘利用率 内存利用率 服务器时间同步 日常数据备份文件检查 2.2 k8s集群状态 证书过期检查 API通信是否正常 各名称空间下的pod运行状态 ceph共享存储是否正常 2.3 业务状态 业务平台登录是否正常 kafka是否积压 kafka消费速率 三、实现方式前期：前期巡检同事登录各客户环境进行人工手动巡检(登录VPN、连接跳板机、登录业务平台、登录grafana平台等等)一些列操作下来，一轮巡检工作大约在2-3hours。 目前：通过自动化的方式(shell+python+web框架flask)实现了，人工在windows跳板机上、Linux服务器中的模拟人工操作连接VPN、登录业务平台、登录grafana平台等一系列操作，定期定时将巡检任务结果发送至企业微信群内；从单人单次巡检的2-3小时，直接提效到了5-10min，极大程度上提高了日常巡检的工作效率。 那么，在客户环境数量达到一定体量时，群消息接收也会造成巡检遗漏的情况，在这种情况下需要一个集中化的平台作为展示，于是将巡检结果发送到群内的同时也会将消息格式化(注：图片是通过Python截图生成后将其转换为base64编码，然后将其他巡检结果内容格式化为json后post到web后端，再在web前端进行展示) Detail: 通过点击后弹出整个巡检过程以及结果信息； Screenshoot: 通过点击后将会弹出base64编码转换为图片的业务平台及grafana截图 最后，巡检人员只需要定期浏览此汇总展示平台即可！ (人工操作是基础，自动化操作才是王道😀 ) 技术点 Python自动化 Selenium爬虫技术(网页内容获取) Windows端UI自动化(用于自动启动VPN程序，并填写账号密码后进行登录操作) Flask前后端开发(平台展示，内容接收入库) 企业微信机器人Webhook集成","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"},{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/tags/Python/"},{"name":"Flask","slug":"Flask","permalink":"https://blog.sctux.cc/tags/Flask/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"如何让添加定时作业任务变得更加优雅","slug":"Flask结合APScheduler实现定时任务框架平台","date":"2019-03-19T06:32:01.000Z","updated":"2025-09-01T01:59:08.973Z","comments":true,"path":"2019/03/19/Flask结合APScheduler实现定时任务框架平台/","permalink":"https://blog.sctux.cc/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/","excerpt":"序 APSCheduler 简介在平常的工作中有些工作都需要定时任务来推动，例如项目中有一个定时刷新排行榜的程序脚本、定时爬出网站的URL程序、定时检测钓鱼网站的程序等等，都涉及到了关于定时任务的问题；虽然这些定时任务在服务器上面都能通过crontab来做；其次想到的是利用time模块的time.sleep()方法使程序休眠来达到定时任务的目的，虽然前两者也可以，但是总觉得不是那么的专业，😁所以就找到了python的定时任务模块APScheduler; APScheduler基于Quartz的一个Python定时任务框架，实现了Quartz的所有功能，使用起来十分方便。提供了基于日期、固定时间间隔以及crontab类型的任务，并且可以持久化任务。基于这些功能，我们可以很方便的实现一个python定时任务系统。 同时APScheduler还提供了Flask的扩展Flask-Apscheduler, 那正好就可以拿来做一个集成定时任务平台; APScheduler有四个组成部分 触发器(trigger)包含调度逻辑，每一个作业有它自己的触发器，用于决定接下来哪一个作业会运行。除了他们自己初始配置意外，触发器完全是无状态的。 1234567891011121314151617181920212223242526272829303132333435363738cron: 类linux下的crontab格式,属于定时调度interval:每隔多久调度一次date:一次性调度#1. cron风格(int|str) 表示参数既可以是int类型，也可以是str类型(datetime | str) 表示参数既可以是datetime类型，也可以是str类型year (int|str) – 4-digit year -（表示四位数的年份，如2008年）month (int|str) – month (1-12) -（表示取值范围为1-12月）day (int|str) – day of the (1-31) -（表示取值范围为1-31日）week (int|str) – ISO week (1-53) -（格里历2006年12月31日可以写成2006年-W52-7（扩展形式）或2006W527（紧凑形式））day_of_week (int|str) – number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun) - （表示一周中的第几天，既可以用0-6表示也可以用其英语缩写表示）hour (int|str) – hour (0-23) - （表示取值范围为0-23时）minute (int|str) – minute (0-59) - （表示取值范围为0-59分）second (int|str) – second (0-59) - （表示取值范围为0-59秒）start_date (datetime|str) – earliest possible date/time to trigger on (inclusive) - （表示开始时间）end_date (datetime|str) – latest possible date/time to trigger on (inclusive) - （表示结束时间）timezone (datetime.tzinfo|str) – time zone to use for the date/time calculations (defaults to scheduler timezone) -（表示时区取值）#如:在6,7,8,11,12月份的第三个星期五的00:00,01:00,02:00,03:00 执行该程序sched.add_job(my_job, 'cron', month='6-8,11-12', day='3rd fri', hour='0-3')#2.interval风格weeks (int) – number of weeks to waitdays (int) – number of days to waithours (int) – number of hours to waitminutes (int) – number of minutes to waitseconds (int) – number of seconds to waitstart_date (datetime|str) – starting point for the interval calculationend_date (datetime|str) – latest possible date/time to trigger ontimezone (datetime.tzinfo|str) – time zone to use for the date/time calculations#如:每隔2分钟执行一次scheduler.add_job(myfunc, 'interval', minutes=2)#3.date风格run_date (datetime|str) – the date/time to run the job at -（任务开始的时间）timezone (datetime.tzinfo|str) – time zone for run_date if it doesn’t have one already#如:在2009年11月6号16时30分5秒时执行sched.add_job(my_job, 'date', run_date=datetime(2009, 11, 6, 16, 30, 5), args=['text']) 作业存储(job store)存储被调度的作业，默认的作业存储是简单地把作业保存在内存中，其他的作业存储是将作业保存在数据库中。一个作业的数据讲在保存在持久化作业存储时被序列化，并在加载时被反序列化。调度器不能分享同一个作业存储。 jobstore则是指的是job持久化,默认job运行在内存中,可持久化在数据库,指定为mongo的MongoDBJobStore或者是使用sqlite的SQLAlchemyJobStore,同时可指定多种jobstore 执行器(executor)处理作业的运行，他们通常通过在作业中提交制定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。 说白了就是指定任务是以线程池/进程池里运行,这在初始化时可以指定,同时可以指定最大的工作池,默认的为default: ThreadPoolExecutor,max-worker为20,当然也可以指定为processpool,默认max-worker为5 调度器(scheduler)是其他的组成部分。你通常在应用只有一个调度器，应用的开发者通常不会直接处理作业存储、调度器和触发器，相反，调度器提供了处理这些的合适的接口。配置作业存储和执行器可以在调度器中完成，例如添加、修改和移除作业。调度器分为以下几种,可根据不同的使用场景选用不同的调度器: 1234567BlockingScheduler: 很明显这是种阻塞型,一般用在没有其它进程运行的场景下BackGroundScheduler: 后台式,也就是单起一个进程/线程运行该任务,不影响主程序ASyncIOScheduler:GeventScheduler:TornadoScheduler:TwistedScheduler:QtScheduler: 本人只使用过BlockingScheduler跟BackGroundScheduler,flask-scheduler使用的即为BackGroundScheduler,其它的后续再研究研究 选择类型也很简单,初始化时直接实例化: 1234from apscheduler.schedulers.background import BackgroundSchedulerscheduler = BackgroundScheduler()#启动scheduler.start() 以上就是apscheduler主要组成的四个方面。那么既然有提供flask-apscheduler这个扩展，那一定是把apscheduler封装过后为我们提供更加方便的集成开发，这里我的存储选择了MySQL作为作业任务存储，因为我这里的平台的用户是用它来存储的；为了方便统一使用了MySQL;","text":"序 APSCheduler 简介在平常的工作中有些工作都需要定时任务来推动，例如项目中有一个定时刷新排行榜的程序脚本、定时爬出网站的URL程序、定时检测钓鱼网站的程序等等，都涉及到了关于定时任务的问题；虽然这些定时任务在服务器上面都能通过crontab来做；其次想到的是利用time模块的time.sleep()方法使程序休眠来达到定时任务的目的，虽然前两者也可以，但是总觉得不是那么的专业，😁所以就找到了python的定时任务模块APScheduler; APScheduler基于Quartz的一个Python定时任务框架，实现了Quartz的所有功能，使用起来十分方便。提供了基于日期、固定时间间隔以及crontab类型的任务，并且可以持久化任务。基于这些功能，我们可以很方便的实现一个python定时任务系统。 同时APScheduler还提供了Flask的扩展Flask-Apscheduler, 那正好就可以拿来做一个集成定时任务平台; APScheduler有四个组成部分 触发器(trigger)包含调度逻辑，每一个作业有它自己的触发器，用于决定接下来哪一个作业会运行。除了他们自己初始配置意外，触发器完全是无状态的。 1234567891011121314151617181920212223242526272829303132333435363738cron: 类linux下的crontab格式,属于定时调度interval:每隔多久调度一次date:一次性调度#1. cron风格(int|str) 表示参数既可以是int类型，也可以是str类型(datetime | str) 表示参数既可以是datetime类型，也可以是str类型year (int|str) – 4-digit year -（表示四位数的年份，如2008年）month (int|str) – month (1-12) -（表示取值范围为1-12月）day (int|str) – day of the (1-31) -（表示取值范围为1-31日）week (int|str) – ISO week (1-53) -（格里历2006年12月31日可以写成2006年-W52-7（扩展形式）或2006W527（紧凑形式））day_of_week (int|str) – number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun) - （表示一周中的第几天，既可以用0-6表示也可以用其英语缩写表示）hour (int|str) – hour (0-23) - （表示取值范围为0-23时）minute (int|str) – minute (0-59) - （表示取值范围为0-59分）second (int|str) – second (0-59) - （表示取值范围为0-59秒）start_date (datetime|str) – earliest possible date/time to trigger on (inclusive) - （表示开始时间）end_date (datetime|str) – latest possible date/time to trigger on (inclusive) - （表示结束时间）timezone (datetime.tzinfo|str) – time zone to use for the date/time calculations (defaults to scheduler timezone) -（表示时区取值）#如:在6,7,8,11,12月份的第三个星期五的00:00,01:00,02:00,03:00 执行该程序sched.add_job(my_job, 'cron', month='6-8,11-12', day='3rd fri', hour='0-3')#2.interval风格weeks (int) – number of weeks to waitdays (int) – number of days to waithours (int) – number of hours to waitminutes (int) – number of minutes to waitseconds (int) – number of seconds to waitstart_date (datetime|str) – starting point for the interval calculationend_date (datetime|str) – latest possible date/time to trigger ontimezone (datetime.tzinfo|str) – time zone to use for the date/time calculations#如:每隔2分钟执行一次scheduler.add_job(myfunc, 'interval', minutes=2)#3.date风格run_date (datetime|str) – the date/time to run the job at -（任务开始的时间）timezone (datetime.tzinfo|str) – time zone for run_date if it doesn’t have one already#如:在2009年11月6号16时30分5秒时执行sched.add_job(my_job, 'date', run_date=datetime(2009, 11, 6, 16, 30, 5), args=['text']) 作业存储(job store)存储被调度的作业，默认的作业存储是简单地把作业保存在内存中，其他的作业存储是将作业保存在数据库中。一个作业的数据讲在保存在持久化作业存储时被序列化，并在加载时被反序列化。调度器不能分享同一个作业存储。 jobstore则是指的是job持久化,默认job运行在内存中,可持久化在数据库,指定为mongo的MongoDBJobStore或者是使用sqlite的SQLAlchemyJobStore,同时可指定多种jobstore 执行器(executor)处理作业的运行，他们通常通过在作业中提交制定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。 说白了就是指定任务是以线程池/进程池里运行,这在初始化时可以指定,同时可以指定最大的工作池,默认的为default: ThreadPoolExecutor,max-worker为20,当然也可以指定为processpool,默认max-worker为5 调度器(scheduler)是其他的组成部分。你通常在应用只有一个调度器，应用的开发者通常不会直接处理作业存储、调度器和触发器，相反，调度器提供了处理这些的合适的接口。配置作业存储和执行器可以在调度器中完成，例如添加、修改和移除作业。调度器分为以下几种,可根据不同的使用场景选用不同的调度器: 1234567BlockingScheduler: 很明显这是种阻塞型,一般用在没有其它进程运行的场景下BackGroundScheduler: 后台式,也就是单起一个进程/线程运行该任务,不影响主程序ASyncIOScheduler:GeventScheduler:TornadoScheduler:TwistedScheduler:QtScheduler: 本人只使用过BlockingScheduler跟BackGroundScheduler,flask-scheduler使用的即为BackGroundScheduler,其它的后续再研究研究 选择类型也很简单,初始化时直接实例化: 1234from apscheduler.schedulers.background import BackgroundSchedulerscheduler = BackgroundScheduler()#启动scheduler.start() 以上就是apscheduler主要组成的四个方面。那么既然有提供flask-apscheduler这个扩展，那一定是把apscheduler封装过后为我们提供更加方便的集成开发，这里我的存储选择了MySQL作为作业任务存储，因为我这里的平台的用户是用它来存储的；为了方便统一使用了MySQL; 以下是这个项目的结构: 123456789101112131415161718192021 /data/study/github_repo/JobCenter $ tree -L 2.├── LICENSE├── Pipfile├── Pipfile.lock├── README.md├── app│&nbsp;&nbsp; ├── __init__.py│&nbsp;&nbsp; ├── auth/ # 平台登录相关│&nbsp;&nbsp; ├── commands.py│&nbsp;&nbsp; ├── config.py # 平台配置相关│&nbsp;&nbsp; ├── decorators.py│&nbsp;&nbsp; ├── email.py # 邮件发送│&nbsp;&nbsp; ├── extensions.py│&nbsp;&nbsp; ├── job/ # 定时任务主要模块│&nbsp;&nbsp; ├── main/ # 主要用于路由│&nbsp;&nbsp; ├── models.py # 数据模型│&nbsp;&nbsp; ├── static │&nbsp;&nbsp; ├── templates└── test.py 目前主要实现的功能有: 可视化界面操作 定时任务统一管理 完全兼容Crontab 支持秒级定时任务 作业任务可搜索、暂停、编辑、删除 作业任务持久化存储、三种不同触发器类型作业动态添加 欢迎star 欢迎提建议~项目地址: https://github.com/guomaoqiu/JobCenterDemo地址: https://jobcenter.sctux.cc","categories":[{"name":"定时任务","slug":"定时任务","permalink":"https://blog.sctux.cc/categories/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"}],"tags":[{"name":"Flask APScheduler","slug":"Flask-APScheduler","permalink":"https://blog.sctux.cc/tags/Flask-APScheduler/"}],"keywords":[{"name":"定时任务","slug":"定时任务","permalink":"https://blog.sctux.cc/categories/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"}]},{"title":"基于Harbor搭建企业级私有镜像仓库","slug":"docker-harbor","date":"2019-01-07T10:23:27.000Z","updated":"2025-09-01T01:59:08.881Z","comments":true,"path":"2019/01/07/docker-harbor/","permalink":"https://blog.sctux.cc/2019/01/07/docker-harbor/","excerpt":"基于harbor搭建企业docker镜像仓库背景docker中要使用镜像，一般会从本地、docker Hup公共仓库和其它第三方公共仓库中下载镜像，一般出于安全和外网(墙)资源下载速率的原因考虑企业级上不会轻易使用。那么有没有一种办法可以存储自己的镜像又有安全认证的仓库呢? —-&gt; 企业级环境中基于Harbor搭建自己的安全认证仓库。 Harbor是VMware公司最近开源的企业级Docker Registry项目, 其目标是帮助用户迅速搭建一个企业级的Docker registry服务。 安装Harborharbor需要安装docker和docker-compose才能使用，安装docker步骤省略， 安装docker-domposedocker-dompose安装步骤如下： 下载最新版的docker-compose文件","text":"基于harbor搭建企业docker镜像仓库背景docker中要使用镜像，一般会从本地、docker Hup公共仓库和其它第三方公共仓库中下载镜像，一般出于安全和外网(墙)资源下载速率的原因考虑企业级上不会轻易使用。那么有没有一种办法可以存储自己的镜像又有安全认证的仓库呢? —-&gt; 企业级环境中基于Harbor搭建自己的安全认证仓库。 Harbor是VMware公司最近开源的企业级Docker Registry项目, 其目标是帮助用户迅速搭建一个企业级的Docker registry服务。 安装Harborharbor需要安装docker和docker-compose才能使用，安装docker步骤省略， 安装docker-domposedocker-dompose安装步骤如下： 下载最新版的docker-compose文件 1$ curl -L https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose 添加可执行权限 1$ chmod +x /usr/local/bin/docker-compose 验证版本 12$ docker-compose -vdocker-compose version 1.23.2, build 1110ad01 获取Harbor软件包1https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-offline-installer-v1.7.1.tgz 解压 1tar -xf harbor-offline-installer-v1.7.1.tgz -C /usr/local/ 编辑配置文件 12345678910$ cd /usr/local/harbor$ vim harbor.cfghostname = reg.for-k8s.com # 本机外网IP或域名，该地址供用户通过UI进行访问，不要使用127.0.0.1ui_url_protocol = https # 用户访问私仓时使用的协议，默认时http，配置成httpsdb_password = root123 # 指定mysql数据库管理员密码harbor_admin_password：Harbor12345 # harbor的管理员账户密码ssl_cert = /data/cert/reg.for-k8s.com.crt # 设置证书文件路径ssl_cert_key = /data/cert/reg.for-k8s.com.key # 设置证书密钥文件路径# 其他配置选项按需填写即可 生成ssl证书生成根证书 123$ cd /dada/cert/$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout ca.key -x509 -days 365 -out ca.crt -subj \"/C=CN/L=Shanghai/O=harbor/CN=harbor-registry\" 生成一个证书签名, 设置访问域名为 reg.for-k8s.com 1$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout reg.for-k8s.com.key -out server.csr -subj \"/C=CN/L=Shanghai/O=harbor/CN=reg.for-k8s.com\" 生成主机证书 1$ openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out reg.for-k8s.com.crt 通过自带脚本一键安装12345678910$ cd /usr/local/harbor/./install.sh..................✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at https://reg.for-k8s.com.For more details, please visit https://github.com/goharbor/harbor . 然后绑定hosts访问即可: 默认账号密码admin / Harbor12345 ok 那上面的私有仓库服务已经搭建完毕了，该怎么使用呢？ 首先在harbor上创建一个项目myproject(我这里不使用默认的libary)这里我选择私有仓库, pull/push都需要在主机上面执行docker login才行; 当我通过Dockerfile构建一个新镜像的时候, 直接指明registry和标签, 比如: 123456789101112131415$ docker build -t reg.for-k8s.com/myproject/mydocker-image:v1.0.1 .Sending build context to Docker daemon 97.21MBStep 1/12 : FROM 1and1internet/ubuntu-16 ---&gt; dbf985f1f449Step 2/12 : MAINTAINER guomaoqiu &lt;guomaoqiu@gmail.com&gt; ---&gt; Using cache ---&gt; 598894333db9............Successfully built b190966f3773Successfully tagged reg.for-k8s.com/myproject/mydocker-image:v1.0.1$ docker images | grep myprojectreg.for-k8s.com/myproject/mydocker-image v1.0.1 b190966f3773 44 seconds ago 482MB 加入当你从别处获取的镜像想上传到私有仓库呢？就是打个tag就行啦, 比如我想把从官网的这个nginx镜像放到我的仓库: 1234$ docker tag nginx reg.for-k8s.com/myproject/mynginx:latest$ docker images | grep myprojectreg.for-k8s.com/myproject/mydocker-image v1.0.1 b190966f3773 2 minutes ago 482MBreg.for-k8s.com/myproject/mynginx latest 568c4670fa80 5 weeks ago 109MB 登录仓库 123456789$ docker login -u admin -p Harbor12345 reg.for-k8s.comUsername: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded 最后把本地的镜像push到仓库当我执行这个的时候报错了： 12docker push reg.for-k8s.com/myproject/mynginx:latestError response from daemon: Get https://reg.for-k8s.com/v2/: x509: certificate signed by unknown authority 解决办法就是如果不在客户端部署证书，那么在Docker启动时设置参数 “–insecure-registry IP/仓库域名”,然后重载服务重启docker进程；注意的是我这里使用的这个域名是自定义的，那么需要在需要上传下载镜像的机器上，同样需要修改docker进程参数，并且绑定hosts,否则即使配置了参数，这个域名没法解析也是push/pull不到镜像的。 再次执行push操作: 12345678910111213141516171819202122$ docker push reg.for-k8s.com/myproject/mynginx:latestThe push refers to repository [reg.for-k8s.com/myproject/mynginx]b7efe781401d: Pushedc9c2a3696080: Pushed7b4e562e58dc: Pushedlatest: digest: sha256:e2847e35d4e0e2d459a7696538cbfea42ea2d3b8a1ee8329ba7e68694950afd3 size: 948$ [root@k8s-m1 kubectl-terminal-ubuntu]# docker push reg.for-k8s.com/myproject/mydocker-image:v1.0.1The push refers to repository [reg.for-k8s.com/myproject/mydocker-image]96dca48ee72c: Pushedfa879b69764c: Pushed4d823b00e6b7: Pushed6bf6e96da4a0: Pushedeedda540c6a8: Pushedf2a971e53afa: Pushed3ee1a3b3fd18: Pushed8a225cfa6dea: Pushed428c1ba11354: Pushedb097f5edab7b: Pushed27712caf4371: Pushed8241afc74c6f: Pushedv1.0.1: digest: sha256:a20629f62d73cff93bf73b31958878a1d76c2dd42e36ebb2cb6d0ac294a46da7 size: 2826 以上push成功； 测试pull那为了测试pull并且能成功运行，我这里通过kuernetes运行一个DaemonSet，镜像采用: mynginx ,并且设置镜像pull策略为Always, 然后创建一个服务在集群内部通过ClusterIP能够访问, yaml如下: 1234567891011121314151617181920212223242526272829303132333435363738394041$ cat &gt;&gt; test.yaml &lt;&lt; EOFapiVersion: v1kind: Servicemetadata: labels: app: mynginx-service name: mynginx-servicespec: ports: - name: 80-80 port: 80 protocol: TCP targetPort: 80 selector: run: mynginx type: ClusterIP---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: run: mynginx name: mynginxspec: selector: matchLabels: run: mynginx template: metadata: labels: run: mynginx spec: containers: - image: reg.for-k8s.com/myproject/mynginx:latest imagePullPolicy: Always name: mynginxEOF$ kubectl apply -f daemonset.yamlservice/mynginx-service createddaemonset.extensions/mynginx create 由于我刚才创建仓库的时候设置的仓库隐私性为私有的需要docker login 登录成功之后，k8s kubectl create 就拉取不了镜像；如果设置为公开，那么久不需要配置这一步骤。只需要docker login 登录成功之后，k8s kubectl create 就可以拉取镜像; 但是我不想让其为公开的；所以还需要配置如下步骤： 配置一个私有仓库harbor的secret： 123kubectl create secret docker-registry registry-secret --namespace=default \\--docker-server=https://reg.for-k8s.com --docker-username=admin \\--docker-password=Harbor12345 部署时指定imagePullSecrets, 修改在上面的yaml中添加这个选项: 12345678910111213141516171819202122232425262728293031323334353637383940414243$ cat &gt;&gt; test.yaml &lt;&lt; EOFapiVersion: v1kind: Servicemetadata: labels: app: mynginx-service name: mynginx-servicespec: ports: - name: 80-80 port: 80 protocol: TCP targetPort: 80 selector: run: mynginx type: ClusterIP---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: run: mynginx name: mynginxspec: selector: matchLabels: run: mynginx template: metadata: labels: run: mynginx spec: containers: - image: reg.for-k8s.com/myproject/mynginx:latest imagePullPolicy: Always name: mynginx imagePullSecrets: - name: registry-secretEOF$ kubectl apply -f daemonset.yamlservice/mynginx-service createddaemonset.extensions/mynginx create","categories":[{"name":"docker","slug":"docker","permalink":"https://blog.sctux.cc/categories/docker/"}],"tags":[{"name":"docker-registry","slug":"docker-registry","permalink":"https://blog.sctux.cc/tags/docker-registry/"}],"keywords":[{"name":"docker","slug":"docker","permalink":"https://blog.sctux.cc/categories/docker/"}]},{"title":"CKA(Certified Kubernetes Adminsitrator)认证获取历程","slug":"road-to-cka","date":"2019-01-07T02:00:40.000Z","updated":"2025-09-01T01:59:08.876Z","comments":true,"path":"2019/01/07/road-to-cka/","permalink":"https://blog.sctux.cc/2019/01/07/road-to-cka/","excerpt":"my experience with cka exam preparation What is CKA?CKA全称就是Certified Kubernetes Adminsitrator，是由CNCF(Cloud Native Computing Foundation 云原生计算基金会)提供的认证项目，考试费用为300美金，必须要双币行用卡，考试过程为3个小时。我办理的是招行的MasterCard，给了20块加急；缴考试费到考完整个流程方面还是非常简单的。 ![](road-to-cka/WechatIMG319.jpeg) 如果考试第一次考试不通过，账号内会生成一个`Free Retake`，在一年之内有一次免费重考的机会。 Why should obtain certification很有幸在春节前(2019/01/06)完成了2018下半年既定的目标，将CKA顺利拿下。很巧合的是在6年前也就是2013年1月5号我通过了RHCE认证考试(虽然我的RHCE认证早已过期，但是我更想说的是: 万变不离其宗 ) 当K8S已经进入到各行各业，那么一套公正权威的管理员测评体系必然就呼之欲出了。也许你会说互联网企业不相信认证，确实一张纸并不能说明什么。这些认证的意义何在，或者怎么才能向别人证明你会用呢？答案自然就是考证了。RHCE也好、CKA也好对我而言其实更多的是一种鞭策，让自己心里有个小目标，同时能够以运维人员的身份去系统的学习、使用他们。 经过我个人的学习，参考过程，我认为这个认证的意义和价值如下： 从考试大纲可以看出考试内容涵盖的知识点很全面，也就是说你需要学习或者了解的知识点也就很多; 对于计划构建自己k8s集群的个人和企业，有CKA在你的团队里能让你更快更稳的开始你的实践。毕竟这帮人是从徒手搭集群开始的。 参加这个考试的时间非常紧张，参加人必须对k8s大部分资源的yaml和命令烂熟于心。你随便找个cka来，让他徒手给你写一个应用的编排出来，从deploy 到pv到service，也许他个别细节和词法可能会写错，大体写出来是一点问题都没有的，期间完全不用参考文档。我要说的是cka的基本技能非常过硬。 截至到现在，我没有见过任何组织和个人提供cka必过手册（也就是传说中的bible）。这个认证诞生也就一、两年的时间， 因此现在通过的人都是货真价实考出来的（相对于其他公司的某cm，某某ca等等），这个价值会体现在证书编号上。再过几年，以我们中国人的考试能力来说，你懂的… 本考试从报名考试到接洽参考，全程英文交流，能考过的人英文水平还算说得过去吧。","text":"my experience with cka exam preparation What is CKA?CKA全称就是Certified Kubernetes Adminsitrator，是由CNCF(Cloud Native Computing Foundation 云原生计算基金会)提供的认证项目，考试费用为300美金，必须要双币行用卡，考试过程为3个小时。我办理的是招行的MasterCard，给了20块加急；缴考试费到考完整个流程方面还是非常简单的。 ![](road-to-cka/WechatIMG319.jpeg) 如果考试第一次考试不通过，账号内会生成一个`Free Retake`，在一年之内有一次免费重考的机会。 Why should obtain certification很有幸在春节前(2019/01/06)完成了2018下半年既定的目标，将CKA顺利拿下。很巧合的是在6年前也就是2013年1月5号我通过了RHCE认证考试(虽然我的RHCE认证早已过期，但是我更想说的是: 万变不离其宗 ) 当K8S已经进入到各行各业，那么一套公正权威的管理员测评体系必然就呼之欲出了。也许你会说互联网企业不相信认证，确实一张纸并不能说明什么。这些认证的意义何在，或者怎么才能向别人证明你会用呢？答案自然就是考证了。RHCE也好、CKA也好对我而言其实更多的是一种鞭策，让自己心里有个小目标，同时能够以运维人员的身份去系统的学习、使用他们。 经过我个人的学习，参考过程，我认为这个认证的意义和价值如下： 从考试大纲可以看出考试内容涵盖的知识点很全面，也就是说你需要学习或者了解的知识点也就很多; 对于计划构建自己k8s集群的个人和企业，有CKA在你的团队里能让你更快更稳的开始你的实践。毕竟这帮人是从徒手搭集群开始的。 参加这个考试的时间非常紧张，参加人必须对k8s大部分资源的yaml和命令烂熟于心。你随便找个cka来，让他徒手给你写一个应用的编排出来，从deploy 到pv到service，也许他个别细节和词法可能会写错，大体写出来是一点问题都没有的，期间完全不用参考文档。我要说的是cka的基本技能非常过硬。 截至到现在，我没有见过任何组织和个人提供cka必过手册（也就是传说中的bible）。这个认证诞生也就一、两年的时间， 因此现在通过的人都是货真价实考出来的（相对于其他公司的某cm，某某ca等等），这个价值会体现在证书编号上。再过几年，以我们中国人的考试能力来说，你懂的… 本考试从报名考试到接洽参考，全程英文交流，能考过的人英文水平还算说得过去吧。 我的学习过程、途径: 其实早在2015年就在接触容器技术，从LXC到Docker再到现在的kubernetes, 日常工作中的也尽量把一些应用做成docker去跑；省下了不少精力时间去做一些重复复杂性的工作；比如以前跑个gitLab的环境，亦或是想要个Python应用运行的环境，除了要把服务器初始化完成之后，还需要一些繁琐的配置，有了docker之后，通过dockerfile/docker-compose就可以很轻松的实现一些应用跑在容器里面了。2018年5月份开始研究学习kubernetes相关东西，虽然学习得不是很系统，但是还是完成了一个小小的项目，感兴趣的点这里 可以作为学习练习的项目;既然学了就要用上；不然也是徒劳。 K8s.io里面tasks相关的东西都需要自己实践下，还是有些日常没怎么用到的,最好把整个文档都看一遍。 宋大神的博客: https://jimmysong.io/kubernetes-handbook/ 学习实操环境做最好是通过二进制的方式来搭建，毕竟如果你使用kubeadm来搭建的话很多的事情它已经帮你完成设定了，也就没理解为什么要这么做。每个组件结合起来之后整个架构在脑子里面就会非常的清楚，排错起来也更加容易一些。考试不难，只要经过大量的实践练习就可以啦~ 注: 这次CKA考试，我没有参加任何国内培训，主要是根据考试大纲、官方文档以及CNCF推出的一套免费课程进行自学。 考前注意事项 考试平台是由CNCF委托的一个计算机方面非常专业的服务商PSI来进行监督考试； 官方认证证件：需要有照片和Latin字母写的全名，我用的是护照; 考试中不能喝水、吃东西，可以申请休息，但是不清楚能不能离开摄像头监视范围内，没请过，反正我全程无尿点。 考试的网页一半是试题、一半是GateOne的终端界面，最好能有一台mac电脑然后外接显示器、键盘，鼠标，mac系统比windows有优势，再一个就是终端就显得不是很小了，操作也顺手; (另外为了提前熟悉使用gateone终端我这里也做了一个镜像，具体食用方法参见README.md) 一个没有其他人的空房间，空桌子，不能带手机，不能有书本，监考官会让你拿着电脑展示你周围的所有环境，我的考试是预约在周末下午在公司一个会议室完成的; 按照官方考试流程指导还需要做一下硬件要求性的测试: 使用Chrome浏览器访问https://www.examslocal.com/ScheduleExam/Home/CompatibilityCheck 选择”Linux Foundation” as the Exam Sponsor and “CKA” as the Exam根据提示安装一个chrome插件（实测有项网速的检查，需要有个稳定的科学上网工具） 180分钟，24道题(涵盖所有k8s基础知识点),平均一道题7.5分钟，时间是非常紧的；一道题做完要去验证，有把握没问题的就果断下一题，千万不要犹豫；打脑壳的那种题记住题号，做完简单的在回过头来整。 考试结束注意事项在考试结束之后，36个小时之内，CNCF就会通过邮件告诉你考试成绩了，如果你的分数大于74%，那么恭喜你通过了！并且附件就直接会有你的通过的证书。如果考试不通过，你的账号上就会直接有一次Free Retake考试的机会。 😢丢了3分，官方也不会告诉你到底错在哪里了；就只有这么一个成绩跟证书发过来。 以上，希望可以帮到你。 ps: _该考试签订了保密协议，So 考试原题无法分享~~~_~~","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/tags/kubernetes/"},{"name":"CKA","slug":"CKA","permalink":"https://blog.sctux.cc/tags/CKA/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"使用Bootstrap Token完成TLS Bootstrapping","slug":"kubernetes-bootstrapping","date":"2018-12-30T10:24:10.000Z","updated":"2025-09-01T01:59:08.973Z","comments":true,"path":"2018/12/30/kubernetes-bootstrapping/","permalink":"https://blog.sctux.cc/2018/12/30/kubernetes-bootstrapping/","excerpt":"一、概述本文主要记录一下搭建一个kubelet搭建到加入集群到手动、自动(证书轮换的)证书认证过程, 那么我的环境是用的v1.12.4版本，有一台master,除了master上面的一些组件已经搭建完毕，主要重点是在kubelet上面，所以现在就是master已经工作正常的情况下，加入新的kubelet(worker节点)需要做一些什么样的操作。然后需要实现的是: 通过bootstrap token以及boostrap.kubeconfig来引导worker节点申请证书 如果第一步申请发出去之后，在手动在master端进行手动批准证书申请，然后发放kubelet证书 实现kubelet证书的自动批准 实现kubelet证书的自动轮换操作 简单理一下TLS Bootstrapping的一个引导过程 master端创建API Server和Kubelet Client通信凭证即生成 apiserver.pem/apiserver-key.pem/apiserver.csr; 在集群内创建特定的 Bootstrap Token Secret，该 Secret 将替代以前的 token.csv 内置用户声明文件; 在集群内创建首次 TLS Bootstrap 申请证书的 ClusterRole 即system:node-bootstrapper,并将上面的bootstrap token secret进行绑定即 clusterrolebinding kubelet-bootstrap，这样通过绑定就能以这个内置用户组信息去发起CSR请求啦； 生成特定的包含 TLS Bootstrapping Token 的 bootstrap.kubeconfig; 调整kubelete启动参数，指定引导文件bootstrap.kubeconfig; 重载配置、重启服务，master端手动批准完成worker节点的CSR请求; 后续如果没有配置证书轮换的话，就会一直使用由controller-manager批准颁发的证书文件了，有效期是一年。(自动批准&amp;证书轮换下面再说) 当然以我的理解要要实现证书轮换那么肯定是没有外界干预的情况下自动执行的，那这个功能也肯定是需要自动批准这个功能的。 那么自动批准需要做的一些事情却是在手动批准的基础上做了一些操作: 创建CSR请求的 TLS Bootstrap 申请证书的renew Kubelet client/server 的 ClusterRole，以及其相关对应的 ClusterRoleBinding；并绑定到对应的组或用户 调整 Controller Manager 配置，以使其能自动签署相关证书和自动清理过期的 TLS Bootstrapping Token 可选的: 集群搭建成功后立即清除 Bootstrap Token Secret，或等待 Controller Manager 待其过期后删除，以防止被恶意利用 二、手动批准2.1 创建API Server和Kubelet Client通信凭证","text":"一、概述本文主要记录一下搭建一个kubelet搭建到加入集群到手动、自动(证书轮换的)证书认证过程, 那么我的环境是用的v1.12.4版本，有一台master,除了master上面的一些组件已经搭建完毕，主要重点是在kubelet上面，所以现在就是master已经工作正常的情况下，加入新的kubelet(worker节点)需要做一些什么样的操作。然后需要实现的是: 通过bootstrap token以及boostrap.kubeconfig来引导worker节点申请证书 如果第一步申请发出去之后，在手动在master端进行手动批准证书申请，然后发放kubelet证书 实现kubelet证书的自动批准 实现kubelet证书的自动轮换操作 简单理一下TLS Bootstrapping的一个引导过程 master端创建API Server和Kubelet Client通信凭证即生成 apiserver.pem/apiserver-key.pem/apiserver.csr; 在集群内创建特定的 Bootstrap Token Secret，该 Secret 将替代以前的 token.csv 内置用户声明文件; 在集群内创建首次 TLS Bootstrap 申请证书的 ClusterRole 即system:node-bootstrapper,并将上面的bootstrap token secret进行绑定即 clusterrolebinding kubelet-bootstrap，这样通过绑定就能以这个内置用户组信息去发起CSR请求啦； 生成特定的包含 TLS Bootstrapping Token 的 bootstrap.kubeconfig; 调整kubelete启动参数，指定引导文件bootstrap.kubeconfig; 重载配置、重启服务，master端手动批准完成worker节点的CSR请求; 后续如果没有配置证书轮换的话，就会一直使用由controller-manager批准颁发的证书文件了，有效期是一年。(自动批准&amp;证书轮换下面再说) 当然以我的理解要要实现证书轮换那么肯定是没有外界干预的情况下自动执行的，那这个功能也肯定是需要自动批准这个功能的。 那么自动批准需要做的一些事情却是在手动批准的基础上做了一些操作: 创建CSR请求的 TLS Bootstrap 申请证书的renew Kubelet client/server 的 ClusterRole，以及其相关对应的 ClusterRoleBinding；并绑定到对应的组或用户 调整 Controller Manager 配置，以使其能自动签署相关证书和自动清理过期的 TLS Bootstrapping Token 可选的: 集群搭建成功后立即清除 Bootstrap Token Secret，或等待 Controller Manager 待其过期后删除，以防止被恶意利用 二、手动批准2.1 创建API Server和Kubelet Client通信凭证我这里已经有ca根证书文件了，那就从创建apiserver证书开始 12345678910111213141516171819202122232425262728$ cat &gt;&gt; apiserver-csr.json &lt;&lt; EOF{ \"CN\": \"kube-apiserver\", \"hosts\": [ \"10.96.0.1\", \"127.0.0.1\", \"192.168.56.201\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"Hangzhou\", \"L\": \"Hangzhou\", \"O\": \"Kubernetes\", \"OU\": \"Kubernetes-manual\" } ]}EOF 我这里就单台master,所以就写了个这一个IP地址 12345678910# 执行创建:$cfssl gencert \\ -ca=/etc/kubernetes/pki/ca.pem \\ -ca-key=/etc/kubernetes/pki/ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ apiserver-csr.json | cfssljson -bare ${PKI_DIR}/apiserverls /etc/kubernetes/pki/apiserver*.pem/etc/kubernetes/pki/apiserver-key.pem /etc/kubernetes/pki/apiserver.pem 2.2 创建引导配置文件 bootstrap-kubelet.kubeconfig2.2.1 生成bootsrap—token123export TOKEN_ID=$(openssl rand 3 -hex)export TOKEN_SECRET=$(openssl rand 8 -hex)export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET} 关于这个token的格式要求参见这里 2.2.2 创建boostrap token secret123456789101112131415161718192021222324252627282930313233343536cat &gt;&gt; bootstrap-token-Secret.yml &lt;&lt; EOFapiVersion: v1kind: Secretmetadata: name: bootstrap-token-{TOKEN_ID} namespace: kube-systemtype: bootstrap.kubernetes.io/tokenstringData: token-id: {TOKEN_ID} token-secret: {TOKEN_SECRET} usage-bootstrap-authentication: \"true\" usage-bootstrap-signing: \"true\" auth-extra-groups: system:bootstrappers:default-node-tokenEOF# 将以上变量赋值进去sed -ri \"s#\\{TOKEN_ID\\}#${TOKEN_ID}#g\" bootstrap-token-Secret.ymlsed -ri \"/token-id/s#\\S+\\$#'&amp;'#\" resources/bootstrap-token-Secret.ymlsed -ri \"s#\\{TOKEN_SECRET\\}#${TOKEN_SECRET}#g\" resources/bootstrap-token-Secret.ymlkubectl create -f resources/bootstrap-token-Secret.yml# 查看详情，一定要放到kube-system这个namespace$ kubectl describe secret bootstrap-token-b8cf79 -n kube-systemName: bootstrap-token-b8cf79Namespace: kube-systemLabels: &lt;none&gt;Annotations:Type: bootstrap.kubernetes.io/tokenData====auth-extra-groups: 39 bytestoken-id: 6 bytestoken-secret: 16 bytesusage-bootstrap-authentication: 4 bytesusage-bootstrap-signing: 4 bytes 2.2.2 创建启动引导配置123456789101112131415161718192021# bootstrap set cluster$ kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.pem \\ --embed-certs=true \\ --server=192.168.56.202 \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig# bootstrap set credentials$ kubectl config set-credentials tls-bootstrap-token-user \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=/etc/kubernetes//bootstrap-kubelet.kubeconfig# bootstrap set context$ kubectl config set-context tls-bootstrap-token-user@kubernetes \\ --cluster=kubernetes \\ --user=tls-bootstrap-token-user \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig# bootstrap use default context$ kubectl config use-context tls-bootstrap-token-user@kubernetes \\ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig 该文件生成之后需要拷贝至worker节点上面去，并且修改配置，指定该引导配置文件这个流程在官方文档中也有描述:bootstrap初始化过程 2.2.3 调整worker节点kubelet启动配置文件加上这个引导文件(参数)123456789101112[Service]ExecStart=/usr/local/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/opt/cni/bin \\ --logtostderr=false \\ --log-dir=/etc/kubernetes/log \\ --config=/etc/kubernetes/kubelet-conf.yml \\ --node-labels=node-role.kubernetes.io/master='' \\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \\ --v=2 此时如果启动kubelet服务的话在apiserver将会报错： 这是因为在默认情况下，kubelet 通过 bootstrap.kubeconfig 中的预设用户 Token 声明了自己的身份，然后创建 CSR 请求；但是不要忘记这个用户在我们不处理的情况下他没任何权限的，包括创建 CSR 请求；所以需要如下命令创建一个 ClusterRoleBinding，将预设用户 kubelet-bootstrap 与内置的 ClusterRole system:node-bootstrapper 绑定到一起，使其能够发起 CSR 请求 1234567891011121314cat &gt;&gt; kubelet-bootstrap-clusterrolebinding.yaml &lt;&lt; EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubelet-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers:default-node-tokenEOF 将上面创建后就可以看到我们通过手动批准之后worker节点就加入到集群当中来了而且在worker节点的/var/lib/kubelet/pki目录下已经获得controller-manager颁发下来的证书,并且还分发了一个kubelet.kubeconfig的文件，这个文件虽然配置中指定了，但这个也是由controller-manager分发给worker节点的。 123456[root@k8s-n0 kubernetes]# ll /var/lib/kubelet/pki/total 12-rw------- 1 root root 1293 Dec 30 10:14 kubelet-client-2018-12-30-10-14-50.pemlrwxrwxrwx 1 root root 59 Dec 30 10:14 kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2018-12-30-10-14-50.pem-rw-r--r-- 1 root root 2153 Dec 30 10:14 kubelet.crt-rw------- 1 root root 1679 Dec 30 10:14 kubelet.key 还有个问题就是，集群已经完成，在我执行exec想进入一个容器的时候却出现了forbidden的问题: 12[root@k8s-m1 resources]# kubectl exec -it kubectl-terminal-ubuntu /bin/basherror: unable to upgrade connection: Forbidden (user=kube-apiserver, verb=create, resource=nodes, subresource=proxy) 这是因为kube-apiserveruser并没有nodes的资源存取权限,属于正常。由于 API 权限,故需要建立一个 RBAC Role 来获取存取权限; 1234567891011121314151617181920212223242526272829303132333435cat &gt;&gt; apiserver-to-kubelet-rbac.yml &lt;&lt; EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\"---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: \"\"roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kube-apiserverEOF 创建好之后就可以exec啦~ 以上，TLS Bootstrapping的手动批准已经完成。 三、自动批准上面已经完成了手动批准的操作，那接下来就是实现以下自动批准以及证书轮换的实现 上面需要的配置其实都差不多了，只需要再做一些配置即可完成； kubelet 所发起的 CSR 请求是由 controller manager 签署的；如果想要是实现自动续期，就需要让 controller manager 能够在 kubelet 发起证书请求的时候自动帮助其签署证书；那么 controller manager 不可能对所有的 CSR 证书申请都自动签署，这时候就需要配置 RBAC 规则，保证 controller manager 只对 kubelet 发起的特定 CSR 请求自动批准即可；在 TLS bootstrapping 官方文档中，CSR有三种请求类型: nodeclient: kubelet 以 O=system:nodes 和 CN=system:node:(node name) 形式发起的 CSR 请求 selfnodeclient: kubelet client renew 自己的证书发起的 CSR 请求(与上一个证书就有相同的 O 和 CN) selfnodeserver: kubelet server renew 自己的证书发起的 CSR 请求 通俗点讲就是:nodeclient 类型的 CSR 仅在第一次启动时会产生，selfnodeclient 类型的 CSR 请求实际上就是 kubelet renew 自己作为 client 跟 apiserver 通讯时使用的证书产生的，selfnodeserver 类型的 CSR 请求则是 kubelet 首次申请或后续 renew 自己的 10250 api 端口证书时产生的 那么针对以上3种CSR请求分别给出了3种对应的 ClusterRole，如下所示 1234567891011121314151617181920212223242526272829303132# A ClusterRole which instructs the CSR approver to approve a user requesting# node client credentials.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:nodeclientrules:- apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/nodeclient\"] verbs: [\"create\"]---# A ClusterRole which instructs the CSR approver to approve a node renewing its# own client credentials.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclientrules:- apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeclient\"] verbs: [\"create\"]---# A ClusterRole which instructs the CSR approver to approve a node requesting a# serving cert matching its client cert.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"] 其中selfnodeclient,nodeclient 已经是集群内置的了; 1234567$ kubectl get clusterrole............system:certificates.k8s.io:certificatesigningrequests:nodeclient 28hsystem:certificates.k8s.io:certificatesigningrequests:selfnodeclient 28h............ 然后是三个 ClusterRole 对应的 三个ClusterRoleBinding；需要注意的是 在使用 Bootstrap Token 进行引导时，Kubelet 组件使用 Token 发起的请求其用户名为system:bootstrap:&lt;token id&gt;，用户组为 system:bootstrappers；所以 我们在创建 ClusterRoleBinding 时要绑定到这个用户或者组上；这里我选择全部绑定到组上： 12345678910111213cat &gt;&gt; kubelet-bootstrap-rbac.yml &lt;&lt; EOF# 允许 system:bootstrappers 组用户创建 CSR 请求# (这一步我在上面已经做过啦)kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes 即: kubelet worker节点证书手动或自动批准需要用到的角色以及角色绑定有: 1234567891011121314151617181920212223# ClusterRole$ kubectl get clusterrole............system:certificates.k8s.io:certificatesigningrequests:nodeclient 2dsystem:certificates.k8s.io:certificatesigningrequests:selfnodeclient 2dsystem:certificates.k8s.io:certificatesigningrequests:selfnodeserver 5msystem:node-bootstrapper 2dsystem:kube-apiserver-to-kubelet 17h............# ClusterRoleBinding$ kubectl get clusterrolebinding | grep auto............node-client-auto-approve-csr 16hnode-client-auto-renew-crt 16hnode-server-auto-renew-crt 16hsystem:kube-apiserver 17hkubelet-bootstrap 18h............ 创建好对应的角色及角色绑定之后就可以修改这个证书的有效期啦；只要在Description=Kubernetes Controller Manager 服务的启动配置参数中加入: 123--experimental-cluster-signing-duration=10m0s \\--feature-gates=RotateKubeletClientCertificate=true \\--feature-gates=RotateKubeletServerCertificate=true 以及kubelet服务启动配置参数中加入: 12--feature-gates=RotateKubeletClientCertificate=true \\--feature-gates=RotateKubeletServerCertificate=true 通过文档了解到--feature-gates 这个是kubernetes中特有的一些功能，有些功能是处于开发版本的，具体可以参看这里 以下就是kubelet运行一段时间后，通过在controller-manager设置的证书有效期快过期的时候通过自动申请并自动批准的结果；可以看出来node-csr-ysGrNbyioCNnj3UUFiQGn5WwLGF1P6wJ4DTIib1IcqQ 这个CSR是第一次启动时的用户system:bootstrap:fd2f4f的证书请求。 并且在证书轮换的过程也可以通过日志发现 1Dec 31 04:10:06 k8s-n0 kubelet: I1231 17:10:06.753064 18294 transport.go:132] certificate rotation detected, shutting down client connections to start using new credentials 那么以上就是kubelet证书的整个批准以及证书轮换的过程。 四、TLS Bootstrapping总结流程总结 kubelet 首次启动通过加载 bootstrap.kubeconfig 中的用户 Token 和 apiserver CA 证书发起首次 CSR 请求，这个 Token 被预先内置在 apiserver 节点的 token.csv(新版本中已经推荐使用Bootstrap Token Secert) 中，其身份为 kubelet-bootstrap 用户和 system:bootstrappers 用户组；想要首次 CSR 请求能成功(成功指的是不会被 apiserver 401 拒绝)，则需要先将 kubelet-bootstrap 用户和 system:node-bootstrapper 内置 ClusterRole 绑定； 对于首次 CSR 请求可以手动签发，也可以将 system:bootstrappers 用户组与 approve-node-client-csr ClusterRole 绑定实现自动签发(1.8 之前这个 ClusterRole 需要手动创建，1.8 后 apiserver 自动创建，并更名为 system:certificates.k8s.io:certificatesigningrequests:nodeclient) 默认签署的的证书只有 1 年有效期，如果想要调整证书有效期可以通过设置 kube-controller-manager 的 --experimental-cluster-signing-duration 参数实现，该参数默认值为 8760h0m0s 对于证书轮换，需要通过协调两个方面实现；第一，想要 kubelet 在证书到期后自动发起续期请求，则需要在 kubelet 启动时增加 --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true 来实现；第二，想要让 controller manager 自动批准续签的 CSR 请求需要在 controller manager 启动时增加 --feature-gates=RotateKubeletServerCertificate=true 参数，并绑定对应的 RBAC 规则","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/tags/kubernetes/"},{"name":"TLS bootstrapping","slug":"TLS-bootstrapping","permalink":"https://blog.sctux.cc/tags/TLS-bootstrapping/"},{"name":"证书手动签发","slug":"证书手动签发","permalink":"https://blog.sctux.cc/tags/%E8%AF%81%E4%B9%A6%E6%89%8B%E5%8A%A8%E7%AD%BE%E5%8F%91/"},{"name":"证书自动签发","slug":"证书自动签发","permalink":"https://blog.sctux.cc/tags/%E8%AF%81%E4%B9%A6%E8%87%AA%E5%8A%A8%E7%AD%BE%E5%8F%91/"},{"name":"证书轮换(自动续期)","slug":"证书轮换-自动续期","permalink":"https://blog.sctux.cc/tags/%E8%AF%81%E4%B9%A6%E8%BD%AE%E6%8D%A2-%E8%87%AA%E5%8A%A8%E7%BB%AD%E6%9C%9F/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"理解Kubernetes安全的持久化保存键值-Etcd","slug":"kubernetes-etcd-secret","date":"2018-12-23T06:47:38.000Z","updated":"2025-09-01T01:59:08.942Z","comments":true,"path":"2018/12/23/kubernetes-etcd-secret/","permalink":"https://blog.sctux.cc/2018/12/23/kubernetes-etcd-secret/","excerpt":"概述etcd是CoreOS团队于2013年6月发起的开源项目，它的目标是构建一个高可用的分布式键值(key-value)数据库。etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。 etcd作为服务发现系统，有以下的特点： 简单：安装配置简单，而且提供了HTTP API进行交互，使用也很简单安全：支持SSL证书验证快速：根据官方提供的benchmark数据，单实例支持每秒2k+读操作可靠：采用raft算法，实现分布式系统数据的可用性和一致性 etcd项目地址：https://github.com/coreos/etcd/ kubernetes中的使用目前etcd是作为kubernetes集群当中的存储后端 在kuernetes中etcd涉及到的安全相关的主要有: etcd支持备份恢复机制，防止数据被误删导致数据丢失 用户的敏感信息建议放在secret类型的资源中，该类型资源是加密存储在etcd中的 etcd支持https, kube-apiserver访问etcd使用https协议","text":"概述etcd是CoreOS团队于2013年6月发起的开源项目，它的目标是构建一个高可用的分布式键值(key-value)数据库。etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。 etcd作为服务发现系统，有以下的特点： 简单：安装配置简单，而且提供了HTTP API进行交互，使用也很简单安全：支持SSL证书验证快速：根据官方提供的benchmark数据，单实例支持每秒2k+读操作可靠：采用raft算法，实现分布式系统数据的可用性和一致性 etcd项目地址：https://github.com/coreos/etcd/ kubernetes中的使用目前etcd是作为kubernetes集群当中的存储后端 在kuernetes中etcd涉及到的安全相关的主要有: etcd支持备份恢复机制，防止数据被误删导致数据丢失 用户的敏感信息建议放在secret类型的资源中，该类型资源是加密存储在etcd中的 etcd支持https, kube-apiserver访问etcd使用https协议 在kubernetes中的配置: Client -&gt; Server123456789101112client-transport-security: # 通道以TLS协议加密 ca-file: '/etc/etcd/ssl/etcd-ca.pem' cert-file: '/etc/etcd/ssl/etcd.pem' key-file: '/etc/etcd/ssl/etcd-key.pem' # 服务端会认证客户端证书是否受信任CA签发 client-cert-auth: true trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # 是否系统自动生成证书 auto-tls: true Server -&gt; Server123456789101112peer-transport-security: # 通道以TLS协议加密 ca-file: '/etc/etcd/ssl/etcd-ca.pem' cert-file: '/etc/etcd/ssl/etcd.pem' key-file: '/etc/etcd/ssl/etcd-key.pem' # 服务端会认证客户端证书是否受信任CA签发 peer-client-cert-auth: true trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # 是否系统自动生成证书 auto-tls: true etcd的备份对于 API 3 备份与恢复方法官方 v3 admin guide在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。在命令行设置： 1$ export ETCDCTL_API=3 备份数据： 12$ etcdctl --endpoints=[localhost:2379] snapshot save snapshot.db 恢复： 1$ etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data 恢复后的文件需要修改权限为 etcd:etcd–name:重新指定一个数据目录，可以不指定，默认为 default.etcd–data-dir：指定数据目录建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/tags/kubernetes/"},{"name":"etcd cluster","slug":"etcd-cluster","permalink":"https://blog.sctux.cc/tags/etcd-cluster/"},{"name":"etcd secret","slug":"etcd-secret","permalink":"https://blog.sctux.cc/tags/etcd-secret/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"理解kubernetes中的静态Pod","slug":"kubernetes-static-pod","date":"2018-12-22T03:02:30.000Z","updated":"2025-09-01T01:59:08.872Z","comments":true,"path":"2018/12/22/kubernetes-static-pod/","permalink":"https://blog.sctux.cc/2018/12/22/kubernetes-static-pod/","excerpt":"概述今儿是冬至，尽管如此，学习的脚步还是不能停下，今天学习实践一下kubernetes中的静态pod是什么？ 我们知道在前面Pod的声明周期管理都是通过像DaemonSet、StatefulSet、Deployment这种方式创建管理的，而官方文档介绍了一种特殊的pod就是静态Pod， 什么是静态Pod静态Pod是由kubelet进行管理，仅存在于特定Node上的Pod。它们不能通过API Server进行管理，无法与ReplicationController、Deployment或DaemonSet进行关联，并且kubelet也无法对其健康检查。 静态Pod的创建:静态pod可以通过两种方式创建：使用配置文件或HTTP。 通过配置文件创建配置文件只是特定目录中json或yaml格式的标准pod定义。他通过在kubelet守护进程中添加配置参数--pod-manifest-path=&lt;the directory&gt; 来运行静态Pod，kubelet经常会它定期扫描目录； 例如，如何将一个简单web服务作为静态pod启动","text":"概述今儿是冬至，尽管如此，学习的脚步还是不能停下，今天学习实践一下kubernetes中的静态pod是什么？ 我们知道在前面Pod的声明周期管理都是通过像DaemonSet、StatefulSet、Deployment这种方式创建管理的，而官方文档介绍了一种特殊的pod就是静态Pod， 什么是静态Pod静态Pod是由kubelet进行管理，仅存在于特定Node上的Pod。它们不能通过API Server进行管理，无法与ReplicationController、Deployment或DaemonSet进行关联，并且kubelet也无法对其健康检查。 静态Pod的创建:静态pod可以通过两种方式创建：使用配置文件或HTTP。 通过配置文件创建配置文件只是特定目录中json或yaml格式的标准pod定义。他通过在kubelet守护进程中添加配置参数--pod-manifest-path=&lt;the directory&gt; 来运行静态Pod，kubelet经常会它定期扫描目录； 例如，如何将一个简单web服务作为静态pod启动 选择运行静态pod的节点服务器不一定是node节点，只要有kubelet进程所在的节点都可以运行静态pod 我在某个节点上创建一个放置一个Web服务器pod定义的描述文件文件夹，例如/etc/kubelet.d/static-web.yaml 1234567891011121314151617181920$ mkdir /etc/kubelet.d/$ cat &lt;&lt;EOF &gt;/etc/kubelet.d/static-web.yamlapiVersion: v1kind: Podmetadata: name: static-web labels: role: myrolespec: containers: - name: web image: nginx ports: - name: web containerPort: 80 protocol: TCPEOF$ ls /etc/kubelet.d/static-web.yaml 通过使用–pod-manifest-path=/etc/kubelet.d/参数运行它，在节点上配置我的kubelet守护程序以使用此目录。比如我这里kubelet启动参数位于/etc/systemd/system/kubelet.service.d/10-kubelet.conf, 修改配置，然后将参数加入到现有参数配置项中(安装方式不尽相同，但是道理一样) 123456$ vim /etc/systemd/system/kubelet.service.d/10-kubelet.conf············Environment=\"KUBELET_EXTRA_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local --pod-manifest-path=/etc/kubelet.d/\"············ 保存退出，reload一下systemd daeomon ,重启kubelet服务进程 12$ systemctl daemon-reload$ systemctl restart kubelet 前面说啦，当kubelet启动时，它会自动启动在指定的目录–pod-manifest-path=或–manifest-url=参数中定义的所有pod ，即我们的static-web。再该节点上检查是否创建成功： 123$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE static-web-k8s-m1 1/1 Running 0 2m 10.244.2.32 k8s-m1 上面也提到了，他不归任何部署方式来管理，即使我们尝试kubelet命令去删除 12345$ kubectl delete pod static-web-k8s-m1pod \"static-web-k8s-m1\" deleted$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEstatic-web-k8s-m1 0/1 Pending 0 2s &lt;none&gt; k8s-m1 &lt;none&gt; 可以看出静态pod通过这种方式是没法删除的 那我如何去删除或者说是动态的添加一个pod呢？这种机制已经知道，kubelet进程会定期扫描配置的目录（/etc/kubelet.d在我的示例）以进行更改，并在文件出现/消失在此目录中时添加/删除pod。 通过url来创建:例如，我在这个url放置了一个pod创建的描述文件 那么上面提到了如何修改？通过url来创建的？就是kubelet启动配置参数中配置--manifest-url指向即可 ok 以上就是如何创建静态Pod的具体食用方法，那什么场景需要用到它，这个我个人觉得还是因人或因环境而已，毕竟kubernetes开发出了这个功能，就说明有用武之地的。 同时在kubelet-conf.yml可以指定直接指定这个StaticPod的描述文件存放目录: 123456$ cat kubelet-conf.yml............staticPodPath: /etc/kubernetes/manifests............ 但是还是需要在启动参数中配置 --pod-manifest-path","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/tags/kubernetes/"},{"name":"Static Pod","slug":"Static-Pod","permalink":"https://blog.sctux.cc/tags/Static-Pod/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"理解kubernetes中的Secret","slug":"kubernetes-secret","date":"2018-12-20T14:50:30.000Z","updated":"2025-09-01T01:59:08.921Z","comments":true,"path":"2018/12/20/kubernetes-secret/","permalink":"https://blog.sctux.cc/2018/12/20/kubernetes-secret/","excerpt":"概述Secret对象与ConfigMap对象类似，但它主要用于存储以下敏感信息，例如密码，OAuth token和SSH key等等。将这些信息存储在secret中，和直接存储在Pod的定义中，或Docker镜像定义中相比，更加安全和灵活。 kuberntes中内置了三种secret类型: Opaque：使用base64编码存储信息，可以通过base64 –decode解码获得原始数据，因此安全性弱。 kubernetes.io/dockerconfigjson：用于存储docker registry的认证信息。 kubernetes.io/service-account-token：用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中。(前面博文记录过实践后的食用方法) Opaque SecretOpaque类型的Secret，其value为base64编码后的值。 创建方式从文件中创建Secret12$ echo -n \"admin\" &gt; ./username.txt$ echo -n \"1f2d1e2e67df\" &gt; ./password.txt 使用kubectl create secret命令创建secret：","text":"概述Secret对象与ConfigMap对象类似，但它主要用于存储以下敏感信息，例如密码，OAuth token和SSH key等等。将这些信息存储在secret中，和直接存储在Pod的定义中，或Docker镜像定义中相比，更加安全和灵活。 kuberntes中内置了三种secret类型: Opaque：使用base64编码存储信息，可以通过base64 –decode解码获得原始数据，因此安全性弱。 kubernetes.io/dockerconfigjson：用于存储docker registry的认证信息。 kubernetes.io/service-account-token：用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中。(前面博文记录过实践后的食用方法) Opaque SecretOpaque类型的Secret，其value为base64编码后的值。 创建方式从文件中创建Secret12$ echo -n \"admin\" &gt; ./username.txt$ echo -n \"1f2d1e2e67df\" &gt; ./password.txt 使用kubectl create secret命令创建secret： 12345678910111213141516171819# 创建$ kubectl create secret generic db-user-passwd --from-file=./username.txt --from-file=./password.txtsecret \"db-user-passwd\" created# 查看创建结果$ kubectl get secrets db-user-passwdapiVersion: v1data: password.txt: MWYyZDFlMmU2N2Rm username.txt: YWRtaW4=kind: Secretmetadata: creationTimestamp: 2018-12-21T08:58:33Z name: db-user-passwd namespace: default resourceVersion: \"57310\" selfLink: /api/v1/namespaces/default/secrets/db-user-passwd uid: 98488947-04fe-11e9-97cd-00505621dd5btype: Opaque 使用描述文件创建Secret首先使用base64对数据进行编码： 1234$ echo -n \"admin\" | base64YWRtaW4=$ echo -n \"1f2d1e2e67df\" | base64MWYyZDFlMmU2N2Rm 创建一个类型为Secret的描述文件： 123456789101112131415161718192021222324252627282930$ cat &gt;&gt; secret.yaml &lt;&lt; EOFapiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: username: YWRtaW4= password: MWYyZDFlMmU2N2RmEOF# 创建$ kubectl create -f secret.yaml secret/mysecret created# 查看创建结果$ kubectl get secrets mysecret -o yamlapiVersion: v1data: password: MWYyZDFlMmU2N2Rm username: YWRtaW4=kind: Secretmetadata: creationTimestamp: 2018-12-21T09:03:52Z name: mysecret namespace: default resourceVersion: \"57767\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 564bd4da-04ff-11e9-97cd-00505621dd5btype: Opaque 食用方式创建好Secret之后，可以通过两种方式食用： 以Volume方式 以环境变量方式 将 Secret 挂载到 Volume 中1234567891011121314151617181920212223242526$ cat &gt;&gt; redis-pod.yaml &lt;&lt; EOFapiVersion: v1kind: Podmetadata: name: redis labels: app: redisspec: nodeName: k8s-m1 containers: - name: container-0 image: redis imagePullPolicy: IfNotPresent volumeMounts: - name: foo mountPath: /etc/foo readOnly: true volumes: - name: foo secret: secretName: db-user-passwdEOF$ 创建:$ kubectl create -f redis-pod.yamlpod/redis created 进入容器检查 12345678$ ls -l /etc/foo/total 0lrwxrwxrwx 1 root root 19 Dec 21 09:14 password.txt -&gt; ..data/password.txtlrwxrwxrwx 1 root root 19 Dec 21 09:14 username.txt -&gt; ..data/username.txt$ cat /etc/foo/username.txt admin$ cat /etc/foo/password.txt 1f2d1e2e67df 也可以只挂载Secret中特定的key： 1234567891011121314151617181920212223242526272829$ cat &gt;&gt; redis-pod2.yaml &lt;&lt; EOFapiVersion: v1kind: Podmetadata: name: redis2 labels: app: redisspec: nodeName: k8s-m1 containers: - name: redis image: redis imagePullPolicy: IfNotPresent volumeMounts: - name: foo mountPath: /etc/foo readOnly: true volumes: - name: foo secret: secretName: mysecret items: - key: username path: my-group/my-usernameEOF# 创建$ kubectl create -f redis-pod2.yaml pod/redis2 created 进入容器检查 12345$ kubectl exec -it redis5 /bin/bash$ ls -l /etc/foo/my-group/my-username -rw-r--r-- 1 root root 5 Dec 21 10:26 /etc/foo/my-group/my-username$ cat /etc/foo/my-group/my-usernameadmin 在这种情况下： username 存储在/etc/foo/my-group/my-username中password未被挂载注意: 指定key的这种挂载方式只适用于是通过使用描述文件创建的Secret,从文件中创建的那种Secret挂载会报错: 12345$ 当我挂载 Secret db-user-passwd 的时候pod创建事件:Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedMount 5s (x5 over 12s) kubelet, k8s-m1 MountVolume.SetUp failed for volume \"foo\" : references non-existent secret key 具体原因目前无解,有知道的大哥可以评论留言告知一二，谢谢 将 Secret 导出到环境变量中1234567891011121314151617181920212223242526272829303132$ cat &gt;&gt; redis3.yaml &lt;&lt; EOFapiVersion: v1kind: Podmetadata: name: redis3 labels: app: redisspec: nodeName: k8s-m1 containers: - name: redis image: redis imagePullPolicy: IfNotPresent env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: passwordEOF# 进入容器查看结果$ kubectl exec -it redis3 /bin/bash$ echo $SECRET_USERNAMEadmin$ echo $SECRET_PASSWORD1f2d1e2e67df ok , 通过不同方式挂载进容器中，我们的应用程序就可以拿来用啦，具体选择哪种方式挂载还是需要看实际环境；好啦，以上就是Secret的简单食用方法，更多可以看官方文档","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kuberntes","slug":"kuberntes","permalink":"https://blog.sctux.cc/tags/kuberntes/"},{"name":"secret","slug":"secret","permalink":"https://blog.sctux.cc/tags/secret/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"Kubernetes容器存活探测&应用自恢复","slug":"kubernetes-liveness","date":"2018-12-18T15:12:40.000Z","updated":"2025-09-01T01:59:08.959Z","comments":true,"path":"2018/12/18/kubernetes-liveness/","permalink":"https://blog.sctux.cc/2018/12/18/kubernetes-liveness/","excerpt":"概述当你使用kuberentes的时候，有没有遇到过Pod在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过kubernetes是如何检测pod是否还存活？虽然容器已经启动，但是kubernetes如何知道容器的进程是否准备好对外提供服务了呢？ 本博文主要记录实践如何配置容器的存活和就绪探针。 liveness probe（存活探针）用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。 readiness probe（就绪探针）用于判断容器是否启动完成，即容器的Ready是否为True，可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。 每类探针都支持三种探测方法 exec：通过执行命令来检查服务是否正常，针对复杂检测或无HTTP接口的服务，命令返回值为0则表示容器健康。 httpGet：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康。 tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康。 每种方式都可以定义在readiness 或者liveness 中。比如定义readiness 中http get 就是意思说如果我定义的这个path的http get 请求返回200-400以外的http code 就把我从所有有我的服务里面删了吧，如果定义在liveness里面就是把我kill 了。注意，liveness不会重启pod，pod是否会重启由你的restart policy 控制。 探针探测的结果有以下三者之一 Success：Container通过了检查。 Failure：Container未通过检查。 Unknown：未能执行检查，因此不采取任何措施。","text":"概述当你使用kuberentes的时候，有没有遇到过Pod在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过kubernetes是如何检测pod是否还存活？虽然容器已经启动，但是kubernetes如何知道容器的进程是否准备好对外提供服务了呢？ 本博文主要记录实践如何配置容器的存活和就绪探针。 liveness probe（存活探针）用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。 readiness probe（就绪探针）用于判断容器是否启动完成，即容器的Ready是否为True，可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。 每类探针都支持三种探测方法 exec：通过执行命令来检查服务是否正常，针对复杂检测或无HTTP接口的服务，命令返回值为0则表示容器健康。 httpGet：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康。 tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康。 每种方式都可以定义在readiness 或者liveness 中。比如定义readiness 中http get 就是意思说如果我定义的这个path的http get 请求返回200-400以外的http code 就把我从所有有我的服务里面删了吧，如果定义在liveness里面就是把我kill 了。注意，liveness不会重启pod，pod是否会重启由你的restart policy 控制。 探针探测的结果有以下三者之一 Success：Container通过了检查。 Failure：Container未通过检查。 Unknown：未能执行检查，因此不采取任何措施。 重启策略 Always: 总是重启 OnFailure: 如果失败就重启 Never: 永远不重启 LivenessProbe探针配置示例一: 通过exec方式做健康探测12345678910111213141516171819202122exec-liveness.yaml apiVersion: v1kind: Podmetadata: labels: test: liveness name: liveness-execspec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 在该配置文件中，对容器执行livenessProbe检查，periodSeconds字段指定kubelet每5s执行一次检查，检查的命令为cat /tmp/healthy，initialDelaySeconds字段告诉kubelet应该在执行第一次检查之前等待5秒，如果命令执行成功，则返回0，那么kubelet就认为容器是健康的，如果为非0，则Kubelet会Kill掉容器并根据重启策略来决定是否需要重启(kubernetes默认为POD配置的重启策略为Always) 当容器启动时，它会执行以下命令： 1/bin/sh -c \"touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600\" 对于容器的前30秒，有一个/tmp/healthy文件。因此，在前30秒内，该命令cat /tmp/healthy返回成功代码。30秒后，cat /tmp/healthy返回失败代码。 在30秒内，查看Pod事件： 1234567891011$ kubectl describe pod liveness-exec............Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 15m default-scheduler Successfully assigned default/liveness-exec to k8s-m3 Normal Pulled 3m (x3 over 5m) kubelet, k8s-m3 Successfully pulled image \"k8s.gcr.io/busybox\" Normal Created 3m (x3 over 5m) kubelet, k8s-m3 Created container Normal Started 3m (x3 over 5m) kubelet, k8s-m3 Started container 在30秒后，查看Pod事件： 1234567891011$ kubectl describe pod liveness-execEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 16m default-scheduler Successfully assigned default/liveness-exec to k8s-m3 Normal Pulled 5m (x3 over 7m) kubelet, k8s-m3 Successfully pulled image \"k8s.gcr.io/busybox\" Normal Created 5m (x3 over 7m) kubelet, k8s-m3 Created container Normal Started 5m (x3 over 7m) kubelet, k8s-m3 Started container Warning Unhealthy 4m (x9 over 7m) kubelet, k8s-m3 Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Pulling 4m (x4 over 7m) kubelet, k8s-m3 pulling image \"k8s.gcr.io/busybox\" Normal Killing 2m (x4 over 6m) kubelet, k8s-m3 Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated. 再等30秒，确认Container已重新启动, 下面输出中RESTARTS的次数已增加： 123$ kubectl get pod liveness-execNAME READY STATUS RESTARTS AGEliveness-exec 1/1 Running 1 1m 示例二: 通过HTTP方式做健康探测123456789101112131415161718192021apiVersion: v1kind: Podmetadata: labels: test: liveness name: liveness-httpspec: containers: - name: liveness image: k8s.gcr.io/liveness # 官方用户测试的demo镜像 args: - /server livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 在配置文件中，使用k8s.gcr.io/liveness镜像，创建出一个Pod，其中periodSeconds字段指定kubelet每3秒执行一次探测，initialDelaySeconds字段告诉kubelet延迟等待3秒，探测方式为向容器中运行的服务发送HTTP GET请求，请求8080端口下的/healthz, 任何大于或等于200且小于400的代码表示成功。任何其他代码表示失败。 10秒后，查看Pod事件以验证liveness探测失败并且Container已重新启动： 123$ kubectl describe pod liveness-httpNAME READY STATUS RESTARTS AGEliveness-http 1/1 RUNNING 1 1m httpGet探测方式有如下可选的控制字段 host：要连接的主机名，默认为Pod IP，可以在http request head中设置host头部。 scheme: 用于连接host的协议，默认为HTTP。 path：http服务器上的访问URI。 httpHeaders：自定义HTTP请求headers，HTTP允许重复headers。 port： 容器上要访问端口号或名称。 示例三：通过TCP方式做健康探测Kubelet将尝试在指定的端口上打开容器上的套接字，如果能建立连接，则表明容器健康。 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: goproxy labels: app: goproxyspec: containers: - name: goproxy image: k8s.gcr.io/goproxy:0.1 ports: - containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 TCP检查方式和HTTP检查方式非常相似，示例中两种探针都使用了，在容器启动5秒后，kubelet将发送第一个readinessProbe探针，这将连接到容器的8080端口，如果探测成功，则该Pod将被标识为ready，10秒后，kubelet将进行第二次连接。除此之后，此配置还包含了livenessProbe探针，在容器启动15秒后，kubelet将发送第一个livenessProbe探针，仍然尝试连接容器的8080端口，如果连接失败则重启容器。 12345678ports:- name: liveness-port containerPort: 8080 hostPort: 8080livenessProbe: httpGet: path: /healthz port: liveness-port ReadinessProbe探针配置：ReadinessProbe探针的使用场景livenessProbe稍有不同，有的时候应用程序可能暂时无法接受请求，比如Pod已经Running了，但是容器内应用程序尚未启动成功，在这种情况下，如果没有ReadinessProbe，则Kubernetes认为它可以处理请求了，然而此时，我们知道程序还没启动成功是不能接收用户请求的，所以不希望kubernetes把请求调度给它，则使用ReadinessProbe探针。ReadinessProbe和livenessProbe可以使用相同探测方式，只是对Pod的处置方式不同，ReadinessProbe是将Pod IP:Port从对应的EndPoint列表中删除，而livenessProbe则Kill容器并根据Pod的重启策略来决定作出对应的措施。 ReadinessProbe探针探测容器是否已准备就绪，如果未准备就绪则kubernetes不会将流量转发给此Pod。 ReadinessProbe探针与livenessProbe一样也支持exec、httpGet、TCP的探测方式，配置方式相同，只不过是将livenessProbe字段修改为ReadinessProbe。 1234567readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 示例一: ReadinessProbe示例现在来看一个加入ReadinessProbe探针和一个没有ReadinessProbe探针的示例：该示例中，创建了一个deploy，名为gogs，启动的容器运行一个类似于gitlab的应用程序，程序监听端口为3000。这里为了模拟效果我这里原镜像做了一下修改，主要是为了延迟他的启动时间为40s后再去启动gogs的应用程序，此时就会开启3000端口， (感兴趣的同学可以了解一下，非常爽的一个自助gitweb平台Gogs) 12345678910111213141516171819202122232425262728293031323334353637383940414243kind: ServiceapiVersion: v1metadata: name: gogs namespace: defaultspec: selector: test: gogs ports: - protocol: TCP port: 3000---kind: DeploymentapiVersion: apps/v1metadata: name: gogs namespace: defaultspec: replicas: 1 selector: matchLabels: test: gogs template: metadata: labels: test: gogs spec: containers: - image: test imagePullPolicy: IfNotPresent name: gogs ports: - containerPort: 3000 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-m1 ![Not add readinessProbe result](kubernetes-liveness/image1.png) 从上图可以看出来，当我创建部署之后，Pod启动18s，自身状态已Running，其READ字段，1/1 表示1个容器状态已准备就绪了，此时，对于kubernetes而言，它已经可以接收请求了，而实际上我在去访问的时候服务还无法访问，因为Gogo程序还尚启动起来，40s之后方可正常访问，所以针对于服务启动慢或者其他原因的此类程序，必须配置ReadinessProbe。 下面我加入readinessProbe 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748kind: ServiceapiVersion: v1metadata: name: gogs namespace: defaultspec: selector: test: gogs ports: - protocol: TCP port: 3000---kind: DeploymentapiVersion: apps/v1metadata: name: gogs namespace: defaultspec: replicas: 1 selector: matchLabels: test: gogs template: metadata: labels: test: gogs spec: containers: - image: test imagePullPolicy: IfNotPresent name: gogs ports: - containerPort: 3000 readinessProbe: tcpSocket: port: 3000 initialDelaySeconds: 10 # 启动后10秒开始探测 periodSeconds: 5 # 每5秒探测一次 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-m1 ![Add readinessProbe result](kubernetes-liveness/image1.png) 上图可以看出Pod虽然已处于Runnig状态，但是由于第一次探测时间未到，所以READY字段为0/1，即容器的状态为未准备就绪，在未准备就绪的情况下，其Pod对应的Service下的Endpoint也为空，所以不会有任何请求被调度进来。 当通过第一次探测的检查通过后，容器的状态自然会转为READ状态。 此后根据指定的间隔时间10s后再次探测，如果不通过，则kubernetes就会将Pod IP从EndPoint列表中移除。 配置探针(Probe)相关属性探针(Probe)有许多可选字段，可以用来更加精确的控制Liveness和Readiness两种探针的行为(Probe)： initialDelaySeconds：Pod启动后延迟多久才进行检查，单位：秒。periodSeconds：检查的间隔时间，默认为10，单位：秒。timeoutSeconds：探测的超时时间，默认为1，单位：秒。successThreshold：探测失败后认为成功的最小连接成功次数，默认为1，在Liveness探针中必须为1，最小值为1。failureThreshold：探测失败的重试次数，重试一定次数后将认为失败，在readiness探针中，Pod会被标记为未就绪，默认为3，最小值为1。 参考:https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"liveness probe","slug":"liveness-probe","permalink":"https://blog.sctux.cc/tags/liveness-probe/"},{"name":"readiness probe","slug":"readiness-probe","permalink":"https://blog.sctux.cc/tags/readiness-probe/"},{"name":"restartPolicy","slug":"restartPolicy","permalink":"https://blog.sctux.cc/tags/restartPolicy/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"理解Kubernetes中的认证&授权&准入机制","slug":"kubernetes-auth","date":"2018-12-16T07:35:24.000Z","updated":"2025-09-01T01:59:08.920Z","comments":true,"path":"2018/12/16/kubernetes-auth/","permalink":"https://blog.sctux.cc/2018/12/16/kubernetes-auth/","excerpt":"概述首先需要了解这三种机制的区别： 认证(Authenticating)是对客户端的认证，通俗点就是用户名密码验证， 授权(Authorization)是对资源的授权，k8s中的资源无非是容器，最终其实就是容器的计算，网络，存储资源，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。 准入(Admission Control)机制是一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。在k8s中，这三种机制如下图：k8s的整体架构也是一个微服务的架构，所有的请求都是通过一个GateWay，也就是kube-apiserver这个组件（对外提供REST服务），由图中可以看出，k8s中客户端有两类，一种是普通用户，一种是集群内的Pod，这两种客户端的认证机制略有不同，后文会详述。但无论是哪一种，都需要依次经过认证，授权，准入这三个机制。 kubernetes 中的认证机制需要注意的是，kubernetes虽然提供了多种认证机制，但并没有提供user 实体信息的存储，也就是说，账户体系需要我们自己去做维护。当然，也可以接入第三方账户体系（如谷歌账户），也可以使用开源的keystone去做整合。kubernetes 支持多种认证机制，可以配置成多个认证体制共存，这样，只要有一个认证通过，这个request就认证通过了。下面列举的是官网几种常见认证机制： X509 Client Certs Static Token File Bootstrap Tokens Static Password File Service Account Tokens OpenID Connect Tokens 这里我主要还是理解一下常用认证方式: X509 Client Certs也叫作双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。默认在kubeadm创建的集群中是enabled的，可以在master node上查看kube-apiserver的pod配置文件： 12345678910111213141516171819202122232425$ cat /usr/lib/systemd/system/kube-apiserver.service....................ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --insecure-port=0 \\ --advertise-address=192.168.56.110 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.56.111:2379,https://192.168.56.112:2379,https://192.168.56.113:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\....................","text":"概述首先需要了解这三种机制的区别： 认证(Authenticating)是对客户端的认证，通俗点就是用户名密码验证， 授权(Authorization)是对资源的授权，k8s中的资源无非是容器，最终其实就是容器的计算，网络，存储资源，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。 准入(Admission Control)机制是一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。在k8s中，这三种机制如下图：k8s的整体架构也是一个微服务的架构，所有的请求都是通过一个GateWay，也就是kube-apiserver这个组件（对外提供REST服务），由图中可以看出，k8s中客户端有两类，一种是普通用户，一种是集群内的Pod，这两种客户端的认证机制略有不同，后文会详述。但无论是哪一种，都需要依次经过认证，授权，准入这三个机制。 kubernetes 中的认证机制需要注意的是，kubernetes虽然提供了多种认证机制，但并没有提供user 实体信息的存储，也就是说，账户体系需要我们自己去做维护。当然，也可以接入第三方账户体系（如谷歌账户），也可以使用开源的keystone去做整合。kubernetes 支持多种认证机制，可以配置成多个认证体制共存，这样，只要有一个认证通过，这个request就认证通过了。下面列举的是官网几种常见认证机制： X509 Client Certs Static Token File Bootstrap Tokens Static Password File Service Account Tokens OpenID Connect Tokens 这里我主要还是理解一下常用认证方式: X509 Client Certs也叫作双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。默认在kubeadm创建的集群中是enabled的，可以在master node上查看kube-apiserver的pod配置文件： 12345678910111213141516171819202122232425$ cat /usr/lib/systemd/system/kube-apiserver.service....................ExecStart=/usr/local/bin/kube-apiserver \\ --v=2 \\ --logtostderr=true \\ --allow-privileged=true \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --insecure-port=0 \\ --advertise-address=192.168.56.110 \\ --service-cluster-ip-range=10.96.0.0/12 \\ --service-node-port-range=30000-32767 \\ --etcd-servers=https://192.168.56.111:2379,https://192.168.56.112:2379,https://192.168.56.113:2379 \\ --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \\ --etcd-certfile=/etc/etcd/ssl/etcd.pem \\ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\ --client-ca-file=/etc/kubernetes/pki/ca.pem \\ --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \\ --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \\ --kubelet-client-certificate=/etc/kubernetes/pki/apiserver.pem \\ --kubelet-client-key=/etc/kubernetes/pki/apiserver-key.pem \\.................... 相关的三个启动参数： client-ca-file: 指定CA根证书文件为/etc/kubernetes/pki/ca.pem tls-private-key-file: 指定ApiServer私钥文件为/etc/kubernetes/pki/apiserver-key.pem tls-cert-file：指定ApiServer证书文件为/etc/kubernetes/pki/apiserver.pem 请求中需要带有由该证书签名的证书，才能认证通过，客户端签署的证书里包含user,group信息，具体为证书的subject.CommonName(username)以及subject.Organization(group) ![x509](kubernetes-auth/x509.png) Service Account TokensService Account Token 是一种比较特殊的认证机制，适用于上文中提到的pod内部服务需要访问apiserver的认证情况，默认enabled。还是看上文中apiserver 的启动配置参数有–service-account-key-file，如果没有指明文件，默认使用–tls-private-key-file的值，即API Server的私钥。 ![ServiceAccount工作流程](kubernetes-auth/sa.png) 通过控制器ServiceAccountController会去list,watch k8s apiserver对于命名空间的创建、删除；就会在新创建的名称空间下创建一个名为”default”的service account; TokenController 根据创建的ServiceAccount下面关联生成一个带有Token的secret。Admission 是在通过认证进行鉴权的一个阶段，那么他会默认给当前名称空间下的pod打上，当前名称空间的那个ServiceAccount。 service accout本身是作为一种资源在k8s集群中，我们可以通过命令行获取 1234567891011121314151617181920212223242526272829303132333435363738394041$ kubectl get sa --all-namespacesNAMESPACE NAME SECRETS AGEdefault default 1 16dkube-system default 1 16dkube-public default 1 16dkube-system attachdetach-controller 1 16dkube-system bootstrap-signer 1 16dkube-system calico-node 1 16......................$ kubectl describe serviceaccount/default -n kube-systemName: defaultNamespace: kube-systemLabels: &lt;none&gt;Annotations: &lt;none&gt;Image pull secrets: &lt;none&gt;Mountable secrets: default-token-q9s9lTokens: default-token-q9s9lEvents: &lt;none&gt;$ kubectl get secret default-token-q9s9l -o yaml -n kube-systemapiVersion: v1data: ca.crt: LS0tLS1CRUdJ ...............略 namespace: a3ViZS1zeXN0ZW0= token: ZXlKaGJHY2lPaUpTVX- ...............略kind: Secretmetadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: a5568634-f445-11e8-b4c4-000c295134cf creationTimestamp: 2018-11-30T02:14:19Z name: default-token-q9s9l namespace: kube-system resourceVersion: \"337\" selfLink: /api/v1/namespaces/kube-system/secrets/default-token-q9s9l uid: a56521fd-f445-11e8-aa44-000c29fe5618type: kubernetes.io/service-account-token[root@k8s-m1 kubelet]# 可以看到service-account-token的secret资源包含的数据有三部分： ca.crt，这是API Server的CA公钥证书，用于Pod中的Process对API Server的服务端数字证书进行校验时使用的； namespace，这是Secret所在namespace的值的base64编码：# echo -n “kube-system”|base64 =&gt; “a3ViZS1zeXN0ZW0=” token：该token就是由service-account-key-file的值签署(sign)生成。 API Server的service account authentication(身份验证)前面说过，service account为Pod中的Process提供了一种身份标识，在Kubernetes的身份校验(authenticating)环节，以某个service account提供身份的Pod的用户名为： 1system:serviceaccount:(NAMESPACE):(SERVICEACCOUNT) 以上面那个kube-system namespace下的default service account为例，使用它的Pod的username全称为： 1system:serviceaccount:kube-system:default 默认的service accountKubernetes会为每个cluster中的namespace自动创建一个默认的service account资源，并命名为”default”. 如果Pod中没有显式指定spec.serviceAccount字段值(自定义Admission)，那么Kubernetes会默认将该namespace下的default service account自动mount到在这个namespace中创建的Pod里，那这个操作就是通过上面所说的ServiceAccountAdmission来实现的。我们以namespace “default”为例，我们查看一下其中的一个Pod的信息： 123456789101112131415161718192021222324$ kubectl describe pod nginx-64f497f8fd-lkmdrName: nginx-64f497f8fd-lkmdrNamespace: default............Containers:............ Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-d5d8g (ro)Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled TrueVolumes: ...... ......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 27s default-scheduler Successfully assigned default/nginx-64f497f8fd-lkmdr to k8s-m3 Normal Pulling 17s kubelet, k8s-m3 pulling image \"nginx\" 可以看到，kubernetes将default namespace中的service account “default”的service account token挂载(mount)到了Pod中容器的/var/run/secrets/kubernetes.io/serviceaccount路径下。 深入容器内部，查看mount的serviceaccount路径下的结构： 123456$ kubectl exec nginx-64f497f8fd-lkmdr -- ls /var/run/secrets/kubernetes.io/serviceaccountca.crtnamespacetoken# 这三个文件与上面提到的service account的token中的数据是一一对应的。 如何创建一个ServiceAccount1234567891011121314# 通过命令行直接创建$ kubectl create sa myk8ssaserviceaccount/myk8ssa created$ kubectl describe sa/myk8ssaName: mysaNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Image pull secrets: &lt;none&gt;Mountable secrets: mysa-token-kcwqmTokens: mysa-token-kcwqmEvents: &lt;none&gt;# 如何使用？下面我在讨论 kubernetes 中的鉴权机制目前，k8s 中一共有 4 种鉴权权限模式： Node: 一种特殊目的的授权模式，主要用来让 kubernetes 遵从 node 的编排规则，实际上是 RBAC 的一部分，相当于只定义了 node 这个角色以及它的权限； ABAC: Attribute-based access control； RBAC: Role-based access control； Webhook: 以 HTTP Callback 的方式，利用外部授权接口来进行权限控制； 这里我主要学习总结最常用的鉴权方式—RBACRBAC的鉴权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC 在 Kubernetes 中被映射为 API 资源和操作。 ![RBAC](kubernetes-auth/RBAC.png) 需要理解 RBAC 一些基础的概念和思路，RBAC 是让用户能够访问 Kubernetes API 资源的授权方式。 role角色是一系列权限的集合，例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限， ClusterRole 跟 Role 类似，但是可以在集群中到处使用（ Role 是 namespace 一级的）。role bindingRoleBinding 把角色映射到用户，从而让这些用户继承角色在 namespace 中的权限。ClusterRoleBinding 让用户继承 ClusterRole 在整个集群中的权限。 另外还要考虑cluster roles和cluster role binding。cluster role和cluster role binding方法跟role和role binding一样，出了它们有更广的scope。 Kubernetes中的RBACRBAC 现在被 Kubernetes 深度集成，并使用他给系统组件进行授权。System Roles 一般具有前缀system:，很容易识别： 1234567891011121314151617181920212223242526$ kubectl get clusterroles --namespace=kube-systemNAME AGEadmin 16danonymous-dashboard-proxy-role 16dcalico-node 16dcalicoctl 16dcluster-admin 16dedit 16dsystem:aggregate-to-admin 16dsystem:aggregate-to-edit 16dsystem:aggregate-to-view 16dsystem:auth-delegator 16dsystem:aws-cloud-provider 16dsystem:basic-user 16dsystem:certificates.k8s.io:certificatesigningrequests:nodeclient 16dsystem:certificates.k8s.io:certificatesigningrequests:selfnodeclient 16dsystem:controller:attachdetach-controller 16dsystem:controller:certificate-controller 16dsystem:controller:clusterrole-aggregation-controller 16dsystem:controller:cronjob-controller 16dsystem:controller:daemon-set-controller 16dsystem:controller:deployment-controller 16dsystem:controller:disruption-controller 16dsystem:controller:endpoint-controller 16dsystem:controller:expand-controller 16d......略 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657$ cat &gt;&gt; role.yaml &lt;&lt; EOFkind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: default name: pod-readerrules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: [\"pods\"] # 可用操作资源对象 verbs: [\"get\", \"watch\", \"list\"] # 对上面定义资源有哪些操作权限EOF$ cat &gt;&gt; rolebinding.yaml &lt;&lt; EOFkind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: pod-reader-binding namespace: defaultsubjects: - kind: ServiceAccount name: mysa namespace: defaultroleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.ioEOF# 执行创建$ kubectl create -f role.yamlrole.rbac.authorization.k8s.io/pod-reader created$ kubectl create -f rolebinding.yamlrolebinding.rbac.authorization.k8s.io/pod-reader-binding created[root@k8s-m1 ~]# kubectl get roleNAME AGEpod-reader 36s[root@k8s-m1 ~]# kubectl describe role pod-readerName: pod-readerLabels: &lt;none&gt;Annotations: &lt;none&gt;PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- pods [] [] [get watch list]$ kubectl describe rolebinding pod-reader-bindingName: pod-reader-bindingLabels: &lt;none&gt;Annotations: &lt;none&gt;Role: Kind: Role Name: pod-readerSubjects: Kind Name Namespace ---- ---- --------- ServiceAccount mysa default 上面， 我定义了一个pod-reader的role,其中它的权限定义如上方定义的get,watch,list; 然后定义了一个pod-reader-binding的rolebinding前面提到过；用户通过认证之后就是鉴权，通过SA的认证方式之后拿到userinfo,然后绑定到role里面；那么此时对应的用户权限就是role里面定义的权限，subjects应用的对象类型也可以是User,Group;总之此时这一步就是获取到userinfo信息，然后通过角色绑定。相对应的权限就会赋予该用户。那用户从何而来？下面就需要创建一个用户来访问我们的集群了。 那如何使用我们自己创建的ServiceAccount来登录并且通过自定义的这个RBAC来鉴权呢？这就涉及到了kubeconfig文件生成的问题了 创建kubeconfig获取上面已创建的ServiceAccount的Seter名称这里我是在默认namespace下面创建的。 123456789$ kubectl describe sa/myk8ssaName: myk8ssaNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Image pull secrets: &lt;none&gt;Mountable secrets: myk8ssa-token-2kntdTokens: myk8ssa-token-2kntdEvents: &lt;none&gt; 获取secret下的token，并base64解码获取token明文123$ token=`kubectl get secret myk8ssa-token-2kntd -oyaml |grep token: | awk '{print $2}' | xargs echo -n | base64 -d`$ echo $tokeneyJhbGciOiJSUzI1......略 新增用户guomaoqiu123456789101112131415$ kubectl config set-cluster my-k8s --server=https://192.168.56.110:8443 \\--certificate-authority=/etc/kubernetes/pki/ca.pem \\--embed-certs=trueCluster \"my-k8s\" set.$ kubectl config set-credentials guomaoqiu --token=$tokenUser \"guomaoqiu\" set.# 设置上下文$ kubectl config set-context my-k8s --cluster=kubernetesContext \"my-k8s\" created.# 确认用户信息$ kubectl config set-context my-k8s --user=guomaoqiuContext \"my-k8s\" modified. 验证123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354$ kubectl config get-contextsCURRENT NAME CLUSTER AUTHINFO NAMESPACE kubernetes-admin@kubernetes kubernetes kubernetes-admin* my-k8s kubernetes guomaoqiu# 将其设置为当前默认上下文:$ kubectl config use-context my-k8sSwitched to context \"my-k8s\".# 获取POD:$ kubectl get podNAME READY STATUS RESTARTS AGEnginx-64f497f8fd-4qdnt 1/1 Running 0 10m# 尝试删除POD:$ kubectl delete pod nginx-64f497f8fd-4qdntError from server (Forbidden): pods \"nginx-64f497f8fd-4qdnt\" is forbidden: User \"system:serviceaccount:default:myk8ssa\" cannot delete pods in the namespace \"default\"# 可见，该用户的的权限只有可读，而没有删除的权限；# 说明配置是成功了的。# 浏览一下当前我环境中有哪些kubeconfig?$ kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority-data: REDACTED server: https://192.168.56.110:8443 name: kubernetes- cluster: certificate-authority-data: REDACTED server: https://192.168.56.110:8443 name: my-k8scontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes- context: cluster: kubernetes user: guomaoqiu name: my-k8scurrent-context: my-k8s # 当前环境kind: Configpreferences: {}users:- name: guomaoqiu user: token:......略- name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 通过namespace我们可以做到不同业务之间的隔离，如果再加上如上这种权限控制将会更加细化、以上实验主要是在默认的namespace下面操作，除此之外也可以新建namespace然后进行验证操作。kubeconfig切换上下文也是非常实用的一种功能。常用命令： 12345678kubectl - 使用kubectl来管理Kubernetes集群。kubectl config set - 在kubeconfig配置文件中设置一个单独的值。kubectl config set-cluster - 在kubeconfig配置文件中设置一个集群项。kubectl config set-context - 在kubeconfig配置文件中设置一个环境项。kubectl config set-credentials - 在kubeconfig配置文件中设置一个用户项。kubectl config unset - 在kubeconfig配置文件中清除一个单独的值。kubectl config use-context - 使用kubeconfig中的一个环境项作为当前配置。kubectl config view - 显示合并后的kubeconfig设置，或者一个指定的kubeconfig配置文件。 kubernetes 中的准入机制Kubernetes的Admission Control实际上是一个准入控制器(Admission Controller)插件列表，发送到APIServer的请求都需要经过这个列表中的每个准入控制器插件的检查，如果某一个控制器插件准入失败，就准入失败。更多可查看: http://docs.kubernetes.org.cn/144.html这里我们主要学习PodSecurityPolicy 安全上下文(Pod SecurityContext)分为Pod级别和容器级别，容器级别的会覆盖Pod级别的相同设置。在有PodSecurityPolicy策略的情况下，两者需要配合使用; 12345678910111213141516171819202122232425262728293031323334353637383940414243cat &gt;&gt; pod-security-policy.yaml &lt;&lt; EOFapiVersion: v1kind: Podmetadata: name: test-podspec: volumes: - name: test emptyDir: {} containers: - name: test-pod image: alpine imagePullPolicy: IfNotPresent command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 36000'] volumeMounts: - name: test mountPath: /data/test securityContext: # 设置容器安全上下文 readOnlyRootFilesystem: false # 容器文件系统是否是只读 privileged: false # 是否为特权容器 runAsUser: 1000 # 进程运行用户UIDEOF# 执行创建:$ kubectl create -f pod-security-policy.yamlpod/test-pod created# 进入容器验证：$ kubectl exec -it test-pod sh/ $ iduid=1000 gid=0(root)/ $ ps -efPID USER TIME COMMAND 1 1000 0:00 sleep 36000 6 1000 0:00 sh 15 1000 0:00 sh 21 1000 0:00 ps -ef/ $ sysctl -w net.ipv4.tcp_recovery=2sysctl: error setting key 'net.ipv4.tcp_recovery': Read-only file system/ $# 以上我设置限制了pod是否开启特权模式，并且运行用户uid为1000# 那么如果在pod级别设置上下文的话，容器级别的会覆盖Pod级别的相同设置(已验证) 其他更多参数参见: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ 运行态的安全控制—网络策略(NetworkPolicy)Kubernetes要求集群中所有pod，无论是节点内还是跨节点，都可以直接通信，或者说所有pod工作在同一跨节点网络，此网络一般是二层虚拟网络，称为pod网络。在安装引导kubernetes时，由选择并安装的network plugin实现。默认情况下，集群中所有pod之间、pod与节点之间可以互通。 网络主要解决两个问题，一个是连通性，实体之间能够通过网络互通。另一个是隔离性，出于安全、限制网络流量的目的，又要控制实体之间的连通性。Network Policy用来实现隔离性，只有匹配规则的流量才能进入pod，同理只有匹配规则的流量才可以离开pod。 但请注意，kubernetes支持的用以实现pod网络的network plugin有很多种，并不是全部都支持Network Policy，为kubernetes选择network plugin时需要考虑到这点，是否需要隔离？可用network plugin及是否支持Network Policy请参考这里。 基本原理Network Policy是kubernetes中的一种资源类型，它从属于某个namespace。其内容从逻辑上看包含两个关键部分，一是pod选择器，基于标签选择相同namespace下的pod，将其中定义的规则作用于选中的pod。另一个就是规则了，就是网络流量进出pod的规则，其采用的是白名单模式，符合规则的通过，不符合规则的拒绝。 123456789101112131415161718192021222324252627282930313233apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: # 规则选择器，选择匹配的POD role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: # 远端(访问端)IP白名单开放 cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: # 远端(访问端)namespaces白名单开放 matchLabels: project: myproject - podSelector: # 远端(访问端)Pod白名单开放 matchLabels: role: frontend ports: # 本端(被访问端)允许被访问的端口和协议 - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP 对象创建方法与其它如ReplicaSet相同。apiVersion、kind、metadata与其它类型对象含义相同，不详细描述。 .spec.PodSelector顾名思义，它是pod选择器，基于标签选择与Network Policy处于同一namespace下的pod，如果pod被选中，则对其应用Network Policy中定义的规则。此为可选字段，当没有此字段时，表示选中所有pod。 .spec.PolicyTypesNetwork Policy定义的规则可以分成两种，一种是入pod的Ingress规则，一种是出pod的Egress规则。本字段可以看作是一个开关，如果其中包含Ingress，则Ingress部分定义的规则生效，如果是Egress则Egress部分定义的规则生效，如果都包含则全部生效。当然此字段也可选，如果没有指定的话，则默认Ingress生效，如果Egress部分有定义的话，Egress才生效。怎么理解这句话，下文会提到，没有明确定义Ingress、Egress部分，它也是一种规则，默认规则而非没有规则。 .spec.ingress与.spec.egress前者定义入pod规则，后者定义出pod规则，详细参考这里，这里只讲一下重点。上例中ingress与egress都只包含一条规则，两者都是数组，可以包含多条规则。当包含多条时，条目之间的逻辑关系是“或”，只要匹配其中一条就可以。.spec.ingress[].from也是数组，数组成员对访问pod的外部source进行描述，符合条件的source才可以访问pod，有多种方法，如示例中的ip地址块、名称空间、pod标签等，数组中的成员也是逻辑或的关系。spec.ingress[].from.prots表示允许通过的协议及端口号。 .spec.egress.to 定义的是pod想要访问的外部destination，其它与ingress相同。 kubernetes 默认NetworkPolicy:12345678910111213141516171819202122232425262728293031323334353637383940414243#默认禁止所有出口请求apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: default-denyspec: podSelector: {} policyTypes: - Egress# 默认禁止所有入口请求apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: default-denyspec: podSelector: {} policyTypes: - Ingress# 默认允许所有出口请求apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-allspec: podSelector: {} egress: - {} policyTypes: - Egress# 默认允许所有入口请求apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-allspec: podSelector: {} egress: - {} policyTypes: - Ingress 默认策略 无需详解，但请注意，pod与所运行节点之间流量不受Network Policy限制。 示例下面通过一个真实示例展示Network Policy普通用法。 用Deployment创建nginx pod实例并用service暴露1234$ kubectl run nginx --image=nginx --replicas=3deployment.apps/nginx created$ kubectl expose deployment nginx --port=80service/nginx exposed 确认创建结果123456789$ kubectl get pod,svcNAME READY STATUS RESTARTS AGEpod/nginx-64f497f8fd-9mfd7 1/1 Running 0 2mpod/nginx-64f497f8fd-nss94 1/1 Running 0 2mpod/nginx-64f497f8fd-zs926 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 3dservice/nginx ClusterIP 10.111.254.191 &lt;none&gt; 80/TCP 14s 测试nginx服务连通性12345$ kubectl run busybox --rm -it --image=busybox /bin/shIf you don't see a command prompt, try pressing enter./ # wget --spider --timeout=3 nginxConnecting to nginx (10.111.254.191:80) # 说明通的/ # 通过创建Network Policy对象添加隔离性123456789101112131415161718192021222324252627282930313233343536$ cat &gt;&gt; pod-network-policy.yaml &lt;&lt; EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: access-nginxspec: podSelector: matchLabels: # 默认该pod就有run=app这个标签了，我这里无需在从新定义 run: nginx ingress: - from: - podSelector: matchLabels: access: \"true\"EOF# 执行创建$ kubectl create -f pod-network-policy.yamlnetworkpolicy.networking.k8s.io/access-nginx created# 查看具体隔离策略$ kubectl describe networkpolicy access-nginxName: access-nginxNamespace: defaultCreated on: 2018-12-17 01:16:57 -0500 ESTLabels: &lt;none&gt;Annotations: &lt;none&gt;Spec: PodSelector: run=nginx Allowing ingress traffic: To Port: &lt;any&gt; (traffic allowed to all ports) From: PodSelector: access=true Allowing egress traffic: &lt;none&gt; (Selected pods are isolated for egress connectivity) Policy Types: Ingress 测试隔离性12345$ kubectl run busybox --rm -it --image=busybox /bin/shIf you don't see a command prompt, try pressing enter./ # wget --spider --timeout=3 nginxConnecting to nginx (10.111.254.191:80)wget: download timed out # 已经超时，说明已经不能访问了 为pod添加access: “true”标签后再次测试连通性1234$ kubectl run busybox --rm -it --labels=\"access=true\" --image=busybox /bin/shIf you don't see a command prompt, try pressing enter./ # wget --spider --timeout=3 nginxConnecting to nginx (10.111.254.191:80) 参考:https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/https://kubernetes.io/docs/reference/access-authn-authz/rbac/https://kubernetes.io/docs/concepts/services-networking/network-policies/https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/https://kubernetes.io/docs/concepts/policy/pod-security-policy/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/tags/kubernetes/"},{"name":"ServiceAccount","slug":"ServiceAccount","permalink":"https://blog.sctux.cc/tags/ServiceAccount/"},{"name":"Kubeconfig","slug":"Kubeconfig","permalink":"https://blog.sctux.cc/tags/Kubeconfig/"},{"name":"RBAC","slug":"RBAC","permalink":"https://blog.sctux.cc/tags/RBAC/"},{"name":"Admission","slug":"Admission","permalink":"https://blog.sctux.cc/tags/Admission/"},{"name":"PodSecurityContext","slug":"PodSecurityContext","permalink":"https://blog.sctux.cc/tags/PodSecurityContext/"},{"name":"NetworkPolicy","slug":"NetworkPolicy","permalink":"https://blog.sctux.cc/tags/NetworkPolicy/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"理解kubernetes中的Storage","slug":"kubernetes-storage","date":"2018-12-15T00:27:08.000Z","updated":"2025-09-01T01:59:08.868Z","comments":true,"path":"2018/12/15/kubernetes-storage/","permalink":"https://blog.sctux.cc/2018/12/15/kubernetes-storage/","excerpt":"序、为何需要存储卷容器部署过程中一般有以下三种数据: 启动时需要的初始数据，可以是配置文件 启动过程中产生的临时数据，该临时数据需要多个容器间共享 启动过程中产生的持久化数据 以上三种数据都不希望在容器重启时就消失，存储卷由此而来，它可以根据不同场景提供不同类型的存储能力。各类卷: ![截图来源华为云cce课堂](kubernetes-storage/volume.png) spec.volumes：通过此字段提供指定的存储卷 spec.containers.volumeMounts：通过此字段将存储卷挂接到容器中 一、普通存储卷的用法:容器启动时依赖数据1. ConfigMap上图也说了它是在容器启动的时候以来的数据来源","text":"序、为何需要存储卷容器部署过程中一般有以下三种数据: 启动时需要的初始数据，可以是配置文件 启动过程中产生的临时数据，该临时数据需要多个容器间共享 启动过程中产生的持久化数据 以上三种数据都不希望在容器重启时就消失，存储卷由此而来，它可以根据不同场景提供不同类型的存储能力。各类卷: ![截图来源华为云cce课堂](kubernetes-storage/volume.png) spec.volumes：通过此字段提供指定的存储卷 spec.containers.volumeMounts：通过此字段将存储卷挂接到容器中 一、普通存储卷的用法:容器启动时依赖数据1. ConfigMap上图也说了它是在容器启动的时候以来的数据来源 示例：12345678910111213141516171819202122232425262728293031323334353637383940# 1.创建configmap预制数据卷cat &gt;&gt; configmap.yaml &lt;&lt; EOFapiVersion: v1data: guomaoqiu: hello-worldkind: ConfigMapmetadata: name: testEOF$ kubectl create -f configmap.yamlconfigmap/test created# 2. 创建pod来使用这个configmap# 这里我创建了一个nginx pod 让他启动是挂载这个数据$ cat nginx-deployment.yaml............ spec: containers: - image: nginx name: nginx resources: {} volumeMounts: - name: test mountPath: /tmp # 挂载点 volumes: - name: test configMap: name: test # ConfigMap name defaultMode: 420 # 指定挂载到pod文件的权限............# 3.检查数据是否挂载进podkubectl exec nginx-7d8bb9c4fc-bxxkt -- ls /tmpguomaoqiukubectl exec nginx-7d8bb9c4fc-bxxkt -- cat /tmp/guomaoqiuhello-world $# 以上可以看到将configmap的key作为了文件名，将value作为了文件的内容。# 他的作用也就是允许您将配置文件从容器镜像中解耦，从而增强容器应用的可移植性# 更多可查看: https://k8smeetup.github.io/docs/tasks/configure-pod-container/configmap/ 2. SerectSecret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。 用户可以创建 secret，同时系统也创建了一些 secret。要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 volume 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。 示例：假设有些 pod 需要访问数据库。这些 pod 需要使用的用户名和密码在您本地机器的 ./username.txt 和 ./password.txt 文件里。 123# Create files needed for rest of example.echo -n \"admin\" &gt; ./username.txtecho -n \"1f2d1e2e67df\" &gt; ./password.txt kubectl create secret 命令将这些文件打包到一个 Secret 中并在 API server 中创建了一个对象。 1234567891011121314151617$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txtsecret/db-user-pass created$ kubectl describe secret db-user-passName: db-user-passNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: OpaqueData====password.txt: 12 bytesusername.txt: 5 bytes# 默认情况下，get 和 describe 命令都不会显示文件的内容。# 这是为了防止将 secret 中的内容被意外暴露给从终端日志记录中刻意寻找它们的人。 在 Pod 中使用 Secret 文件在挂载的 secret volume 的容器内，secret key 将作为文件，并且 secret 的值使用 base-64 解码并存储在这些文件中。这是在上面的示例容器内执行的命令的结果： 12345 kubectl exec -it mypod /bin/bashroot@mypod:/# cat /etc/foo/username.txtadminroot@mypod:/# cat /etc/foo/password.txt1f2d1e2e67df 容器中的程序负责从文件中读取 secret。关于secret挂载使用方法 更多：https://kubernetes.io/zh/docs/concepts/configuration/secret/#secret-%E4%B8%8E-pod-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%9A%84%E8%81%94%E7%B3%BB 临时数据存储1. emptryDir当 Pod 被分配给节点时，首先创建 emptyDir 卷，并且只要该 Pod 在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod 中的容器可以读取和写入 emptyDir 卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除 Pod 时，emptyDir 中的数据将被永久删除。 注意：容器崩溃不会从节点中移除 pod，因此 emptyDir 卷中的数据在容器崩溃时是安全的。 缺省情况下，EmptyDir 是使用主机磁盘进行存储的，也可以设置emptyDir.medium 字段的值为Memory，来提高运行速度，但是这种设置，对该卷的占用会消耗容器的内存份额。 emptyDir 的用法有： 暂存空间，例如用于基于磁盘的合并排序 用作长时间计算崩溃恢复时的检查点 Web服务器容器提供数据时，保存内容管理器容器提取的文件 可以在同一 Pod 内的不同容器之间共享工作过程中产生的文件。 示例123456789101112131415apiVersion: v1kind: Podmetadata: name: test-emptydir-podspec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-emptydir-pod volumeMounts: - mountPath: /temp-data # 挂载点 name: temp-data volumes: - name: temp-data emptyDir: {} 创建pod 12345 kubectl create -f test-emptydir-pod.yamlpod/test-emptydir-pod created$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEtest-emptydir-pod 1/1 Running 0 18s 10.244.0.23 k8s-m3 &lt;none&gt; 此时Emptydir已经创建成功，在宿主机上的访问路径为/var/lib/kubelet/pods//volumes/kubernetes.io~empty-dir/temp-data,如果在此目录中创建删除文件，都将对容器中的/data目录有影响，如果删除Pod，文件将全部删除，即使是在宿主机上创建的文件也是如此，在宿主机上删除容器则k8s会再自动创建一个容器，此时文件仍然存在。 12345678# 获取 pod uidkubectl get pod test-emptydir-pod -o yaml | grep -n uid11: uid: 3328c9e1-ff8f-11e8-b6d6-00505621dd5b# 登录到节点k8s-m3 查看 ll /var/lib/kubelet/pods/3328c9e1-ff8f-11e8-b6d6-00505621dd5b/volumes/kubernetes.io~empty-dir/total 0drwxrwxrwx 2 root root 6 Dec 14 05:58 temp-data 2. hostPathhostPath volume映射node文件系统中的文件或者目录到pod里。大多数Pod都不需要这个功能，但对于一些特定的场景，该特性还是很有作用的。这些场景包括： 运行的容器需要访问Docker内部结构：使用hostPath映射/var/lib/docker 在容器中运行cAdvisor，使用hostPath映射/dev/cgroups 不过，使用这种volume要小心，因为： 配置相同的pod（如通过podTemplate创建），可能在不同的Node上表现不同，因为不同节点上映射的文件内容不同 当Kubernetes增加了资源敏感的调度程序，hostPath使用的资源不会被计算在内 宿主机下创建的目录只有root有写权限。你需要让你的程序运行在privileged container上，或者修改宿主机上的文件权限。 示例：假设我们在创建docker镜像的时候忘记了更改容器中的时区文件；这时候想把宿主机的/usr/share/zoneinfo/Asia/Shanghai挂到pod中的容器内(这里为了学习挂载方式，暂且不谈不同系统或不同宿主机的一些配置或路径) 12345678910111213141516apiVersion: v1kind: Podmetadata: name: test-emptydir-podspec: containers: - image: nginx imagePullPolicy: IfNotPresent name: test-emptydir-pod volumeMounts: - name: container-time mountPath: /etc/localtime # 挂载点，container启动后直接挂载覆盖此文件 volumes: - name: container-time hostPath: path: /usr/share/zoneinfo/Asia/Shanghai # 挂载时区文件到container中 通过创建pod后登录检查时区正常. 二、持久存储卷的用法:学习持久化存储之前需要学习一下以下概念：Volume 提供了非常好的数据持久化方案，不过在可管理性上还有不足。拿前面 AWS EBS 的例子来说，要使用 Volume，Pod 必须事先知道如下信息： 当前 Volume 来自 AWS EBS。 EBS Volume 已经提前创建，并且知道确切的 volume-id。 Pod 通常是由应用的开发人员维护，而 Volume 则通常是由存储系统的管理员维护。开发人员要获得上面的信息： 要么询问管理员。 要么自己就是管理员。 这样就带来一个管理上的问题：应用开发人员和系统管理员的职责耦合在一起了。如果系统规模较小或者对于开发环境这样的情况还可以接受。但当集群规模变大，特别是对于生成环境，考虑到效率和安全性，这就成了必须要解决的问题。 Kubernetes 给出的解决方案是 PersistentVolume 和 PersistentVolumeClaim。 PersistentVolume (PV) 是外部存储系统中的一块存储空间，由管理员创建和维护。与 Volume 一样，PV 具有持久性，生命周期独立于 Pod。 PersistentVolumeClaim (PVC) 是对 PV 的申请 (Claim)。PVC 通常由普通用户创建和维护。需要为 Pod 分配存储资源时，用户可以创建一个 PVC，指明存储资源的容量大小和访问模式（比如只读）等信息，Kubernetes 会查找并提供满足条件的 PV。 有了 PersistentVolumeClaim，用户只需要告诉 Kubernetes 需要什么样的存储资源，而不必关心真正的空间从哪里分配，如何访问等底层细节信息。这些 Storage Provider 的底层信息交给管理员来处理，只有管理员才应该关心创建 PersistentVolume 的细节信息。 Kubernetes 支持多种类型的 PersistentVolume，比如 AWS EBS、Ceph、NFS 等，完整列表请参考 https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes 主要包括:NFS/GlusterFS/CephFS/AWS/GCE等等作为一个容器集群，支持网络存储自然是重中之重了,Kubernetes支持为数众多的云提供商和网络存储方案。不同公司选择的方案也是不相同。 下节我用 NFS 来体会它存储的使用方法。 NFSNFS的搭建1. 在NFS存储的服务器创建存储目录:12# 这里先创建三个共享目录稍后会用到.$ mkdir /{nfs_data_1,nfs_data_2,nfs_data_3} 2. 安装nfs:1$ yum -y install nfs-utils rpcbind 3. 写入配置123echo \"/nfs_data_1 *(rw,sync,no_subtree_check,no_root_squash)\" &gt; /etc/exportsecho \"/nfs_data_2 *(rw,sync,no_subtree_check,no_root_squash)\" &gt;&gt; /etc/exportsecho \"/nfs_data_3 *(rw,sync,no_subtree_check,no_root_squash)\" &gt;&gt; /etc/exports 4. 重启nfs服务验证12345678systemctl restart nfs-servershowmount -e 192.168.56.113 # NFS ServerExport list for 192.168.56.113:/nfs_data_3 */nfs_data_2 */nfs_data_1 *# nfs安装并且共享目录已经创建完毕 NFS作为普通存储卷在Kubernetes中，可以通过nfs类型的存储卷将现有的NFS（网络文件系统）到的挂接到Pod中。在移除Pod时，NFS存储卷中的内容被不会被删除，只是将存储卷卸载而已。这意味着在NFS存储卷总可以预先填充数据，并且可以在Pod之间共享数据。NFS可以被同时挂接到多个Pod中，并能同时进行写入。需要注意的是：在使用nfs存储卷之前，必须已正确部署和运行NFS服务器，并已经设置了共享目录。 创建一个pod，让其挂载上面创建的共享目录12345678910111213141516171819202122232425cat &gt;&gt; nfs-busybox &lt;&lt; EOFapiVersion: v1kind: Podmetadata: name: nfs-busybox-podspec: containers: - name: nfs-busybox-pod image: busybox imagePullPolicy: IfNotPresent command: - \"/bin/sh\" args: - \"-c\" - \"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1\" volumeMounts: - name: nfs-busybox-storage mountPath: \"/mnt\" restartPolicy: \"Never\" volumes: - name: nfs-busybox-storage nfs: path: /nfs_data_1 # NFS 共享目录 server: 192.168.56.113 # NFS ServerEOF 启动POD，一会儿POD就是completed状态，说明执行完毕。 12345kubectl apply -f nfs-busybox.yamlpod/nfs-busybox-pod created$ kubectl get podsNAME READY STATUS RESTARTS AGEnfs-busybox-pod 0/1 Completed 0 7s 我们去NFS Server共享目录查看有没有SUCCESS文件。 12ls /nfs_data_1/SUCCESS 说明当NFS作为普通存储卷时我们挂载到pod容器中产生数据将会保存到这个存储卷当中，并且删除pod不会影响数据。 如何基于NFS文件系统创建持久化存储？Provisioning: PV的预制创建有两种模式：静态和动态供给模式，他们的含义是： 静态供给模式: 需要先手动创建PV, 然后通过 PVC 申请 PV 并在 Pod 中使用，这种方式叫做静态供给（Static Provision）。动态供给模式: 只需要创建PVC，系统根据PVC创建PV, 如果没有满足 PVC 条件的 PV，会动态创建 PV。相比静态供给，动态供给(Dynamical Provision）有明显的优势：不需要提前创建 PV，减少了管理员的工作量，效率高. 动态供给是通过 StorageClass 实现的，StorageClass 定义了如何创建 PV。 下面就分别实践这两种模式的创建跟使用: NFS作为静态供给模式持久化存储卷1手动创建PV---&gt;手动创建PVC---&gt;POD挂载使用 1. 创建PV12345678910111213141516171819cat &gt;&gt; nfs-test-pv.yaml &lt;&lt; EOFapiVersion: v1kind: PersistentVolumemetadata: name: nfs-test-pv namespace: default labels: app: nfs-test-pvspec: capacity: storage: 5Gi # 指定PV容量为5G accessModes: - ReadWriteOnce # 指定访问模式为 ReadWriteOnce persistentVolumeReclaimPolicy: Recycle # 指定当 PV 的回收策略为 Recycle storageClassName: nfs # 定 PV 的 class 为 nfs。相当于为 PV 设置了一个分类，PVC 可以指定 class 申请相应 class 的 PV。 nfs: path: /nfs_data_2 # 指定 PV 在 NFS 服务器上对应的目录 server: 192.168.56.113 # NFS Server地址EOF STATUS 为 Available，表示 nfs-test-pv 就绪，可以被 PVC 申请。接下来创建 PVC nfs-test-pv-claim： 2. 创建PVC1234567891011121314cat &gt;&gt; nfs-test-pv-claim.yaml &lt;&lt; EOFapiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-test-pv-claim namespace: defaultspec: accessModes: # 存储访问模式，此能力依赖存储厂商能力 - ReadWriteOnce resources: requests: storage: 2Gi # 请求获得的pvc存储大小 storageClassName: nfs EOF 从 kubectl get pvc 和 kubectl get pv 的输出可以看到 nfs-test-pv-claim 已经 Bound 到 nfs-test-pv，申请成功。接下来就可以在 Pod 中使用存储了： 3. 创建Pod123456789101112131415161718192021cat &gt;&gt; nfs-test-pod.yaml &lt;&lt; EOFapiVersion: v1kind: Podmetadata: name: nfs-test-podspec: containers: - name: nfs-test-pod image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/nfs-data\" name: nfs-data volumes: - name: nfs-data persistentVolumeClaim: claimName: nfs-test-pv-claimEOF 与使用普通 Volume 的格式类似，在 volumes 中通过 persistentVolumeClaim 指定使用 nfs-test-pv-claim 申请的 Volume。验证 PV 是否可用： 1kubectl exec nfs-test-pod touch /nfs-data/SUCCESS ![create_result](kubernetes-storage/create_result.png) 可见，在 Pod 中创建的文件 /nfs-data/SUCCESS 确实已经保存到了 NFS 服务器目录 /nfs_data_2/中。如果不再需要使用 PV，可用删除 PVC 回收 PV: 4. 回收PVC持久化卷声明的保护PVC 保护的目的是确保由 pod 正在使用的 PVC 不会从系统中移除，因为如果被移除的话可能会导致数据丢失。注意：当 pod 状态为 Pending 并且 pod 已经分配给节点或 pod 为 Running 状态时，PVC 处于活动状态。当启用PVC保护功能时，如果用户删除了一个 pod 正在使用的 PVC，则该 PVC 不会被立即删除。PVC 的删除将被推迟，直到 PVC 不再被任何 pod 使用。您可以看到，当我直接删除上面POD正在使用的PVC时命令直接hang住了，此时虽然为 Teminatiing，但PVC 受到保护，Finalizers 列表中包含 kubernetes.io/pvc-protection： 1kubectl delete pvc nfs-test-pv-claim 等待 pod 状态变为 Terminated（删除 pod 或者等到它结束），然后检查，确认 PVC 被移除。 反之，如果一个PVC没有被pod使用则可以直接删除。 用户用完 volume 后，可以从允许回收资源的 API 中删除 PVC 对象。PersistentVolume 的回收策略告诉集群在存储卷声明释放后应如何处理该卷。目前，volume 的处理策略有: Retain，不清理, 保留 Volume（需要手动清理） Recycle，删除数据，即 rm -rf /thevolume/*（只有 NFS 和 HostPath 支持） Delete，删除存储资源，比如删除 AWS EBS 卷（只有 AWS EBS, GCE PD, Azure Disk 和 Cinder 支持） OK 以上是NFS创建静态模式创建PVC,以及PVC跟POD生命周期的一些实践，下面实践动态模式 NFS作为动态持久化存储卷利用NFS client provisioner动态提供Kubernetes后端存储卷 想要动态生成PV，需要运行一个NFS-Provisioner服务，将已配置好的NFS系统相关参数录入，并向用户提供创建PV的服务。官方推荐使用Deployment运行一个replica来实现，当然也可以使用Daemonset等其他方式，这些都在官方文档中提供了。 前提条件是有已经安装好的NFS服务器，并且NFS服务器与Kubernetes的Slave节点都能网络连通。 所有下文用到的文件来自于git clone https://github.com/kubernetes-incubator/external-storage.git 的nfs-client目录 nfs-client-provisioner 是一个Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储 PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上） PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上） 1. 获取nfs-client-provisioner配置文件12git clone https://github.com/kubernetes-incubator/external-storage.git 2. 安装部署:1. 修改deployment文件并部署 deploy/deployment.yaml需要修改的地方只有NFS服务器所在的IP地址（192.168.56.113），以及NFS服务器共享的路径（/nfs_data_3），两处都需要修改为你实际的NFS服务器和共享目录 123456789101112131415161718192021222324252627282930313233343536373839404142cat deploy/deployment.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.56.113 - name: NFS_PATH value: /nfs_data_3 volumes: - name: nfs-client-root nfs: server: 192.168.56.113 path: /nfs_data_3$ kubectl apply -f deploy/deployment.yaml # 执行部署serviceaccount/nfs-client-provisioner createddeployment.extensions/nfs-client-provisioner created 2. 修改StorageClass文件并部署 deploy/class.yaml此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致。 1234567891011121314151617181920212223cat deploy/class.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storageprovisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME'parameters: archiveOnDelete: \"false\" # 执行部署 kubectl apply -f deploy/class.yamlstorageclass.storage.k8s.io/managed-nfs-storage created# 查看StorageClasskubectl get scNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 16m# 设置这个managed-nfs-storage名字的SC为Kubernetes的默认存储后端kubectl patch storageclass managed-nfs-storage -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'$ kubectl get scNAME PROVISIONER AGEmanaged-nfs-storage (default) fuseim.pri/ifs 19m 3. 授权如果您的集群启用了RBAC，或者您正在运行OpenShift，则必须授权provisioner。 如果你在非默认的“default”名称空间/项目之外部署，可以编辑deploy/rbac.yaml或编辑`oadm policy“指令。 如果启用了RBAC需要执行如下的命令来授权。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960cat deploy/rbac.yamlkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionerrules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.iokubectl create -f deploy/rbac.yamlclusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner createdrole.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner createdrolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created 4. 执行部署:测试创建PVC123456789101112131415161718192021222324cat deploy/test-claim.yamlkind: PersistentVolumeClaimapiVersion: v1metadata: name: test-claim annotations: volume.beta.kubernetes.io/storage-class: \"managed-nfs-storage\"spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi$ kubectl create -f deploy/test-claim.yamlpersistentvolumeclaim/test-claim created$ kubectl get pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/pvc-137f0450-0048-11e9-af7a-00505621dd5b 1Mi RWX Delete Bound default/test-claim managed-nfs-storage 9mNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/test-claim Bound pvc-137f0450-0048-11e9-af7a-00505621dd5b 1Mi RWX managed-nfs-storage 9m# 以上可以看到我们的pvc已经申请成功，等待POD挂载使用 测试创建PODPOD文件如下，作用就是在test-claim的PV里touch一个SUCCESS文件。 1234567891011121314151617181920212223242526272829303132cat deploy/test-pod.yamlkind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - \"/bin/sh\" args: - \"-c\" - \"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1\" volumeMounts: - name: nfs-pvc mountPath: \"/mnt\" restartPolicy: \"Never\" volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim kubectl create -f deploy/test-pod.yamlpod/test-pod created# 启动POD，一会儿POD就是completed状态，说明执行完毕。kubectl get pod | grep test-podNAME READY STATUS RESTARTS AGEtest-pod 0/1 Completed 0 18s 在NFS服务器上的共享目录下的卷子目录中检查创建的NFS PV卷下是否有”SUCCESS” 文件。以上，说明我们部署正常，并且可以通过动态分配NFS的持久共享卷 参考https://k8smeetup.github.io/docs/concepts/storage/volumes/https://k8smeetup.github.io/docs/concepts/storage/persistent-volumes/#回收-1https://k8smeetup.github.io/docs/tasks/administer-cluster/pvc-protection/#lichuqianghttps://jimmysong.io/kubernetes-handbook/practice/using-nfs-for-persistent-storage.html","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"https://blog.sctux.cc/tags/kubernets/"},{"name":"持久化存储","slug":"持久化存储","permalink":"https://blog.sctux.cc/tags/%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/"},{"name":"Storage","slug":"Storage","permalink":"https://blog.sctux.cc/tags/Storage/"},{"name":"NFS PV静态供给","slug":"NFS-PV静态供给","permalink":"https://blog.sctux.cc/tags/NFS-PV%E9%9D%99%E6%80%81%E4%BE%9B%E7%BB%99/"},{"name":"NFS PV动态供给","slug":"NFS-PV动态供给","permalink":"https://blog.sctux.cc/tags/NFS-PV%E5%8A%A8%E6%80%81%E4%BE%9B%E7%BB%99/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"理解kubernetes中的有状态服务StatefulSet","slug":"kubernetes-statefulset","date":"2018-12-14T07:10:56.000Z","updated":"2025-09-01T01:59:08.980Z","comments":true,"path":"2018/12/14/kubernetes-statefulset/","permalink":"https://blog.sctux.cc/2018/12/14/kubernetes-statefulset/","excerpt":"概述概念StatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状态服务而设计），其应用场景包括 稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现 稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有 Cluster IP 的 Service）来实现 有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依序进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers 来实现 有序收缩，有序删除（即从 N-1 到 0） 从上面的应用场景可以发现，StatefulSet 由以下几个部分组成： 用于定义网络标志（DNS domain）的 Headless Service 用于创建 PersistentVolumes 的 volumeClaimTemplates 定义具体应用的 StatefulSet StatefulSet 中每个 Pod 的 DNS 格式为 statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local，其中 serviceName 为 Headless Service 的名字 0..N-1 为 Pod 所在的序号，从 0 开始到 N-1 statefulSetName 为 StatefulSet 的名字 namespace 为服务所在的 namespace，Headless Service 和 StatefulSet 必须在相同的 namespace .cluster.local 为 Cluster Domain 限制 给定 Pod 的存储必须由 PersistentVolume Provisioner 根据请求的 storage class 进行配置，或由管理员预先配置。 删除或 scale StatefulSet 将不会删除与 StatefulSet 相关联的 volume。 这样做是为了确保数据安全性，这通常比自动清除所有相关 StatefulSet 资源更有价值。 StatefulSets 目前要求 Headless Service 负责 Pod 的网络身份。 您有责任创建此服务。","text":"概述概念StatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状态服务而设计），其应用场景包括 稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现 稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有 Cluster IP 的 Service）来实现 有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依序进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers 来实现 有序收缩，有序删除（即从 N-1 到 0） 从上面的应用场景可以发现，StatefulSet 由以下几个部分组成： 用于定义网络标志（DNS domain）的 Headless Service 用于创建 PersistentVolumes 的 volumeClaimTemplates 定义具体应用的 StatefulSet StatefulSet 中每个 Pod 的 DNS 格式为 statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local，其中 serviceName 为 Headless Service 的名字 0..N-1 为 Pod 所在的序号，从 0 开始到 N-1 statefulSetName 为 StatefulSet 的名字 namespace 为服务所在的 namespace，Headless Service 和 StatefulSet 必须在相同的 namespace .cluster.local 为 Cluster Domain 限制 给定 Pod 的存储必须由 PersistentVolume Provisioner 根据请求的 storage class 进行配置，或由管理员预先配置。 删除或 scale StatefulSet 将不会删除与 StatefulSet 相关联的 volume。 这样做是为了确保数据安全性，这通常比自动清除所有相关 StatefulSet 资源更有价值。 StatefulSets 目前要求 Headless Service 负责 Pod 的网络身份。 您有责任创建此服务。 部署Statefulset服务首先我们下面使用的是用前面博文中使用到的NFS做动态供给PVC作为持久化存储。那么他的创建步骤这里不在赘述，我们在编排yaml文件中申请即可。 获取动态卷信息1234567891011$ kubectl describe storageclass managed-nfs-storageName: managed-nfs-storageIsDefaultClass: NoAnnotations: &lt;none&gt;Provisioner: fuseim.pri/ifsParameters: archiveOnDelete=falseAllowVolumeExpansion: &lt;unset&gt;MountOptions: &lt;none&gt;ReclaimPolicy: DeleteVolumeBindingMode: ImmediateEvents: &lt;none&gt; 编写service一个名为 nginx 的 headless service，用于控制网络域。 1234567891011121314151617181920$ cat &gt;&gt; statefulset_service.yaml &lt;&lt; EOFapiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginxEOF# 执行创建$ kubectl create -f statefulset_service.yamlservice/nginx created 编写statefulset yaml 一个名为 web 的 StatefulSet，它的 Spec 中指定在有 2 个运行 nginx 容器的 Pod。 volumeClaimTemplates 使用 PersistentVolume Provisioner 提供的 PersistentVolumes 作为稳定存储。 volumeClaimTemplates: 表示一类PVC的模板，系统会根据Statefulset配置的replicas数量，创建相应数量的PVC。这些PVC除了名字不一样之外其他配置都是一样的。 下面storageClassName配置为我之前通过NFS创建的。 12345678910111213141516171819202122232425262728293031323334$ cat &gt;&gt; statefulset.yaml &lt;&lt; EOFapiVersion: apps/v1beta1kind: StatefulSetmetadata: name: webspec: serviceName: \"nginx\" replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www-disk mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www-disk annotations: volume.beta.kubernetes.io/storage-class: \"managed-nfs-storage\" spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"managed-nfs-storage\" resources: requests: storage: 1GiEOF 验证服务伸缩性创建Statefulset服务：123456789101112131415# 执行创建$ kubectl create -f statefulset.yamlstatefulset.apps/web created$ kubectl get podNAME READY STATUS RESTARTS AGEpod/nfs-client-provisioner-6c6997c674-z7m9r 1/1 Running 0 18mpod/web-0 1/1 Running 0 13mpod/web-1 1/1 Running 0 12m$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/www-disk-web-0 Bound pvc-a440540e-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 13mpersistentvolumeclaim/www-disk-web-1 Bound pvc-af5c7006-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 12m 扩容服务到3个Pod，显示会创建新的云盘卷：123456789101112131415$ kubectl scale sts web --replicas=3statefulset.apps/web scaled$ kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-6c6997c674-z7m9r 1/1 Running 0 19mweb-0 1/1 Running 0 14mweb-1 1/1 Running 0 14mweb-2 1/1 Running 0 40s$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEwww-disk-web-0 Bound pvc-a440540e-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 14mwww-disk-web-1 Bound pvc-af5c7006-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 14mwww-disk-web-2 Bound pvc-97afce59-01df-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 38s 缩容服务到1个Pod，显示pvc/pv并不会一同删除：1234567891011121314$ kubectl scale sts web --replicas=1statefulset.apps/web scaled$ kubectl get podNAME READY STATUS RESTARTS AGEnfs-client-provisioner-6c6997c674-z7m9r 1/1 Running 0 21mweb-0 1/1 Running 0 16m$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEwww-disk-web-0 Bound pvc-a440540e-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 16mwww-disk-web-1 Bound pvc-af5c7006-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 16mwww-disk-web-2 Bound pvc-97afce59-01df-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 2m 再次扩容到3个Pod，新的pod会复用原来的PVC/PV：123456789101112131415$ kubectl scale sts web --replicas=3statefulset.apps/web scaled$ kubectl get podNAME READY STATUS RESTARTS AGEpod/nfs-client-provisioner-6c6997c674-z7m9r 1/1 Running 1 46mpod/web-0 1/1 Running 0 3mpod/web-1 1/1 Running 0 1mpod/web-2 1/1 Running 0 18s$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/www-disk-web-0 Bound pvc-a440540e-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 41mpersistentvolumeclaim/www-disk-web-1 Bound pvc-af5c7006-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 40mpersistentvolumeclaim/www-disk-web-2 Bound pvc-97afce59-01df-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 27m 删除一个pod/web0前，Pod引用PVC：www-disk-web-012kubectl describe pod/web-0 | grep ClaimName ClaimName: www-disk-web-0 删除Pod后，重新创建的Pod名字与删除的一致，且使用同一个PVC：123456789101112131415161718$ kubectl delete pod/web-0pod \"web-0\" deleted$ kubectl get podNAME READY STATUS RESTARTS AGEpod/nfs-client-provisioner-6c6997c674-z7m9r 1/1 Running 1 49mpod/web-0 1/1 Running 0 10spod/web-1 1/1 Running 0 4mpod/web-2 1/1 Running 0 3m$ kubectl describe pod/web-0 | grep ClaimName ClaimName: www-disk-web-0$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/www-disk-web-0 Bound pvc-a440540e-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 44mpersistentvolumeclaim/www-disk-web-1 Bound pvc-af5c7006-01dd-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 43mpersistentvolumeclaim/www-disk-web-2 Bound pvc-97afce59-01df-11e9-b006-00505621dd5b 1Gi RWO managed-nfs-storage 30m 验证服务高可用性共享持久卷中新建文件12$ kubectl exec web-1 ls /usr/share/nginx/htmlstatefulset 删除Pod，验证数据持久性：12345678$ kubectl delete pod web-1pod \"web-1\" deleted# 待新的pod创建之后再次检查文件是否还在$ kubectl exec web-1 ls /usr/share/nginx/htmlstatefulset# 以上说明删除pod对pvc没有任何的影响，我们的数据还是保存着的。 数据保存:上面也说啦, 删除或 scale StatefulSet 将不会删除与 StatefulSet 相关联的 volume。 这样做是为了确保数据安全性，这通常比自动清除所有相关 StatefulSet 资源更有价值。 所以我到NFS Server上面check一下: 可见 我们在pod/web-1上创建的文件还是保存着的。 附:该yaml为华为云那边的PV申请，以及StatefulSet应用的创建 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566---# PVC存储申请apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-evs-auto-example namespace: default annotations: volume.beta.kubernetes.io/storage-class: sata volume.beta.kubernetes.io/storage-provisioner: flexvolume-huawei.com/fuxivol labels: failure-domain.beta.kubernetes.io/region: cn-north-1 failure-domain.beta.kubernetes.io/zone: cn-north-1aspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi---# StatefulSet应用创建apiVersion: apps/v1kind: StatefulSetmetadata: name: cce21days-app11-guomaoqiu namespace: defaultspec: podManagementPolicy: OrderedReady serviceName: cce21days-app11-guomaoqiu replicas: 3 revisionHistoryLimit: 10 selector: matchLabels: app: cce21days-app11-guomaoqiu failure-domain.beta.kubernetes.io/region: cn-north-1 failure-domain.beta.kubernetes.io/zone: cn-north-1a template: metadata: labels: app: cce21days-app11-guomaoqiu failure-domain.beta.kubernetes.io/region: cn-north-1 failure-domain.beta.kubernetes.io/zone: cn-north-1a spec: affinity: {} containers: - image: 100.125.0.198:20202/guomaoqiu/tank:1.0.1 imagePullPolicy: IfNotPresent name: container-0 resources: {} volumeMounts: - mountPath: /tmp name: pvc-evs-example dnsPolicy: ClusterFirst imagePullSecrets: - name: default-secret restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: pvc-evs-example persistentVolumeClaim: claimName: pvc-evs-auto-example updateStrategy: type: RollingUpdate","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"StatefulSet","slug":"StatefulSet","permalink":"https://blog.sctux.cc/tags/StatefulSet/"},{"name":"有状态","slug":"有状态","permalink":"https://blog.sctux.cc/tags/%E6%9C%89%E7%8A%B6%E6%80%81/"},{"name":"持久化存储","slug":"持久化存储","permalink":"https://blog.sctux.cc/tags/%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"管理Kubernetes日志","slug":"kubernetes-logs","date":"2018-12-13T12:47:40.000Z","updated":"2025-09-01T01:59:08.984Z","comments":true,"path":"2018/12/13/kubernetes-logs/","permalink":"https://blog.sctux.cc/2018/12/13/kubernetes-logs/","excerpt":"概述日志是我们在运维部署当中非常重要的一个东西，对于我个人工作经验而言，一般出现问题，第一步事情就是查看日志，其次是服务器资源查看这样去定位问题，那么在kubernets集群当中呢，我主要给其分了两种类型: kubernetes组件日志 运行于kubernetes上的容器应用日志 kubernetes组件日志我们知道在kubernetes集群是有多个组件组成，协同为我们提供一个运行容器的环境。当运行当中出现问题，也是需要查看对应组件的日志，进行问题排查、处理。那么常见的日志如下： 12345/var/log/kube-apiserver.log/var/log/kube-proxy.log/var/log/kube-controller-manager.log/var/log/kube-scheduler.log/var/log/kubelet.log 当然根据搭建集群的方式不同，我们配置的日志目录也不尽相同，所以只是列举一下；如果组件的安装方式由systemd来管理的话 我们还可以通过以下命令进行排错 123journalctl -u kubelet或systemctl status kubelet -l 如果使用的是kuernetes插件式方式部署的组件则使用以下命令 1kubectl logs -f &lt;组件名称&gt;","text":"概述日志是我们在运维部署当中非常重要的一个东西，对于我个人工作经验而言，一般出现问题，第一步事情就是查看日志，其次是服务器资源查看这样去定位问题，那么在kubernets集群当中呢，我主要给其分了两种类型: kubernetes组件日志 运行于kubernetes上的容器应用日志 kubernetes组件日志我们知道在kubernetes集群是有多个组件组成，协同为我们提供一个运行容器的环境。当运行当中出现问题，也是需要查看对应组件的日志，进行问题排查、处理。那么常见的日志如下： 12345/var/log/kube-apiserver.log/var/log/kube-proxy.log/var/log/kube-controller-manager.log/var/log/kube-scheduler.log/var/log/kubelet.log 当然根据搭建集群的方式不同，我们配置的日志目录也不尽相同，所以只是列举一下；如果组件的安装方式由systemd来管理的话 我们还可以通过以下命令进行排错 123journalctl -u kubelet或systemctl status kubelet -l 如果使用的是kuernetes插件式方式部署的组件则使用以下命令 1kubectl logs -f &lt;组件名称&gt; 运行于kubernetes上的容器应用日志运行于kubernetes上的，比如一个nginx容器；我们如何查看这个应用的日志呢？ 从容器标准输出截获用法类似于docker 1kubectl logs -f {POD_NAME} -c {Container_NAME} 进入容器进行查看12kubectl exec -it {POD_NAME} -c {Container_NAME} /bin/shdocker exec -it {Container_NAME} /bin/sh 将日志文件挂载到主机目录比如我要把nginx的日志挂载到运行于该POD的宿主机的某个目录 1234567891011121314151617181920212223242526272829303132333435363738# 编写yaml文件# 这里我们知道手写一个yaml是很烦的，而且那么多关键词不是那么容易记住，# 那么此时就需要在现有环境中找一个pod然后把他的yaml导出再做修改# 如果环境中还是没有pod，那就直接run一个$ kubectl get pod nginx-67ccc95d8c-fd5nk -o yaml --export &gt; nginx.yaml# 删除一些不必要的字段$ vim nginx.yaml apiVersion: v1kind: Podmetadata: name: nginx labels: run: nginxspec: containers: - image: nginx:latest imagePullPolicy: IfNotPresent name: nginx volumeMounts: - mountPath: /var/log/nginx # nginx应用默认日志目录 name: nginx-log-volume volumes: - name: nginx-log-volume hostPath: path: /var/k8s/log # 宿主机目录 $ kubectl create -f nginx.yamlpod \"nginx\" created# 查看宿主机，这样容器的日志目录我就挂到了宿主# 而不用在进入容器中查看了$ ll /var/k8s/log/ total 0-rw-r----- 1 root root 0 Dec 18 15:17 access.log-rw-r----- 1 root root 0 Dec 18 15:17 error.log 注意点: 写yaml的时候一定要养成导出现有资源类型的yaml的习惯再去修改；这样能避免手动编写时格式的一些问题，如果关键字记不住那就一定要用kubectl explain获取资源文档","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/tags/kubernetes/"},{"name":"log","slug":"log","permalink":"https://blog.sctux.cc/tags/log/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"理解kubernetes中Pod访问方式","slug":"kubernetes_pod_access","date":"2018-12-12T10:39:11.000Z","updated":"2025-09-01T01:59:08.959Z","comments":true,"path":"2018/12/12/kubernetes_pod_access/","permalink":"https://blog.sctux.cc/2018/12/12/kubernetes_pod_access/","excerpt":"概述Pod的IP是在docker0网段动态分配的，当发生重启，扩容等操作时，IP地址会随之变化。当某个Pod(frontend)需要去访问其依赖的另外一组Pod(backend)时，如果backend的IP发生变化时，如何保证fronted到backend的正常通信变的非常重要。由此，引出了Service的概念。这里docker0是一个网桥，docker daemon启动container时会根据docker0的网段来划粉container的IP地址Docker网络在实际生产环境中，对Service的访问可能会有两种来源：Kubernetes集群内部的程序（Pod）和Kubernetes集群外部，为了满足上述的场景，Kubernetes service有以下三种类型： ClusterIP:提供一个集群内部的虚拟IP以供Pod访问。 NodePort:在每个Node上打开一个端口以供外部访问。 LoadBalancer:通过外部的负载均衡器来访问。 那么它是怎么实现的，通过下面的示例来理解。(创建方式可以是通过yaml文件或者是命令行方式，这里为了理解我先用命令行方式创建，如果不合适的地方我们通过kubectl edit (RESOURCE/NAME | -f FILENAME) [options]这种方式先修改) 其次，还需要理解NodePort,TargetPort以及port他们的区别:NodePort外部机器可访问的端口。比如一个Web应用需要被其他用户访问，那么需要配置type=NodePort，而且配置nodePort=30001，那么其他机器就可以通过浏览器访问scheme://node:30001访问到该服务，例如http://node:30001。例如MySQL数据库可能不需要被外界访问，只需被内部服务访问，那么不必设置NodePort TargetPort容器的端口（最根本的端口入口），与制作容器时暴露的端口一致（DockerFile中EXPOSE），例如docker.io官方的nginx暴露的是80端口。docker.io官方的nginx容器的DockerFile参考https://github.com/nginxinc/docker-nginx 123456789101112apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: type: NodePort # 有配置NodePort，外部流量可访问k8s中的服务 ports: - port: 30080 # 服务访问端口 targetPort: 80 # 容器端口 nodePort: 30001 # NodePort selector: name: nginx-pod port kubernetes中的服务之间访问的端口，尽管mysql容器暴露了3306端口（参考https://github.com/docker-library/mysql/的DockerFile），但是集群内其他容器需要通过33306端口访问该服务，外部机器不能访问mysql服务，因为他没有配置NodePort类型 12345678910apiVersion: v1kind: Servicemetadata: name: mysql-servicespec: ports: - port: 33306 targetPort: 3306 selector: name: mysql-pod","text":"概述Pod的IP是在docker0网段动态分配的，当发生重启，扩容等操作时，IP地址会随之变化。当某个Pod(frontend)需要去访问其依赖的另外一组Pod(backend)时，如果backend的IP发生变化时，如何保证fronted到backend的正常通信变的非常重要。由此，引出了Service的概念。这里docker0是一个网桥，docker daemon启动container时会根据docker0的网段来划粉container的IP地址Docker网络在实际生产环境中，对Service的访问可能会有两种来源：Kubernetes集群内部的程序（Pod）和Kubernetes集群外部，为了满足上述的场景，Kubernetes service有以下三种类型： ClusterIP:提供一个集群内部的虚拟IP以供Pod访问。 NodePort:在每个Node上打开一个端口以供外部访问。 LoadBalancer:通过外部的负载均衡器来访问。 那么它是怎么实现的，通过下面的示例来理解。(创建方式可以是通过yaml文件或者是命令行方式，这里为了理解我先用命令行方式创建，如果不合适的地方我们通过kubectl edit (RESOURCE/NAME | -f FILENAME) [options]这种方式先修改) 其次，还需要理解NodePort,TargetPort以及port他们的区别:NodePort外部机器可访问的端口。比如一个Web应用需要被其他用户访问，那么需要配置type=NodePort，而且配置nodePort=30001，那么其他机器就可以通过浏览器访问scheme://node:30001访问到该服务，例如http://node:30001。例如MySQL数据库可能不需要被外界访问，只需被内部服务访问，那么不必设置NodePort TargetPort容器的端口（最根本的端口入口），与制作容器时暴露的端口一致（DockerFile中EXPOSE），例如docker.io官方的nginx暴露的是80端口。docker.io官方的nginx容器的DockerFile参考https://github.com/nginxinc/docker-nginx 123456789101112apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: type: NodePort # 有配置NodePort，外部流量可访问k8s中的服务 ports: - port: 30080 # 服务访问端口 targetPort: 80 # 容器端口 nodePort: 30001 # NodePort selector: name: nginx-pod port kubernetes中的服务之间访问的端口，尽管mysql容器暴露了3306端口（参考https://github.com/docker-library/mysql/的DockerFile），但是集群内其他容器需要通过33306端口访问该服务，外部机器不能访问mysql服务，因为他没有配置NodePort类型 12345678910apiVersion: v1kind: Servicemetadata: name: mysql-servicespec: ports: - port: 33306 targetPort: 3306 selector: name: mysql-pod 一、Create service (type: ClusterIP)此模式会提供一个集群内部的虚拟IP（与Pod不在同一网段)，以供集群内部的pod之间通信使用。ClusterIP也是Kubernetes service的默认类型。 为了实现图上的功能主要需要以下几个组件的协同工作 apiserver 用户通过kubectl命令向apiserver发送创建service的命令，apiserver接收到请求以后将数据存储到etcd中。 kube-proxy kubernetes的每个节点中都有一个叫做kube-proxy的进程，这个进程负责感知service，pod的变化，并将变化的信息写入本地的iptables中。 iptables 使用NAT等技术将virtualIP的流量转至endpoint中。 下面我们实际发布一个Service，能够更清晰的了解到Service是如何工作的。 1. 通过命令行方式创建service,类型为clusterip12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455$ kubectl create service clusterip my-svc-cp --tcp=8080:80$ kubectl describe service/my-svc-cpName: my-svc-cpNamespace: defaultLabels: app=my-svc-cpAnnotations: &lt;none&gt;Selector: app=my-svc-cpType: ClusterIP # 类型IP: 10.247.52.210Port: 80-8080 8080/TCP # 映射到集群的端口TargetPort: 80/TCP # 目标pod暴露端口Endpoints: &lt;none&gt; # 此时还没有后端容器Session Affinity: NoneEvents: &lt;none&gt;$ kubectl get svc -o widemy-svc-cp ClusterIP 10.247.52.210 &lt;none&gt; 80/TCP 6s app=my-svc-cp# 以上我创建了一个svc, 那么他是为pod服务的,# 那么 pod跟service是通过`selector label`来做关联的, 所以我们还需要对这个svc做下调整$ kubectl edit svc my-svc-cp# Please edit the object below. Lines beginning with a '#' will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1kind: Servicemetadata: creationTimestamp: 2018-12-12T17:41:54Z labels: app: my-svc-cp name: my-svc-cp namespace: default resourceVersion: \"32742\" selfLink: /api/v1/namespaces/default/services/my-svc-cp uid: 372f6f2c-fe35-11e8-b967-fa163e874e90spec: clusterIP: 10.247.52.210 ports: - name: 80-8080 port: 8080 protocol: TCP targetPort: 80 selector: app: my-svc-cp-pod # 这里我修改了selector ，需要与pod name与之对应起来 sessionAffinity: None type: ClusterIPstatus: loadBalancer: {}# :wq 保存退出# kubectl describe service/my-svc-cp 查看修改是否成功 2.创建pod注意selector label 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485$ vim my-svc-cp-pod.yamlapiVersion: v1kind: Podmetadata: name: my-svc-cp-pod labels: app: my-svc-cp-podspec: containers: - image: nginx:latest imagePullPolicy: Always name: nginx # 注意这里的亲和性是为了将pod调度至有外网的node节点便于pull镜像 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.253.183 restartPolicy: Always schedulerName: default-scheduler# 执行:$ kubectl create -f my-svc-cp-pod.yaml$ kubectl get pods,svc,endpoints,deployment -o wideNAME READY STATUS RESTARTS AGE IP NODEpo/my-svc-cp-pod 1/1 Running 0 36m 172.16.0.36 192.168.253.183NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORsvc/kubernetes ClusterIP 10.247.0.1 &lt;none&gt; 443/TCP 1h &lt;none&gt;svc/my-svc-cp ClusterIP 10.247.52.210 &lt;none&gt; 8080/TCP 3m app=my-svc-cp-podNAME ENDPOINTS AGEep/kubernetes 192.168.103.50:5444,192.168.174.46:5444,192.168.236.124:5444 1hep/my-svc-cp 172.16.0.36:80 3m# 再次查看service是否关联到了pod$ kubectl describe ep/my-svc-cpName: my-svc-cpNamespace: defaultLabels: app=my-svc-cpAnnotations: &lt;none&gt;Subsets: Addresses: 172.16.0.36 # 可见endpoints中已经有了后端的那个pod NotReadyAddresses: &lt;none&gt; Ports: Name Port Protocol ---- ---- -------- 80-8080 80 TCPEvents: &lt;none&gt;# 集群内部通过Cluster IP访问nginx服务# 指定了端口映射到Cluster IP$ curl -I 10.247.52.210:8080HTTP/1.1 200 OKServer: nginx/1.15.5Date: Wed, 12 Dec 2018 17:48:56 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 02 Oct 2018 14:49:27 GMTConnection: keep-aliveETag: \"5bb38577-264\"Accept-Ranges: bytes# 集群内部通过POD IP访问nginx服务# 因为pod暴露的是80 默认不用加端口$ curl -I 172.16.0.36HTTP/1.1 200 OKServer: nginx/1.15.5Date: Wed, 12 Dec 2018 17:47:52 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 02 Oct 2018 14:49:27 GMTConnection: keep-aliveETag: \"5bb38577-264\"Accept-Ranges: bytes# 通过这种方式也只是集群内部能够访问到，如果集群外部要访问我们的pod又该如何做呢？# 下面继续 二、Create service (type: NodePort)1.通过命令行方式创建service,类型为nodeportCluster service 的 IP 地址是虚拟的，因此，只能从node节点上使用该IP 地址访问应用。为了从集群外访问应用，K8S 提供了使用 node 节点的IP 地址访问应用的方式。 基本上，NodePort 服务与普通的 “ClusterIP” 服务 YAML 定义有两点区别。 首先，type 是 “NodePort”。还有一个称为 nodePort 的附加端口，指定在节点上打开哪个端口。 如果你不指定这个端口，它会选择一个随机端口。 下图中是 32591. 该端口号的范围是 kube-apiserver 的启动参数 –service-node-port-range指定的，在当前测试环境中其值是 30000-32767。 1234567891011121314151617181920212223242526272829303132333435363738394041$ kubectl create service nodeport my-svc-np --tcp=8080:80[root@linux-node1 ~]# kubectl describe service my-svc-npName: my-svc-npNamespace: defaultLabels: app=my-svc-npAnnotations: &lt;none&gt;Selector: app=my-svc-npType: NodePort # 类型IP: 10.103.115.169 # 分配的ClusterIPPort: 8080-80 8080/TCP # 容器映射到node节点的端口TargetPort: 80/TCP # 容器将要暴露的端口NodePort: 8080-80 32591/TCP # 附加端口Endpoints: &lt;none&gt; # 此时后端没有podSession Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt;# 有了svc我们就需要创建一个或者一组pod来跟这个svc建立连接# 像前面一样我们需要更改这个svc的选择器：$ kubectl edit svc my-svc-np............ externalTrafficPolicy: Cluster ports: - name: 8080-80 nodePort: 32591 port: 8080 protocol: TCP targetPort: 80 selector: app: my-svc-np-pod # 与即将创建的pod对应上 sessionAffinity: None type: NodePortstatus: loadBalancer: {}............# :wq保存退出 2.创建pod与之建立关系123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# 我这里通过命令行方式创建一个deployment的3个副本的nginx pod# 1. 首先通过命令行生成yaml文件$ kubectl run nginx-deployment --image=nginx --replicas=3 --dry-run -o yaml &gt; nginx-deployment.yaml# 2. 进行调整$ cat &gt;&gt; nginx-deployment.yaml &lt;&lt; EOFapiVersion: apps/v1beta1kind: Deploymentmetadata: creationTimestamp: null labels: run: nginx-deployment name: nginx-deploymentspec: replicas: 3 selector: matchLabels: app: my-svc-np-pod # 此处修改为svc selector相同 strategy: {} template: metadata: creationTimestamp: null labels: app: my-svc-np-pod # 此处修改为svc selector相同 spec: containers: - image: nginx name: nginx-deployment resources: {}status: {}EOF# 3. 执行$ kubectl create -f nginx-deployment.yamldeployment.apps/nginx-deployment created# 4. 查看状态:$ kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-6794c779fb-8x92g 1/1 Running 0 4m 10.244.1.19 k8s-m2 &lt;none&gt;nginx-deployment-6794c779fb-rf8qj 1/1 Running 0 4m 10.244.0.19 k8s-m3 &lt;none&gt;nginx-deployment-6794c779fb-tdscx 1/1 Running 0 4m 10.244.2.35 k8s-m1 &lt;none&gt;$ kubectl describe svc my-svc-npName: my-svc-npNamespace: defaultLabels: app=my-svc-npAnnotations: &lt;none&gt;Selector: app=my-svc-np-podType: NodePortIP: 10.103.115.169Port: 8080-80 8080/TCPTargetPort: 80/TCPNodePort: 8080-80 32591/TCPEndpoints: 10.244.0.19:80,10.244.1.19:80,10.244.2.35:80 # 可以看到我们的三个endpointSession Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt;$ kubectl get svc my-svc-npNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmy-svc-np NodePort 10.103.115.169 &lt;none&gt; 8080:32591/TCP 42m# 5.如何访问?# a.集群内部访问pod ip$ curl -I 10.244.0.19HTTP/1.1 200 OKServer: nginx/1.15.7Date: Thu, 13 Dec 2018 08:43:25 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 27 Nov 2018 12:31:56 GMTConnection: keep-aliveETag: \"5bfd393c-264\"Accept-Ranges: bytes# b.集群内部clusterip+端口$ curl -I 10.103.115.169:8080HTTP/1.1 200 OKServer: nginx/1.15.7Date: Thu, 13 Dec 2018 08:42:53 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 27 Nov 2018 12:31:56 GMTConnection: keep-aliveETag: \"5bfd393c-264\"Accept-Ranges: bytes# c.集群外部访问nodeip+附加端口curl -I 192.168.56.111:32591HTTP/1.1 200 OKServer: nginx/1.15.7Date: Thu, 13 Dec 2018 08:45:39 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 27 Nov 2018 12:31:56 GMTConnection: keep-aliveETag: \"5bfd393c-264\"Accept-Ranges: bytes 以上就是nodeport网络类型的简单实现 三、Create service (type: HeadlessClusterIP)所谓的HeadlessClusterIP 就是我们在创建这种网络类型的时候将 spec.clusterIP 设置成 None,这样k8s就不会给service分配clusterIp了。但指定了selector，那么endpoints controller还是会创建Endpoints的，会创建一个新的DNS记录直接指向这个service描述的后端pod。否则，不会创建Endpoints记录。这种ClusterIP，kube-proxy 并不处理此类服务，因为没有load balancing或 proxy 代理设置，在访问服务的时候回返回后端的全部的Pods IP地址，主要用于开发者自己根据pods进行负载均衡器的开发(设置了selector)。 下面通过实践理解 1.通过命令行方式创建svc,类型为headlessCluserIP12345678910111213$ kubectl create svc clusterip my-svc-headless --clusterip=\"None\" --tcp=8080:80service/my-svc-headless created$ kubectl describe svc my-svc-headlessName: my-svc-headlessNamespace: defaultLabels: app=my-svc-headlessAnnotations: &lt;none&gt;Selector: app=my-svc-headlessType: ClusterIPIP: NoneSession Affinity: NoneEvents: &lt;none&gt; 2. 创建pod与之关联：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586$ cat nginx-deployment.yamlapiVersion: apps/v1beta1kind: Deploymentmetadata: creationTimestamp: null labels: run: nginx-deployment name: nginx-deploymentspec: replicas: 3 selector: matchLabels: app: my-svc-headless # 对应到svc strategy: {} template: metadata: creationTimestamp: null labels: app: my-svc-headless # 对应到svc spec: containers: - image: nginx name: nginx-deployment resources: {}status: {}$ kubectl create -f nginx-deployment.yamldeployment.apps/nginx-deployment created$ [root@k8s-m1 ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx-deployment-54bfc79477-b78c6 1/1 Running 0 40s 10.244.1.21 k8s-m2 &lt;none&gt;nginx-deployment-54bfc79477-dvp2t 1/1 Running 0 40s 10.244.0.22 k8s-m3 &lt;none&gt;nginx-deployment-54bfc79477-x6v7s 1/1 Running 0 40s 10.244.2.38 k8s-m1 &lt;none&gt;$ kubectl describe svc my-svc-headlessName: my-svc-headlessNamespace: defaultLabels: app=my-svc-headlessAnnotations: &lt;none&gt;Selector: app=my-svc-headlessType: ClusterIPIP: NonePort: 8080-80 8080/TCPTargetPort: 80/TCPEndpoints: 10.244.0.20:80,10.244.1.20:80,10.244.2.36:80Session Affinity: NoneEvents: &lt;none&gt;# 以上可以看出后端的pod列表已经加到该svc# 我们看通过内部域名是否能够解析访问到pod# 1.我们先获取到dns服务的IP#kubectl get svc -n kube-system | grep dnskube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP 13d# 2.登录到pod 内部获取dns域$ kubectl exec -it nginx-deployment-54bfc79477-b78c6 -- cat /etc/resolv.confnameserver 10.96.0.10search default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5# 3.指定dns服务器并查询该域名：`nginx-deployment.default.svc.cluster.local`$ dig @10.96.0.10 my-svc-headless.default.svc.cluster.local; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-72.el7 &lt;&lt;&gt;&gt; @10.96.0.10 my-svc-headless.default.svc.cluster.local; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 41743;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;my-svc-headless.default.svc.cluster.local. IN A;; ANSWER SECTION:my-svc-headless.default.svc.cluster.local. 30 IN A 10.244.0.22my-svc-headless.default.svc.cluster.local. 30 IN A 10.244.1.21my-svc-headless.default.svc.cluster.local. 30 IN A 10.244.2.38;; Query time: 29 msec;; SERVER: 10.96.0.10#53(10.96.0.10);; WHEN: Thu Dec 13 04:37:13 EST 2018;; MSG SIZE rcvd: 107 由上可见 dns服务为service和pod生成不同格式的DNS记录Service A记录：生成my-svc.my-namespace.svc.cluster.local域名，解析成 IP 地址，分为两种情况： 1.普通 Service：解析成 ClusterIP 2.Headless Service：解析为指定 Pod的IP列表(上述示例就是headless) SRV记录：为命名的端口（普通 Service 或 Headless Service）生成_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local的域名 Pod A记录：生成域名 pod-ip.my-namespace.pod.cluster.local 上述示例个人理解是 这种无头ClusterIP类型 在其集群内部直接可以通过该域名去访问pod，并且该域名也起到了通过dns做负载的能力。 其他访问法方式可查阅官网学习,此篇博文主要是再次学习巩固一下知识~诸如ingress这种访问方式之前也在学习实践过程中已经实现过了，感兴趣可以撩我","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/tags/kubernetes/"},{"name":"ClusterIP","slug":"ClusterIP","permalink":"https://blog.sctux.cc/tags/ClusterIP/"},{"name":"NodePort","slug":"NodePort","permalink":"https://blog.sctux.cc/tags/NodePort/"},{"name":"TargetPort","slug":"TargetPort","permalink":"https://blog.sctux.cc/tags/TargetPort/"},{"name":"Port","slug":"Port","permalink":"https://blog.sctux.cc/tags/Port/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"Kubernetes中deployment升级回滚","slug":"kubernetes-deployment-rollingupdate","date":"2018-12-10T14:49:01.000Z","updated":"2025-09-01T01:59:08.885Z","comments":true,"path":"2018/12/10/kubernetes-deployment-rollingupdate/","permalink":"https://blog.sctux.cc/2018/12/10/kubernetes-deployment-rollingupdate/","excerpt":"在kubernetes中使用deployment管理rs时，可以利用deployment滚动升级的特性，达到服务零停止升级的目的 命令行创建一个deploymen1kubectl run nginx --image=nginx 多副本deployment1kubectl run nginx --image=nginx --replicas=2 --record 滚动升级 （可以加上–record参数可以记录命令）12345678kubectl set image deploy/nginx nginx=nginx:1.9.1# 滚动策略:kubectl edit deployment nginx strategy: rollingUpdate: maxSurge: 1 # 升级过程中可以比预设的 pod 的数量多出的个数，默认值是25% maxUnavailable: 1 # 最多有几个 pod 处于无法工作的状态，默认值是25% 滚动升级状态查看1kubectl rollout status deploy/nginx 查看升级历史","text":"在kubernetes中使用deployment管理rs时，可以利用deployment滚动升级的特性，达到服务零停止升级的目的 命令行创建一个deploymen1kubectl run nginx --image=nginx 多副本deployment1kubectl run nginx --image=nginx --replicas=2 --record 滚动升级 （可以加上–record参数可以记录命令）12345678kubectl set image deploy/nginx nginx=nginx:1.9.1# 滚动策略:kubectl edit deployment nginx strategy: rollingUpdate: maxSurge: 1 # 升级过程中可以比预设的 pod 的数量多出的个数，默认值是25% maxUnavailable: 1 # 最多有几个 pod 处于无法工作的状态，默认值是25% 滚动升级状态查看1kubectl rollout status deploy/nginx 查看升级历史123kubectl rollout history deploy/nginx# 查看单个revision 的详细信息：kubectl rollout history deploy/nginx --revision=2 资源调整:123456789101112131415161718192021222324252627kubectl set resources deploy/nginx -c=nginx --limits=cpu=100m,memory=200m# 此时在查看历史状态:$ kubectl rollout history deploy/nginxdeployments \"nginx\"REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;3 &lt;none&gt;# 查看revision 3 的内容,可以看出刚才调整了pod的资源记录了到了升级历史当中$ kubectl rollout history deploy/nginx --revision=3eployments \"nginx\" with revision #3Pod Template: Labels: pod-template-hash=440294079 run=nginx Containers: nginx: Image: nginx:1.9.1 Port: &lt;none&gt; Limits: cpu: 100m memory: 200m Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; 回滚操作:123# 回滚到第二个版本去$ kubectl rollout undo deploy nginx --to-revision=2deployment \"nginx\" 应用弹性伸缩:12$ kubectl scale deploy nginx --replicas=10deployment \"nginx\" scaled 如果集群中对接了heapster，和HPA联动后，可以通过autoscale自动弹性伸缩 1$ kubectl autoscale deployment nginx --min=10 --max=15 --cpu-percent=80 暂停和恢复Deployment您可以在发出一次或多次更新前暂停一个 Deployment，然后再恢复它。这样您就能多次暂停和恢复 Deployment，在此期间进行一些更新，修复工作，而不会发出不必要的 rollout。 例如使用刚刚创建 Deployment： 1234$ kubectl get deploy nginxNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEnginx 3 3 3 3 27s 使用以下命令暂停 Deployment： 12$ kubectl rollout pause deployment/nginxdeployment \"nginx\" paused 然后更新 Deplyment中的镜像： 123$ kubectl set image deploy/nginx nginx=nginx:1.9.1deployment \"nginx-deployment\" image updated 您可以进行任意多次更新，例如更新使用的资源： 123$ kubectl set resources deployment nginx -c=nginx --limits=cpu=200m,memory=256Mideployment \"nginx\" resource requirements updated Deployment 暂停前的初始状态将继续它的功能，而不会对 Deployment 的更新产生任何影响，只要 Deployment是暂停的。那么新的更新或者改动的结果将是在暂停期间所有做的操作都是顺序生效取相同操作的最后一步，比如,暂停后我做了以下操作:1.更新了版本: 1.9.12.更新了资源: memory=222Mi3.又更新了版本: 1.9.34.又调整了资源: memory=211Mi 最后，恢复这个 Deployment，观察完成更新的 ReplicaSet 已经创建出来了： 注意： 在恢复 Deployment 之前您无法回退一个已经暂停的 Deployment。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}],"tags":[{"name":"deployment","slug":"deployment","permalink":"https://blog.sctux.cc/tags/deployment/"},{"name":"rolling","slug":"rolling","permalink":"https://blog.sctux.cc/tags/rolling/"},{"name":"update","slug":"update","permalink":"https://blog.sctux.cc/tags/update/"},{"name":"pausing","slug":"pausing","permalink":"https://blog.sctux.cc/tags/pausing/"},{"name":"resuming","slug":"resuming","permalink":"https://blog.sctux.cc/tags/resuming/"}],"keywords":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://blog.sctux.cc/categories/kubernetes/"}]},{"title":"K8s Yaml编写小技巧","slug":"k8s-yaml-bian-xie-xiao-ji-qiao","date":"2018-12-09T15:31:58.000Z","updated":"2025-09-01T01:59:08.973Z","comments":true,"path":"2018/12/09/k8s-yaml-bian-xie-xiao-ji-qiao/","permalink":"https://blog.sctux.cc/2018/12/09/k8s-yaml-bian-xie-xiao-ji-qiao/","excerpt":"","text":"学习使用k8s的童鞋都知道我们在部署pod的时候有时候需要手动去编写一些yaml文件；比如我需要编写deployment,那除了在其他地方粘贴拷贝外有没有其他方法呢？答案是有的 1.用run命令生成，然后作为模板进行编辑。12345678910kubectl run --image=nginx my-deploy -o yaml --dry-run &gt; my-deploy.yaml ``` ### 2.用get命令导出，然后作为模板进行编辑。``` # 注意: --export 是为了去除当前正在运行的这个deployment生成的一些状态，我们用不到就过滤掉kubectl get deployment/nginx -o=yaml --export &gt; new.yaml``` ### 3.Pod亲和性下面字段的拼写忘记了 kubectl explain pod.spec.affinity.podAffinity 示例: --- 我想生成一个有三个副本的redis pod的yaml，然后我想把这三个pod 通过node亲和性调度到同一个node节点上面； ### 1\\. 我这里用kubectl run来生成: kubectl run redis --image=redis --replicas=3 --dry-run -o yaml &gt; redis_node_affinity.yaml ### 2\\. 手写亲和性策略： 额 问题来了亲和性策略的字段我记不住啊，怎么办？那就需要通过 `kubectl explain RESOURCE [options]`来获取资源文档 怎么用? 比如我这里是要为pod做node的亲和性，那么一定是这个api接口下面的配置文档:想看pod的资源文档: [root@k8s-m1 ~]# kubectl explain pod.spec.affinity KIND: Pod VERSION: v1 RESOURCE: affinity &lt;Object&gt; DESCRIPTION: If specified, the pod's scheduling constraints Affinity is a group of affinity scheduling rules. FIELDS: nodeAffinity &lt;Object&gt; Describes node affinity scheduling rules for the pod. podAffinity &lt;Object&gt; Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). podAntiAffinity &lt;Object&gt; Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). [root@k8s-m1 ~]# 上面我们通过 `pod.spec.affinity` 定位到了`nodeAffinity` 文档, 这些字段也是yaml种使用的字段，随后我通过一层一层的定位就大体上知道这些字段在yaml中是怎么使用的啦~ [root@k8s-m1 ~]# kubectl explain pods.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchExpressions 最后快速生成并且编辑的deployment yaml就写好了。 apiVersion: apps/v1beta1 kind: Deployment metadata: creationTimestamp: null labels: run: redis name: redis spec: replicas: 3 selector: matchLabels: run: redis strategy: {} template: metadata: creationTimestamp: null labels: run: redis spec: containers: - image: redis name: redis resources: {} # 以下内容就是我通过explain参数来查询到的我想要的字段写的 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-m1 status: {} ![](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/12/15443718852925.jpg)￼ 还是非常快速高效的。","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/tags/k8s/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"K8s集群中pause容器是干嘛的~","slug":"k8s-ji-qun-zhongpause-rong-qi-shi-gan-ma-de","date":"2018-12-07T08:01:08.000Z","updated":"2025-09-01T01:59:08.981Z","comments":true,"path":"2018/12/07/k8s-ji-qun-zhongpause-rong-qi-shi-gan-ma-de/","permalink":"https://blog.sctux.cc/2018/12/07/k8s-ji-qun-zhongpause-rong-qi-shi-gan-ma-de/","excerpt":"当我们在检查k8s集群状态的时候会发现有很多 pause 容器运行于服务器上面，然后每次启动一个容器，都会伴随一个pause容器的启动。那它究竟是干啥子的？ Pause容器，又叫Infra容器，下面通过实验来理解它。 我们知道在搭建k8s集群的时候，kubelet的配置中有这样一个参数： [root@linux-node1 cfg]# more /usr/lib/systemd/system/kubelet.service ······ ······ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \\ ······ ······ 我这里是直接将这些配置参数通过启动脚本来补充进去的。Pause容器，是可以自己来定义，官方使用的gcr.io/google_containers/pause-amd64:3.0容器的代码见Github，使用C语言编写。 Pause容器的作用我们检查nod节点的时候会发现每个node上都运行了很多的pause容器，例如如下。 [root@linux-node1 cfg]# docker ps ······ ······ CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a007c18b8dc0 568c4670fa80 \"nginx -g 'daemon of…\" 42 hours ago Up 42 hours k8s_nginx_nginx-pod-7d9f9876cc-75sf7_default_a688bb46-f872-11e8-ae6b-000c29c6d12b_1 9866c08d1f4b 568c4670fa80 \"nginx -g 'daemon of…\" 42 hours ago Up 42 hours k8s_nginx_nginx-pod-7d9f9876cc-wpv4h_default_a6a899c0-f872-11e8-ae6b-000c29c6d12b_1 aafef6727026 mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_flask-app-6f5b6cc447-kbxks_flask-app-extions-stage_374b8aa0-f873-11e8-ae6b-000c29c6d12b_1 c4f48f90b27f mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_flask-app-6f5b6cc447-f9wjn_flask-app-extions-stage_373be9db-f873-11e8-ae6b-000c29c6d12b_1 9f452e6961f6 mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_nginx-pod-7d9f9876cc-ccx94_default_a6a8c440-f872-11e8-ae6b-000c29c6d12b_1 7e68043469d1 mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_nginx-pod-7d9f9876cc-sskpk_default_a6ac43bd-f872-11e8-ae6b-000c29c6d12b_1 ······ ······ kubernetes中的pause容器主要为每个业务容器提供以下功能： 在pod中担任Linux命名空间共享的基础； 启用pid命名空间，开启init进程。","text":"当我们在检查k8s集群状态的时候会发现有很多 pause 容器运行于服务器上面，然后每次启动一个容器，都会伴随一个pause容器的启动。那它究竟是干啥子的？ Pause容器，又叫Infra容器，下面通过实验来理解它。 我们知道在搭建k8s集群的时候，kubelet的配置中有这样一个参数： [root@linux-node1 cfg]# more /usr/lib/systemd/system/kubelet.service ······ ······ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \\ ······ ······ 我这里是直接将这些配置参数通过启动脚本来补充进去的。Pause容器，是可以自己来定义，官方使用的gcr.io/google_containers/pause-amd64:3.0容器的代码见Github，使用C语言编写。 Pause容器的作用我们检查nod节点的时候会发现每个node上都运行了很多的pause容器，例如如下。 [root@linux-node1 cfg]# docker ps ······ ······ CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a007c18b8dc0 568c4670fa80 \"nginx -g 'daemon of…\" 42 hours ago Up 42 hours k8s_nginx_nginx-pod-7d9f9876cc-75sf7_default_a688bb46-f872-11e8-ae6b-000c29c6d12b_1 9866c08d1f4b 568c4670fa80 \"nginx -g 'daemon of…\" 42 hours ago Up 42 hours k8s_nginx_nginx-pod-7d9f9876cc-wpv4h_default_a6a899c0-f872-11e8-ae6b-000c29c6d12b_1 aafef6727026 mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_flask-app-6f5b6cc447-kbxks_flask-app-extions-stage_374b8aa0-f873-11e8-ae6b-000c29c6d12b_1 c4f48f90b27f mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_flask-app-6f5b6cc447-f9wjn_flask-app-extions-stage_373be9db-f873-11e8-ae6b-000c29c6d12b_1 9f452e6961f6 mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_nginx-pod-7d9f9876cc-ccx94_default_a6a8c440-f872-11e8-ae6b-000c29c6d12b_1 7e68043469d1 mirrorgooglecontainers/pause-amd64:3.0 \"/pause\" 42 hours ago Up 42 hours k8s_POD_nginx-pod-7d9f9876cc-sskpk_default_a6ac43bd-f872-11e8-ae6b-000c29c6d12b_1 ······ ······ kubernetes中的pause容器主要为每个业务容器提供以下功能： 在pod中担任Linux命名空间共享的基础； 启用pid命名空间，开启init进程。 在The Almighty Pause Container这篇文章中做出了详细的说明，pause容器的作用可以从这个例子中看出，首先见下图：￼ 1.我们首先在节点上运行一个pause容器。[root@k8s-node1 ~]# docker run -d --name pause -p 8880:80 registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 38d2aa8366d5aa6fe4c57aa0d879de4b5259c67c83d17428dd4d9f8937205c02 [root@k8s-node1 ~]# docker ps | grep pause 38d2aa8366d5 registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \"/pause\" 14 seconds ago Up 13 seconds 0.0.0.0:8880-&gt;80/tcp pause 2.然后再运行一个nginx容器，nginx将为localhost:2368创建一个代理。[root@k8s-node1 ~]# cat &lt;&lt;EOF &gt;&gt; nginx.conf error_log stderr; events { worker_connections 1024; } http { access_log /dev/stdout combined; server { listen 80 default_server; server_name example.com www.example.com; location / { proxy_pass http://127.0.0.1:2368; } } } EOF [root@k8s-node1 ~]# docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf --net=container:pause --ipc=container:pause --pid=container:pause nginx fa078473c01e040db795004ad16db525dea8a113893d3052cc6ab1c5e117ba10 3.然后再为ghost创建一个应用容器，这是一款博客软件。[root@linux-node2 ~]# docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost # 查看结果： [root@k8s-node1 ~]# docker ps | grep -E \"pause|nginx|ghost\" 9b796efd95a5 ghost \"docker-entrypoint...\" 47 seconds ago Up 46 seconds ghost fa078473c01e nginx \"nginx -g 'daemon ...\" About a minute ago Up About a minute nginx 38d2aa8366d5 registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \"/pause\" 3 minutes ago Up 3 minutes 0.0.0.0:8880-&gt;80/tcp pause [root@k8s-node1 ~]# # 现在访问http://119.3.198.128:8880/就可以看到ghost博客的界面了吗 # 这里我直接curl 然后浏览器访问也是正常的 [root@k8s-node1 ~]# curl -I http://119.3.198.128:8880/ HTTP/1.1 200 OK Server: nginx/1.15.5 Date: Fri, 07 Dec 2018 08:35:49 GMT Content-Type: text/html; charset=utf-8 Content-Length: 17381 Connection: keep-alive X-Powered-By: Express Cache-Control: public, max-age=0 ETag: W/\"43e5-ELHSnbaoapp3YOyz+PU502oJo5E\" Vary: Accept-Encoding [root@k8s-node1 ~]# 解析: pause 容器将内部的80端口映射到了宿主机的8880端口; pause容器在宿主机上设置好了网络namespace后，nginx容器加入到该网络namespace中; nginx容器启动的时候指定了–net=container:pause; ghost容器启动的时候同样加入到了该网络namespace中; 这样三个容器就共享了网络，互相之间就可以使用localhost直接通信， –ipc=contianer:pause –pid=container:pause就是三个容器处于同一个namespace中，init进程为pause; 我们到ghost容器中查看一下: [root@k8s-node1 ~]# docker exec -it ghost /bin/bash root@38d2aa8366d5:/var/lib/ghost# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 1012 4 ? Ss 08:27 0:00 /pause root 5 0.0 0.0 32472 3168 ? Ss 08:29 0:00 nginx: master process nginx -g daemon off; systemd+ 9 0.0 0.0 32932 1812 ? S 08:29 0:00 nginx: worker process node 10 0.5 2.1 1262748 84688 ? Ssl 08:30 0:03 node current/index.js root 83 0.2 0.0 20240 1912 pts/0 Ss 08:41 0:00 /bin/bash root 87 0.0 0.0 17496 1148 pts/0 R+ 08:41 0:00 ps aux root@38d2aa8366d5:/var/lib/ghost# 在ghost容器中同时可以看到pause和nginx容器的进程，并且pause容器的PID是1。而在kubernetes中容器的PID=1的进程即为容器本身的业务进程。 其他概念： PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID； 网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围； IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信； UTS命名空间：Pod中的多个容器共享一个主机名；Volumes（共享存储卷）： Pod中的各个容器可以访问在Pod级别定义的Volumes；","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/tags/k8s/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"理解Kubernetes的亲和性调度","slug":"li-jiekubernetes-de-qin-he-xing-diao-du","date":"2018-12-04T15:53:16.000Z","updated":"2025-09-01T01:59:08.920Z","comments":true,"path":"2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/","permalink":"https://blog.sctux.cc/2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/","excerpt":"NodeName、nodeSelector、nodeAffinity、podAffinity、Taints以及Tolerations用法 1. NodeNamePod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是强制匹配。例子：我的预期是将该pod运行于节点名称为 192.168.56.11 这个节点： 1.1 例如(test-nodename.yaml)apiVersion: v1 kind: Pod metadata: labels: app: with-nodename-busybox-pod name: with-nodename spec: nodeName: 192.168.56.11 #通过这里指定 containers: - command: - sleep - \"3600\" image: busybox imagePullPolicy: IfNotPresent name: test-busybox 1.2 查看创建事件、与预期相符:Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m default-scheduler Successfully assigned with-pod-affinity to 192.168.56.11 Normal SuccessfulMountVolume 3m kubelet, 192.168.56.11 MountVolume.SetUp succeeded for volume \"default-token-5htws\" Normal Pulling 3m kubelet, 192.168.56.11 pulling image \"nginx\" Normal Pulled 1m kubelet, 192.168.56.11 Successfully pulled image \"nginx\" Normal Created 1m kubelet, 192.168.56.11 Created container Normal Started 1m kubelet, 192.168.56.11 Started container 2. NodeSelectorPod.spec.nodeSelector是通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是强制约束。使用节点选择器的步骤为： 2.1 Node添加label标记,标记规则：","text":"NodeName、nodeSelector、nodeAffinity、podAffinity、Taints以及Tolerations用法 1. NodeNamePod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是强制匹配。例子：我的预期是将该pod运行于节点名称为 192.168.56.11 这个节点： 1.1 例如(test-nodename.yaml)apiVersion: v1 kind: Pod metadata: labels: app: with-nodename-busybox-pod name: with-nodename spec: nodeName: 192.168.56.11 #通过这里指定 containers: - command: - sleep - \"3600\" image: busybox imagePullPolicy: IfNotPresent name: test-busybox 1.2 查看创建事件、与预期相符:Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m default-scheduler Successfully assigned with-pod-affinity to 192.168.56.11 Normal SuccessfulMountVolume 3m kubelet, 192.168.56.11 MountVolume.SetUp succeeded for volume \"default-token-5htws\" Normal Pulling 3m kubelet, 192.168.56.11 pulling image \"nginx\" Normal Pulled 1m kubelet, 192.168.56.11 Successfully pulled image \"nginx\" Normal Created 1m kubelet, 192.168.56.11 Created container Normal Started 1m kubelet, 192.168.56.11 Started container 2. NodeSelectorPod.spec.nodeSelector是通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是强制约束。使用节点选择器的步骤为： 2.1 Node添加label标记,标记规则：kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt; kubectl label nodes 192.168.56.14 server_type=game_server 2.2 确认标记[root@linux-node1 ~]# kubectl get nodes 192.168.56.14 --show-labels NAME STATUS ROLES AGE VERSION LABELS 192.168.56.14 Ready node 82d v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.56.14,node-role.kubernetes.io/node=true,server_type=game_server 2.3 例如(test-nodeselector.yaml)apiVersion: v1 kind: Pod metadata: labels: app: with-nodeselector-busybox-pod name: with-node-selector spec: containers: - command: - sleep - \"3600\" image: busybox imagePullPolicy: IfNotPresent name: test-busybox # 通过节点标签进行预设该pod将会运行于有此标签的节点上面， # 如果多个节点拥有此标签，调度器将会择优进行调度 nodeSelector: server_type: game_server 2.4 查看创建事件:Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 16s default-scheduler Successfully assigned with-node-selector to 192.168.56.14 Normal SuccessfulMountVolume 16s kubelet, 192.168.56.14 MountVolume.SetUp succeeded for volume \"default-token-5htws\" Normal Pulling 11s kubelet, 192.168.56.14 pulling image \"busybox\" 通过上面的例子我们可以感受到nodeSelector的方式比较直观，但是还够灵活，控制粒度偏大，下面我们再看另外一种更加灵活的方式：nodeAffinity。 3. NodeAffinitynodeAffinity就是节点亲和性，相对应的是Anti-Affinity，就是反亲和性，这种方法比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。调度可以分成软策略和硬策略两种方式，软策略就是如果你没有满足调度要求的节点的话，POD 就会忽略这条规则，继续完成调度过程。 nodeAffinity就有两上面两种策略： # 软策略:(满足条件最好了，没有的话也无所谓了的策略) preferredDuringSchedulingIgnoredDuringExecution # 硬策略:(你必须满足我的要求，不然我就不干) requiredDuringSchedulingIgnoredDuringExecution 3.1 例如(test-node-affinity.yaml)apiVersion: v1 kind: Pod metadata: name: with-node-affinity labels: app: node-affinity-pod spec: containers: - name: with-node-affinity image: nginx imagePullPolicy: IfNotPresent affinity: nodeAffinity: # 硬性策略 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - 192.168.56.11 - 192.168.56.12 # 软性策略 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: server_type operator: In values: - game_server 上面的Pod我们进行一下解读:1、该pod 通过硬性策略约束后将禁止调度到节点11，12上面去2、排除节点11，12后，选择其他节点含有label为server_type:game_server的节点运行3、如果其他节点也没有含有label server_type:game_server, 那么将会调度到任意节点运行同样的我们可以使用descirbe命令查看具体的调度情况是否满足我们的要求。这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 POD。 4. PodAffinity上面三种方式都是让POD去选择节点的，有的时候我们也希望能够根据 POD 之间的关系进行调度，Kubernetes在1.4版本引入的podAffinity概念就可以实现我们这个需求。 和nodeAffinity类似，podAffinity也有requiredDuringSchedulingIgnoredDuringExecution(硬性策略)和 preferredDuringSchedulingIgnoredDuringExecution(软性策略)两种调度策略，唯一不同的是如果要使用互斥性，我们需要使用podAntiAffinity字段。 如下例子，我们希望【with-pod-affinity】和【with-nodename-busybox-pod】能够就近部署，而不希望和【node-affinity-pod】部署在同一个拓扑域下面： 4.1 例如(test-pod-affinity.yaml)apiVersion: v1 kind: Pod metadata: name: with-pod-affinity labels: app: pod-affinity-pod spec: containers: - name: with-pod-affinity image: nginx affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - with-nodename-busybox-pod topologyKey: kubernetes.io/hostname podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - node-affinity-pod topologyKey: kubernetes.io/hostname 上面的pod我们解读一下:1、POD 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 POD：这个 POD 有一个app=busybox-pod的 label。2、podAntiAffinity则是希望最好不要调度到这样的节点：这个节点上运行了某个 POD，而这个 POD 有app=node-affinity-pod的 label。3、根据前面两个 POD 的定义，我们可以预见上面这个 POD 应该会被调度到192.168.56.11的节点上，因为前面实践的时候【with-nodename-busybox-pod】被调度到了192.168.56.11节点，而【node-affinity-pod】被调度到了192.168.56.11以为的节点，正好满足上面的需求。通过describe查看： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 53m default-scheduler Successfully assigned with-pod-affinity to 192.168.56.11 Normal SuccessfulMountVolume 52m kubelet, 192.168.56.11 MountVolume.SetUp succeeded for volume \"default-token-5htws\" Normal Pulling 52m kubelet, 192.168.56.11 pulling image \"nginx\" Normal Pulled 51m kubelet, 192.168.56.11 Successfully pulled image \"nginx\" Normal Created 51m kubelet, 192.168.56.11 Created container Normal Started 51m kubelet, 192.168.56.11 Started container 在labelSelector和 topologyKey的同级，还可以定义 namespaces 列表，表示匹配哪些 namespace 里面的 pod，默认情况下，会匹配定义的 pod 所在的 namespace；如果定义了这个字段，但是它的值为空，则匹配所有的 namespaces。 so，以上我们创建的四个例子结果为下: [root@linux-node1 affinity_study]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE with-node-affinity 1/1 Running 2 10h 10.2.70.16 192.168.56.14 with-node-selector 1/1 Running 0 8m 10.2.70.15 192.168.56.14 with-nodename 1/1 Running 1 11h 10.2.88.9 192.168.56.11 with-pod-affinity 1/1 Running 0 10h 10.2.88.10 192.168.56.11 5. 污点（Taints）与容忍（tolerations）对于nodeAffinity无论是硬策略还是软策略方式，都是调度 POD 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 POD 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度pod。 比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 POD，则污点就很有用了，POD 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下： 5.1 设置污点 kubectl taint node [node] key=value[effect] 其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ] NoSchedule ：一定不能被调度。POD 不会被调度到标记为 taints 节点。 PreferNoSchedule：尽量不要调度。NoSchedule 的软策略版本。 NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。 示例：kubectl taint node 192.168.56.11 key1=value1:NoSchedule 5.2 去除污点#比如设置污点： kubectl taint nodes 192.168.56.11 key1=value1:NoSchedule kubectl taint nodes 192.168.56.11 key2=value2:NoExecute kubectl taint nodes 192.168.56.11 key3=value3:PreferNoSchedule #去除指定key及其effect： kubectl taint nodes 192.168.56.11 key:[effect]- #(这里的key不用指定value) #去除指定key所有的effect: kubectl taint nodes 192.168.56.11 key- #示例： kubectl taint node 192.168.56.11 key1:NoSchedule- kubectl taint node 192.168.56.11 key2:NoExecute- kubectl taint node 192.168.56.11 key3:PreferNoSchedule- kubectl taint node 192.168.56.11 key3- (去除指定key的所有effct) 5.3 实践# 5.3.1 给节点设置污点首先我的环境中还没有设置污点，我们执行以下例子(nginx-deployment.yaml): apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod labels: app: nginx-pod spec: replicas: 10 # 这里为了实践效果特意设定了10个副本 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 部署结果: [root@linux-node1 ~]# kubectl get pods -o wide | grep nginx nginx-pod-7d9f9876cc-95fj2 1/1 Running 0 1m 10.2.70.26 192.168.56.14 nginx-pod-7d9f9876cc-jclvf 1/1 Running 0 1m 10.2.44.28 192.168.56.13 nginx-pod-7d9f9876cc-k2m6x 1/1 Running 0 1m 10.2.44.31 192.168.56.13 nginx-pod-7d9f9876cc-l74hf 1/1 Running 0 1m 10.2.88.15 192.168.56.11 nginx-pod-7d9f9876cc-n8g6c 1/1 Running 0 1m 10.2.44.29 192.168.56.13 nginx-pod-7d9f9876cc-p2cft 1/1 Running 0 1m 10.2.88.14 192.168.56.11 nginx-pod-7d9f9876cc-p7hvt 1/1 Running 0 1m 10.2.69.25 192.168.56.12 nginx-pod-7d9f9876cc-rjj97 1/1 Running 0 1m 10.2.70.27 192.168.56.14 nginx-pod-7d9f9876cc-t2wlw 1/1 Running 0 1m 10.2.69.26 192.168.56.12 nginx-pod-7d9f9876cc-whkx9 1/1 Running 0 1m 10.2.44.30 192.168.56.13 以上例子可以看出在我的环境中已经分配到了每个节点包括我们接下来准备设置污点的节点192.168.56.11。 下面我把master节点192.168.56.11 设置污点(这里我用的effect是NoSchedule) [root@linux-node1 affinity_study]# kubectl taint nodes 192.168.56.11 server_type=k8s_system:NoSchedule node \"192.168.56.11\" tainted # 然后部署下面这个设置例子: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod-taints labels: app: nginx-pod-taints spec: replicas: 10 # 这里为了实践效果特意设定了10个副本 selector: matchLabels: app: nginx-taints template: metadata: labels: app: nginx-taints spec: containers: - name: nginx-taints image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 部署结果: [root@linux-node1 ~]# kubectl get pods -o wide | grep nginx-pod-taints nginx-pod-taints-57757d7677-5h6nj 1/1 Running 0 2m 10.2.69.31 192.168.56.12 nginx-pod-taints-57757d7677-dkd8h 1/1 Running 0 2m 10.2.70.34 192.168.56.14 nginx-pod-taints-57757d7677-f2vdn 1/1 Running 0 2m 10.2.44.38 192.168.56.13 nginx-pod-taints-57757d7677-fnx2n 1/1 Running 0 2m 10.2.44.36 192.168.56.13 nginx-pod-taints-57757d7677-gsc5g 1/1 Running 0 2m 10.2.70.32 192.168.56.14 nginx-pod-taints-57757d7677-kjl89 1/1 Running 0 2m 10.2.44.35 192.168.56.13 nginx-pod-taints-57757d7677-mqx27 1/1 Running 0 2m 10.2.70.33 192.168.56.14 nginx-pod-taints-57757d7677-skdd4 1/1 Running 0 2m 10.2.69.32 192.168.56.12 nginx-pod-taints-57757d7677-spwfh 1/1 Running 0 2m 10.2.69.30 192.168.56.12 nginx-pod-taints-57757d7677-tpcm8 1/1 Running 0 2m 10.2.44.37 192.168.56.13 以上例子可以看出我们将master节点设置污点之后调度器并没有把pod调度到master节点上。这里我选择的effect是NoSchedule，也就是一定不要调度到该节点。 # 5.3.2 给部署配置设置容忍性首先 我们上面设置了污点即: taints: - effect: NoSchedule key: server_type value: k8s_system 然后我们部署以下例子(test-tolertations.yaml) apiVersion: apps/v1 kind: Deployment metadata: name: nginx-pod-taints-tolerations labels: app: nginx-pod-taints-tolerations spec: replicas: 10 # 这里为了实践效果特意设定了10个副本 selector: matchLabels: app: nginx-taints-tolerations template: metadata: labels: app: nginx-taints-tolerations spec: # 设置容忍性 tolerations: - key: \"server_type\" operator: \"Equal\" value: \"k8s_system\" effect: \"NoSchedule\" containers: - name: nginx-taints-tolerations image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 部署结果: [root@linux-node1 affinity_study]# kubectl get pods -o wide | grep nginx-pod-taints-tolerations nginx-pod-taints-tolerations-5b4988bd4-2pk46 1/1 Running 0 29m 10.2.69.33 192.168.56.12 nginx-pod-taints-tolerations-5b4988bd4-854xx 1/1 Running 0 29m 10.2.70.35 192.168.56.14 nginx-pod-taints-tolerations-5b4988bd4-88xrt 1/1 Running 0 29m 10.2.69.34 192.168.56.12 nginx-pod-taints-tolerations-5b4988bd4-8czpg 1/1 Running 0 29m 10.2.88.17 192.168.56.11 nginx-pod-taints-tolerations-5b4988bd4-czdss 1/1 Running 0 21m 10.2.44.41 192.168.56.13 nginx-pod-taints-tolerations-5b4988bd4-dj6mw 1/1 Running 0 29m 10.2.88.16 192.168.56.11 nginx-pod-taints-tolerations-5b4988bd4-g4ltd 1/1 Running 0 29m 10.2.44.39 192.168.56.13 nginx-pod-taints-tolerations-5b4988bd4-npthj 1/1 Running 0 29m 10.2.44.40 192.168.56.13 nginx-pod-taints-tolerations-5b4988bd4-ptlqg 1/1 Running 0 29m 10.2.88.18 192.168.56.11 nginx-pod-taints-tolerations-5b4988bd4-t9gs6 1/1 Running 0 29m 10.2.69.35 192.168.56.12 以上实践我们解读一下:1. 在节点master 192.168.56.11上面设置了污点；但期望该节点能容忍调度;2. 于是通过设置tolerations来实现，其中Pod要容忍的有污点的Node的key是server_type Equal k8s_system,效果是NoSchedule.3. 通过设置容忍机制结果与预期相符。 对于tolerations属性的写法： 其中的key、value、effect 与Node的Taint设置需保持一致， 还有以下几点说明： 1、如果operator的值是Exists，则value属性可省略。 2、如果operator的值是Equal，则表示其key与value之间的关系是equal(等于)。 3、如果不指定operator属性，则默认值为Equal。 另外，还有两个特殊值： 1、空的key 如果再配合Exists 就能匹配所有的key与value ，也是是能容忍所有node的所有Taints。 2、空的effect 匹配所有的effect。 # 5.3.3 其他一个node上可以有多个污点： [root@linux-node1 affinity_study]# kubectl describe node/192.168.56.11 ........ ........ Taints: server_type=k8s_system:NoSchedule test=wahaha:PreferNoSchedule ........ ........ 如果按照我上面的例子来的话结果将不会有任何pod调度到该节点，因为上述我只容忍了一个污点；所以可以新加一个污点容忍: [root@linux-node1 affinity_study]# more test-tolertations.yaml ........ ........ tolerations: - key: \"server_type\" operator: \"Equal\" value: \"k8s_system\" effect: \"NoSchedule\" - key: \"test\" operator: \"Equal\" value: \"wahaha\" effect: \"PreferNoSchedule\" ........ ........ 重新部署后结果也可以预见 只要两个容忍满足就可以调度过去啦~ 设置容忍的效果： 如果在设置node的Taints(污点)之前，就已经运行了一些Pod，那么这些Pod是否还能继续在此Node上运行？ 这就要看设置Taints污点时的effect(效果)了。 如果effect的值是NoSchedule或PreferNoSchedule，那么已运行的Pod仍然可以运行，只是新Pod(如果没有容忍)不会再往上调度。 如果 pod 不能忍受effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐 如果 pod 能够忍受effect 值为 NoExecute 的 taint，但是在 toleration 定义中没有指定 tolerationSeconds，则 pod 还会一直在这个节点上运行。 如果 pod 能够忍受effect 值为 NoExecute 的 taint，而且指定了 tolerationSeconds，则 pod 还能在这个节点上继续运行这个指定的时间长度。 虽然是立刻被驱逐，但是K8S为了彰显人性化，又给具有NoExecute效果的污点， 在容忍属性中有一个可选的tolerationSeconds字段，用来设置这些Pod还可以在这个Node之上运行多久，给它们一点宽限的时间，到时间才驱逐。 不同的部署启动方式： 如果是以Pod来启动的，那么Pod被驱逐后， 将不会再被运行，就等于把它删除了。 如果是deployment/rc，那么删除的pod会再其它节点运行。 如果是DaemonSet在此Node上启动的Pod，那么也不会再被运行，直到Node上的NoExecute污被去除或者Pod容忍。 #设置Pod的宽限时间spec: tolerations: #设置容忍性 key: “test”operator: “Equal” #如果操作符为Exists，那么value属性可省略value: “16”effect: “wahaha”tolerationSeconds: 180 如果运行此Pod的Node，被设置了具有NoExecute效果的Taint(污点)，这个Pod将在存活180s后才被驱逐。如果没有设置tolerationSeconds字段，将永久运行。 通过对Taints和Tolerations的了解，可以知道，通过它们可以让某些特定应用，独占一个Node，给特定的Node设置一个Taint，只让某些特定的应用来容忍这些污点，容忍后就有可能会被调度到此特定Node，但是也不一定会调度给此特定Node，设置容忍并不阻止调度器调度给其它Node，那么如何让特定应用的Node，只能被调度到此特定的Node呢，这就要结合NodeAffinity节点亲和性，给Node打个标签，然后在Pod属性里设置NodeAffinity到Node。如此就能达到要求了。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://blog.sctux.cc/categories/Docker/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/tags/k8s/"}],"keywords":[{"name":"Docker","slug":"Docker","permalink":"https://blog.sctux.cc/categories/Docker/"}]},{"title":"kubernetes调度之NodeSelector","slug":"kubernetes-diao-du-zhinodeselector-trashed","date":"2018-12-03T10:34:48.000Z","updated":"2025-09-01T01:59:08.921Z","comments":true,"path":"2018/12/03/kubernetes-diao-du-zhinodeselector-trashed/","permalink":"https://blog.sctux.cc/2018/12/03/kubernetes-diao-du-zhinodeselector-trashed/","excerpt":"1 NodeNamePod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是强制匹配。例子： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: nodeName: 192.168.56.13 # 指定pod调度到该节点 containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent # 镜像拉取策略 ports: - containerPort: 80 2 NodeSelectorPod.spec.nodeSelector是通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是强制约束。启用节点选择器的步骤为： Node添加label标记 标记规则：kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt; kubectl label nodes 192.168.56.13 server_type=game_server 确认标记[root@linux-node1 ~]# kubectl get nodes 192.168.56.14 --show-labels NAME STATUS ROLES AGE VERSION LABELS 192.168.56.14 Ready &lt;none&gt; 81d v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.56.14,server_type=game_server Pod定义中添加nodeSelector apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 nodeSelector: server_type: game_server #指定调度节点为带有label标记为server_type=game_server","text":"1 NodeNamePod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是强制匹配。例子： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: nodeName: 192.168.56.13 # 指定pod调度到该节点 containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent # 镜像拉取策略 ports: - containerPort: 80 2 NodeSelectorPod.spec.nodeSelector是通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是强制约束。启用节点选择器的步骤为： Node添加label标记 标记规则：kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt; kubectl label nodes 192.168.56.13 server_type=game_server 确认标记[root@linux-node1 ~]# kubectl get nodes 192.168.56.14 --show-labels NAME STATUS ROLES AGE VERSION LABELS 192.168.56.14 Ready &lt;none&gt; 81d v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.56.14,server_type=game_server Pod定义中添加nodeSelector apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 nodeSelector: server_type: game_server #指定调度节点为带有label标记为server_type=game_server","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"Prometheus 实践","slug":"prometheus-e5-ae-9e-e8-b7-b5","date":"2018-11-16T10:12:27.000Z","updated":"2025-09-01T01:59:08.959Z","comments":true,"path":"2018/11/16/prometheus-e5-ae-9e-e8-b7-b5/","permalink":"https://blog.sctux.cc/2018/11/16/prometheus-e5-ae-9e-e8-b7-b5/","excerpt":"PrometheusServer端安装 prometheus / alertmanager / node_expoter 一、安装prometheus server0.获取软件包(略)、解压复制二进制文件12tar -xf ./packages/alertmanager-0.15.3.linux-amd64.tar.gzmv alertmanager-0.15.3.linux-amd64/alertmanager /usr/local/sbin/ 1.创建运行用户1adduser -M -s /sbin/nologin prometheus 2.创建用户所有者目录12345678910111213# 创建prometheus server运行的目录主要存放配置文件mdkir /usr/local/share/prometheus/prometheus_server -pchown -R prometheus:prometheus /usr/local/share/prometheus# 创建prometheus server数据存储目录mkdir /var/lib/prometheus/datachown -R prometheus:prometheus /var/lib/prometheus/data# 创建prometheus alertmanager运行的目录主要存放配置文件mdkir /usr/local/share/prometheus/prometheus_alertmanager# 创建prometheus alertmanager数据存储目录mkdir /var/lib/alertmanager/data -pchown -R prometheus:prometheus /var/lib/alertmanager/data 3.创建prometheus server systemd文件","text":"PrometheusServer端安装 prometheus / alertmanager / node_expoter 一、安装prometheus server0.获取软件包(略)、解压复制二进制文件12tar -xf ./packages/alertmanager-0.15.3.linux-amd64.tar.gzmv alertmanager-0.15.3.linux-amd64/alertmanager /usr/local/sbin/ 1.创建运行用户1adduser -M -s /sbin/nologin prometheus 2.创建用户所有者目录12345678910111213# 创建prometheus server运行的目录主要存放配置文件mdkir /usr/local/share/prometheus/prometheus_server -pchown -R prometheus:prometheus /usr/local/share/prometheus# 创建prometheus server数据存储目录mkdir /var/lib/prometheus/datachown -R prometheus:prometheus /var/lib/prometheus/data# 创建prometheus alertmanager运行的目录主要存放配置文件mdkir /usr/local/share/prometheus/prometheus_alertmanager# 创建prometheus alertmanager数据存储目录mkdir /var/lib/alertmanager/data -pchown -R prometheus:prometheus /var/lib/alertmanager/data 3.创建prometheus server systemd文件12345678910cp ./conf/systemd_conf/prometheus.service /etc/systemd/system/# 服务操作systemctl daemon-reloadsystemctl start prometheussystemctl enable prometheussystemctl status prometheus# 保证端口以及进程ps aux | grep prometheus | grep -v grep &amp;&amp; ss -tunl | grep 9090 | grep -v grep 4.访问:1http://xxxxxxx:9090 二、安装prometheus AlertManager0.获取软件包(略)、解压复制二进制文件12tar -xf ./packages/prometheus-2.4.2.linux-amd64.tar.gzmv prometheus-2.4.2.linux-amd64/prometheus /usr/local/sbin/ 1.创建prometheus alertmanager systemd文件123456789cp ./conf/systemd_conf/alertmanager.service /etc/systemd/system/# 服务操作systemctl daemon-reloadsystemctl start alertmanagersystemctl enable alertmanagersystemctl status alertmanager# 保证端口以及进程ps aux | grep alertmanager | grep -v grep &amp;&amp; ss -tunl | grep 9093 | grep -v grep 2.将alertmanager跟prometheus server结合修改prometheus server配置: 123456vim /usr/local/share/prometheus/prometheus_server/prometheus.yml# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: ['localhost:9093'] 3.访问:1http://xxxxxxx:9093 三、安装Node Exporter收集主机信息数据收集的任务由不同的 exporter 来完成，如果要收集 linux 主机的信息，可以使用 node exporter。然后由 Prometheus Server 从 node exporter 上拉取信息。 0.获取软件包(略)、解压复制二进制文件12tar -xf ./packages/node_exporter-0.17.0-rc.0.linux-amd64.tar.gzmv node_exporter-0.17.0-rc.0.linux-amd64/node_exporter /usr/local/sbin/ 1.把 node exporter 也配置成通过 systemd 管理, 创建文件 /etc/systemd/system/node-exporter.service1234567891011cp ./conf/systemd_conf/node_exporter.service /etc/systemd/system/node-exporter.service#服务操作systemctl daemon-reloadsystemctl enable node-exportersystemctl start node-exportersystemctl status node-exporter# 保证端口以及进程ps aux | grep node_exporter | grep -v grep &amp;&amp; ss -tunl | grep 9100 | grep -v grep Prometheus Server 可以从不同的 exporter 上拉取数据，对于上面的 node exporter 我们可以利用 Prometheus 的static_configs 来拉取 node exporter 的数据。conf/prometheus_server_conf/prometheus/prometheus.yml中已经定义好了 123- job_name: 'node' static_configs: - targets: ['66.112.211.12:9100'] 重启各服务；重启后 prometheus 服务会每隔 15s 从 node exporter 上拉取一次数据。Prometheus Server 提供了简易的 WebUI 可以进数据查询并展示，它默认监听的端口为 9090。接下来我们进行一次简单的查询来验证本文安装配置的系统。 关于各项指标的规则还需要通过编写rule条目来实现；这里简单实现了wechat跟email的报警配置，具体可看规则配置文件conf/prometheus_server_conf/prometheus/rules/hoststas-alert.rules以及报警触发配置文件alertmanager_conf/alertmanager.yml；","categories":[{"name":"monitor","slug":"monitor","permalink":"https://blog.sctux.cc/categories/monitor/"}],"tags":[{"name":"monitor","slug":"monitor","permalink":"https://blog.sctux.cc/tags/monitor/"}],"keywords":[{"name":"monitor","slug":"monitor","permalink":"https://blog.sctux.cc/categories/monitor/"}]},{"title":"Flask RestApi 后端开发项目说明","slug":"flask-restapi-hou-duan-kai-fa-xiang-mu-shuo-ming","date":"2018-10-17T09:54:56.000Z","updated":"2025-09-01T01:59:08.875Z","comments":true,"path":"2018/10/17/flask-restapi-hou-duan-kai-fa-xiang-mu-shuo-ming/","permalink":"https://blog.sctux.cc/2018/10/17/flask-restapi-hou-duan-kai-fa-xiang-mu-shuo-ming/","excerpt":"最近一直在做的一个项目就是打算将之前的MVC风格的后台,重构为前后端分离式,由于个人对于Flask框架熟悉程度比起Django来更熟悉一些，所以最终还是选择他作为开发框架来进行后端的开发，目前呢打算的是把基础的平台功能做出来作为一个模板，然后通过这个模板再去结合业务方面的开发。前端方面暂时未开始，目前后端开发进度: 功能 完成度 methods api 备注 用户注册 🚀%100 POST","text":"最近一直在做的一个项目就是打算将之前的MVC风格的后台,重构为前后端分离式,由于个人对于Flask框架熟悉程度比起Django来更熟悉一些，所以最终还是选择他作为开发框架来进行后端的开发，目前呢打算的是把基础的平台功能做出来作为一个模板，然后通过这个模板再去结合业务方面的开发。前端方面暂时未开始，目前后端开发进度: 功能 完成度 methods api 备注 用户注册 🚀%100 POST /auth/register null 用户登录 🚀%100 POST /auth/login null 用户登出 🚀%100 POST /auth/logout null 邮件确认 🚀%100 POST /auth/confirm/{confirm_token} null Token刷新 🚀%100 GET /auth/refresh_token null 用户获取 🚀%100 GET /user/ null 用户删除 🚀%100 POST /user/{email} null 用户禁用/启用 🚀%0 POST null 任务添加 🚀%0 POST null 任务获取 🚀%0 GET null 任务删除 🚀%0 POST null 任务修改 🚀%0 PUT null API添加 🚀%0 POST 第三方(ex:saltapi,zabbixapi) API获取 🚀%0 GET 第三方(ex:saltapi,zabbixapi) API删除 🚀%0 POST 第三方(ex:saltapi,zabbixapi) API修改 🚀%0 PUT 第三方(ex:saltapi,zabbixapi) 设计思路:1.用户权限管理通过角色管理，分为user,admin,sa三种角色2.采用了jwt token认证机制，访问资源必须携带access_token以验证其访问资源的权限……","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"All","slug":"All","permalink":"https://blog.sctux.cc/tags/All/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"通过Consul-Template实现动态配置服务","slug":"tong-guoconsultemplate-shi-xian-dong-tai-pei-zhi-f","date":"2018-08-06T08:18:58.000Z","updated":"2025-09-01T01:59:08.932Z","comments":true,"path":"2018/08/06/tong-guoconsultemplate-shi-xian-dong-tai-pei-zhi-f/","permalink":"https://blog.sctux.cc/2018/08/06/tong-guoconsultemplate-shi-xian-dong-tai-pei-zhi-f/","excerpt":"背景:公司的测试、预发布环境的配置修改在前期都是通过手工登录到服务器上去vim配置文件的，这样一来就会产生一定的安全或者误操作以及频繁的操作真的是有些恶心的；去年在此基础上也为运营/测试使用Flask 写了一个平台让他们自己用；但是由于一些不定因素，不能够满足这方面的需求；但是本人还是坚持以自动化的理念来操作；所以学习了解了一下自动配置的一些工具，比如Consul，当然他的原理功能网上有很多；也没有必要在这里再一次说了；主要记录一下对于这个需求的一个想法到实现的过程。”妈妈再也不担心我vim错配置啦~~~😁” 这是前期为运营人员使用Flask开发的一个平台,主要日常涉及到的操作(测试/预发布环境)￼ 下面是针对以上一些操作规划的一个拓扑图:￼ 这里我主要使用到了consul的k/v存储以及consul-template动态的配置系统 操作人员通过Opsplatform操作后，通过调用consul-http-api将最新的key/value put到consul-datacenter; 客户端服务器添加对应的模板文件，通过consul-template命令启动监控模板文件 当有新的key/value put到consul-datacenter时，consul-template根据模板文件替换掉里面的kv 在启动监控模板文件时也可以增加后续的操作，例如: ￼ 客户端连接到consul-http-api，指定模板文件以及输出文件，然后指定模板文件替换成功后执行其他命令例如上图中的date命令… 整个过程无需人员操作。 当然这里只是用到了consul的冰山一角，也是作为一个方向去实现各种需求~ 通过reqeusts来操作key/value￼","text":"背景:公司的测试、预发布环境的配置修改在前期都是通过手工登录到服务器上去vim配置文件的，这样一来就会产生一定的安全或者误操作以及频繁的操作真的是有些恶心的；去年在此基础上也为运营/测试使用Flask 写了一个平台让他们自己用；但是由于一些不定因素，不能够满足这方面的需求；但是本人还是坚持以自动化的理念来操作；所以学习了解了一下自动配置的一些工具，比如Consul，当然他的原理功能网上有很多；也没有必要在这里再一次说了；主要记录一下对于这个需求的一个想法到实现的过程。”妈妈再也不担心我vim错配置啦~~~😁” 这是前期为运营人员使用Flask开发的一个平台,主要日常涉及到的操作(测试/预发布环境)￼ 下面是针对以上一些操作规划的一个拓扑图:￼ 这里我主要使用到了consul的k/v存储以及consul-template动态的配置系统 操作人员通过Opsplatform操作后，通过调用consul-http-api将最新的key/value put到consul-datacenter; 客户端服务器添加对应的模板文件，通过consul-template命令启动监控模板文件 当有新的key/value put到consul-datacenter时，consul-template根据模板文件替换掉里面的kv 在启动监控模板文件时也可以增加后续的操作，例如: ￼ 客户端连接到consul-http-api，指定模板文件以及输出文件，然后指定模板文件替换成功后执行其他命令例如上图中的date命令… 整个过程无需人员操作。 当然这里只是用到了consul的冰山一角，也是作为一个方向去实现各种需求~ 通过reqeusts来操作key/value￼","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"consul","slug":"consul","permalink":"https://blog.sctux.cc/tags/consul/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Kubernetes Session亲和性设置","slug":"kubernetes-session-bao-chi-deng-she-zhi","date":"2018-07-15T04:12:31.000Z","updated":"2025-09-01T01:59:08.880Z","comments":true,"path":"2018/07/15/kubernetes-session-bao-chi-deng-she-zhi/","permalink":"https://blog.sctux.cc/2018/07/15/kubernetes-session-bao-chi-deng-she-zhi/","excerpt":"当我们在部署了多个pod，以及一个Service后，就可以在集群内部通过ServiceIP访问pod提供的服务了；￼￼ 当不设置session保持时，service向后台pod转发规则是轮询:￼￼￼以上我通过点击页面请求，可以就看出service将我的请求分发到了后面的三个pod; k8s会根据访问的ip来把请求转发给他以前访问过的pod，这样session就保持住了。查看创建service时的yaml文件内容，如果没有设置的话 该项是为None的￼","text":"当我们在部署了多个pod，以及一个Service后，就可以在集群内部通过ServiceIP访问pod提供的服务了；￼￼ 当不设置session保持时，service向后台pod转发规则是轮询:￼￼￼以上我通过点击页面请求，可以就看出service将我的请求分发到了后面的三个pod; k8s会根据访问的ip来把请求转发给他以前访问过的pod，这样session就保持住了。查看创建service时的yaml文件内容，如果没有设置的话 该项是为None的￼","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/tags/k8s/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"如何将pod中的container时区更改为同一时区的城市或UTC时区偏移","slug":"ru-he-jiangpod-zhong-decontainer-shi-qu-geng-gai-w","date":"2018-07-14T07:55:49.000Z","updated":"2025-09-01T01:59:08.872Z","comments":true,"path":"2018/07/14/ru-he-jiangpod-zhong-decontainer-shi-qu-geng-gai-w/","permalink":"https://blog.sctux.cc/2018/07/14/ru-he-jiangpod-zhong-decontainer-shi-qu-geng-gai-w/","excerpt":"问题:在创建pod container后发现里面的时区是UTC,对于国内习惯还是CST时区比较易读；那如何解决这种问题嘛？暂时想到的两种办法: [root@linux-node1 ~]# kubectl exec flask-app-nginx-66b56f556c-zb84s date -n flask-app-extions-stage Mon Jul 14 07:32:52 UTC 2018 [root@linux-node1 ~]# date Mon Jul 14 15:32:52 CST 2018 直接修改镜像的时间设置，好处是应用部署时无需特殊设置，但是需要手动从新构建Docker镜像 部署应用时，单独读取主机的”/etc/localtime”,无需修改镜像，但是每个应用都要单独设置。 解决:为了快速，简单的解决此问题，先使用第二种方法；yaml文件中映射主机的”/etc/localtime”文件, 添加yaml配置: ...... ...... spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: nginx-conf mountPath: \"/etc/nginx/nginx.conf\" subPath: nginx.conf - name: host-time mountPath: /etc/localtime volumes: - name: nginx-conf configMap: name: nginx-conf - name: host-time hostPath: path: /usr/share/zoneinfo/Asia/Shanghai ...... ...... [root@linux-node1 flask_app_nginx]# kubectl apply -f flask_app_nginx_deploy.yaml [root@linux-node1 flask_app_nginx]# kubectl exec flask-app-nginx-f4d9759b4-xq4rk date -n flask-app-extions-stage Mon Jul 14 15:50:29 CST 2018 [root@linux-node1 flask_app_nginx]# date Mon Jul 14 15:50:29 CST 2018 # 以上，就完成了pod container的时区修改问题... 其实还有一种方法就是将 /usr/share/zoneinfo/Asia/Shanghai 这个文件做成之前我挂载nginx配置文件那样通过ConfigMap的形式挂载.","text":"问题:在创建pod container后发现里面的时区是UTC,对于国内习惯还是CST时区比较易读；那如何解决这种问题嘛？暂时想到的两种办法: [root@linux-node1 ~]# kubectl exec flask-app-nginx-66b56f556c-zb84s date -n flask-app-extions-stage Mon Jul 14 07:32:52 UTC 2018 [root@linux-node1 ~]# date Mon Jul 14 15:32:52 CST 2018 直接修改镜像的时间设置，好处是应用部署时无需特殊设置，但是需要手动从新构建Docker镜像 部署应用时，单独读取主机的”/etc/localtime”,无需修改镜像，但是每个应用都要单独设置。 解决:为了快速，简单的解决此问题，先使用第二种方法；yaml文件中映射主机的”/etc/localtime”文件, 添加yaml配置: ...... ...... spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: nginx-conf mountPath: \"/etc/nginx/nginx.conf\" subPath: nginx.conf - name: host-time mountPath: /etc/localtime volumes: - name: nginx-conf configMap: name: nginx-conf - name: host-time hostPath: path: /usr/share/zoneinfo/Asia/Shanghai ...... ...... [root@linux-node1 flask_app_nginx]# kubectl apply -f flask_app_nginx_deploy.yaml [root@linux-node1 flask_app_nginx]# kubectl exec flask-app-nginx-f4d9759b4-xq4rk date -n flask-app-extions-stage Mon Jul 14 15:50:29 CST 2018 [root@linux-node1 flask_app_nginx]# date Mon Jul 14 15:50:29 CST 2018 # 以上，就完成了pod container的时区修改问题... 其实还有一种方法就是将 /usr/share/zoneinfo/Asia/Shanghai 这个文件做成之前我挂载nginx配置文件那样通过ConfigMap的形式挂载.","categories":[{"name":"虚拟化&amp;云计算&amp;大数据","slug":"虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/tags/k8s/"}],"keywords":[{"name":"虚拟化&amp;云计算&amp;大数据","slug":"虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"How to Change Ingress-Nginx Nginx's Configuration？","slug":"how-to-change-ingressnginx-nginxs-configuration","date":"2018-07-03T05:51:33.000Z","updated":"2025-09-01T01:59:08.921Z","comments":true,"path":"2018/07/03/how-to-change-ingressnginx-nginxs-configuration/","permalink":"https://blog.sctux.cc/2018/07/03/how-to-change-ingressnginx-nginxs-configuration/","excerpt":"序今天访问了一下之前搭建的那套k8s应用，当我客户端通过域名访问(ingress-nginx通过域名转发请求到后端)时，出现了504的情况；第一想到的就是当后端一直没返回时；proxy超时了； 192.168.56.1 - [192.168.56.1] - - [03/Jul/2018:09:16:54 +0000] \"GET / HTTP/1.1\" 504 586 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36\" 412 10.002 [flask-app-extions-stage-flask-app-nginx-80] 10.2.17.5:80 0 10.003 504 497fe6db73abcd3ed749fdc646870432 # 响应10.003秒就报504😢 ￼问题就出在这里,ingress-nginx代理用户请求到flask-app-nginx SVC￼ 而且后端在集群内部直接访问应用的ClusterIP是没有问题；￼ 那解决办法就是修改ingress-nginx 的nginx配置；前面实验中我使用了configmap来挂nginx配置，ingerss-nginx的配置也是通过这种方式，只是说实现的方式不一样；他是在创建了ingress之后根据规则生成的nginx配置；进入pod容器中发现默认的nginx配置参数为: proxy_connect_timeout 5s; proxy_send_timeout 60s; proxy_read_timeout 60s; 这个时间还是蛮短的，所以我需要修改它，那咋个修改嘛？不可能vi nginx.conf 然后保存，再从新reload pod？ 这样能解决，但是不合理，于是官网提供给了这种方式:https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/也就是在官方提供的configmap.yaml中 添加覆盖原有默认值 [root@linux-node1 ingress-nginx]# more configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: nginx-configuration namespace: ingress-nginx labels: app: ingress-nginx data: proxy-connect-timeout: \"30\" proxy-read-timeout: \"120\" proxy-send-timeout: \"120\" 然后apply一下","text":"序今天访问了一下之前搭建的那套k8s应用，当我客户端通过域名访问(ingress-nginx通过域名转发请求到后端)时，出现了504的情况；第一想到的就是当后端一直没返回时；proxy超时了； 192.168.56.1 - [192.168.56.1] - - [03/Jul/2018:09:16:54 +0000] \"GET / HTTP/1.1\" 504 586 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36\" 412 10.002 [flask-app-extions-stage-flask-app-nginx-80] 10.2.17.5:80 0 10.003 504 497fe6db73abcd3ed749fdc646870432 # 响应10.003秒就报504😢 ￼问题就出在这里,ingress-nginx代理用户请求到flask-app-nginx SVC￼ 而且后端在集群内部直接访问应用的ClusterIP是没有问题；￼ 那解决办法就是修改ingress-nginx 的nginx配置；前面实验中我使用了configmap来挂nginx配置，ingerss-nginx的配置也是通过这种方式，只是说实现的方式不一样；他是在创建了ingress之后根据规则生成的nginx配置；进入pod容器中发现默认的nginx配置参数为: proxy_connect_timeout 5s; proxy_send_timeout 60s; proxy_read_timeout 60s; 这个时间还是蛮短的，所以我需要修改它，那咋个修改嘛？不可能vi nginx.conf 然后保存，再从新reload pod？ 这样能解决，但是不合理，于是官网提供给了这种方式:https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/也就是在官方提供的configmap.yaml中 添加覆盖原有默认值 [root@linux-node1 ingress-nginx]# more configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: nginx-configuration namespace: ingress-nginx labels: app: ingress-nginx data: proxy-connect-timeout: \"30\" proxy-read-timeout: \"120\" proxy-send-timeout: \"120\" 然后apply一下 kubectl apply -f configmap.yaml 在执行这个命令的时候pod会去重新加载配置configmap的信息通过pod的日志可以看到: [root@linux-node1 ~]# kubectl logs -f pod/nginx-ingress-controller-t9jh9 -n ingress-nginx I0703 10:21:24.014062 5 event.go:218] Event(v1.ObjectReference{Kind:\"ConfigMap\", Namespace:\"ingress-nginx\", Name:\"nginx-configuration\", UID:\"df01bd6b-79f4-11e8-95a2-000c29c6d12b\", APIVersion:\"v1\", ResourceVersion:\"1013650\", FieldPath:\"\"}): type: 'Normal' reason: 'UPDATE' ConfigMap ingress-nginx/nginx-configuration I0703 10:21:24.015721 5 controller.go:169] Configuration changes detected, backend reload required. I0703 10:21:24.539971 5 controller.go:179] Backend successfully reloaded. 再一次进入容器查看nginx的配置参数是否改变: proxy_connect_timeout 30s; proxy_send_timeout 120s; proxy_read_timeout 120s; 可以看到原来默认的配置参数值已经修改成功；客户端再次访问就不会出现这种问题了：￼","categories":[{"name":"Docker","slug":"Docker","permalink":"https://blog.sctux.cc/categories/Docker/"},{"name":"虚拟化&amp;云计算&amp;大数据","slug":"Docker/虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/Docker/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"Docker","slug":"Docker","permalink":"https://blog.sctux.cc/categories/Docker/"},{"name":"虚拟化&amp;云计算&amp;大数据","slug":"Docker/虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/Docker/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"劝君莫惜金缕衣，劝君惜取少年时。","slug":"e5-8a-9d-e5-90-9b-e8-8e-ab-e6-83-9c-e9-87-91-e7-bc-95-e8-a1-a3-ef-bc-8c-e5-8a-9d-e5-90-9b-e6-83-9c-e5-8f-96-e5-b0-91-e5-b9-b4-e6-97-b6-e3-80-82-trashed","date":"2018-07-02T12:32:00.000Z","updated":"2025-09-01T01:59:08.869Z","comments":true,"path":"2018/07/02/e5-8a-9d-e5-90-9b-e8-8e-ab-e6-83-9c-e9-87-91-e7-bc-95-e8-a1-a3-ef-bc-8c-e5-8a-9d-e5-90-9b-e6-83-9c-e5-8f-96-e5-b0-91-e5-b9-b4-e6-97-b6-e3-80-82-trashed/","permalink":"https://blog.sctux.cc/2018/07/02/e5-8a-9d-e5-90-9b-e8-8e-ab-e6-83-9c-e9-87-91-e7-bc-95-e8-a1-a3-ef-bc-8c-e5-8a-9d-e5-90-9b-e6-83-9c-e5-8f-96-e5-b0-91-e5-b9-b4-e6-97-b6-e3-80-82-trashed/","excerpt":"人生短短几十年、工作、学习(专业技能知识)也许就占了我们生命中的大部分的时间、也是我们生存下来的基础。但是能不能在这基础之上找点缝隙学习、尝试一些不一样的东西呢，这个还是看个人吧~~~我个人倒是现在除了工作、学习还是需要培养一点其他的兴趣爱好，从各个方面不断丰富、完善自己。 回想起年幼时父亲用最严厉的方式逼着我练字，当时根本不懂为什么非要逼着我练习，现在回想起来我只能打心底的感激父亲让我有了这个习惯或者说是培养了我这个兴趣爱好。以至于能写出还算拿得出手的字体。 劝君莫惜金缕衣，劝君惜取少年时。 花开堪折直须折，莫待花开空折枝。","text":"人生短短几十年、工作、学习(专业技能知识)也许就占了我们生命中的大部分的时间、也是我们生存下来的基础。但是能不能在这基础之上找点缝隙学习、尝试一些不一样的东西呢，这个还是看个人吧~~~我个人倒是现在除了工作、学习还是需要培养一点其他的兴趣爱好，从各个方面不断丰富、完善自己。 回想起年幼时父亲用最严厉的方式逼着我练字，当时根本不懂为什么非要逼着我练习，现在回想起来我只能打心底的感激父亲让我有了这个习惯或者说是培养了我这个兴趣爱好。以至于能写出还算拿得出手的字体。 劝君莫惜金缕衣，劝君惜取少年时。 花开堪折直须折，莫待花开空折枝。","categories":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}],"tags":[],"keywords":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}]},{"title":"10-K8s集群搭建---新增Node节点","slug":"10k8s-ji-qun-da-jianxin-zengnode-jie-dian","date":"2018-06-29T07:29:10.000Z","updated":"2025-09-01T01:59:08.984Z","comments":true,"path":"2018/06/29/10k8s-ji-qun-da-jianxin-zengnode-jie-dian/","permalink":"https://blog.sctux.cc/2018/06/29/10k8s-ji-qun-da-jianxin-zengnode-jie-dian/","excerpt":"什么？资源不够用了？ 怼服务器配置啊(向上扩展)，怼机器啊(横向扩展)！ (注: 该节点添加是基于之前k8s集群搭建的环境进行) 1.系统初始化:a. 主机名配置 node4: echo \"linux-node4.example.com\" &gt; /etc/hostname b. 设置/etc/hosts保证主机名能够解析 node4: echo \"192.168.56.14 linux-node4 linux-node4.example.com\" &gt;&gt; /etc/hosts c. 关闭SELinux及防火墙 node4: systemctl disable firewalld; systemctl stop firewalld sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config d. 环境变量配置(后续k8s相关命令都会放到/opt/kubernetes/bin目录下)","text":"什么？资源不够用了？ 怼服务器配置啊(向上扩展)，怼机器啊(横向扩展)！ (注: 该节点添加是基于之前k8s集群搭建的环境进行) 1.系统初始化:a. 主机名配置 node4: echo \"linux-node4.example.com\" &gt; /etc/hostname b. 设置/etc/hosts保证主机名能够解析 node4: echo \"192.168.56.14 linux-node4 linux-node4.example.com\" &gt;&gt; /etc/hosts c. 关闭SELinux及防火墙 node4: systemctl disable firewalld; systemctl stop firewalld sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config d. 环境变量配置(后续k8s相关命令都会放到/opt/kubernetes/bin目录下) echo \"PATH=$PATH:$HOME/bin:/opt/kubernetes/bin\" &gt;&gt; ~/.bash_profile source ~/.bash_profile 4.安装Dockera：使用国内Docker源 [root@linux-node4 ~]# cd /etc/yum.repos.d/ [root@linux-node4 yum.repos.d]# wget \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo b：Docker安装： [root@linux-node4 ~]# yum install -y docker-ce c：启动后台进程： [root@linux-node4 ~]# systemctl enable docker [root@linux-node4 ~]# systemctl start docker 5.准备部署目录[root@linux-node4 ~]# mkdir -p /opt/kubernetes/{cfg,bin,ssl,log} # 目录结构, 所有文件均存放在/opt/kubernetes目录下： [root@linux-node4 ~]# tree -L 1 /opt/kubernetes/ /opt/kubernetes/ ├── bin #二进制文件 ├── cfg #配置文件 ├── log #日志文件 └── ssl #证书文件 6.做好master节点跟其他node节点的ssh互信,便于搭建[root@linux-node1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.56.14 7.拷贝配置(from master)1.拷贝CFSSL[root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.14:/opt/kubernetes/bin 2.分发证书[root@linux-node1 ssl]# scp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.14:/opt/kubernetes/ssl 3.拷贝kubernetes 证书和私钥[root@linux-node1 ssl]# scp /opt/kubernetes/ssl/kubernetes*.pem 192.168.56.14:/opt/kubernetes/ssl/ 4.拷贝kubelet，kube-proxy软件包[root@linux-node1 ~]# cd /usr/local/src/kubernetes/server/bin/ [root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.14:/opt/kubernetes/bin/ 5.拷贝角色绑定文件[root@linux-node1 ~]# scp /opt/kubernetes/cfg/bootstrap.kubeconfig 192.168.56.14:/opt/kubernetes/cfg 6.拷贝CNI支持配置[root@linux-node1 ~]# ssh linux-node4 \"mkdir /etc/cni/net.d -p\" [root@linux-node1 ~]# scp /etc/cni/net.d/10-default.conf linux-node4:/etc/cni/net.d/ 7.创建kubelet所需目录[root@linux-node1 ~]# ssh linux-node4 \"mkdir /var/lib/kubelet\" 8.拷贝Flannel网络证书[root@linux-node1 ~]# scp /opt/kubernetes/ssl/flanneld*.pem 192.168.56.14:/opt/kubernetes/ssl/ 9.复制flanner相关软件包[root@linux-node1 ~]# ssh linux-node4 \"mkdir /opt/kubernetes/bin/cni\" [root@linux-node1 ~]# scp /opt/kubernetes/bin/cni/* 192.168.56.14:/opt/kubernetes/bin/cni/ [root@linux-node1 ~]# scp /usr/lib/systemd/system/flannel.service 192.168.56.14:/usr/lib/systemd/system/ [root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.14:/opt/kubernetes/cfg/ [root@linux-node1 ~]# cd /usr/local/src [root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.14:/opt/kubernetes/bin/ [root@linux-node1 src]# cd /usr/local/src/kubernetes/cluster/centos/node/bin/ [root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.14:/opt/kubernetes/bin/ # docker 服务脚本 [root@linux-node1 bin]# scp /usr/lib/systemd/system/docker.service 192.168.56.14:/usr/lib/systemd/system/ 10.拷贝kubelet系统服务配置[root@linux-node1 ~]# scp /usr/lib/systemd/system/kubelet.service 192.168.56.14:/usr/lib/systemd/system/ [root@linux-node1 ~]# ssh linux-node4 \"systemctl daemon-reload\" [root@linux-node1 ~]# ssh linux-node4 \"systemctl enable kubelet\" [root@linux-node1 ~]# ssh linux-node4 \"systemctl start kubelet\" [root@linux-node1 ~]# ssh linux-node4 \"systemctl status kubelet\" # 然后在node1节点查看TLS证书请求，通过以下就行了，如果结果为空，就需要查看kubelet的日志啦，反正我在这里就跳进了一回自己挖的坑o(╥﹏╥)o [root@linux-node1 ~]# kubectl get csr [root@linux-node1 ~]# kubectl get csr|grep 'Pending' | awk 'NR&gt;0{print $1}'| xargs kubectl certificate [root@linux-node1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION 192.168.56.11 Ready master 1d v1.10.1 192.168.56.12 Ready node 31d v1.10.1 192.168.56.13 Ready node 31d v1.10.1 192.168.56.14 Ready node 1d v1.10.1 # 以上，新的计算节点192.168.56.14已经成功添加 11.配置kube-proxy使用LVS[root@linux-node4 ~]# yum install -y ipvsadm ipset conntrack 12.分发kube-proxy证书[root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.14:/opt/kubernetes/ssl/ 13.分发kubeconfig配置文件[root@linux-node1 ssl]# scp ../cfg/kube-proxy.kubeconfig 192.168.56.14:/opt/kubernetes/cfg/ 14.安装nfs(因为node节点作为客户端也需要安装nfs客户端工具)[root@linux-node1 ssl]# ssh linux-node4 \"yum -y install nfs-utils rpcbind\" 15.创建工作目录[root@linux-node1 ~]# ssh linux-node4 \"mkdir /var/lib/kube-proxy\" 16.拷贝服务配置文件[root@linux-node1 ~]# cp /usr/lib/systemd/system/kube-proxy.service 192.168.56.14:/usr/lib/systemd/system/kube-proxy.service # 注意需要修改配置文件的IP [root@linux-node1 ~]# ssh linux-node4 \"systemctl daemon-reload\" [root@linux-node1 ~]# ssh linux-node4 \"systemctl enable kube-proxy\" [root@linux-node1 ~]# ssh linux-node4 \"systemctl start kube-proxy\" [root@linux-node1 ~]# ssh linux-node4 \"systemctl status kube-proxy\" 验证我们先来Scaling flask-app pod到20个, 然后看Kubenertes Scheduler是否能把请求调度到新的节点上面 [root@linux-node1 ~]# kubectl scale --replicas=20 deployment.extensions/flask-app -n flask-app-extions-stage deployment.extensions \"flask-app\" scaled 上图可以看到 Scaling的pod已经由Kubenertes Scheduler调度到各个node运行，并且状态已经是Running 以上，可以看到我新添加的节点已经成功加到了集群当中，而且运行结果也是我预期的；唯一不足的地方是这个步骤如果后期在工作中用到的话是不太方便维护的,但是手动这样会知道一个节点需要用到哪些东西，需要怎么配置，也算是对k8s的运行机制有更深点的认知。后续有时间还是搞一下用SaltStack来部署吧~🍺🍺🍺","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"Python App on Kubernets Cluster","slug":"flask_kubenertes","date":"2018-06-27T02:25:20.000Z","updated":"2025-09-01T01:59:08.873Z","comments":true,"path":"2018/06/27/flask_kubenertes/","permalink":"https://blog.sctux.cc/2018/06/27/flask_kubenertes/","excerpt":"序此次部署是在前面博文中搭建的K8S集群基础之上进行的，涉及使用到的容器已经推到Dockerhub代码、文件：https://github.com/guomaoqiu/flask_kubernetes 配置清单 逻辑流程图 初始配置1.创建一个namespace 供此次部署使用1234567[root@linux-node1 ~]# kubectl create namespace flask-app-extions-stage[root@linux-node1 ~]# kubectl get nsNAME STATUS AGEdefault Active 29dflask-app-extions-stage Active 1mkube-public Active 29dkube-system Active 29d 1. 创建用NFS存储目录","text":"序此次部署是在前面博文中搭建的K8S集群基础之上进行的，涉及使用到的容器已经推到Dockerhub代码、文件：https://github.com/guomaoqiu/flask_kubernetes 配置清单 逻辑流程图 初始配置1.创建一个namespace 供此次部署使用1234567[root@linux-node1 ~]# kubectl create namespace flask-app-extions-stage[root@linux-node1 ~]# kubectl get nsNAME STATUS AGEdefault Active 29dflask-app-extions-stage Active 1mkube-public Active 29dkube-system Active 29d 1. 创建用NFS存储目录1234# 用于flask-app代码存放目录，当pod启动时通过NFS的方式挂载进去[root@linux-node1 ~]# mkdir /data/flask-app-data# 用于flask的mysql数据存放目录，当pod启动时通过NFS挂载进去[root@linux-node1 ~]# mkdir /data/flask-app-db 2. 安装NFS Server(略)3. 写入配置12[root@linux-node1 ~]# echo \"/data/flask-app-db *(rw,sync,no_subtree_check,no_root_squash)\" &gt; /etc/exports[root@linux-node1 ~]# echo \"/data/flask-app-data *(rw,sync,no_subtree_check,no_root_squash)\" &gt;&gt; /etc/exports 4.重启验证1234567[root@linux-node1 ~]# systemctl restart nfs-server[root@linux-node1 ~]# exportfs# mysql data/data/flask-app-db# flask app code/data/flask-app-data 5.创建flask-app使用的pv及pvc1234[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask-app/flask_app_data_pv.yamlpersistentvolume \"flask-app-data-pv\" created[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask-app/flask_app_data_pvc.yamlpersistentvolumeclaim \"flask-app-data-pv-claim\" created 6.创建flask-app-db使用的pv及pvc12345678910111213[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app_db/flask_app_db_pv.yamlpersistentvolume \"flask-app-db-pv\" created[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app_db/flask_app_db_pvc.yamlpersistentvolumeclaim \"flask-app-db-pv-claim\" created[root@linux-node1 ~]# kubectl get pv -n flask-app-extions-stageNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEflask-app-data-pv 5Gi RWO Recycle Bound flask-app-extions-stage/flask-app-data-pv-claim 5mflask-app-db-pv 5Gi RWO Recycle Bound flask-app-extions-stage/flask-app-db-pv-claim 26s[root@linux-node1 ~]# kubectl get pvc -n flask-app-extions-stageNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEflask-app-data-pv-claim Bound flask-app-data-pv 5Gi RWO 4mflask-app-db-pv-claim Bound flask-app-db-pv 5Gi RWO 28s[root@linux-node1 ~]# 部署 flask-app-db运行flask交互数据的数据库使用的是mysql，上面我已经创建了用于基于NFS存储mysql数据的持久存储目录 /data/flask-app-db 1. 创建配置 MySQL 密码的 Secret12345[root@linux-node1 ~]# kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD[root@linux-node1 ~]# kubectl get secret -n flask-app-extions-stageNAME TYPE DATA AGEdefault-token-fr2sg kubernetes.io/service-account-token 3 47mmysql-pass Opaque 1 14s 2. 部署 MySQL：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app_db/flask_app_db_deploy.yamldeployment.apps \"flask-app-db\" created[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app_db/flask_app_db_service.yamlservice \"flask-app-db\" created# 查看状态NAME READY STATUS RESTARTS AGE IP NODEpod/flask-app-db-6f55458666-h2dk7 1/1 Running 1 22h 10.2.15.108 192.168.56.12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/flask-app-db NodePort 10.1.68.29 &lt;none&gt; 3306:30006/TCP 22h app=flask-app-dbNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.extensions/flask-app-db 1 1 1 1 22h mysql mysql:5.6 app=flask-app-db,tier=mysql# 以上可以知道该pod运行在节点192.168.56.12上面，我这里使用的是NodePort方式，然后映射了一个30006端口出来到节点上面# 可以尝试登陆测试[root@linux-node1 flask_app_db]# mysql -uroot -pdevopsdemo -h192.168.56.12 -P 30006Welcome to the MariaDB monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.6.40 MySQL Community Server (GPL)Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.MySQL [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema |+--------------------+3 rows in set (0.02 sec)MySQL [(none)]&gt;# 顺便我们在这里手动创建一下flask-app需要用到的数据库 devopsdemo, 因为在flask-app在启动过程中需要去初始化数据库并创建数据表。MySQL [(none)]&gt; CREATE DATABASE devopsdemo;Query OK, 1 row affected (0.01 sec)#数据库目录,可以看到mysql数据已经通过nfs的方式存储到了我们nfs server所在主机映射出去的目录[root@linux-node1 ~]# tree /data/flask-app-db/ -L 1/data/flask-app-db/├── auto.cnf├── devopsdemo├── ibdata1├── ib_logfile0├── ib_logfile1├── mysql└── performance_schema3 directories, 4 files 部署flask-app1.将flask程序代码放到/data/flask-app-data/123456789101112131415[root@linux-node1 ~]# cp -rf flask_app_code/* /data/flask-app-data/[root@linux-node1 ~]# tree /data/flask-app-data/ -L 1/data/flask-app-data/├── app├── config.py├── flask_uwsgi.ini├── LICENSE├── manage.py├── README.md├── requirements.txt├── screenshots├── supervisord.conf└── tests3 directories, 9 files 2. 修改flask程序连接数据库的配置信息123456789[root@linux-node1 ~]# vim /data/flask-app-data/config.py............ db_host = 'flask-app-db' # 在pod启动过程中会去加载k8s的环境变量；这个flask-app-db 就是mysql的svc db_user = 'root' # 默认为root db_pass = \"devopsdemo\" # 数据库密码 db_name = 'devopsdemo' # 数据库名称............ 3. 部署flask-app12345678910111213141516171819202122232425[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app/flask_app_deployment.yamldeployment.apps \"flask-app\" created[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app/flask_app_service.yamlservice \"flask-app\" created# 查看状态:[root@linux-node1 ~]# kubectl get pod,svc,deployment,rc -o wide -n flask-app-extions-stageNAME READY STATUS RESTARTS AGE IP NODEpod/flask-app-65646687ff-4gg7g 1/1 Running 0 17h 10.2.42.125 192.168.56.13pod/flask-app-65646687ff-7d5g6 1/1 Running 0 17h 10.2.42.124 192.168.56.13pod/flask-app-65646687ff-xkq6k 1/1 Running 0 17h 10.2.42.123 192.168.56.13pod/flask-app-db-6f55458666-h2dk7 1/1 Running 1 22h 10.2.15.108 192.168.56.12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/flask-app ClusterIP 10.1.173.169 &lt;none&gt; 3032/TCP 22h app=flask-appservice/flask-app-db NodePort 10.1.68.29 &lt;none&gt; 3306:30006/TCP 22h app=flask-app-dbNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.extensions/flask-app 3 3 3 3 22h flask-app guomaoqiu/python27baseenv:v2 app=flask-app,tier=frontenddeployment.extensions/flask-app-db 1 1 1 1 22h mysql mysql:5.6 app=flask-app-db,tier=mysql# 以上可知 我的flask-app运行了三个副本；# 并且采用的是ClusterIP Type,3032是flask+supervisor+uwsgi 之后 uwsgsi 暴露出来的二端口# 此时我们访问是没有用的；因为uwgsi需要结合用到Nginx来访问；于是我下面将会部署nginx ---&gt; uwgsi(flask-app) 部署flask-app-nginx因为这个nginx的配置我这里需要修改做一些定制方面的配置，所以有一个将单个配置文件挂载到pod里面的需求；于是这里使用到了k8s的ConfigMap功能 1. 找到需要挂载进pod的nginx配置文件:123456[root@linux-node1 ~]# kubectl create configmap nginx-conf --from-file=/root/flask_kubernetes/flask_app_nginx/nginx.conf -n flask-app-extions-stage[root@linux-node1 ~]# kubectl get configmap -n flask-app-extions-stageNAME DATA AGEnginx-conf 1 11s# 通过describe就可以看到这个conf的详细内容，其实就在我们nginx.conf配置文件的基础上加上了k8s一些特有的属性值[root@linux-node1 ~]# kubectl describe configmap/nginx-conf -n flask-app-extions-stage 2.部署flask-app-nginx123456789101112131415161718192021222324252627282930313233343536[root@linux-node1 flask_app_nginx]# kubectl create -f flask_app_nginx_deploy.yamldeployment.apps \"flask-app-nginx\" created[root@linux-node1 flask_app_nginx]# kubectl create -f flask_app_nginx_service.yamlservice \"flask-app-nginx\" created# 查看状态：[root@linux-node1 ~]# kubectl get pod,svc,deployment,rc -o wide -n flask-app-extions-stageNAME READY STATUS RESTARTS AGE IP NODEpod/flask-app-65646687ff-4gg7g 1/1 Running 0 17h 10.2.42.125 192.168.56.13pod/flask-app-65646687ff-7d5g6 1/1 Running 0 17h 10.2.42.124 192.168.56.13pod/flask-app-65646687ff-xkq6k 1/1 Running 0 17h 10.2.42.123 192.168.56.13pod/flask-app-db-6f55458666-h2dk7 1/1 Running 1 22h 10.2.15.108 192.168.56.12pod/flask-app-nginx-657fd4c57c-p6qdx 1/1 Running 12 18h 10.2.42.119 192.168.56.13pod/flask-app-nginx-657fd4c57c-v4qsp 1/1 Running 12 18h 10.2.42.117 192.168.56.13pod/flask-app-nginx-657fd4c57c-xtpmm 1/1 Running 12 18h 10.2.42.118 192.168.56.13NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/flask-app ClusterIP 10.1.173.169 &lt;none&gt; 3032/TCP 22h app=flask-appservice/flask-app-db NodePort 10.1.68.29 &lt;none&gt; 3306:30006/TCP 22h app=flask-app-dbservice/flask-app-nginx ClusterIP 10.1.193.82 &lt;none&gt; 80/TCP 21h app=flask-app-nginxNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.extensions/flask-app 3 3 3 3 22h flask-app guomaoqiu/python27baseenv:v2 app=flask-app,tier=frontenddeployment.extensions/flask-app-db 1 1 1 1 22h mysql mysql:5.6 app=flask-app-db,tier=mysqldeployment.extensions/flask-app-nginx 3 3 3 3 21h nginx nginx:latest app=flask-app-nginx# 以上 运行了三个flak-app-nginx ,采用的是ClusterIP Type# 访问验证,直接访问的是flask-app-nginx 的VIP(ClusterIP)[root@linux-node1 ~]# curl -I 10.1.193.82/auth/loginHTTP/1.1 200 OKServer: nginx/1.15.0Date: Wed, 27 Jun 2018 04:35:12 GMTContent-Type: text/html; charset=utf-8Content-Length: 4026Connection: keep-aliveSet-Cookie: session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiT0RrNVpXUmpZV014T1daalkyUmlOamRsWXpBNVpqUTVZMk0wTkRnMU9ERm1NVFUzTURrME5BPT0ifX0.DhSlgA.Biu7EYC7qfj4x8--HlR8VUZFUgk; HttpOnly; Path=/ 以上说明我们能够成功的访问到我们的flask-app服务啦,在用linux 终端下的w3m http://10.1.193.82/auth/login 访问一下呢，说明也是没问题的；那后端uwgsi or flask-app or flask-nginx的访问日志此时毋庸置疑是已经有了的。 那问题来了；此时我只是在集群内部能够访问，那如何在外面访问呢？那就主要将flask_nginx_service.yaml中的Type该为nodePort,然后从新部署一下flask_app_nginx_services 12345678910111213141516171819202122232425262728293031[root@linux-node1 ~]# more /root/flask_kubernetes/flask_app_nginx/flask_app_nginx_service.yamlkind: ServiceapiVersion: v1metadata: name: flask-app-nginx namespace: flask-app-extions-stagespec: type: NodePort selector: app: flask-app-nginx ports: - protocol: TCP port: 80 targetPort: 80 nodePort: 30001 selector: app: flask-app-nginx[root@linux-node1 ~]# kubectl describe svc/flask-app-nginx -n flask-app-extions-stageName: flask-app-nginxNamespace: flask-app-extions-stageLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=flask-app-nginxType: ClusterIPIP: 10.1.193.82Port: &lt;unset&gt; 80/TCPTargetPort: 80/TCPEndpoints: 10.2.42.117:80,10.2.42.118:80,10.2.42.119:80Session Affinity: NoneEvents: &lt;none&gt; 此时如果是外网访问就应该是NodeIP+nodePort啦： 但是这种方式并不是很理想；本来就是要让他直接访问80 port的，于是采用另外一种暴露端口的方式——Ingress （这里我还是把flask-app-nginx的service改回了ClusterIP) 部署 Ingress-NginxIngress 使用开源的反向代理负载均衡器来实现对外暴漏服务，比如 Nginx、Apache、Haproxy等。Nginx Ingress 一般有三个组件组成： Nginx 反向代理负载均衡器 Ingress Controller 可以理解为控制器，它通过不断的跟 Kubernetes API 交互，实时获取后端 Service、Pod 等的变化，比如新增、删除等，然后结合 Ingress 定义的规则生成配置，然后动态更新上边的 Nginx 负载均衡器，并刷新使配置生效，来达到服务自动发现的作用。 Ingress 则是定义规则，通过它定义某个域名的请求过来之后转发到集群中指定的 Service。它可以通过 Yaml 文件定义，可以给一个或多个 Service 定义一个或多个 Ingress 规则。 1.获取官方提供的yaml12345678910111213[root@linux-node1 ~]# cd flask_8s &amp;&amp; git clone https://github.com/kubernetes/ingress-nginx.git[root@linux-node1 ~]# tree flask_8s/ingress-nginx/flask_8s/ingress-nginx/├── configmap.yaml :提供configmap可以在线更行nginx的配置├── default-backend.yaml :提供一个缺省的后台错误页面 404├── mandatory.yaml :这个文件包含了这个目录下面所有yaml的内容，可以不用├── namespace.yaml :创建一个独立的命名空间 ingress-nginx├── rbac.yaml :创建对应的role rolebinding 用于rbac├── tcp-services-configmap.yaml :修改L4负载均衡配置的configmap├── udp-services-configmap.yaml :修改L4负载均衡配置的configmap└── with-rbac.yaml :有应用rbac的nginx-ingress-controller组件 0 directories, 8 files 2.修改官方的配置 kind: DaemonSet：官方原始文件使用的是deployment，replicate 为 1，这样将会在某一台节点上启动对应的nginx-ingress-controller pod。外部流量访问至该节点，由该节点负载分担至内部的service。测试环境考虑防止单点故障，改为DaemonSet然后删掉replicate ，配合亲和性部署在制定节点上启动nginx-ingress-controller pod，确保有多个节点启动nginx-ingress-controller pod，后续将这些节点加入到外部硬件负载均衡组实现高可用性。 hostNetwork: true：添加该字段，暴露nginx-ingress-controller pod的服务端口（80） nodeSelector: 增加亲和性部署，有custom/ingress-controller-ready 标签的节点才会部署该DaemonSet 3.为需要部署nginx-ingress-controller的节点设置lable12345678[root@linux-node1 ~]# kubectl label nodes 192.168.56.12 custom/ingress-controller-ready=true[root@linux-node1 ~]# kubectl label nodes 192.168.56.13 custom/ingress-controller-ready=true[root@linux-node1 ~]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELS192.168.56.12 Ready &lt;none&gt; 28d v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,custom/ingress-controller-ready=true,kubernetes.io/hostname=192.168.56.12192.168.56.13 Ready &lt;none&gt; 27d v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,custom/ingress-controller-ready=true,kubernetes.io/hostname=192.168.56.13# 以上，因为我这里就两个计算节点；所以都打上custom/ingress-controller-ready=true这个标签 4.执行创建yaml文件:1234567[root@linux-node1 ~]# kubectl create -f namespace.yaml[root@linux-node1 ~]# kubectl create -f default-backend.yaml[root@linux-node1 ~]# kubectl create -f configmap.yaml[root@linux-node1 ~]# kubectl create -f tcp-services-configmap.yaml[root@linux-node1 ~]# kubectl create -f udp-services-configmap.yaml[root@linux-node1 ~]# kubectl create -f rbac.yaml[root@linux-node1 ~]# kubectl create -f with-rbac.yaml 5.查看创建状态：创建过程会去拉镜像比较慢，可能会不成功，前面说过那都是因为`%#@%￥#@·的原因，所以还是记得给docker一把梯子。 1234567891011121314[root@linux-node1 ~]# kubectl get pod,svc,deployment,rc -o wide -n ingress-nginxNAME READY STATUS RESTARTS AGE IP NODEpod/default-http-backend-5c6d95c48-dwl56 1/1 Running 0 16h 10.2.42.130 192.168.56.13pod/nginx-ingress-controller-55trv 1/1 Running 0 15h 192.168.56.12 192.168.56.12pod/nginx-ingress-controller-58nf4 1/1 Running 0 15h 192.168.56.13 192.168.56.13NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/default-http-backend ClusterIP 10.1.89.141 &lt;none&gt; 80/TCP 16h app=default-http-backendNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.extensions/default-http-backend 1 1 1 1 16h default-http-backend gcr.io/google_containers/defaultbackend:1.4 app=default-http-backend# 以上可以看到nginx-ingress-controller已经成功运行在了这两个打了标签的节点上面 此时只是把ingress给搭建好，如果要用得根据实际情况写转发规则了，当前我们的目的是通过它定义某个域名的请求过来之后转发到集群中指定的 Service，即此次部署的 flask-app-nginx 6.创建规则：123456789101112131415161718192021222324252627282930313233343536373839404142[root@linux-node1 ingress-nginx]# cat &gt; test.ingress.yaml &lt; EOFapiVersion: extensions/v1beta1kind: Ingressmetadata: name: test-ingress namespace: flask-app-extions-stagespec: rules: - host: test.flaskapp.ingress http: paths: - path: / backend: serviceName: flask-app-nginx servicePort: 80EOF# host: 对应的域名 # path: url上下文 # backend:后向转发 到对应的 serviceName: servicePort:[root@linux-node1 ingress-nginx]# kubectl apply -f test-ingress.yaml[root@linux-node1 ingress-nginx]# kubectl get ingress -n flask-app-extions-stageNAME HOSTS ADDRESS PORTS AGEtest-ingress test.flaskapp.ingress 80 15h[root@linux-node1 ingress-nginx]# kubectl describe ingress/test-ingress -n flask-app-extions-stageName: test-ingressNamespace: flask-app-extions-stageAddress:Default backend: default-http-backend:80 (&lt;none&gt;)Rules: Host Path Backends ---- ---- -------- test.flaskapp.ingress / flask-app-nginx:80 (&lt;none&gt;)Annotations:Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 15h nginx-ingress-controller Ingress flask-app-extions-stage/test-ingress Normal CREATE 15h nginx-ingress-controller Ingress flask-app-extions-stage/test-ingress Normal CREATE 15h nginx-ingress-controller Ingress flask-app-extions-stage/test-ingress Normal CREATE 15h nginx-ingress-controller Ingress flask-app-extions-stage/test-ingress 7.测试：既然是通过域名访问，那这里我就在node1上面直接修改hosts的方式然后访问 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@linux-node1 ~]# echo \"192.168.56.12 test.flaskapp.ingress\" &gt;&gt; /etc/hosts[root@linux-node1 ~]# echo \"192.168.56.13 test.flaskapp.ingress\" &gt;&gt; /etc/hosts[root@linux-node1 ingress-nginx]# curl -I test.flaskapp.ingress/auth/loginHTTP/1.1 200 OKServer: nginx/1.13.12Date: Thu, 28 Jun 2018 02:59:16 GMTContent-Type: text/html; charset=utf-8Content-Length: 4030Connection: keep-aliveVary: Accept-EncodingSet-Cookie: session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiTXpGbFlUWmlOelV4WXpabVlqTXlNVFJpWTJVMk1HWTVObVV5TURRd09HSTNPVFV4WXprd01RPT0ifX0.DhXghA.3HHbX3fvShem9KlkINr8jDGwcSc; HttpOnly; Path=/# 或者指定模拟的域名:[root@linux-node1 ingress-nginx]# curl -vI http://192.168.56.12/auth/login -H 'host: test.flaskapp.ingress'* About to connect() to 192.168.56.12 port 80 (#0)* Trying 192.168.56.12...* Connected to 192.168.56.12 (192.168.56.12) port 80 (#0)&gt; HEAD /auth/login HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Accept: */*&gt; host: test.flaskapp.ingress&gt;&lt; HTTP/1.1 200 OKHTTP/1.1 200 OK&lt; Server: nginx/1.13.12Server: nginx/1.13.12&lt; Date: Thu, 28 Jun 2018 03:00:21 GMTDate: Thu, 28 Jun 2018 03:00:21 GMT&lt; Content-Type: text/html; charset=utf-8Content-Type: text/html; charset=utf-8&lt; Content-Length: 4030Content-Length: 4030&lt; Connection: keep-aliveConnection: keep-alive&lt; Vary: Accept-EncodingVary: Accept-Encoding&lt; Set-Cookie: session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiT1RNMVpUWTBZV1V6TkdWaU5EazBZMkl5TmpZMFl6VTNPVFF3TVdaa09UVXpNall3T1RRNE1RPT0ifX0.DhXgxQ.BnjwR-swXvmD-kUGhtlvhpHuUIY; HttpOnly; Path=/Set-Cookie: session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiT1RNMVpUWTBZV1V6TkdWaU5EazBZMkl5TmpZMFl6VTNPVFF3TVdaa09UVXpNall3T1RRNE1RPT0ifX0.DhXgxQ.BnjwR-swXvmD-kUGhtlvhpHuUIY; HttpOnly; Path=/&lt;* Connection #0 to host 192.168.56.12 left intact 在外部如果要访问也需要绑定域名到node节点，然后访问 ok至此该项目就部署得差不多了，上面flask-app登录，注册正常！那pod如何做到伸缩呢；执行以下命令就行了 1234567891011# 目前我的flask-app是运行了3个，怼20个flask-app应用[root@linux-node1 ~]# kubectl get deploy/flask-app -n flask-app-extions-stageNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEflask-app 3 3 3 3 23h[root@linux-node1 ~]# kubectl scale --replicas=20 deployment.extensions/flask-app -n flask-app-extions-stagedeployment.extensions \"flask-app\" scaled[root@linux-node1 ~]# kubectl get deploy/flask-app -n flask-app-extions-stageNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEflask-app 20 20 20 20 23h# 所以副本数就是靠deploment中的 replicas 或者 命令行的scale --replicas来控制 部署总结：坑:在上面我部署好ingress-nginx后，通过访问哪一步报错了；于是去查了 pod/nginx-ingress-controller-58nf4 的日志，错误日志刷屏啊 官方文档不完整啊，少了创建ingress-services的内容；解决办法： 123456789101112131415161718192021[root@linux-node1 ingress-nginx]# cat &gt; ingress-nginx-services.yaml &lt; EOFapiVersion: v1kind: Servicemetadata: name: ingress-nginx namespace: ingress-nginxspec: type: NodePort ports: - name: http port: 80 targetPort: 80 protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: app: ingress-nginxEOF[root@linux-node1 ingress-nginx]# kubectl create -f ingress-nginx-services.yaml 部署遇到的主要知识点： K8S deployment的yaml文件编写； K8S 持久化存储NFS方式；配置管理ConfigMap; K8S 集群中各种端口/IP类型的工作模式以及服务暴露方式； by the way: 可能看到我创建的pod等资源的AGE已经是过去十几个小时，这个没关系的；前面创建了之后笔记就停顿了一下。后续才继续写的🍺🍺🍺","categories":[{"name":"kubernets","slug":"kubernets","permalink":"https://blog.sctux.cc/categories/kubernets/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"https://blog.sctux.cc/tags/kubernets/"}],"keywords":[{"name":"kubernets","slug":"kubernets","permalink":"https://blog.sctux.cc/categories/kubernets/"}]},{"title":"Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker 部署","slug":"nginx-uwsgi-flask-supervisord-redis-mysql-docker-e9-83-a8-e7-bd-b2-2","date":"2018-06-18T09:37:35.000Z","updated":"2025-09-01T01:59:08.863Z","comments":true,"path":"2018/06/18/nginx-uwsgi-flask-supervisord-redis-mysql-docker-e9-83-a8-e7-bd-b2-2/","permalink":"https://blog.sctux.cc/2018/06/18/nginx-uwsgi-flask-supervisord-redis-mysql-docker-e9-83-a8-e7-bd-b2-2/","excerpt":"之前使用Flask开发了两三个公司或个人使用的平台；在搭建过程当中如果换了环境的话比较麻烦；这次尝试放到docker里面去跑；下面是搭建的一个过程以及对于学习的一个记录，此次web框架还是使用的之前用Flask写的一个基础后台。 部署架构:. ├── README.md ├── docker-compose.yaml # 使用docker-compose来编排部署 ├── flask_app # 用于跑Flask应用的容器 │&nbsp;&nbsp; ├── Dockerfile │&nbsp;&nbsp; └── wait_for_db_complete.sh ├── flask_app_code # 后端项目应用代码目 │&nbsp;&nbsp; ├── LICENSE │&nbsp;&nbsp; ├── README.md │&nbsp;&nbsp; ├── app │&nbsp;&nbsp; ├── config.py │&nbsp;&nbsp; ├── manage.py │&nbsp;&nbsp; ├── requirements.txt │&nbsp;&nbsp; ├── screenshots │&nbsp;&nbsp; └── tests ├── nginx # Nginx用于前端接收用户请求的容器 │&nbsp;&nbsp; └── nginx.conf ├── python27_baseenv # 基础Python环境镜像 │&nbsp;&nbsp; ├── Dockerfile │&nbsp;&nbsp; └── README.md ├── supervisor # 用于管理uwsgi服务进程 │&nbsp;&nbsp; └── supervisord.conf └── uwsgi # 通过uWsgi来为Nginx-Flask牵线搭桥 └── flask_uwsgi.ini 访问流程: Nginx Web服务器层作为前端接收用户请求； uWSGI层作为Web服务器层与Web框架层Flask的一条纽带，将Web服务器层与Web框架连接起来 后端Web框架与数据层MySQL或Redis交互 简单理解起来就是酱紫的: Nginx：Hey，WSGI，我刚收到了一个请求，我需要你作些准备，然后由Flask来处理这个请求。 WSGI：OK，Nginx。我会设置好环境变量，然后将这个请求传递给Flask处理。 Flask：Thanks WSGI！给我一些时间，我将会把请求的响应返回给你。 WSGI：Alright，那我等你。 Flask：Okay，我完成了，这里是请求的响应结果，请求把结果传递给Nginx。 WSGI：Good job！ Nginx，这里是响应结果，已经按照要求给你传递回来了。 Nginx：Cool，我收到了，我把响应结果返回给客户端。大家合作愉快~ 搭建思路: Nginx 单独一个容器 uWSGI+Flask 单独一个容器，其中uWSGI进程由Supervisor来管理 MySQL 单独一个容器，数据目录挂载到宿主机 Redis 单独一个容器","text":"之前使用Flask开发了两三个公司或个人使用的平台；在搭建过程当中如果换了环境的话比较麻烦；这次尝试放到docker里面去跑；下面是搭建的一个过程以及对于学习的一个记录，此次web框架还是使用的之前用Flask写的一个基础后台。 部署架构:. ├── README.md ├── docker-compose.yaml # 使用docker-compose来编排部署 ├── flask_app # 用于跑Flask应用的容器 │&nbsp;&nbsp; ├── Dockerfile │&nbsp;&nbsp; └── wait_for_db_complete.sh ├── flask_app_code # 后端项目应用代码目 │&nbsp;&nbsp; ├── LICENSE │&nbsp;&nbsp; ├── README.md │&nbsp;&nbsp; ├── app │&nbsp;&nbsp; ├── config.py │&nbsp;&nbsp; ├── manage.py │&nbsp;&nbsp; ├── requirements.txt │&nbsp;&nbsp; ├── screenshots │&nbsp;&nbsp; └── tests ├── nginx # Nginx用于前端接收用户请求的容器 │&nbsp;&nbsp; └── nginx.conf ├── python27_baseenv # 基础Python环境镜像 │&nbsp;&nbsp; ├── Dockerfile │&nbsp;&nbsp; └── README.md ├── supervisor # 用于管理uwsgi服务进程 │&nbsp;&nbsp; └── supervisord.conf └── uwsgi # 通过uWsgi来为Nginx-Flask牵线搭桥 └── flask_uwsgi.ini 访问流程: Nginx Web服务器层作为前端接收用户请求； uWSGI层作为Web服务器层与Web框架层Flask的一条纽带，将Web服务器层与Web框架连接起来 后端Web框架与数据层MySQL或Redis交互 简单理解起来就是酱紫的: Nginx：Hey，WSGI，我刚收到了一个请求，我需要你作些准备，然后由Flask来处理这个请求。 WSGI：OK，Nginx。我会设置好环境变量，然后将这个请求传递给Flask处理。 Flask：Thanks WSGI！给我一些时间，我将会把请求的响应返回给你。 WSGI：Alright，那我等你。 Flask：Okay，我完成了，这里是请求的响应结果，请求把结果传递给Nginx。 WSGI：Good job！ Nginx，这里是响应结果，已经按照要求给你传递回来了。 Nginx：Cool，我收到了，我把响应结果返回给客户端。大家合作愉快~ 搭建思路: Nginx 单独一个容器 uWSGI+Flask 单独一个容器，其中uWSGI进程由Supervisor来管理 MySQL 单独一个容器，数据目录挂载到宿主机 Redis 单独一个容器 各个容器之间的关联通过docker-compose编排来实现 部署步骤：主要还是通过编写Dockerfile来定制特定的运行环境镜像 0.安装docker环境cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce docker-compose systemctl start docker 1.构建python基础运行环境镜像，基于alpine镜像cd Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker docker build -f python27_baseenv/Dockerfile . -t python27_baseenv 2.构建Flask应用框架运行所需依赖包镜像cd Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker docker build -f flask_app/Dockerfile . -t flask_app 4.Nginx镜像使用默认，配置文件需要修改，这里通过挂载方式5.Redis镜像使用默认的6.执行docker-composecd Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker docker-compose up 运行状态 登录 用户注册 Flask应用的访问、登录、注册过程日志Nginx uWSGI 部署总结:部署过程中，感觉在宿主机中部署还是没多大的区别，差别可能是在效率上面。宿主机中不能影响系统自带的一些东西，比如python的版本，这时候可能就需要用到virtualenv, 如果服务器迁移了那整个环境就需要重新搭建，还是不太方便。 此次部署呢主要目的还是以这个为一个实践目标去学习docker的compose文件编写，再把各个工具结合在一起跑在docker中实现之前在宿主机中的东西；其实把整个流程梳理清楚后编写yaml文件也很快的。后续尝试放到k8s集群中跑🍺🍺🍺 在我的VPS上面跑起来了… http://blog.sctux.com:8090","categories":[{"name":"Docker","slug":"Docker","permalink":"https://blog.sctux.cc/categories/Docker/"},{"name":"自动化运维","slug":"Docker/自动化运维","permalink":"https://blog.sctux.cc/categories/Docker/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.sctux.cc/tags/Docker/"}],"keywords":[{"name":"Docker","slug":"Docker","permalink":"https://blog.sctux.cc/categories/Docker/"},{"name":"自动化运维","slug":"Docker/自动化运维","permalink":"https://blog.sctux.cc/categories/Docker/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"09-K8s集群搭建---监控展示界面安装","slug":"09k8s-ji-qun-da-jianjian-kong-zhan-shi-jie-mian-an","date":"2018-06-08T08:20:52.000Z","updated":"2025-09-01T01:59:08.937Z","comments":true,"path":"2018/06/08/09k8s-ji-qun-da-jianjian-kong-zhan-shi-jie-mian-an/","permalink":"https://blog.sctux.cc/2018/06/08/09k8s-ji-qun-da-jianjian-kong-zhan-shi-jie-mian-an/","excerpt":"1. 简介Heapster提供了整个集群的资源监控，并支持持久化数据存储到InfluxDB、Google Cloud Monitoring或者其他的存储后端。Heapster从kubelet提供的API采集节点和容器的资源占用。另外，Heapster的 /metrics API提供了Prometheus格式的数据。InfluxDB是一个开源分布式时序、事件和指标数据库；而Grafana则是InfluxDB的 dashboard，提供了强大的图表展示功能。它们常被组合使用展示图表化的监控数据，也可以将Zabbix作为数据源，进行zabbix的监控数据展示。Heapster、InfluxDB和Grafana均以Pod的形式启动和运行，其中Heapster需要与Kubernetes Master进行安全连接。 2. 安装配置Heapster、InfluxDB和Grafana到Heapster获取安装包: [root@linux-node1 tmp]# wget https://github.com/kubernetes/heapster/archive/v1.5.3.tar.gz [root@linux-node1 tmp]# tar -xf v1.5.3.tar.gz &amp;&amp; cd heapster-1.5.3/deploy/kube-config/influxdb/ [root@linux-node1 kube-config]# ll influxdb/ total 12 -rw-rw-r-- 1 root root 2290 May 1 05:13 grafana.yaml -rw-rw-r-- 1 root root 1114 May 1 05:13 heapster.yaml -rw-rw-r-- 1 root root 974 May 1 05:13 influxdb.yaml [root@linux-node1 kube-config]# ll rbac/ total 4 -rw-rw-r-- 1 root root 264 May 1 05:13 heapster-rbac.yaml [root@linux-node1 kube-config]# # 以上是所需要用到的文件 3. 配置Docker Socks5代理(为下载所需镜像做准备)由于某国的 &amp;@#￥%&amp;￥&amp;&amp;*R#￥…… 你懂的. 导致一些镜像无法pull 我这边利用自己搭建的梯子来进行代理 [root@linux-node1 kube-config]# grep \"gcr.io\" influxdb/heapster.yaml image: gcr.io/google_containers/heapster-amd64:v1.5.3 [root@linux-node1 kube-config]# # 修改docker服务文件配置, 在[Service]部分添加你自己的ss服务器地址 [root@linux-node1 kube-config]# vim /usr/lib/systemd/system/docker.service ...... ...... [Service] Environment=\"ALL_PROXY=socks5://192.168.56.1:1080\" ...... ...... 将其复制到其他所有节点，并重载、重启Docker服务, 直接pull一下验证即可 [root@linux-node1 kube-config]# docker pull gcr.io/google_containers/heapster-amd64:v1.5.3","text":"1. 简介Heapster提供了整个集群的资源监控，并支持持久化数据存储到InfluxDB、Google Cloud Monitoring或者其他的存储后端。Heapster从kubelet提供的API采集节点和容器的资源占用。另外，Heapster的 /metrics API提供了Prometheus格式的数据。InfluxDB是一个开源分布式时序、事件和指标数据库；而Grafana则是InfluxDB的 dashboard，提供了强大的图表展示功能。它们常被组合使用展示图表化的监控数据，也可以将Zabbix作为数据源，进行zabbix的监控数据展示。Heapster、InfluxDB和Grafana均以Pod的形式启动和运行，其中Heapster需要与Kubernetes Master进行安全连接。 2. 安装配置Heapster、InfluxDB和Grafana到Heapster获取安装包: [root@linux-node1 tmp]# wget https://github.com/kubernetes/heapster/archive/v1.5.3.tar.gz [root@linux-node1 tmp]# tar -xf v1.5.3.tar.gz &amp;&amp; cd heapster-1.5.3/deploy/kube-config/influxdb/ [root@linux-node1 kube-config]# ll influxdb/ total 12 -rw-rw-r-- 1 root root 2290 May 1 05:13 grafana.yaml -rw-rw-r-- 1 root root 1114 May 1 05:13 heapster.yaml -rw-rw-r-- 1 root root 974 May 1 05:13 influxdb.yaml [root@linux-node1 kube-config]# ll rbac/ total 4 -rw-rw-r-- 1 root root 264 May 1 05:13 heapster-rbac.yaml [root@linux-node1 kube-config]# # 以上是所需要用到的文件 3. 配置Docker Socks5代理(为下载所需镜像做准备)由于某国的 &amp;@#￥%&amp;￥&amp;&amp;*R#￥…… 你懂的. 导致一些镜像无法pull 我这边利用自己搭建的梯子来进行代理 [root@linux-node1 kube-config]# grep \"gcr.io\" influxdb/heapster.yaml image: gcr.io/google_containers/heapster-amd64:v1.5.3 [root@linux-node1 kube-config]# # 修改docker服务文件配置, 在[Service]部分添加你自己的ss服务器地址 [root@linux-node1 kube-config]# vim /usr/lib/systemd/system/docker.service ...... ...... [Service] Environment=\"ALL_PROXY=socks5://192.168.56.1:1080\" ...... ...... 将其复制到其他所有节点，并重载、重启Docker服务, 直接pull一下验证即可 [root@linux-node1 kube-config]# docker pull gcr.io/google_containers/heapster-amd64:v1.5.3 4. 关键点说明:[root@linux-node1 kube-config]# more influxdb/heapster.yaml ...... ...... - /heapster - --source=kubernetes:https://kubernetes.default - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086 ...... ...... source：配置采集源，为Master URL地址：–source=kubernetes:https://kubernetes.default sink：配置后端存储系统，使用InfluxDB系统：–sink=influxdb:http://monitoring-influxdb:8086 这里保持默认即可 【注意】：URL中的主机名地址使用的是InfluxDB的Service名字，这需要DNS服务正常工作，如果没有配置DNS服务，则也可以使用Service的ClusterIP地址。另外，InfluxDB服务的名称没有加上命名空间，是因为Heapster服务与InfluxDB服务属于相同的命名空间kube-system。也可以使用上命名空间的全服务名，例如：http://monitoring-influxdb.kube-system:8086 修改 grafana.yaml文件[root@linux-node1 kube-config]# vim influxdb/grafana.yaml ...... ...... apiVersion: v1 kind: Service metadata: labels: # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-grafana name: monitoring-grafana namespace: kube-system spec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port type: NodePort # 去掉注释即可 ports: - port: 80 targetPort: 3000 selector: k8s-app: grafana ...... ...... 定义端口类型为 NodePort，将Grafana暴露在宿主机Node的端口上，以便后续浏览器访问 grafana 的 admin UI 界面 5. 执行[root@linux-node1 kube-config]# kubectl create -f influxdb/ &amp;&amp; kubectl create -f rbac/ deployment.extensions \"monitoring-grafana\" created service \"monitoring-grafana\" created serviceaccount \"heapster\" created deployment.extensions \"heapster\" created service \"heapster\" created deployment.extensions \"monitoring-influxdb\" created service \"monitoring-influxdb\" created clusterrolebinding.rbac.authorization.k8s.io \"heapster\" created 检查执行结果1. 检查Deployment[root@linux-node1 kube-config]# kubectl get deployments -n kube-system -o wide | grep -E 'heapster|monitoring' heapster 1 1 1 1 11s heapster gcr.io/google_containers/heapster-amd64:v1.5.3 k8s-app=heapster,task=monitoring monitoring-grafana 1 1 1 1 11s grafana gcr.io/google_containers/heapster-grafana-amd64:v4.4.3 k8s-app=grafana,task=monitoring monitoring-influxdb 1 1 1 1 11s influxdb gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 k8s-app=influxdb,task=monitoring 2. 检查POD[root@linux-node1 kube-config]# kubectl get pods -n kube-system -o wide | grep -E 'heapster|monitoring' heapster-589b7db6c9-pwrks 1/1 Running 0 32s 10.2.97.77 192.168.56.13 monitoring-grafana-d8c8d486c-l9dhx 1/1 Running 0 33s 10.2.97.76 192.168.56.13 monitoring-influxdb-54bd58b4c9-q489p 1/1 Running 0 33s 10.2.70.76 192.168.56.12 3. 检查Service[root@linux-node1 kube-config]# kubectl get service -n kube-system -o wide | grep -E 'heapster|monitoring' heapster ClusterIP 10.1.102.233 &lt;none&gt; 80/TCP 50s k8s-app=heapster monitoring-grafana NodePort 10.1.18.167 &lt;none&gt; 80:21240/TCP 50s k8s-app=grafana monitoring-influxdb ClusterIP 10.1.244.92 &lt;none&gt; 8086/TCP 50s k8s-app=influxdb 4. 检查 kubernets dashboard 界面，看是显示各 Nodes、Pods 的 CPU、内存、负载等利用率曲线图 5. Grafana的访问:上面我们不是修改了官方提供的那个grafana.yaml中的那个NodePort嘛，所以我们访问就是http://NodeIP:NodePort","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"08-K8s集群搭建---CoreDNS创建&DashBoard","slug":"08k8s-ji-qun-da-jiancoredns-chuang-jiandashboard","date":"2018-06-03T08:45:37.000Z","updated":"2025-09-01T01:59:08.979Z","comments":true,"path":"2018/06/03/08k8s-ji-qun-da-jiancoredns-chuang-jiandashboard/","permalink":"https://blog.sctux.cc/2018/06/03/08k8s-ji-qun-da-jiancoredns-chuang-jiandashboard/","excerpt":"由于k8s集群内部的服务发现都是通过DNS来发现并实现的，所以需要依赖DNS服务 1.创建coredns.yaml文件:[root@linux-node1 ~]# cat &gt; coredns.yaml &lt;&lt; EOF apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExists data: Corefile: | .:53 { errors health kubernetes cluster.local. in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 } --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: coredns template: metadata: labels: k8s-app: coredns spec: serviceAccountName: coredns tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule - key: \"CriticalAddonsOnly\" operator: \"Exists\" containers: - name: coredns image: coredns/coredns:1.0.6 imagePullPolicy: IfNotPresent resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: coredns clusterIP: 10.1.0.2 #!!!!!!!关键点!!!!!!! ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP EOF # 注意 cluster ip 需要跟service ip是在一个网段 2.运行创建:[root@linux-node1 ~]# kubectl create -f coredns.yaml deployment.extensions \"coredns\" created Error from server (AlreadyExists): error when creating \"coredns.yaml\": serviceaccounts \"coredns\" already exists Error from server (AlreadyExists): error when creating \"coredns.yaml\": clusterroles.rbac.authorization.k8s.io \"system:coredns\" already exists Error from server (AlreadyExists): error when creating \"coredns.yaml\": clusterrolebindings.rbac.authorization.k8s.io \"system:coredns\" already exists Error from server (AlreadyExists): error when creating \"coredns.yaml\": configmaps \"coredns\" already exists Error from server (Invalid): error when creating \"coredns.yaml\": Service \"coredns\" is invalid: spec.clusterIP: Invalid value: \"10.1.0.2\": provided IP is already allocated # 忽略以上错误，由于我已经安装过了。 3.查看创建结果:# 注意: 与k8s相关的组件或者服务都是在kube-system这个命名空间 所以这里 -n 指定了命名空间为kube-system [root@linux-node1 ~]# kubectl get deployment -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE coredns 2 2 2 2 12m # 查看service [root@linux-node1 ~]# kubectl get service -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE coredns ClusterIP 10.1.0.2 &lt;none&gt; 53/UDP,53/TCP 32m # 通过LVS查看这个cluster的信息 [root@linux-node2 ~]# ipvsadm -Ln | grep -v \":80\" IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.1.0.1:443 rr persistent 10800 -&gt; 192.168.56.11:6443 Masq 1 2 0 TCP 10.1.0.2:53 rr -&gt; 10.2.70.216:53 Masq 1 0 0 -&gt; 10.2.97.209:53 Masq 1 0 0 UDP 10.1.0.2:53 rr -&gt; 10.2.70.216:53 Masq 1 0 0 -&gt; 10.2.97.209:53 Masq 1 0 0 # 可以看到通过LVS转向了两个Pod, 查看pod: [root@linux-node1 ~]# kubectl get pod -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE coredns-77c989547b-55kjb 1/1 Running 0 19m 10.2.97.209 192.168.56.13 coredns-77c989547b-qvfw4 1/1 Running 0 19m 10.2.70.216 192.168.56.12 4. 验证创建结果:启动一个容器 # 可以看出来已经获取到了dns服务的VIP地址 [root@linux-node1 ~]# kubectl exec -it nginx-deployment-75d56bb955-b4z4k /bin/sh # cat /etc/resolv.conf nameserver 10.1.0.2 search default.svc.cluster.localping . svc.cluster.local. cluster.local. example.com options ndots:5","text":"由于k8s集群内部的服务发现都是通过DNS来发现并实现的，所以需要依赖DNS服务 1.创建coredns.yaml文件:[root@linux-node1 ~]# cat &gt; coredns.yaml &lt;&lt; EOF apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExists data: Corefile: | .:53 { errors health kubernetes cluster.local. in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 } --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: coredns template: metadata: labels: k8s-app: coredns spec: serviceAccountName: coredns tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule - key: \"CriticalAddonsOnly\" operator: \"Exists\" containers: - name: coredns image: coredns/coredns:1.0.6 imagePullPolicy: IfNotPresent resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: coredns clusterIP: 10.1.0.2 #!!!!!!!关键点!!!!!!! ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP EOF # 注意 cluster ip 需要跟service ip是在一个网段 2.运行创建:[root@linux-node1 ~]# kubectl create -f coredns.yaml deployment.extensions \"coredns\" created Error from server (AlreadyExists): error when creating \"coredns.yaml\": serviceaccounts \"coredns\" already exists Error from server (AlreadyExists): error when creating \"coredns.yaml\": clusterroles.rbac.authorization.k8s.io \"system:coredns\" already exists Error from server (AlreadyExists): error when creating \"coredns.yaml\": clusterrolebindings.rbac.authorization.k8s.io \"system:coredns\" already exists Error from server (AlreadyExists): error when creating \"coredns.yaml\": configmaps \"coredns\" already exists Error from server (Invalid): error when creating \"coredns.yaml\": Service \"coredns\" is invalid: spec.clusterIP: Invalid value: \"10.1.0.2\": provided IP is already allocated # 忽略以上错误，由于我已经安装过了。 3.查看创建结果:# 注意: 与k8s相关的组件或者服务都是在kube-system这个命名空间 所以这里 -n 指定了命名空间为kube-system [root@linux-node1 ~]# kubectl get deployment -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE coredns 2 2 2 2 12m # 查看service [root@linux-node1 ~]# kubectl get service -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE coredns ClusterIP 10.1.0.2 &lt;none&gt; 53/UDP,53/TCP 32m # 通过LVS查看这个cluster的信息 [root@linux-node2 ~]# ipvsadm -Ln | grep -v \":80\" IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.1.0.1:443 rr persistent 10800 -&gt; 192.168.56.11:6443 Masq 1 2 0 TCP 10.1.0.2:53 rr -&gt; 10.2.70.216:53 Masq 1 0 0 -&gt; 10.2.97.209:53 Masq 1 0 0 UDP 10.1.0.2:53 rr -&gt; 10.2.70.216:53 Masq 1 0 0 -&gt; 10.2.97.209:53 Masq 1 0 0 # 可以看到通过LVS转向了两个Pod, 查看pod: [root@linux-node1 ~]# kubectl get pod -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE coredns-77c989547b-55kjb 1/1 Running 0 19m 10.2.97.209 192.168.56.13 coredns-77c989547b-qvfw4 1/1 Running 0 19m 10.2.70.216 192.168.56.12 4. 验证创建结果:启动一个容器 # 可以看出来已经获取到了dns服务的VIP地址 [root@linux-node1 ~]# kubectl exec -it nginx-deployment-75d56bb955-b4z4k /bin/sh # cat /etc/resolv.conf nameserver 10.1.0.2 search default.svc.cluster.localping . svc.cluster.local. cluster.local. example.com options ndots:5 梳理主要内容： 在k8s集群中，服务是运行在Pod中的，Pod的发现和副本间负载均衡是我们面临的问题。 通过Service可以解决这两个问题，但访问Service也需要对应的IP，因此又引入了Service发现的问题。 得益于coredns插件，我们可以通过域名来访问集群内的Service，解决了Service发现的问题。 为了让Pod中的容器可以使用coredns来解析域名，k8s会修改容器的/etc/resolv.conf配置。 有了以上机制的保证，就可以在Pod中通过Service名称和namespace非常方便地访问对应的服务了。 DashBoard部署dashboard[root@linux-node1 ~]# kubectl create -f dashboard/ # 查看集群详情 [root@linux-node1 ~]# kubectl cluster-info Kubernetes master is running at https://192.168.56.11:6443 CoreDNS is running at https://192.168.56.11:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy kubernetes-dashboard is running at https://192.168.56.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. [root@linux-node1 src]# kubectl get services kubernetes-dashboard -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.1.27.148 &lt;none&gt; 443:28966/TCP 3d 登录:￼ 获取登录令牌: [root@linux-node1 src]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') # 复制 token的内容部分 输入 登录 ￼","categories":[{"name":"虚拟化&amp;云计算&amp;大数据","slug":"虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"虚拟化&amp;云计算&amp;大数据","slug":"虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"07-K8s集群搭建---创建K8s应用","slug":"07k8s-ji-qun-da-jianchuang-jiank8s-ying-yong","date":"2018-06-03T02:28:32.000Z","updated":"2025-09-01T01:59:08.876Z","comments":true,"path":"2018/06/03/07k8s-ji-qun-da-jianchuang-jiank8s-ying-yong/","permalink":"https://blog.sctux.cc/2018/06/03/07k8s-ji-qun-da-jianchuang-jiank8s-ying-yong/","excerpt":"1.创建一个测试用的deployment[root@linux-node1 ~]# kubectl run net-test --image=alpine --replicas=3 sleep 360000 2.查看创建情况[root@linux-node1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE net-test-5767cb94df-6crmr 1/1 Running 0 17s 10.2.97.176 192.168.56.13 net-test-5767cb94df-9mpmb 1/1 Running 0 17s 10.2.70.187 192.168.56.12 net-test-5767cb94df-x8l44 1/1 Running 0 17s 10.2.97.175 192.168.56.13 # 使用kubectl创建了一个名为net-test 的deployment, 镜像是alpine 副本数为3， [root@linux-node1 ~]# kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE net-test 3 3 3 3 7m 3.测试联通性ping 10.2.97.176 通过yaml文件部署应用1.编写一个nginx-deployment.yaml文件:[root@linux-node1 ~]# cat &gt; nginx-deployment.yaml &lt;&lt; EOF apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.10.3 ports: - containerPort: 80 EOF 2.执行创建:","text":"1.创建一个测试用的deployment[root@linux-node1 ~]# kubectl run net-test --image=alpine --replicas=3 sleep 360000 2.查看创建情况[root@linux-node1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE net-test-5767cb94df-6crmr 1/1 Running 0 17s 10.2.97.176 192.168.56.13 net-test-5767cb94df-9mpmb 1/1 Running 0 17s 10.2.70.187 192.168.56.12 net-test-5767cb94df-x8l44 1/1 Running 0 17s 10.2.97.175 192.168.56.13 # 使用kubectl创建了一个名为net-test 的deployment, 镜像是alpine 副本数为3， [root@linux-node1 ~]# kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE net-test 3 3 3 3 7m 3.测试联通性ping 10.2.97.176 通过yaml文件部署应用1.编写一个nginx-deployment.yaml文件:[root@linux-node1 ~]# cat &gt; nginx-deployment.yaml &lt;&lt; EOF apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.10.3 ports: - containerPort: 80 EOF 2.执行创建:[root@linux-node1 ~]# kubectl create -f nginx-deployment.yaml deployment.apps \"nginx-deployment\" created # 查看deployment [root@linux-node1 ~]# kubectl get deployment/nginx-deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 1m # 查看deployment 详情 [root@linux-node1 ~]# kubectl describe deployment/nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 31 May 2018 20:20:17 +0800 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=1 Selector: app=nginx Replicas: 10 desired | 10 updated | 10 total | 10 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.10.3 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; Conditions: Type Status Reason ---- ------ ------ Progressing True NewReplicaSetAvailable Available True MinimumReplicasAvailable OldReplicaSets: &lt;none&gt; NewReplicaSet: nginx-deployment-75d56bb955 (10/10 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 5m deployment-controller Scaled up replica set nginx-deployment-75d56bb955 to 3 Normal ScalingReplicaSet 1m (x2 over 3m) deployment-controller Scaled down replica set nginx-deployment-75d56bb955 to 3 Normal ScalingReplicaSet 57s (x3 over 5m) deployment-controller Scaled up replica set nginx-deployment-75d56bb955 to 10 # 查看pod ,可以看出按照我们的描述文件里面的副本数已经有10个POD通过Schedule到了不同的node节点上面 [root@linux-node1 ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE net-test-5767cb94df-6crmr 1/1 Running 0 40m 10.2.97.178 192.168.56.13 net-test-5767cb94df-9mpmb 1/1 Running 0 40m 10.2.70.187 192.168.56.12 net-test-5767cb94df-x8l44 1/1 Running 0 40m 10.2.97.177 192.168.56.13 nginx-deployment-75d56bb955-255w2 1/1 Running 0 3m 10.2.70.196 192.168.56.12 nginx-deployment-75d56bb955-5ckvs 1/1 Running 0 8m 10.2.70.188 192.168.56.12 nginx-deployment-75d56bb955-7dq87 1/1 Running 0 3m 10.2.70.198 192.168.56.12 nginx-deployment-75d56bb955-8vl7p 1/1 Running 0 3m 10.2.97.191 192.168.56.13 nginx-deployment-75d56bb955-9f9ms 1/1 Running 0 3m 10.2.97.189 192.168.56.13 nginx-deployment-75d56bb955-9g9bk 1/1 Running 0 3m 10.2.97.188 192.168.56.13 nginx-deployment-75d56bb955-9kz2r 1/1 Running 0 3m 10.2.70.197 192.168.56.12 nginx-deployment-75d56bb955-s78gs 1/1 Running 0 8m 10.2.70.189 192.168.56.12 nginx-deployment-75d56bb955-v8n5v 1/1 Running 0 3m 10.2.97.190 192.168.56.13 nginx-deployment-75d56bb955-zfb4m 1/1 Running 0 8m 10.2.97.179 192.168.56.13 # 查看某个pod详情 [root@linux-node1 ~]# kubectl describe pod/nginx-deployment-75d56bb955-255w2 Name: nginx-deployment-75d56bb955-255w2 Namespace: default Node: 192.168.56.12/192.168.56.12 Start Time: Thu, 31 May 2018 20:24:31 +0800 Labels: app=nginx pod-template-hash=3181266511 Annotations: &lt;none&gt; Status: Running IP: 10.2.70.196 Controlled By: ReplicaSet/nginx-deployment-75d56bb955 Containers: nginx: Container ID: docker://48ad80730efd6d744ac6d31b34778eb8e75b0f4c6698e1c1fc8f6046a6a77b2b Image: nginx:1.10.3 Image ID: docker-pullable://nginx@sha256:6202beb06ea61f44179e02ca965e8e13b961d12640101fca213efbfd145d7575 Port: 80/TCP Host Port: 0/TCP State: Running Started: Thu, 31 May 2018 20:24:33 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5htws (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-5htws: Type: Secret (a volume populated by a Secret) SecretName: default-token-5htws Optional: false QoS Class: BestEffort Node-Selectors: &lt;none&gt; Tolerations: &lt;none&gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulMountVolume 3m kubelet, 192.168.56.12 MountVolume.SetUp succeeded for volume \"default-token-5htws\" Normal Pulled 3m kubelet, 192.168.56.12 Container image \"nginx:1.10.3\" already present on machine Normal Created 3m kubelet, 192.168.56.12 Created container Normal Started 3m kubelet, 192.168.56.12 Started container Normal Scheduled 2m default-scheduler Successfully assigned nginx-deployment-75d56bb955-255w2 to 192.168.56.12 # 访问Nginx pod [root@linux-node1 ~]# curl -I http://10.2.70.196 HTTP/1.1 200 OK Server: nginx/1.10.3 Date: Thu, 31 May 2018 12:30:45 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 31 Jan 2017 15:01:11 GMT Connection: keep-alive ETag: \"5890a6b7-264\" Accept-Ranges: byte # 更新deployment [root@linux-node1 ~]# kubectl set image deployment/nginx-deployment nginx=nginx:1.12.2 --record deployment.apps \"nginx-deployment\" image updated # 可以看到更新已经成功(滚动更新) [root@linux-node1 ~]# kubectl get deployment/nginx-deployment -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deployment 10 10 10 10 17m nginx nginx:1.12.2 app=nginx [root@linux-node1 ~]# curl -I 10.2.97.192 HTTP/1.1 200 OK Server: nginx/1.12.2 # 版本已经更新成功 Date: Thu, 31 May 2018 12:35:54 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 11 Jul 2017 13:29:18 GMT Connection: keep-alive ETag: \"5964d2ae-264\" Accept-Ranges: bytes # 查看deployment更新历史 [root@linux-node1 ~]# kubectl rollout history deployment/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 &lt;none&gt; # 用于在之前在创建创建nginx deployment的时候 未加参数 --record 所以这个信息为none 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.12.2 --record=true # 查看版本详情(--revison=VERSION_NUM) [root@linux-node1 ~]# kubectl rollout history deployment/nginx-deployment --revision=1 deployments \"nginx-deployment\" with revision #1 Pod Template: Labels: app=nginx pod-template-hash=3181266511 Containers: nginx: Image: nginx:1.10.3 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; # 版本回滚至上一个版本: [root@linux-node1 ~]# kubectl rollout undo deployment/nginx-deployment deployment.apps \"nginx-deployment\" [root@linux-node1 ~]# kubectl get deployment/nginx-deployment -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deployment 10 10 10 10 26m nginx nginx:1.10.3 app=nginx # 扩容pod数量 [root@linux-node1 ~]# kubectl scale --replicas=20 deployment nginx-deployment deployment.extensions \"nginx-deployment\" scaled [root@linux-node1 ~]# kubectl get deployment/nginx-deployment -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deployment 20 20 20 20 28m nginx nginx:1.10.3 app=nginx 那问题来了: 每次收到获取podIP太扯了，总不能每次都要手动改程序或者配置才能访问服务吧，要怎么提前知道podIP呢？ Pod在运行中可能会重建，IP变了怎么解？ 如何在多个Pod中实现负载均衡嘞？ 这些问题使用k8s Service就可以解决。 Service可以将pod IP封装起来，即使Pod发生重建，依然可以通过Service来访问Pod提供的服务。此外，Service还解决了负载均衡的问题，大家可以多访问几次Service，然后通过kubectl logs 来查看Nginx Pod的访问日志来确认 创建一个对于nginx deployment的一个service1.编写nginx-service.yaml文件[root@linux-node1 ~]# cat &gt; nginx-service.yaml &lt;&lt; EOF kind: Service apiVersion: v1 metadata: name: nginx-service spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 EOF 2.执行创建操作:[root@linux-node1 ~]# kubectl create -f nginx-service.yaml service \"nginx-service\" created [root@linux-node1 ~]# kubectl get service -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443/TCP 3d &lt;none&gt; nginx-service ClusterIP 10.1.146.223 &lt;none&gt; 80/TCP 30s app=nginx # 访问VIP [root@linux-node1 ~]# curl -I http://10.1.146.223 HTTP/1.1 200 OK Server: nginx/1.10.3 Date: Thu, 31 May 2018 12:54:54 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 31 Jan 2017 15:01:11 GMT Connection: keep-alive ETag: \"5890a6b7-264\" Accept-Ranges: bytes # 在节点2上查看LVS，通过k8s封装LVS来实现了负载均衡 [root@linux-node2 ~]# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.1.0.1:443 rr persistent 10800 -&gt; 192.168.56.11:6443 Masq 1 1 0 TCP 10.1.146.223:80 rr -&gt; 10.2.70.204:80 Masq 1 0 0 -&gt; 10.2.70.205:80 Masq 1 0 1 -&gt; 10.2.70.206:80 Masq 1 0 0 -&gt; 10.2.70.207:80 Masq 1 0 0 -&gt; 10.2.70.208:80 Masq 1 0 0 -&gt; 10.2.70.209:80 Masq 1 0 0 -&gt; 10.2.70.210:80 Masq 1 0 0 -&gt; 10.2.70.211:80 Masq 1 0 0 -&gt; 10.2.70.212:80 Masq 1 0 0 -&gt; 10.2.70.213:80 Masq 1 0 0 -&gt; 10.2.97.197:80 Masq 1 0 0 -&gt; 10.2.97.198:80 Masq 1 0 0 -&gt; 10.2.97.199:80 Masq 1 0 1 -&gt; 10.2.97.200:80 Masq 1 0 0 -&gt; 10.2.97.201:80 Masq 1 0 0 -&gt; 10.2.97.202:80 Masq 1 0 1 -&gt; 10.2.97.203:80 Masq 1 0 1 -&gt; 10.2.97.204:80 Masq 1 0 0 -&gt; 10.2.97.205:80 Masq 1 0 0","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"06-K8s集群搭建---Flannel网络部署","slug":"06k8s-ji-qun-da-jianflannel-wang-luo-bu-shu","date":"2018-06-02T00:51:42.000Z","updated":"2025-09-01T01:59:08.879Z","comments":true,"path":"2018/06/02/06k8s-ji-qun-da-jianflannel-wang-luo-bu-shu/","permalink":"https://blog.sctux.cc/2018/06/02/06k8s-ji-qun-da-jianflannel-wang-luo-bu-shu/","excerpt":"1.为Flannel生成证书[root@linux-node1 ~]# vim flanneld-csr.json { \"CN\": \"flanneld\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 2.生成证书[root@linux-node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld 3.分发证书[root@linux-node1 ~]# cp flanneld*.pem /opt/kubernetes/ssl/ [root@linux-node1 ~]# scp flanneld*.pem 192.168.56.12:/opt/kubernetes/ssl/ [root@linux-node1 ~]# scp flanneld*.pem 192.168.56.13:/opt/kubernetes/ssl/ 4.下载Flannel软件包[root@linux-node1 ~]# cd /usr/local/src # wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz [root@linux-node1 src]# tar zxf flannel-v0.10.0-linux-amd64.tar.gz [root@linux-node1 src]# cp flanneld mk-docker-opts.sh /opt/kubernetes/bin/ 复制到linux-node2节点 [root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.13:/opt/kubernetes/bin/ 复制对应脚本到/opt/kubernetes/bin目录下。 [root@linux-node1 ~]# cd /usr/local/src/kubernetes/cluster/centos/node/bin/ [root@linux-node1 bin]# cp remove-docker0.sh /opt/kubernetes/bin/ [root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.13:/opt/kubernetes/bin/ 5.配置Flannel[root@linux-node1 ~]# vim /opt/kubernetes/cfg/flannel FLANNEL_ETCD=\"-etcd-endpoints=https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379\" FLANNEL_ETCD_KEY=\"-etcd-prefix=/kubernetes/network\" FLANNEL_ETCD_CAFILE=\"--etcd-cafile=/opt/kubernetes/ssl/ca.pem\" FLANNEL_ETCD_CERTFILE=\"--etcd-certfile=/opt/kubernetes/ssl/flanneld.pem\" FLANNEL_ETCD_KEYFILE=\"--etcd-keyfile=/opt/kubernetes/ssl/flanneld-key.pem\" 复制配置到其它节点上 [root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.12:/opt/kubernetes/cfg/ [root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.13:/opt/kubernetes/cfg/","text":"1.为Flannel生成证书[root@linux-node1 ~]# vim flanneld-csr.json { \"CN\": \"flanneld\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 2.生成证书[root@linux-node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld 3.分发证书[root@linux-node1 ~]# cp flanneld*.pem /opt/kubernetes/ssl/ [root@linux-node1 ~]# scp flanneld*.pem 192.168.56.12:/opt/kubernetes/ssl/ [root@linux-node1 ~]# scp flanneld*.pem 192.168.56.13:/opt/kubernetes/ssl/ 4.下载Flannel软件包[root@linux-node1 ~]# cd /usr/local/src # wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz [root@linux-node1 src]# tar zxf flannel-v0.10.0-linux-amd64.tar.gz [root@linux-node1 src]# cp flanneld mk-docker-opts.sh /opt/kubernetes/bin/ 复制到linux-node2节点 [root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.13:/opt/kubernetes/bin/ 复制对应脚本到/opt/kubernetes/bin目录下。 [root@linux-node1 ~]# cd /usr/local/src/kubernetes/cluster/centos/node/bin/ [root@linux-node1 bin]# cp remove-docker0.sh /opt/kubernetes/bin/ [root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.13:/opt/kubernetes/bin/ 5.配置Flannel[root@linux-node1 ~]# vim /opt/kubernetes/cfg/flannel FLANNEL_ETCD=\"-etcd-endpoints=https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379\" FLANNEL_ETCD_KEY=\"-etcd-prefix=/kubernetes/network\" FLANNEL_ETCD_CAFILE=\"--etcd-cafile=/opt/kubernetes/ssl/ca.pem\" FLANNEL_ETCD_CERTFILE=\"--etcd-certfile=/opt/kubernetes/ssl/flanneld.pem\" FLANNEL_ETCD_KEYFILE=\"--etcd-keyfile=/opt/kubernetes/ssl/flanneld-key.pem\" 复制配置到其它节点上 [root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.12:/opt/kubernetes/cfg/ [root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.13:/opt/kubernetes/cfg/ 6.设置Flannel系统服务[root@linux-node1 ~]# vim /usr/lib/systemd/system/flannel.service [Unit] Description=Flanneld overlay address etcd agent After=network.target Before=docker.service [Service] EnvironmentFile=-/opt/kubernetes/cfg/flannel ExecStartPre=/opt/kubernetes/bin/remove-docker0.sh ExecStart=/opt/kubernetes/bin/flanneld --ip-masq ${FLANNEL_ETCD} ${FLANNEL_ETCD_KEY} ${FLANNEL_ETCD_CAFILE} ${FLANNEL_ETCD_CERTFILE} ${FLANNEL_ETCD_KEYFILE} ExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/docker Type=notify [Install] WantedBy=multi-user.target RequiredBy=docker.service 复制系统服务脚本到其它节点上 # scp /usr/lib/systemd/system/flannel.service 192.168.56.12:/usr/lib/systemd/system/ # scp /usr/lib/systemd/system/flannel.service 192.168.56.13:/usr/lib/systemd/system/ Flannel CNI集成1.下载CNI插件https://github.com/containernetworking/plugins/releases wget https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz [root@linux-node1 ~]# mkdir /opt/kubernetes/bin/cni [root@linux-node1 src]# tar zxf cni-plugins-amd64-v0.7.1.tgz -C /opt/kubernetes/bin/cni # scp -r /opt/kubernetes/bin/cni/* 192.168.56.12:/opt/kubernetes/bin/cni/ # scp -r /opt/kubernetes/bin/cni/* 192.168.56.13:/opt/kubernetes/bin/cni/ 2.创建Etcd的key/opt/kubernetes/bin/etcdctl --ca-file /opt/kubernetes/ssl/ca.pem --cert-file /opt/kubernetes/ssl/flanneld.pem --key-file /opt/kubernetes/ssl/flanneld-key.pem \\ --no-sync -C https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379 \\ mk /kubernetes/network/config '{ \"Network\": \"10.2.0.0/16\", \"Backend\": { \"Type\": \"vxlan\", \"VNI\": 1 }}' &gt;/dev/null 2&gt;&amp;1 3.启动flannel[root@linux-node1 ~]# systemctl daemon-reload [root@linux-node1 ~]# systemctl enable flannel [root@linux-node1 ~]# chmod +x /opt/kubernetes/bin/* [root@linux-node1 ~]# systemctl start flannel 4.查看服务状态[root@linux-node1 ~]# systemctl status flannel 配置Docker使用Flannel[root@linux-node1 ~]# vim /usr/lib/systemd/system/docker.service [Unit] #在Unit下面修改After和增加Requires After=network-online.target firewalld.service flannel.service Wants=network-online.target Requires=flannel.service [Service] #增加EnvironmentFile=-/run/flannel/docker Type=not EnvironmentFile=-/run/flannel/docker ExecStart=/usr/bin/dockerd $DOCKER_OPTS 1.将配置复制到另外的节点[root@linux-node1 ~]# scp /usr/lib/systemd/system/docker.service 192.168.56.12:/usr/lib/systemd/system/ [root@linux-node1 ~]# scp /usr/lib/systemd/system/docker.service 192.168.56.13:/usr/lib/systemd/system/ 2.重启Docker[root@linux-node1 ~]# systemctl daemon-reload [root@linux-node1 ~]# systemctl restart docker # 此时就可以看到docker0网卡的IP 跟flaneel是一个网段的了： [root@linux-node2 ~]# ip a | egrep \"flannel|docker0\" 6: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN inet 10.2.70.0/32 scope global flannel.1 7: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1400 qdisc noqueue state DOWN inet 10.2.70.1/24 brd 10.2.70.255 scope global docker0","categories":[{"name":"虚拟化&amp;云计算&amp;大数据","slug":"虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"虚拟化&amp;云计算&amp;大数据","slug":"虚拟化-amp-云计算-amp-大数据","permalink":"https://blog.sctux.cc/categories/%E8%99%9A%E6%8B%9F%E5%8C%96-amp-%E4%BA%91%E8%AE%A1%E7%AE%97-amp-%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"05-K8s集群搭建---Node节点部署","slug":"05k8s-ji-qun-da-jiannode-jie-dian-bu-shu","date":"2018-06-01T22:47:38.000Z","updated":"2025-09-01T01:59:08.922Z","comments":true,"path":"2018/06/02/05k8s-ji-qun-da-jiannode-jie-dian-bu-shu/","permalink":"https://blog.sctux.cc/2018/06/02/05k8s-ji-qun-da-jiannode-jie-dian-bu-shu/","excerpt":"部署kubelet1.软件包准备[root@linux-node1 ~]# cd /usr/local/src/kubernetes/server/bin/ [root@linux-node1 bin]# cp kubelet kube-proxy /opt/kubernetes/bin/ [root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.13:/opt/kubernetes/bin/ 2.创建角色绑定kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests): [root@linux-node1 ~]# cd /usr/local/src/ssl [root@linux-node1 ~]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap clusterrolebinding \"kubelet-bootstrap\" created --user=kubelet-bootstrap 是在 /opt/kubernetes/ssl/bootstrap-token.csv 文件中指定的用户名，同时也写入了bootstrap.kubeconfig 文件； 3.创建 kubelet bootstrapping kubeconfig 文件 设置集群参数[root@linux-node1 ~]# kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://192.168.56.11:6443 \\ --kubeconfig=bootstrap.kubeconfig Cluster \"kubernetes\" set. # 设置客户端认证参数 [root@linux-node1 ~]# kubectl config set-credentials kubelet-bootstrap \\ --token=ad6d5bb607a186796d8861557df0d17f \\ --kubeconfig=bootstrap.kubeconfig User \"kubelet-bootstrap\" set. # 设置上下文参数 [root@linux-node1 ~]# kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig Context \"default\" created. #选择默认上下文 [root@linux-node1 ~]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig Switched to context \"default\". [root@linux-node1 kubernetes]# cp bootstrap.kubeconfig /opt/kubernetes/cfg [root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.12:/opt/kubernetes/cfg [root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.13:/opt/kubernetes/cfg 4.配置CNI支持","text":"部署kubelet1.软件包准备[root@linux-node1 ~]# cd /usr/local/src/kubernetes/server/bin/ [root@linux-node1 bin]# cp kubelet kube-proxy /opt/kubernetes/bin/ [root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.13:/opt/kubernetes/bin/ 2.创建角色绑定kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests): [root@linux-node1 ~]# cd /usr/local/src/ssl [root@linux-node1 ~]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap clusterrolebinding \"kubelet-bootstrap\" created --user=kubelet-bootstrap 是在 /opt/kubernetes/ssl/bootstrap-token.csv 文件中指定的用户名，同时也写入了bootstrap.kubeconfig 文件； 3.创建 kubelet bootstrapping kubeconfig 文件 设置集群参数[root@linux-node1 ~]# kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://192.168.56.11:6443 \\ --kubeconfig=bootstrap.kubeconfig Cluster \"kubernetes\" set. # 设置客户端认证参数 [root@linux-node1 ~]# kubectl config set-credentials kubelet-bootstrap \\ --token=ad6d5bb607a186796d8861557df0d17f \\ --kubeconfig=bootstrap.kubeconfig User \"kubelet-bootstrap\" set. # 设置上下文参数 [root@linux-node1 ~]# kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig Context \"default\" created. #选择默认上下文 [root@linux-node1 ~]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig Switched to context \"default\". [root@linux-node1 kubernetes]# cp bootstrap.kubeconfig /opt/kubernetes/cfg [root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.12:/opt/kubernetes/cfg [root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.13:/opt/kubernetes/cfg 4.配置CNI支持[root@linux-node2 ~]# mkdir -p /etc/cni/net.d [root@linux-node2 ~]# vim /etc/cni/net.d/10-default.conf { \"name\": \"flannel\", \"type\": \"flannel\", \"delegate\": { \"bridge\": \"docker0\", \"isDefaultGateway\": true, \"mtu\": 1400 } } 5.创建kubelet所需目录[root@linux-node2 ~]# mkdir /var/lib/kubelet 6.创建kubelet系统服务[root@k8s-node2 ~]# cat &gt; /usr/lib/systemd/system/kubelet.service &lt;&lt; EOF [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/opt/kubernetes/bin/kubelet \\ --address=192.168.56.12 \\ --hostname-override=192.168.56.12 \\ --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\ --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\ --cert-dir=/opt/kubernetes/ssl \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/opt/kubernetes/bin/cni \\ --cluster-dns=10.1.0.2 \\ --cluster-domain=cluster.local. \\ --hairpin-mode hairpin-veth \\ --allow-privileged=true \\ --fail-swap-on=false \\ --logtostderr=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/log Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF 7.启动并查看服务状态：[root@linux-node2 ~]# systemctl daemon-reload [root@linux-node2 ~]# systemctl enable kubelet [root@linux-node2 ~]# systemctl start kubelet [root@linux-node2 kubernetes]# systemctl status kubelet 8.查看csr请求 注意是在linux-node1上执行。[root@linux-node1 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-0_w5F1FM_la_SeGiu3Y5xELRpYUjjT2icIFk9gO9KOU 1m kubelet-bootstrap Pending 9.批准kubelet 的 TLS 证书请求[root@linux-node1 ~]# kubectl get csr|grep 'Pending' | awk 'NR&gt;0{print $1}'| xargs kubectl certificate approve #执行完毕后，查看节点状态已经是Ready的状态了 [root@linux-node1 ssl]# kubectl get nodes NAME STATUS ROLES AGE VERSION 192.168.56.12 Ready &lt;none&gt; 2m v1.10.1 部署Kubernetes Proxy1.配置kube-proxy使用LVS[root@linux-node2 ~]# yum install -y ipvsadm ipset conntrack 2.创建 kube-proxy 证书请求[root@linux-node1 ~]# cd /usr/local/src/ssl/ [root@linux-node1 ~]# cat &gt; kube-proxy-csr.json &lt;&lt; EOF { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 3.生成证书[root@linux-node1~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 4.分发证书到所有Node节点[root@linux-node1 ssl]# cp kube-proxy*.pem /opt/kubernetes/ssl/ [root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.12:/opt/kubernetes/ssl/ [root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.12:/opt/kubernetes/ssl/ 5.创建kube-proxy配置文件# 设置集群参数 [root@linux-node1 ssl]# kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://192.168.56.11:6443 \\ --kubeconfig=kube-proxy.kubeconfig Cluster \"kubernetes\" set. # 设置客户端认证参数 [root@linux-node1 ssl]# kubectl config set-credentials kube-proxy \\ --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\ --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig User \"kube-proxy\" set. # 设置上下文参数 [root@linux-node1 ssl]# kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig Context \"default\" created. # 设置默认上下文 [root@linux-node1 ssl]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig Switched to context \"default\". 6.分发kubeconfig配置文件[root@linux-node1 ssl]# cp kube-proxy.kubeconfig /opt/kubernetes/cfg/ [root@linux-node1 ssl]# scp kube-proxy.kubeconfig 192.168.56.12:/opt/kubernetes/cfg/ [root@linux-node1 ssl]# scp kube-proxy.kubeconfig 192.168.56.13:/opt/kubernetes/cfg/ 7.创建kube-proxy系统服务[root@linux-node2 bin]# mkdir /var/lib/kube-proxy [root@k8s-node2 ~]# vim /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/opt/kubernetes/bin/kube-proxy \\ --bind-address=192.168.56.12 \\ --hostname-override=192.168.56.12 \\ --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --logtostderr=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/log Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 8.启动Kubernetes Proxy并查看状态:systemctl status kube-proxy [root@linux-node2 ~]# systemctl daemon-reload [root@linux-node2 ~]# systemctl enable kube-proxy [root@linux-node2 ~]# systemctl start kube-proxy [root@linux-node2 ~]# systemctl status kube-proxy #检查LVS状态 [root@linux-node2 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.1.0.1:443 rr persistent 10800 -&gt; 192.168.56.11:6443 Masq 1 0 0 9.验证node节点是否正常部署(node3按照以上步骤启动服务即可1，7，8)[root@linux-node1 ssl]# kubectl get nodes NAME STATUS ROLES AGE VERSION 192.168.56.12 Ready &lt;none&gt; 12m v1.10.1 192.168.56.13 Ready &lt;none&gt; 2d v1.10.1","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"04-K8s集群搭建---Master节点部署","slug":"04k8s-ji-qun-da-jianmaster-jie-dian-bu-shu","date":"2018-06-01T20:59:27.000Z","updated":"2025-09-01T01:59:08.967Z","comments":true,"path":"2018/06/02/04k8s-ji-qun-da-jianmaster-jie-dian-bu-shu/","permalink":"https://blog.sctux.cc/2018/06/02/04k8s-ji-qun-da-jianmaster-jie-dian-bu-shu/","excerpt":"master节点涉及服务组件ApiServer , Scheduler , ControllerManager, kubectl 1.软件包准备:[root@linux-node1 ~]# cd /usr/local/src/kubernetes [root@linux-node1 kubernetes]# cp server/bin/kube-apiserver /opt/kubernetes/bin/ [root@linux-node1 kubernetes]# cp server/bin/kube-controller-manager /opt/kubernetes/bin/ [root@linux-node1 kubernetes]# cp server/bin/kube-scheduler /opt/kubernetes/bin/ 2.创建生成CSR的 JSON 配置文件:[root@linux-node1 src]# cat &gt; kubernetes-csr.json &lt;&lt; EOF { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"192.168.56.11\", \"10.1.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 3.生成 kubernetes 证书和私钥:[root@linux-node1 src]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes [root@linux-node1 src]# cp kubernetes*.pem /opt/kubernetes/ssl/ [root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.12:/opt/kubernetes/ssl/ [root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.13:/opt/kubernetes/ssl/ 4.创建 kube-apiserver 使用的客户端 token 文件[root@linux-node1 ~]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' ad6d5bb607a186796d8861557df0d17f [root@linux-node1 ~]# vim /opt/kubernetes/ssl/ bootstrap-token.csv ad6d5bb607a186796d8861557df0d17f,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" 5.创建基础用户名/密码认证配置","text":"master节点涉及服务组件ApiServer , Scheduler , ControllerManager, kubectl 1.软件包准备:[root@linux-node1 ~]# cd /usr/local/src/kubernetes [root@linux-node1 kubernetes]# cp server/bin/kube-apiserver /opt/kubernetes/bin/ [root@linux-node1 kubernetes]# cp server/bin/kube-controller-manager /opt/kubernetes/bin/ [root@linux-node1 kubernetes]# cp server/bin/kube-scheduler /opt/kubernetes/bin/ 2.创建生成CSR的 JSON 配置文件:[root@linux-node1 src]# cat &gt; kubernetes-csr.json &lt;&lt; EOF { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"192.168.56.11\", \"10.1.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 3.生成 kubernetes 证书和私钥:[root@linux-node1 src]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes [root@linux-node1 src]# cp kubernetes*.pem /opt/kubernetes/ssl/ [root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.12:/opt/kubernetes/ssl/ [root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.13:/opt/kubernetes/ssl/ 4.创建 kube-apiserver 使用的客户端 token 文件[root@linux-node1 ~]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' ad6d5bb607a186796d8861557df0d17f [root@linux-node1 ~]# vim /opt/kubernetes/ssl/ bootstrap-token.csv ad6d5bb607a186796d8861557df0d17f,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" 5.创建基础用户名/密码认证配置[root@linux-node1 ~]# vim /opt/kubernetes/ssl/basic-auth.csv admin,admin,1 readonly,readonly,2 6.部署K8S ApiServer服务a.将ApiServer配置为系统服务[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] ExecStart=/opt/kubernetes/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --bind-address=192.168.56.11 \\ --insecure-bind-address=127.0.0.1 \\ --authorization-mode=Node,RBAC \\ --runtime-config=rbac.authorization.k8s.io/v1 \\ --kubelet-https=true \\ --anonymous-auth=false \\ --basic-auth-file=/opt/kubernetes/ssl/basic-auth.csv \\ --enable-bootstrap-token-auth \\ --token-auth-file=/opt/kubernetes/ssl/bootstrap-token.csv \\ --service-cluster-ip-range=10.1.0.0/16 \\ --service-node-port-range=20000-40000 \\ --tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\ --client-ca-file=/opt/kubernetes/ssl/ca.pem \\ --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --etcd-cafile=/opt/kubernetes/ssl/ca.pem \\ --etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem \\ --etcd-servers=https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379 \\ --enable-swagger-ui=true \\ --allow-privileged=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/opt/kubernetes/log/api-audit.log \\ --event-ttl=1h \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/log Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target b.启动API Server服务[root@linux-node1 ~]# systemctl daemon-reload [root@linux-node1 ~]# systemctl enable kube-apiserver [root@linux-node1 ~]# systemctl start kube-apiserver #查看API Server服务状态 [root@linux-node1 ~]# systemctl status kube-apiserver 7.部署K8S ControllerManager服务a.将ControllerManager配置为系统服务[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/opt/kubernetes/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.1.0.0/16 \\ --cluster-cidr=10.2.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/opt/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/log Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target b.启动ControllerManager服务[root@linux-node1 ~]# systemctl daemon-reload [root@linux-node1 scripts]# systemctl enable kube-controller-manager [root@linux-node1 scripts]# systemctl start kube-controller-manager # 查看ControllerManager服务状态 [root@linux-node1 scripts]# systemctl status kube-controller-manager 部署K8S Scheduler服务a.将Scheduler配置为系统服务[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/opt/kubernetes/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=2 \\ --logtostderr=false \\ --log-dir=/opt/kubernetes/log Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target b.启动Scheduler服务[root@linux-node1 ~]# systemctl daemon-reload [root@linux-node1 scripts]# systemctl enable kube-scheduler [root@linux-node1 scripts]# systemctl start kube-scheduler # 查看Scheduler服务状态 [root@linux-node1 scripts]# systemctl status kube-scheduler 部署kubectl 命令行工具1.准备二进制命令包[root@linux-node1 ~]# cd /usr/local/src/kubernetes/client/bin [root@linux-node1 bin]# cp kubectl /opt/kubernetes/bin/ 2.创建 admin 证书签名请求fds[root@linux-node1 ~]# cd /usr/local/src/ssl/ [root@linux-node1 ssl]# vim admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } 3.生成 admin 证书和私钥：[root@linux-node1 ssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin [root@linux-node1 ssl]# ls -l admin* -rw-r--r-- 1 root root 1009 Mar 5 12:29 admin.csr -rw-r--r-- 1 root root 229 Mar 5 12:28 admin-csr.json -rw------- 1 root root 1675 Mar 5 12:29 admin-key.pem -rw-r--r-- 1 root root 1399 Mar 5 12:29 admin.pem [root@linux-node1 src]# mv admin*.pem /opt/kubernetes/ssl/ 4.设置集群参数[root@linux-node1 src]# kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://192.168.56.11:6443 Cluster \"kubernetes\" set. 5.设置客户端认证参数[root@linux-node1 src]# kubectl config set-credentials admin \\ --client-certificate=/opt/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/opt/kubernetes/ssl/admin-key.pem User \"admin\" set. 6.设置上下文参数[root@linux-node1 src]# kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin Context \"kubernetes\" created. 7.设置默认上下文[root@linux-node1 src]# kubectl config use-context kubernetes Switched to context \"kubernetes\". 8.使用kubectl工具[root@linux-node1 ~]# kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-1 Healthy {\"health\":\"true\"} etcd-2 Healthy {\"health\":\"true\"} etcd-0 Healthy {\"health\":\"true\"} 至此 k8sMaster端以及etcd集群已经搭建完毕","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"03-K8s集群搭建---部署ETCD集群","slug":"03k8s-ji-qun-da-jian-sanbu-shuetcd-ji-qun","date":"2018-06-01T18:39:59.000Z","updated":"2025-09-01T01:59:08.957Z","comments":true,"path":"2018/06/02/03k8s-ji-qun-da-jian-sanbu-shuetcd-ji-qun/","permalink":"https://blog.sctux.cc/2018/06/02/03k8s-ji-qun-da-jian-sanbu-shuetcd-ji-qun/","excerpt":"0.解压安装分发etcd二进制包:[root@linux-node1 src]# tar zxf etcd-v3.2.18-linux-amd64.tar.gz [root@linux-node1 src]# cd etcd-v3.2.18-linux-amd64 [root@linux-node1 etcd-v3.2.18-linux-amd64]# chmod +x etcdctl [root@linux-node1 etcd-v3.2.18-linux-amd64]# cp etcd etcdctl /opt/kubernetes/bin/ [root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.13:/opt/kubernetes/bin/ 1.创建 etcd 证书签名请求：[root@linux-node1 ~]# cat &gt; etcd-csr.json &lt;&lt; EOF { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"192.168.56.11\", \"192.168.56.12\", \"192.168.56.13\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 2.生成 etcd 证书和私钥：[root@linux-node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd # 会生成以下证书文件 [root@linux-node1 ssl]# ll etc* -rw-r--r-- 1 root root 1062 May 28 15:39 etcd.csr -rw-r--r-- 1 root root 287 May 28 15:38 etcd-csr.json -rw------- 1 root root 1679 May 28 15:39 etcd-key.pem -rw-r--r-- 1 root root 1436 May 28 15:39 etcd.pem 3.将证书移动到/opt/kubernetes/ssl目录下:[root@linux-node1 ~]# cp etcd*.pem /opt/kubernetes/ssl [root@linux-node1 ~]# scp etcd*.pem 192.168.56.12:/opt/kubernetes/ssl [root@linux-node1 ~]# scp etcd*.pem 192.168.56.13:/opt/kubernetes/ssl [root@linux-node1 ~]# rm -f etcd.csr etcd-csr.json 4.增加etcd配置文件:!!!!! 注意这里需要修改相关对应服务器的IP地址 !!!!! [root@linux-node1 ~]# vim /opt/kubernetes/cfg/etcd.conf #[member] ETCD_NAME=\"etcd-node1\" # 修改节点对应名称 ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\" #ETCD_SNAPSHOT_COUNTER=\"10000\" #ETCD_HEARTBEAT_INTERVAL=\"100\" #ETCD_ELECTION_TIMEOUT=\"1000\" ETCD_LISTEN_PEER_URLS=\"https://192.168.56.11:2380\" # 修改对应服务器IP ETCD_LISTEN_CLIENT_URLS=\"https://192.168.56.11:2379,https://127.0.0.1:2379\" #ETCD_MAX_SNAPSHOTS=\"5\" #ETCD_MAX_WALS=\"5\" #ETCD_CORS=\"\" #[cluster] ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://192.168.56.11:2380\" # 修改对应服务器IP # if you use different ETCD_NAME (e.g. test), # set ETCD_INITIAL_CLUSTER value for this name, i.e. \"test=http://...\" ETCD_INITIAL_CLUSTER=\"etcd-node1=https://192.168.56.11:2380,etcd-node2=https://192.168.56.12:2380,etcd-node3=https://192.168.56.13:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"https://192.168.56.11:2379\" # 修改对应服务器IP #[security] CLIENT_CERT_AUTH=\"true\" ETCD_CA_FILE=\"/opt/kubernetes/ssl/ca.pem\" ETCD_CERT_FILE=\"/opt/kubernetes/ssl/etcd.pem\" ETCD_KEY_FILE=\"/opt/kubernetes/ssl/etcd-key.pem\" PEER_CLIENT_CERT_AUTH=\"true\" ETCD_PEER_CA_FILE=\"/opt/kubernetes/ssl/ca.pem\" ETCD_PEER_CERT_FILE=\"/opt/kubernetes/ssl/etcd.pem\" ETCD_PEER_KEY_FILE=\"/opt/kubernetes/ssl/etcd-key.pem\"","text":"0.解压安装分发etcd二进制包:[root@linux-node1 src]# tar zxf etcd-v3.2.18-linux-amd64.tar.gz [root@linux-node1 src]# cd etcd-v3.2.18-linux-amd64 [root@linux-node1 etcd-v3.2.18-linux-amd64]# chmod +x etcdctl [root@linux-node1 etcd-v3.2.18-linux-amd64]# cp etcd etcdctl /opt/kubernetes/bin/ [root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.12:/opt/kubernetes/bin/ [root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.13:/opt/kubernetes/bin/ 1.创建 etcd 证书签名请求：[root@linux-node1 ~]# cat &gt; etcd-csr.json &lt;&lt; EOF { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"192.168.56.11\", \"192.168.56.12\", \"192.168.56.13\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 2.生成 etcd 证书和私钥：[root@linux-node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \\ -ca-key=/opt/kubernetes/ssl/ca-key.pem \\ -config=/opt/kubernetes/ssl/ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd # 会生成以下证书文件 [root@linux-node1 ssl]# ll etc* -rw-r--r-- 1 root root 1062 May 28 15:39 etcd.csr -rw-r--r-- 1 root root 287 May 28 15:38 etcd-csr.json -rw------- 1 root root 1679 May 28 15:39 etcd-key.pem -rw-r--r-- 1 root root 1436 May 28 15:39 etcd.pem 3.将证书移动到/opt/kubernetes/ssl目录下:[root@linux-node1 ~]# cp etcd*.pem /opt/kubernetes/ssl [root@linux-node1 ~]# scp etcd*.pem 192.168.56.12:/opt/kubernetes/ssl [root@linux-node1 ~]# scp etcd*.pem 192.168.56.13:/opt/kubernetes/ssl [root@linux-node1 ~]# rm -f etcd.csr etcd-csr.json 4.增加etcd配置文件:!!!!! 注意这里需要修改相关对应服务器的IP地址 !!!!! [root@linux-node1 ~]# vim /opt/kubernetes/cfg/etcd.conf #[member] ETCD_NAME=\"etcd-node1\" # 修改节点对应名称 ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\" #ETCD_SNAPSHOT_COUNTER=\"10000\" #ETCD_HEARTBEAT_INTERVAL=\"100\" #ETCD_ELECTION_TIMEOUT=\"1000\" ETCD_LISTEN_PEER_URLS=\"https://192.168.56.11:2380\" # 修改对应服务器IP ETCD_LISTEN_CLIENT_URLS=\"https://192.168.56.11:2379,https://127.0.0.1:2379\" #ETCD_MAX_SNAPSHOTS=\"5\" #ETCD_MAX_WALS=\"5\" #ETCD_CORS=\"\" #[cluster] ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://192.168.56.11:2380\" # 修改对应服务器IP # if you use different ETCD_NAME (e.g. test), # set ETCD_INITIAL_CLUSTER value for this name, i.e. \"test=http://...\" ETCD_INITIAL_CLUSTER=\"etcd-node1=https://192.168.56.11:2380,etcd-node2=https://192.168.56.12:2380,etcd-node3=https://192.168.56.13:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"https://192.168.56.11:2379\" # 修改对应服务器IP #[security] CLIENT_CERT_AUTH=\"true\" ETCD_CA_FILE=\"/opt/kubernetes/ssl/ca.pem\" ETCD_CERT_FILE=\"/opt/kubernetes/ssl/etcd.pem\" ETCD_KEY_FILE=\"/opt/kubernetes/ssl/etcd-key.pem\" PEER_CLIENT_CERT_AUTH=\"true\" ETCD_PEER_CA_FILE=\"/opt/kubernetes/ssl/ca.pem\" ETCD_PEER_CERT_FILE=\"/opt/kubernetes/ssl/etcd.pem\" ETCD_PEER_KEY_FILE=\"/opt/kubernetes/ssl/etcd-key.pem\" 5.增加etcd系统服务[root@linux-node1 ~]# cat &gt; /etc/systemd/system/etcd.service &lt;&lt; EOF [Unit] Description=Etcd Server After=network.target [Service] Type=simple WorkingDirectory=/var/lib/etcd EnvironmentFile=-/opt/kubernetes/cfg/etcd.conf # set GOMAXPROCS to number of processors ExecStart=/bin/bash -c \"GOMAXPROCS=$(nproc) /opt/kubernetes/bin/etcd\" Type=notify [Install] WantedBy=multi-user.target EOF 6.重新加载系统服务[root@linux-node1 ~]# systemctl daemon-reload [root@linux-node1 ~]# systemctl enable etcd [root@linux-node1 ~]# scp /opt/kubernetes/cfg/etcd.conf 192.168.56.12:/opt/kubernetes/cfg/ [root@linux-node1 ~]# scp /etc/systemd/system/etcd.service 192.168.56.12:/etc/systemd/system/ [root@linux-node1 ~]# scp /opt/kubernetes/cfg/etcd.conf 192.168.56.13:/opt/kubernetes/cfg/ [root@linux-node1 ~]# scp /etc/systemd/system/etcd.service 192.168.56.13:/etc/systemd/system/ #在所有节点上创建etcd存储目录并启动etcd(注意这里需要三台尽量同时启动) [root@linux-node1 ~]# mkdir /var/lib/etcd [root@linux-node1 ~]# systemctl start etcd [root@linux-node1 ~]# systemctl status etcd # 其他节点重复以上操作，直到所有机器的 etcd 服务都已启动。 7.验证ETCD集群[root@linux-node1 ssl]# etcdctl --endpoints=https://192.168.56.11:2379 \\ &gt; --ca-file=/opt/kubernetes/ssl/ca.pem \\ &gt; --cert-file=/opt/kubernetes/ssl/etcd.pem \\ &gt; --key-file=/opt/kubernetes/ssl/etcd-key.pem cluster-health member 6566e06d7343e1bb is healthy: got healthy result from https://192.168.56.11:2379 member 435fb0a8da627a4c is healthy: got healthy result from https://192.168.56.12:2379 member ce7b884e428b6c8c is healthy: got healthy result from https://192.168.56.13:2379 cluster is healthy # 集群可用","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"02-K8s集群搭建---CA证书生成","slug":"02k8s-ji-qun-da-jianca-zheng-shu-sheng-cheng","date":"2018-06-01T14:15:34.000Z","updated":"2025-09-01T01:59:08.984Z","comments":true,"path":"2018/06/01/02k8s-ji-qun-da-jianca-zheng-shu-sheng-cheng/","permalink":"https://blog.sctux.cc/2018/06/01/02k8s-ji-qun-da-jianca-zheng-shu-sheng-cheng/","excerpt":"kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书； 使用证书的组件如下： etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem； kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。 (后面的操作我们做到哪一步再去做对应组建的证书，以免出现差错 1. 安装 CFSSL(二进制方式)[root@linux-node1 ~]# cd /usr/local/src [root@linux-node1 src]# mv cfssl-certinfo_linux-amd64 /opt/kubernetes/bin/cfssl-certinfo [root@linux-node1 src]# mv cfssljson_linux-amd64 /opt/kubernetes/bin/cfssljson [root@linux-node1 src]# mv cfssl_linux-amd64 /opt/kubernetes/bin/cfssl [root@linux-node1 src]# chmod +x /opt/kubernetes/bin/cf* 复制cfssl命令文件到node2和node3节点。如果实际中多个节点，就都需要同步复制。 [root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.12: /opt/kubernetes/bin [root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.13: /opt/kubernetes/bin 2.初始化cfssl[root@linux-node1 src]# mkdir ssl &amp;&amp; cd ssl [root@linux-node1 ssl]# cfssl print-defaults config &gt; config.json [root@linux-node1 ssl]# cfssl print-defaults csr &gt; csr.json 3.创建用来生成 CA 文件的 JSON 配置文件[root@linux-node1 ssl]# cat &gt; ca-config.json &lt;&lt;EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF 创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件","text":"kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书； 使用证书的组件如下： etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem； kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。 (后面的操作我们做到哪一步再去做对应组建的证书，以免出现差错 1. 安装 CFSSL(二进制方式)[root@linux-node1 ~]# cd /usr/local/src [root@linux-node1 src]# mv cfssl-certinfo_linux-amd64 /opt/kubernetes/bin/cfssl-certinfo [root@linux-node1 src]# mv cfssljson_linux-amd64 /opt/kubernetes/bin/cfssljson [root@linux-node1 src]# mv cfssl_linux-amd64 /opt/kubernetes/bin/cfssl [root@linux-node1 src]# chmod +x /opt/kubernetes/bin/cf* 复制cfssl命令文件到node2和node3节点。如果实际中多个节点，就都需要同步复制。 [root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.12: /opt/kubernetes/bin [root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.13: /opt/kubernetes/bin 2.初始化cfssl[root@linux-node1 src]# mkdir ssl &amp;&amp; cd ssl [root@linux-node1 ssl]# cfssl print-defaults config &gt; config.json [root@linux-node1 ssl]# cfssl print-defaults csr &gt; csr.json 3.创建用来生成 CA 文件的 JSON 配置文件[root@linux-node1 ssl]# cat &gt; ca-config.json &lt;&lt;EOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" } } } } EOF 创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件[root@linux-node1 ssl]# cat &gt; ca-csr.json &lt;&lt;EOF { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } EOF 5.生成CA证书（ca.pem）和密钥（ca-key.pem）[root@linux-node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca [root@linux-node1 ssl]# ls -l ca* -rw-r--r-- 1 root root 290 May 28 15:25 ca-config.json -rw-r--r-- 1 root root 1001 May 28 15:30 ca.csr -rw-r--r-- 1 root root 208 May 28 15:30 ca-csr.json -rw------- 1 root root 1675 May 28 15:30 ca-key.pem -rw-r--r-- 1 root root 1359 May 28 15:30 ca.pem 6.分发证书[root@ linux-node1 ssl]# cp ca.csr ca.pem ca-key.pem ca-config.json /opt/kubernetes/ssl scp证书到node2和node3节点 [root@ linux-node1 ssl]# scp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.12:/opt/kubernetes/ssl [root@ linux-node1 ssl]# scp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.13:/opt/kubernetes/ssl","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"01-K8s集群搭建---系统初始化配置","slug":"01k8s-ji-qun-da-jian-yixi-tong-chu-shi-hua-pei-zhi","date":"2018-05-30T12:14:43.000Z","updated":"2025-09-01T01:59:08.932Z","comments":true,"path":"2018/05/30/01k8s-ji-qun-da-jian-yixi-tong-chu-shi-hua-pei-zhi/","permalink":"https://blog.sctux.cc/2018/05/30/01k8s-ji-qun-da-jian-yixi-tong-chu-shi-hua-pei-zhi/","excerpt":"1.架构说明:￼ 2.版本信息: 系统版本CentOS7.x Kubernets: v1.10 Etcd: v3.3.1 Docker: 18.03.1-ce Flannel: v1.10 CNI-Plugins: v0.7.0 建议部署节点：最少三个节点，请配置好主机名解析（必备） 3.系统初始化(三台服务器分别执行即可，以node1为例):a. 主机名配置 node1: echo \"linux-node1.example.com\" &gt; /etc/hostname b. 设置/etc/hosts保证主机名能够解析 node1: echo \"192.168.56.11 linux-node1 linux-node1.example.com\" &gt;&gt; /etc/hosts c. 关闭SELinux及防火墙","text":"1.架构说明:￼ 2.版本信息: 系统版本CentOS7.x Kubernets: v1.10 Etcd: v3.3.1 Docker: 18.03.1-ce Flannel: v1.10 CNI-Plugins: v0.7.0 建议部署节点：最少三个节点，请配置好主机名解析（必备） 3.系统初始化(三台服务器分别执行即可，以node1为例):a. 主机名配置 node1: echo \"linux-node1.example.com\" &gt; /etc/hostname b. 设置/etc/hosts保证主机名能够解析 node1: echo \"192.168.56.11 linux-node1 linux-node1.example.com\" &gt;&gt; /etc/hosts c. 关闭SELinux及防火墙 node1: systemctl disable firewalld; systemctl stop firewalld sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config d. 环境变量配置(后续k8s相关命令都会放到/opt/kubernetes/bin目录下) PATH=$PATH:$HOME/bin:/opt/kubernetes/bin source ~/.bash_profile 4.安装Dockera：使用国内Docker源 [root@linux-node1 ~]# cd /etc/yum.repos.d/ [root@linux-node1 yum.repos.d]# wget \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo b：Docker安装： [root@linux-node1 ~]# yum install -y docker-ce c：启动后台进程： [root@linux-node1 ~]# systemctl start docker 5.准备部署目录[root@linux-node1 ~]# mkdir -p /opt/kubernetes/{cfg,bin,ssl,log} # 目录结构, 所有文件均存放在/opt/kubernetes目录下： [root@linux-node1 ~]# tree -L 1 /opt/kubernetes/ /opt/kubernetes/ ├── bin #二进制文件 ├── cfg #配置文件 ├── log #日志文件 └── ssl #证书文件 6.准备软件包百度网盘下载地址：https://pan.baidu.com/s/1bQo7PEfKJAAhk4KzQgLrMQ 密码:gm3k 7.解压软件包[root@linux-node1 ~]# cd /tmp/ &amp;&amp; unzip k8s-v1.10.1-manual.zip &amp;&amp; mv k8s-v1.10.1-manual/k8s-v1.10.1/* /usr/local/src/ [root@linux-node1 ~]# cd /usr/local/src [root@linux-node1 ~]# tar zxf kubernetes.tar.gz [root@linux-node1 ~]# tar zxf kubernetes-server-linux-amd64.tar.gz [root@linux-node1 ~]# tar zxf kubernetes-client-linux-amd64.tar.gz [root@linux-node1 ~]# tar zxf kubernetes-node-linux-amd64.tar.gz 8.做好master节点跟其他node节点的ssh互信,便于搭建[root@linux-node1 ~]# ssh-key -t rsa \"\" ...... ...... ...... [root@linux-node1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.56.11 [root@linux-node1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.56.12","categories":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}],"tags":[{"name":"k8s集群搭建","slug":"k8s集群搭建","permalink":"https://blog.sctux.cc/tags/k8s%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"}],"keywords":[{"name":"k8s","slug":"k8s","permalink":"https://blog.sctux.cc/categories/k8s/"}]},{"title":"如何实现Mysql数据库差异化对比","slug":"ru-he-shi-xianmysql-shu-ju-ku-cha-yi-hua-dui-bi","date":"2018-03-18T10:15:49.000Z","updated":"2025-09-01T01:59:08.862Z","comments":true,"path":"2018/03/18/ru-he-shi-xianmysql-shu-ju-ku-cha-yi-hua-dui-bi/","permalink":"https://blog.sctux.cc/2018/03/18/ru-he-shi-xianmysql-shu-ju-ku-cha-yi-hua-dui-bi/","excerpt":"在团队开发中，一般都会存在测试、预发布、正式环境或多版本进行开发；代码的管理一般也有git/svn等等工具； 但是在mysql的管理就有些麻烦了，对于一些正规化的大厂团队，对数据库的每一次表结构都有详细的记录，这样在执行变更/升级的时候只需要执行直接执行变更过的SQL即可，但是有时候也会出现记录不完整或者遗漏造成测试/预发布/正式环境的不一致。 这时候就需要人工去查找两个数据库数据表中的不同；看哪里少什么，哪里多了什么，但是如果人工去每次desc/select是很费时费力的事情；那么这时候我们就需要用到mysql的相关工具；例如mysqldiff 例如:这里有个db1跟db2 数据库，各自里面有两张表student_1,student2,这里只是举个例子，下面的结构用肉眼是可以看出来的； MariaDB [(none)]&gt; use db1; Database changed MariaDB [db1]&gt; show tables; +---------------+ | Tables_in_db1 | +---------------+ | student_1 | +---------------+ 1 row in set (0.00 sec) MariaDB [db1]&gt; desc student_1; +-------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------------+-------------+------+-----+---------+-------+ | studentNo | char(10) | NO | PRI | NULL | | | studentName | varchar(20) | NO | | NULL | | | sex | char(2) | YES | | NULL | | | birthday | date | YES | | NULL | | | native | varchar(20) | YES | | NULL | | | nation | varchar(20) | YES | | NULL | | | classNo | char(6) | YES | | NULL | | +-------------+-------------+------+-----+---------+-------+ 7 rows in set (0.01 sec) MariaDB [db1]&gt; use db2; Database changed MariaDB [db2]&gt; desc student_2; +-------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------------+-------------+------+-----+---------+-------+ | studentNo | char(10) | NO | PRI | NULL | | | studentName | varchar(20) | NO | | NULL | | | sex | char(2) | YES | | NULL | | | birthday | date | YES | | NULL | | | native | varchar(40) | YES | | NULL | | | nation | varchar(20) | YES | | NULL | | +-------------+-------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) MariaDB [db2]&gt; 如果使用mysqldiff工具输出将会是这样的:mysqldiff --server1=root:123.com@127.0.0.1:3306 --server2=root:123.com@127.0.0.1:3306 db1.student_1:db2.student_2; # WARNING: Using a password on the command line interface can be insecure. # server1 on 127.0.0.1: ... connected. # server2 on 127.0.0.1: ... connected. # Comparing db1.student_1 to db2.student_2 [FAIL] # Object definitions differ. (--changes-for=server1) # --- db1.student_1 +++ db2.student_2 @@ -1,10 +1,9 @@ -CREATE TABLE `student_1` ( +CREATE TABLE `student_2` ( `studentNo` char(10) NOT NULL, `studentName` varchar(20) NOT NULL, `sex` char(2) DEFAULT NULL, `birthday` date DEFAULT NULL, - `native` varchar(20) DEFAULT NULL, + `native` varchar(40) DEFAULT NULL, `nation` varchar(20) DEFAULT NULL, - `classNo` char(6) DEFAULT NULL, UNIQUE KEY `studentNo` (`studentNo`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 # Compare failed. One or more differences found. 从以上输出可以看出来, db2的student_2相对于db1的student_1结构1.字段native的vachar类型限制不同；表student_2是40,表student_1是40；2.表student_2缺少字段classNo 这样一来就能很快速的输出两个库中表结构的差异；然后看以那个库为标杆进行Alert操作就行了下面是我改正后再次执行mysqldiff工具命令的结果输出:","text":"在团队开发中，一般都会存在测试、预发布、正式环境或多版本进行开发；代码的管理一般也有git/svn等等工具； 但是在mysql的管理就有些麻烦了，对于一些正规化的大厂团队，对数据库的每一次表结构都有详细的记录，这样在执行变更/升级的时候只需要执行直接执行变更过的SQL即可，但是有时候也会出现记录不完整或者遗漏造成测试/预发布/正式环境的不一致。 这时候就需要人工去查找两个数据库数据表中的不同；看哪里少什么，哪里多了什么，但是如果人工去每次desc/select是很费时费力的事情；那么这时候我们就需要用到mysql的相关工具；例如mysqldiff 例如:这里有个db1跟db2 数据库，各自里面有两张表student_1,student2,这里只是举个例子，下面的结构用肉眼是可以看出来的； MariaDB [(none)]&gt; use db1; Database changed MariaDB [db1]&gt; show tables; +---------------+ | Tables_in_db1 | +---------------+ | student_1 | +---------------+ 1 row in set (0.00 sec) MariaDB [db1]&gt; desc student_1; +-------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------------+-------------+------+-----+---------+-------+ | studentNo | char(10) | NO | PRI | NULL | | | studentName | varchar(20) | NO | | NULL | | | sex | char(2) | YES | | NULL | | | birthday | date | YES | | NULL | | | native | varchar(20) | YES | | NULL | | | nation | varchar(20) | YES | | NULL | | | classNo | char(6) | YES | | NULL | | +-------------+-------------+------+-----+---------+-------+ 7 rows in set (0.01 sec) MariaDB [db1]&gt; use db2; Database changed MariaDB [db2]&gt; desc student_2; +-------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------------+-------------+------+-----+---------+-------+ | studentNo | char(10) | NO | PRI | NULL | | | studentName | varchar(20) | NO | | NULL | | | sex | char(2) | YES | | NULL | | | birthday | date | YES | | NULL | | | native | varchar(40) | YES | | NULL | | | nation | varchar(20) | YES | | NULL | | +-------------+-------------+------+-----+---------+-------+ 6 rows in set (0.00 sec) MariaDB [db2]&gt; 如果使用mysqldiff工具输出将会是这样的:mysqldiff --server1=root:123.com@127.0.0.1:3306 --server2=root:123.com@127.0.0.1:3306 db1.student_1:db2.student_2; # WARNING: Using a password on the command line interface can be insecure. # server1 on 127.0.0.1: ... connected. # server2 on 127.0.0.1: ... connected. # Comparing db1.student_1 to db2.student_2 [FAIL] # Object definitions differ. (--changes-for=server1) # --- db1.student_1 +++ db2.student_2 @@ -1,10 +1,9 @@ -CREATE TABLE `student_1` ( +CREATE TABLE `student_2` ( `studentNo` char(10) NOT NULL, `studentName` varchar(20) NOT NULL, `sex` char(2) DEFAULT NULL, `birthday` date DEFAULT NULL, - `native` varchar(20) DEFAULT NULL, + `native` varchar(40) DEFAULT NULL, `nation` varchar(20) DEFAULT NULL, - `classNo` char(6) DEFAULT NULL, UNIQUE KEY `studentNo` (`studentNo`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 # Compare failed. One or more differences found. 从以上输出可以看出来, db2的student_2相对于db1的student_1结构1.字段native的vachar类型限制不同；表student_2是40,表student_1是40；2.表student_2缺少字段classNo 这样一来就能很快速的输出两个库中表结构的差异；然后看以那个库为标杆进行Alert操作就行了下面是我改正后再次执行mysqldiff工具命令的结果输出: mysqldiff --server1=root:123.com@127.0.0.1:3306 --server2=root:123.com@127.0.0.1:3306 db1.student_1:db2.student_2; # WARNING: Using a password on the command line interface can be insecure. # server1 on 127.0.0.1: ... connected. # server2 on 127.0.0.1: ... connected. # Comparing db1.student_1 to db2.student_2 [PASS] # WARNING: The tables structure is the same, but the columns order is different. Use --change-for to take the order into account. # Success. All objects are the same. 意思就是说检测通过，看起来所有的对象都是一样的。 So, 简单使用方法就是酱紫的了；其实还有更多的选项就行操作的；但是目前暂时没有用到；需要的话找man就好了。","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"},{"name":"数据库","slug":"必备知识/数据库","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"系统运维","slug":"系统运维","permalink":"https://blog.sctux.cc/tags/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"},{"name":"数据库","slug":"必备知识/数据库","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"如何利用Git Webhook 进行部署","slug":"ru-he-li-yonggit-webhook-jin-xing-bu-shu","date":"2018-03-08T12:11:45.000Z","updated":"2025-09-02T08:18:14.293Z","comments":true,"path":"2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/","permalink":"https://blog.sctux.cc/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/","excerpt":"作为一名”伪码农”运维工程师,在接触了开发方面的知识后；也在写项目时一直使用git,可是开发、调试、部署都是在本地进行的；在部署到服务器时也是通过手工去获取仓库的代码；1.开发完代码提交到远程仓库;2.登录远程服务器,并切到代码目录进行git pull;3.重启supervisor应用(我这边开发的python web应用是supervisor进行管理); 当然如果只是一次性部署上去就不再修改的话并没啥问题，但是要是项目持续性修改迭代的话，就比较麻烦了，就在不断的重复着上面的步骤。作为一个”伪码农”，怎么允许不断的重复同样的工作，于是git webhooks闪亮登场；大家对于钩子并不陌生，同样的版本控制SVN也有钩子这个功能；只是个人更加的倾向于使用git; 然后git也催生出来了很多；比如使用最广泛的是GitHub，再其次就是Gitlab,最后就是我这边是用的Gogs;他们都有同样的功能；只是说看需求跟习惯; gogs有个docker版本；能在2分钟之内就可以跑起来非常轻巧方便；感兴趣的请点击这里Gogs git webhook进行自动部署如何实现 Git webhook进行自动部署，其实原理很简单，如图所示:￼ 1.本地代码开发完毕提交到远程仓库;￼ 2.通过 POST 请求将订阅事件信息发送至向指定 URL 地址;￼ ￼ 3.指定的url是使用Flask写的一个应用,触发这个URL就会进行远程仓库的代码clone/pull￼4.拉取代码完毕后,重启服务 Flask web应用代码:# -*- coding:utf-8 -*- # 依赖包: pip install flask gitpython from flask import Flask, request, jsonify,abort import git, os # 远程服务器代码地址 code_dir = \"./code\" # 远程仓库地址 git_url = \"git@192.168.1.105:guomaoqiu/devopscode.git\" #白名单 allow_ip=[\"192.168.1.105\"] app = Flask(__name__) #重启服务 restart_services = os.system(\"supervisorctl -c /etc/supervisord.conf restart devops &amp;&amp; supervisorctl -c /etc/supervisord.conf restart celery\") @app.route('/pullcode', methods=['POST']) def pullcode(): # 只允许指定服务器向Flask应用发起POST请求，否则直接返回403 if request.headers.get('X-Forwarded-For', request.remote_addr) not in allow_ip: return abort(403) if request.method == 'POST': if os.path.isdir(code_dir): local_repo = git.Repo(code_dir) try: print local_repo.git.pull() # 重新加载代码、重启服务 restart_services return jsonify({\"result\":True,\"message\":\"pull success\"}) except Exception,e: return jsonify({\"result\":False,\"message\": \"pull faild\".format(e)}) else: try: print git.Repo.clone_from(url=git_url, to_path=code_dir) # 重新加载代码、重启服务 restart_services return jsonify({\"result\":True,\"message\":\"clone success\"}) except Exception, e: return jsonify({\"result\":False,\"message\": \"clone faild\".format(e)}) if __name__ == '__main__': app.run(host='192.168.1.29', port=5003)","text":"作为一名”伪码农”运维工程师,在接触了开发方面的知识后；也在写项目时一直使用git,可是开发、调试、部署都是在本地进行的；在部署到服务器时也是通过手工去获取仓库的代码；1.开发完代码提交到远程仓库;2.登录远程服务器,并切到代码目录进行git pull;3.重启supervisor应用(我这边开发的python web应用是supervisor进行管理); 当然如果只是一次性部署上去就不再修改的话并没啥问题，但是要是项目持续性修改迭代的话，就比较麻烦了，就在不断的重复着上面的步骤。作为一个”伪码农”，怎么允许不断的重复同样的工作，于是git webhooks闪亮登场；大家对于钩子并不陌生，同样的版本控制SVN也有钩子这个功能；只是个人更加的倾向于使用git; 然后git也催生出来了很多；比如使用最广泛的是GitHub，再其次就是Gitlab,最后就是我这边是用的Gogs;他们都有同样的功能；只是说看需求跟习惯; gogs有个docker版本；能在2分钟之内就可以跑起来非常轻巧方便；感兴趣的请点击这里Gogs git webhook进行自动部署如何实现 Git webhook进行自动部署，其实原理很简单，如图所示:￼ 1.本地代码开发完毕提交到远程仓库;￼ 2.通过 POST 请求将订阅事件信息发送至向指定 URL 地址;￼ ￼ 3.指定的url是使用Flask写的一个应用,触发这个URL就会进行远程仓库的代码clone/pull￼4.拉取代码完毕后,重启服务 Flask web应用代码:# -*- coding:utf-8 -*- # 依赖包: pip install flask gitpython from flask import Flask, request, jsonify,abort import git, os # 远程服务器代码地址 code_dir = \"./code\" # 远程仓库地址 git_url = \"git@192.168.1.105:guomaoqiu/devopscode.git\" #白名单 allow_ip=[\"192.168.1.105\"] app = Flask(__name__) #重启服务 restart_services = os.system(\"supervisorctl -c /etc/supervisord.conf restart devops &amp;&amp; supervisorctl -c /etc/supervisord.conf restart celery\") @app.route('/pullcode', methods=['POST']) def pullcode(): # 只允许指定服务器向Flask应用发起POST请求，否则直接返回403 if request.headers.get('X-Forwarded-For', request.remote_addr) not in allow_ip: return abort(403) if request.method == 'POST': if os.path.isdir(code_dir): local_repo = git.Repo(code_dir) try: print local_repo.git.pull() # 重新加载代码、重启服务 restart_services return jsonify({\"result\":True,\"message\":\"pull success\"}) except Exception,e: return jsonify({\"result\":False,\"message\": \"pull faild\".format(e)}) else: try: print git.Repo.clone_from(url=git_url, to_path=code_dir) # 重新加载代码、重启服务 restart_services return jsonify({\"result\":True,\"message\":\"clone success\"}) except Exception, e: return jsonify({\"result\":False,\"message\": \"clone faild\".format(e)}) if __name__ == '__main__': app.run(host='192.168.1.29', port=5003) 如果其他地址发起POST请求那么就直接返回403,这样就避免了其他人乱来的情况:￼ 总结起来也就是:本地仓库推送代码到远程仓库后，一旦本地仓库变更提交就会触发webhook发送post请求，驱动自动部署、重启服务等。这样一来只需要本地开发好代码提交后直接访问服务器就可以验证我们的应用程序了;","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"如何使用Celery(芹菜)异步神器执行后台任务","slug":"ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin","date":"2017-11-24T01:13:10.000Z","updated":"2025-09-01T01:59:08.930Z","comments":true,"path":"2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/","permalink":"https://blog.sctux.cc/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/","excerpt":"关于异步的知识网上很多，这里就直接上代码，目前结合Flask这个Python框架实现后台任务的执行操作； 1.需要了解的知识点: 了解生产消费模型或者发布订阅模式来实现消息队列 了解异步、同步之间的差别 2.实现过程(1)目录结构├── LICENSE ├── README.md ├── app // Flask APP应用 │&nbsp;&nbsp; ├── __init__.py │&nbsp;&nbsp; ├── auth │&nbsp;&nbsp; ├── email.py │&nbsp;&nbsp; ├── main │&nbsp;&nbsp; ├── models.py │&nbsp;&nbsp; ├── salt │&nbsp;&nbsp; ├── static │&nbsp;&nbsp; └── templates ├── config.py // 通用配置 ├── config.pyc ├── manager.py ├── migrations │&nbsp;&nbsp; ├── README │&nbsp;&nbsp; ├── alembic.ini │&nbsp;&nbsp; ├── env.py │&nbsp;&nbsp; └── versions └── requirements.txt (2)指定broker/backend(此处使用redis)vim app/__init__.py // 部分代码 from celery import Celery app = Flask(__name__) broker = 'redis://127.0.0.1:6379' backend = 'redis://127.0.0.1:6379/0' celery = Celery(app.name, broker=broker, backend=backend) // 部分代码 以上引入了Celery,指定了Celery的生产消费端都为我们的redis","text":"关于异步的知识网上很多，这里就直接上代码，目前结合Flask这个Python框架实现后台任务的执行操作； 1.需要了解的知识点: 了解生产消费模型或者发布订阅模式来实现消息队列 了解异步、同步之间的差别 2.实现过程(1)目录结构├── LICENSE ├── README.md ├── app // Flask APP应用 │&nbsp;&nbsp; ├── __init__.py │&nbsp;&nbsp; ├── auth │&nbsp;&nbsp; ├── email.py │&nbsp;&nbsp; ├── main │&nbsp;&nbsp; ├── models.py │&nbsp;&nbsp; ├── salt │&nbsp;&nbsp; ├── static │&nbsp;&nbsp; └── templates ├── config.py // 通用配置 ├── config.pyc ├── manager.py ├── migrations │&nbsp;&nbsp; ├── README │&nbsp;&nbsp; ├── alembic.ini │&nbsp;&nbsp; ├── env.py │&nbsp;&nbsp; └── versions └── requirements.txt (2)指定broker/backend(此处使用redis)vim app/__init__.py // 部分代码 from celery import Celery app = Flask(__name__) broker = 'redis://127.0.0.1:6379' backend = 'redis://127.0.0.1:6379/0' celery = Celery(app.name, broker=broker, backend=backend) // 部分代码 以上引入了Celery,指定了Celery的生产消费端都为我们的redis (3)这里我将任务写到了视图函数中vim app/main/views.py (1 创建一个任务函数, 并且将这个任务绑定上了celery的标记 部分代码@celery.task(bind=True)def update_cbt_resource(self):‘’’@summary: 创建一个需要后台去执行的任务，我这里只是举个栗子‘’’result=commands.getoutput(“sleep 20 &amp;&amp; echo ‘ok’”)return result (2 创建一个执行任务请求函数 部分代码@main.route(‘/execute_task/‘, methods=[‘GET’, ‘POST’])def execute_task(update_env):“””@summary: 请求函数入口“””# 这里用最笨的办法执行了在我们web中点击执行任务之前去检查celery这个服务是否存在，如果不存在会提示用户result = commands.getoutput(“ps -ef | grep celery | grep -v grep”)if not result: return jsonify({“result”:False,”message”:u’未发现Celery进程，请检查该服务是否正常启动’}) # 将前面写的任务函数直接调用apply_async属性task = update_cbt_resource.apply_async() # 这里就可以获取到这个任务一开始执行就返回的一些属性 # 比如任务ID, 任务状态data = { “task_id”: task.id, “task_status”: task.status}# 将以上的信息作为这请求函数的返回result = {“result”:True,”data”:data,”message”:u’执行开始’}return jsonify(result) (3 创建一个任务状态查询的函数 一般任务触发之后我们想要知道这个任务的执行状态，是处于哪个阶段的，各个状态已经在官网详细说明了；有兴趣的直接看官网 1234567891011121314151617181920212223242526272829### 部分的代码@main.route('/task_result', methods=['GET'])@login_requireddef task_result(): ''' @summary: 任务的状态是通过task_id来获取,在触发了任务之后,通过前端js轮询请求这个函数，就可以得到该任务的当前执行状态 ''' # 点击执行按钮前端会传一个task_id到后端,这里使用form的方式获取到task_id task_id = json.loads(request.form.get('data'))['task_id'] # AsyncResult，它的作用是被用来检查任务状态，等待任务执行完毕或获取任务结果，如果任务失败，它会返回异常信息或者调用栈。 the_task = update_cbt_resource.AsyncResult(task_id) print(\"任务：{0} 当前的 state 为：{1}\".format(task_id, the_task.state)) # 执the_task.state if the_task.state == 'PROGRESS': print the_task.info.get('i', 0) result = {'state': 'progress',\"result_data\":the_task.result} elif the_task.state == 'SUCCESS': result = {'state': \"success\", \"result_data\":the_task.result} elif the_task.state == 'PENDING': result = {'state': 'waitting',\"result_data\":the_task.result} elif the_task.state == 'REVOKED': result = { 'state': 'revoke', \"result_data\":the_task.result} print the_task.result else: result = {'state': the_task.state,'progress':0,\"result_data\":the_task.result} return jsonify(result) (4 尝试在前端页面点击执行任务￼Oops! 这就是上面进行celery进程判断之后得到的提示信息，那下面就把celery启动起来吧~~~ (5 启动Celey服务 celery worker -A manager.celery -l debug 我这里是本地使用的virtualenv环境~~~ ￼ok, 现在启动了celery服务之后我们在前台执行一下:￼通过点击执行后我们看日志：￼ 任务执行完毕了；￼ 再看看Flask的日志,这就是通过js轮询请求这个task_resul函数的结果￼再来看看redis中的内容,这就将结果保存到了backend中￼ (4)如何撤销一个任务呢?还是通过task_id来实现。视图函数中编写一个任务的函数 ### 部分代码 @main.route('/cancel_task/', methods=['GET', 'POST']) @login_required def cancel_task(): # 通过前端的取消按钮来获取到这个任务的id task_id = json.loads(request.form.get('data'))['task_id'] try: celery.control.revoke(task_id, terminate=True, signal='SIGKILL') return Response(json.dumps({'result':True,\"message\": \"取消任务完成\" })) except Exception,e: return Response(json.dumps({'result': True, \"message\": u'取消任务失败.{0}'.format(e)})) 执行过程如下:￼￼￼ ￼需要注意的是,在上面视图函数中的这段代码其实有也可以撤销 task = update_cbt_resource.apply_async() task.revoke() 但是这种方式只是撤销，如果任务已经在执行撤销则无效;所以我这里可以使用下面的方法来撤销 # 通过task_id撤销 celery.control.revoke(task_id) # 撤销正在执行的任务，默认使用TERM信号 celery.control.revoke(task_id, terminate=True) # 撤销正在执行的任务，默认使用KILL信号 celery.control.revoke(task_id, terminate=True, signal='SIGKILL') #在官网文档中也可以将多个task_id组成列表形式然后同时撤销多个任务 celery.control.revoke([task_id1,task_id2,task_id3,task_id4.......]) ok，以上就是Celery 的基本使用。","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Celery","slug":"Celery","permalink":"https://blog.sctux.cc/tags/Celery/"},{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/tags/Python/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Redis集群详细搭建指南","slug":"redis-ji-qun-xiang-xi-da-jian-zhi-nan","date":"2017-06-04T06:17:26.000Z","updated":"2025-09-01T01:59:08.955Z","comments":true,"path":"2017/06/04/redis-ji-qun-xiang-xi-da-jian-zhi-nan/","permalink":"https://blog.sctux.cc/2017/06/04/redis-ji-qun-xiang-xi-da-jian-zhi-nan/","excerpt":"Redis 集群简介Redis 是一个开源的 key-value 存储系统，由于出众的性能，大部分互联网企业都用来做服务器端缓存。Redis 在3.0版本前只支持单实例模式，虽然支持主从模式、哨兵模式部署来解决单点故障，但是现在互联网企业动辄大几百G的数据，可完全是没法满足业务的需求，所以，Redis 在 3.0 版本以后就推出了集群模式。 Redis 从3.0.0正式版开始官方支持集群, Redis 集群采用了P2P的模式，完全去中心化。Redis 把所有的 Key 分成了 16384 个 slot，每个 Redis 实例负责其中一部分 slot 。集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新。 Redis 客户端可以在任意一个 Redis 实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例。 随随便便搭建一个集群安装部署任何一个应用其实都很简单，只要安装步骤一步一步来就行了。下面说一下 Redis 集群搭建规划，由于集群至少需要6个节点（3主3从模式），所以，没有这么多机器给我玩，现在计划是在一台机器上模拟一个集群，当然，这和生产环境的集群搭建没本质区别。 1.获取软件包cd /tmp/ &amp;&amp; wget http://download.redis.io/releases/redis-4.0.11.tar.gz mkdir /usr/local/redis &amp;&amp; tar -xr /tmp/redis-4.0.11.tar.gz -C /usr/local/redis cd /usr/local/redis ./configure make -j 4 make install 2.规划创建文件目录我们计划集群中 Redis 节点的端口号为 9001-9006 ，端口号即集群下各实例文件夹。数据存放在 端口号/data 文件夹中。","text":"Redis 集群简介Redis 是一个开源的 key-value 存储系统，由于出众的性能，大部分互联网企业都用来做服务器端缓存。Redis 在3.0版本前只支持单实例模式，虽然支持主从模式、哨兵模式部署来解决单点故障，但是现在互联网企业动辄大几百G的数据，可完全是没法满足业务的需求，所以，Redis 在 3.0 版本以后就推出了集群模式。 Redis 从3.0.0正式版开始官方支持集群, Redis 集群采用了P2P的模式，完全去中心化。Redis 把所有的 Key 分成了 16384 个 slot，每个 Redis 实例负责其中一部分 slot 。集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新。 Redis 客户端可以在任意一个 Redis 实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例。 随随便便搭建一个集群安装部署任何一个应用其实都很简单，只要安装步骤一步一步来就行了。下面说一下 Redis 集群搭建规划，由于集群至少需要6个节点（3主3从模式），所以，没有这么多机器给我玩，现在计划是在一台机器上模拟一个集群，当然，这和生产环境的集群搭建没本质区别。 1.获取软件包cd /tmp/ &amp;&amp; wget http://download.redis.io/releases/redis-4.0.11.tar.gz mkdir /usr/local/redis &amp;&amp; tar -xr /tmp/redis-4.0.11.tar.gz -C /usr/local/redis cd /usr/local/redis ./configure make -j 4 make install 2.规划创建文件目录我们计划集群中 Redis 节点的端口号为 9001-9006 ，端口号即集群下各实例文件夹。数据存放在 端口号/data 文件夹中。 mkdir /usr/local/redis-cluster/ cd /usr/local/redis-cluster/ for i in {1..6};do mkdir 900${i}/data done 3. 复制执行脚本在 /usr/local/redis-cluster 下创建 bin 文件夹，用来存放集群运行脚本，并把安装好的 Redis 的 src 路径下的运行脚本拷贝过来。看命令： mkdir /usr/local/redis-cluster/bin cd /usr/local/redis/src # 下载后解压编译后的src目录 cp mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-dump redis-cli redis-server redis-trib.rb /usr/local/redis-cluster/bin 4.复制一个新 Redis 实例我们现在从已安装好的 Redis 中复制一个新的实例到 9001 文件夹，并修改 redis.conf 配置。 cp /usr/local/redis/* /usr/local/redis-cluster/9001 port 9001（每个节点的端口号） daemonize yes bind 192.168.56.127（绑定当前机器 IP） dir /usr/local/redis-cluster/9001/data/（数据文件存放位置） pidfile /var/run/redis_9001.pid（pid 9001和port要对应） cluster-enabled yes（启动集群模式） cluster-config-file nodes9001.conf（9001和port要对应） cluster-node-timeout 15000 appendonly yes 5.再复制出五个新 Redis 实例我们已经完成了一个节点了，其实接下来就是机械化的再完成另外五个节点，其实可以这么做：把 9001 实例 复制到另外五个文件夹中，唯一要修改的就是 redis.conf 中的所有和端口的相关的信息即可，其实就那么四个位置。开始操作，看命令： \\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9002 \\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9003 \\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9004 \\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9005 \\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9006 这里我们把配置复制过去之后需要修改对应文件夹中的redis.conf 用sed或者vim全局替换即可，主要是端口 下面，下面我们来启动它们 for i in {1..6};do /usr/local/bin/redis-server /usr/local/redis-cluster/900${i}/redis.conf done [root@localhost redis-cluster]# ps aux | grep redis | grep -v grep root 3995 0.5 0.6 151404 11864 ? Ssl 10:51 0:10 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9001 [cluster] root 4000 0.4 0.6 151404 11856 ? Ssl 10:51 0:09 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9002 [cluster] root 4005 0.4 0.6 151404 11856 ? Ssl 10:51 0:09 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9003 [cluster] root 4010 0.1 0.6 151404 11872 ? Ssl 10:51 0:02 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9004 [cluster] root 4015 0.0 0.6 151404 11876 ? Ssl 10:51 0:01 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9005 [cluster] root 4020 0.0 0.6 151404 11872 ? Ssl 10:51 0:01 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9006 [cluster] root 4509 0.8 0.6 149356 11880 ? Ssl 10:55 0:14 /usr/local/redis-cluster/bin/redis-server [root@localhost redis-cluster]# 以上 可以看出我们的6个实例已经启动了测试一下？ 随便找个节点测试一下: /usr/local/redis-cluster/bin/redis-cli -h 192.168.56.127 -p 9001 192.168.56.127:9001&gt; set name guomaoqiu (error) CLUSTERDOWN Hash slot not served 192.168.56.127:9001&gt; 连接成功了，但好像报错了 这是因为虽然我们配置并启动了 Redis 集群服务，但是他们暂时还并不在一个集群中，互相直接发现不了，而且还没有可存储的位置，就是所谓的slot（槽）。 6.安装集群所需软件由于 Redis 集群需要使用 ruby 命令，所以我们需要安装 ruby 和相关接口。 安装redis需要ruby版本最低是2.2.2，而centos yum库中ruby版本支持到2.0.0。所以，无法满足需求。 解决方案： 1.安装curl sudo yum install curl 2.安装RVM curl -L get.rvm.io | bash -s stable source /usr/local/rvm/scripts/rvm 3.查看rvm库中已知的ruby版本 rvm list known 4.安装一个ruby版本 rvm install 2.3.3 5.设置默认版本 rvm use 2.3.3 6.卸载一个已知版本 rvm remove 2.0.0 7.查看ruby版本 ruby –version 8.再安装redis就可以了 gem install redis 7. 利用官方提供的命令创建集群/usr/local/redis-cluster/bin/redis-trib.rb create --replicas 1 192.168.56.127:9001 192.168.56.127:9002 192.168.56.127:9003 192.168.56.127:9004 192.168.56.127:9005 192.168.56.127:9006 简单解释一下这个命令：调用 ruby 命令来进行创建集群，–replicas 1 表示主从复制比例为 1:1，即一个主节点对应一个从节点；然后，默认给我们分配好了每个主节点和对应从节点服务，以及 solt 的大小，因为在 Redis 集群中有且仅有 16383 个 solt ，默认情况会给我们平均分配，当然你可以指定，后续的增减节点也可以重新分配。 [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb create --replicas 1 192.168.56.127:9001 192.168.56.127:9002 192.168.56.127:9003 192.168.56.127:9004 192.168.56.127:9005 192.168.56.127:9006 &gt;&gt;&gt; Creating cluster &gt;&gt;&gt; Performing hash slots allocation on 6 nodes... Using 3 masters: # 三个主节点 192.168.56.127:9001 192.168.56.127:9002 192.168.56.127:9003 # 三个主节点对应三个从节点 Adding replica 192.168.56.127:9005 to 192.168.56.127:9001 Adding replica 192.168.56.127:9006 to 192.168.56.127:9002 Adding replica 192.168.56.127:9004 to 192.168.56.127:9003 &gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:0-5460 (5461 slots) master M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:5461-10922 (5462 slots) master M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots:10923-16383 (5461 slots) master S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 replicates 474fd53f7199ad70cce7f18bd68f5445b559509a S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 replicates e070e76789352d2492cfc9800850e943e0c4b72d S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 replicates 0fcbaa301451baf87284546513003568e47b5daa Can I set the above configuration? (type 'yes' to accept): yes &gt;&gt;&gt; Nodes configuration updated &gt;&gt;&gt; Assign a different config epoch to each node &gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join... &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:0-5460 (5461 slots) master 1 additional replica(s) M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:5461-10922 (5462 slots) master 1 additional replica(s) M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates e070e76789352d2492cfc9800850e943e0c4b72d [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. 以上执行结果代表集群搭建成功啦！！！ 验证： /usr/local/redis-cluster/bin/redis-cli -c -h 192.168.56.127 -p 9001 192.168.56.127:9001&gt; cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 cluster_size:3 cluster_current_epoch:6 cluster_my_epoch:1 cluster_stats_messages_ping_sent:167 cluster_stats_messages_pong_sent:172 cluster_stats_messages_sent:339 cluster_stats_messages_ping_received:167 cluster_stats_messages_pong_received:167 cluster_stats_messages_meet_received:5 cluster_stats_messages_received:339 192.168.56.127:9001&gt; cluster nodes 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002@19002 master - 0 1536205294229 2 connected 5461-10922 e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003@19003 master - 0 1536205293000 3 connected 10923-16383 bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006@19006 slave 0fcbaa301451baf87284546513003568e47b5daa 0 1536205295234 6 connected 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001@19001 myself,master - 0 1536205294000 1 connected 0-5460 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004@19004 slave 474fd53f7199ad70cce7f18bd68f5445b559509a 0 1536205293217 4 connected 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005@19005 slave e070e76789352d2492cfc9800850e943e0c4b72d 0 1536205292212 5 connected 192.168.56.127:9001&gt; set name guomaoqiu -&gt; Redirected to slot [5798] located at 192.168.56.127:9002 OK 192.168.56.127:9002&gt; get name \"guomaoqiu\" 192.168.56.127:9002&gt; 通过命令，可以详细的看出集群信息和各个节点状态，主从信息以及连接数、槽信息等。这么看到，我们已经真的把 Redis 集群搭建部署成功啦！ 如何新增节点？新增节点无非就是复制配置，修改配置，然后启动；我这里还是创建了一对一主一从的配置，端口分别对应的是9007、9008 1.启动两个实例(假设我这里已经复制配置，修改完配置了)for i in {7..8};do /usr/local/redis-cluster/bin/redis-server /usr/local/redis-cluster/900${i}/redis.conf done 6002:C 06 Sep 11:50:53.048 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 6002:C 06 Sep 11:50:53.048 # Redis version=4.0.11, bits=64, commit=00000000, modified=0, pid=6002, just started 6002:C 06 Sep 11:50:53.048 # Configuration loaded 6004:C 06 Sep 11:50:53.060 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 6004:C 06 Sep 11:50:53.060 # Redis version=4.0.11, bits=64, commit=00000000, modified=0, pid=6004, just started 6004:C 06 Sep 11:50:53.060 # Configuration loaded ps aux | grep redis | grep -v grep redis 1180 0.0 0.3 142900 5828 ? Ssl 11:28 0:01 /usr/bin/redis-server 0.0.0.0:6379 root 4257 0.0 0.6 149356 11880 ? Ssl 11:32 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9001 [cluster] root 4262 0.0 0.6 149356 11880 ? Ssl 11:32 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9002 [cluster] root 4267 0.0 0.6 149356 11876 ? Ssl 11:32 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9003 [cluster] root 4272 0.0 0.6 149356 11908 ? Ssl 11:32 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9004 [cluster] root 4277 0.0 0.6 149356 11920 ? Ssl 11:32 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9005 [cluster] root 4282 0.0 0.6 149356 11912 ? Ssl 11:32 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9006 [cluster] root 6003 0.1 0.5 147308 9616 ? Ssl 11:50 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9007 [cluster] root 6008 0.0 0.5 147308 9616 ? Ssl 11:50 0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9008 [cluster] 以上 可以看到我们启动了后面两个实例，并且进程中标注了是以集群方式启动, 但是此时还并没有将实例加入到集群当中。 2.添加实例到集群[root@locahost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb add-node 192.168.56.127:9007 192.168.56.127:9001 &gt;&gt;&gt; Adding node 192.168.56.127:9007 to cluster 192.168.56.127:9001 &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:0-5460 (5461 slots) master 1 additional replica(s) M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:5461-10922 (5462 slots) master 1 additional replica(s) M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates e070e76789352d2492cfc9800850e943e0c4b72d [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. &gt;&gt;&gt; Send CLUSTER MEET to node 192.168.56.127:9007 to make it join the cluster. [OK] New node added correctly. [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-cli -c -h 192.168.56.127 -p 9001 cluster nodes 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002@19002 master - 0 1536215194000 2 connected 5461-10922 e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003@19003 master - 0 1536215194000 3 connected 10923-16383 bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006@19006 slave 0fcbaa301451baf87284546513003568e47b5daa 0 1536215193000 6 connected 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001@19001 myself,master - 0 1536215192000 1 connected 0-5460 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004@19004 slave 474fd53f7199ad70cce7f18bd68f5445b559509a 0 1536215195858 4 connected 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007@19007 master - 0 1536215195000 0 connected 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005@19005 slave e070e76789352d2492cfc9800850e943e0c4b72d 0 1536215194855 5 connected 以上可以看到我们通过 add-node 参数添加了 9007这个实例；命令最后一个参数是已经成为集群的集群成员host:port而且实例9007获得了一个集群ID:1714784542b3afc9e2a8b2c93fff49d095e7d72b 5. 增加实例的从节点到集群以上我们将实例9007作为master增加到了集群中，但是按照前面的规划，每个master对应一个slave所以我们需要将实例9008加入集群并将其称为9007的slave,命令如下 [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb add-node --slave --master-id 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9008 192.168.56.127:9001 &gt;&gt;&gt; Adding node 192.168.56.127:9008 to cluster 192.168.56.127:9001 &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:0-5460 (5461 slots) master 1 additional replica(s) M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:5461-10922 (5462 slots) master 1 additional replica(s) M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007 slots: (0 slots) master 0 additional replica(s) S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates e070e76789352d2492cfc9800850e943e0c4b72d [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. &gt;&gt;&gt; Send CLUSTER MEET to node 192.168.56.127:9008 to make it join the cluster. Waiting for the cluster to join. &gt;&gt;&gt; Configure node as replica of 192.168.56.127:9007. [OK] New node added correctly. [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb check 192.168.56.127:9001 &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:0-5460 (5461 slots) master 1 additional replica(s) M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:5461-10922 (5462 slots) master 1 additional replica(s) M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008 slots: (0 slots) slave replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007 slots: (0 slots) master 1 additional replica(s) S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates e070e76789352d2492cfc9800850e943e0c4b72d [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. 以上我们将两个节点加入到了集群中，但是细心的同学会发现我们的实例9007所拥有的slot数量为0，那此时就需要将重新调整哈希槽啦~再次之前我们三个master 平均将16384 分成了三等分，那我们新增加了一个master ，为了平衡我们可以计算出这个新的master需要多少哈希槽，即 16384/4 = 4096 我们需要移动4096个槽点到9007上。 [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb reshard 192.168.56.127:9001 &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:0-5460 (5461 slots) master 1 additional replica(s) S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008 slots: (0 slots) slave replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007 slots:5461-5798 (0 slots) master 1 additional replica(s) S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates e070e76789352d2492cfc9800850e943e0c4b72d M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:5799-10922 (5462 slots) master 1 additional replica(s) M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. # 这里需要输入我们所需要的哈希槽 How many slots do you want to move (from 1 to 16384)? 4097 # 这里需要输入我们实例9007的ID What is the receiving node ID? 1714784542b3afc9e2a8b2c93fff49d095e7d72b Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node #1:all #输入all 表示从所有的主节点中随机转移，凑够1000个哈希槽 #然后再输入yes，redis集群就开始分配哈希槽了。 以上:redis-trib 会向你询问重新分片的源节点（source node），即，要从特点的哪个节点中取出 4096 个哈希槽，还是从全部节点提取4096个哈希槽， 并将这些槽移动到9007节点上面。 如果我们不打算从特定的节点上取出指定数量的哈希槽，那么可以向redis-trib输入 all，这样的话， 集群中的所有主节点都会成为源节点，redis-trib从各个源节点中各取出一部分哈希槽，凑够4096个，然后移动到9007节点上： 验证： [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb check 192.168.56.127:9001 &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:1280-5460 (4096 slots) master 1 additional replica(s) S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008 slots: (0 slots) slave replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007 slots:0-1279,5461-6998,10923-12201 (4096 slots) master 1 additional replica(s) S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates e070e76789352d2492cfc9800850e943e0c4b72d M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:6999-10922 (4096 slots) master 1 additional replica(s) M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots:12202-16383 (4096 slots) master 1 additional replica(s) S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. 以上我们的集群搭建以及新增节点就完成了； 6. 从集群中删除节点:和节点添加一样，移除节点也有移除主节点，从节点 1、移除主节点移除节点使用redis-trib的del-node命令 /usr/local/redis-cluster/bin/redis-trib.rb del-node 192.168.56.127:9001 ${node-id} 192.168.56.127:9001其中一个集群节点，node-id为要删除的主节点,这里和添加节点不同,移除节点node-id是必需的，测试删除9003主节点 [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb del-node 192.168.56.127:9001 e070e76789352d2492cfc9800850e943e0c4b72d &gt;&gt;&gt; Removing node e070e76789352d2492cfc9800850e943e0c4b72d from cluster 192.168.56.127:9001 [ERR] Node 192.168.56.127:9003 is not empty! Reshard data away and try again. redis cluster提示9003已经有数据了，不能够被删除，需要将他的数据转移出去，也就是和新增主节点一样需重新分片。 /usr/local/redis-cluster/bin/redis-trib.rb reshard 192.168.56.127:9001 执行以后会提示我们移除的大小，因为9003占用了4096个槽点 &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. How many slots do you want to move (from 1 to 16384)? 输入4096提示移动的node id，填写9007的node id。 &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. How many slots do you want to move (from 1 to 16384)? 4182 What is the receiving node ID? 1714784542b3afc9e2a8b2c93fff49d095e7d72b 需要移动到全部主节点上还是单个主节点 Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node #1: 将4096个槽点移动到9007上，填写9003的node id ：e070e76789352d2492cfc9800850e943e0c4b72d Source node #1:e070e76789352d2492cfc9800850e943e0c4b72d Source node #2:done 确认之后会一个一个将9003的卡槽移到到9007上。 [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb check 192.168.56.127:9001 &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:1280-5460 (4096 slots) master 1 additional replica(s) S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008 slots: (0 slots) slave replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007 slots:0-1279,5461-6998,10923-16383 (8192 slots) master 2 additional replica(s) S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:6999-10922 (4096 slots) master 1 additional replica(s) M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003 slots: (0 slots) master 0 additional replica(s) S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. 以上我们将实例9003的哈希槽全部转移到了9007，已经为0 ,此时再来删除我们的节点就行了~ [root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb del-node 192.168.56.127:9001 e070e76789352d2492cfc9800850e943e0c4b72d &gt;&gt;&gt; Removing node e070e76789352d2492cfc9800850e943e0c4b72d from cluster 192.168.56.127:9001 &gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster... &gt;&gt;&gt; SHUTDOWN the node. # 看下面结果 执行完del-node 9003节点就已经从集群中剔除啦~😁 &gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001) M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001 slots:1280-5460 (4096 slots) master 1 additional replica(s) S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008 slots: (0 slots) slave replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007 slots:0-1279,5461-6998,10923-16383 (8192 slots) master 2 additional replica(s) S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005 slots: (0 slots) slave replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002 slots:6999-10922 (4096 slots) master 1 additional replica(s) S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006 slots: (0 slots) slave replicates 0fcbaa301451baf87284546513003568e47b5daa S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004 slots: (0 slots) slave replicates 474fd53f7199ad70cce7f18bd68f5445b559509a [OK] All nodes agree about slots configuration. &gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage... [OK] All 16384 slots covered. 最后当然我们集群搭建已经完成，那集群的作用是为了解决分片的问题，如果需要搭建高可用集群；也就是主的失败后从的能够切换角色,所以可以使用哨兵机制来解决HA.sentinel和cluster主要区别，sentinel用来解决HA（高可用）问题，而cluster主要解决sharding（分片）问题。两个经常结合使用搭建高可用集群。 关于哈希槽的概念：Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。 Redis 集群没有使用一致性hash, 而是引入了哈希槽的概念。 Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽。这种结构很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。 使用哈希槽的好处就在于可以方便的添加或移除节点。 当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了； 当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了； 在这一点上，我们以后新增或移除节点的时候不用先停掉所有的 redis 服务。 “用了哈希槽的概念，而没有用一致性哈希算法，不都是哈希么？这样做的原因是为什么呢？”Redis Cluster是自己做的crc16的简单hash算法，没有用一致性hash。Redis的作者认为它的crc16(key) mod 16384的效果已经不错了，虽然没有一致性hash灵活，但实现很简单，节点增删时处理起来也很方便。 “为了动态增删节点的时候，不至于丢失数据么？”节点增删时不丢失数据和hash算法没什么关系，不丢失数据要求的是一份数据有多个副本。 “还有集群总共有2的14次方，16384个哈希槽，那么每一个哈希槽中存的key 和 value是什么？”当你往Redis Cluster中加入一个Key时，会根据crc16(key) mod 16384计算这个key应该分布到哪个hash slot中，一个hash slot中会有很多key和value。你可以理解成表的分区，使用单节点时的redis时只有一个表，所有的key都放在这个表里；改用Redis Cluster以后会自动为你生成16384个分区表，你insert数据时会根据上面的简单算法来决定你的key应该存在哪个分区，每个分区里有很多key。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.sctux.cc/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"redis-cluster","slug":"redis-cluster","permalink":"https://blog.sctux.cc/tags/redis-cluster/"}],"keywords":[{"name":"数据库","slug":"数据库","permalink":"https://blog.sctux.cc/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"蓝鲸平台实践及应用...","slug":"lan-jing-ping-tai-shi-jian-ji-ying-yong-2","date":"2017-05-30T23:15:49.000Z","updated":"2025-09-01T01:59:08.871Z","comments":true,"path":"2017/05/31/lan-jing-ping-tai-shi-jian-ji-ying-yong-2/","permalink":"https://blog.sctux.cc/2017/05/31/lan-jing-ping-tai-shi-jian-ji-ying-yong-2/","excerpt":"前言:在今年三月份蓝鲸推出了一套免费的蓝鲸DevOps技能培训，借助蓝鲸平台致力于让每个运维屌丝都具备一定的开发能力；有幸在此次培训课程坚持了下来，也收获了关于腾讯蓝鲸运维的一些理念、思维；同时自己的工作方式、学习方式也有了一定的提升、改变；再此感谢各位导师的孜孜不倦的教授—–授人以鱼不如授人以渔。 一、如何学习它？ Linux基础(必备)； 需要有一定的Python基础知识； 前端方面也需要多写； 学习能力 + 学习方法； 二、学习途径：链接: 蓝鲸DevOps技能培训 三、我的学习记录:","text":"前言:在今年三月份蓝鲸推出了一套免费的蓝鲸DevOps技能培训，借助蓝鲸平台致力于让每个运维屌丝都具备一定的开发能力；有幸在此次培训课程坚持了下来，也收获了关于腾讯蓝鲸运维的一些理念、思维；同时自己的工作方式、学习方式也有了一定的提升、改变；再此感谢各位导师的孜孜不倦的教授—–授人以鱼不如授人以渔。 一、如何学习它？ Linux基础(必备)； 需要有一定的Python基础知识； 前端方面也需要多写； 学习能力 + 学习方法； 二、学习途径：链接: 蓝鲸DevOps技能培训 三、我的学习记录:链接: 作业记录 四、目前学习成果: 蓝鲸paas平台页面截图: 蓝鲸开发者中心: ￼ 自主开发App权限控制平台: ￼ 自主开发运维管控平台: ￼ 五、小结:上面的两个应用已经应用在生产环境;其他各个部门的平台开发ing;现在小伙伴儿们如果去学习的话；应该非常的顺利，毕竟坑差不多都被填完了。不想说太多；此篇博文记录一下这三个月以来的一些收获吧;￼ 你的对手在看书，你的仇人在磨刀，你的闺密在减肥，隔壁老王在练腰，我们必须不断学习，才能让自己变得更强。","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"系统运维","slug":"系统运维","permalink":"https://blog.sctux.cc/tags/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"How to Use Mojo/webqq Send QQ Message","slug":"how-to-use-mojowebqq-send-qq-message","date":"2017-04-06T07:13:02.000Z","updated":"2025-09-01T01:59:08.959Z","comments":true,"path":"2017/04/06/how-to-use-mojowebqq-send-qq-message/","permalink":"https://blog.sctux.cc/2017/04/06/how-to-use-mojowebqq-send-qq-message/","excerpt":"1. get docker imagesdocker pull sjdy521/mojo-webqq 2. run itdocker run -d -p 9999:5000 -v /tmp/:/tmp/ sjdy521/mojo-webqq 3. check logsdocker logs -f CONTAINER_ID [17/04/07 15:57:13] [info] 当前正在使用 Mojo-Webqq v2.0.8 [17/04/07 15:57:13] [warn] 当前版本与1.x.x版本不兼容，改动详情参见更新日志 [17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::UploadQRcode ] [17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::ShowMsg ] [17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::Openqq ] [17/04/07 15:57:13] [info] Listening at \"http://0.0.0.0:5000\" Server available at http://0.0.0.0:5000 [17/04/07 15:57:13] [info] 初始化 smartqq 客户端参数... [17/04/07 15:57:13] [info] 正在获取登录二维码... [17/04/07 15:57:13] [info] 二维码已下载到本地[ /tmp/mojo_webqq_qrcode_default.png ] [17/04/07 15:57:15] [info] 二维码已上传云存储[ https://ooo.0o0.ooo/2017/04/07/58e7465b89582.png ] [17/04/07 15:57:15] [info] 等待手机QQ扫描二维码... [17/04/07 15:58:01] [info] 手机QQ扫码成功，请在手机上点击[允许登录smartQQ]按钮... [17/04/07 15:59:11] [info] 检查安全代码... [17/04/07 15:59:11] [info] 获取数据验证参数... [17/04/07 15:59:11] [info] 尝试进行登录(2)... [17/04/07 15:59:12] [info] 帐号(xxxxxxxxxxxxx)登录成功 4. scan qrcode/tmp/mojo_webqq_qrcode_default.png 5. sendmessage:curl \"http://YOUR_SERVER_IP:9999/openqq/send_friend_message?uid=friends_qq_num&amp;content=hello\" curl \"http://YOUR_SERVER_IP:9999/openqq/send_group_message?uid=group_qq_number&amp;content=hehe\"","text":"1. get docker imagesdocker pull sjdy521/mojo-webqq 2. run itdocker run -d -p 9999:5000 -v /tmp/:/tmp/ sjdy521/mojo-webqq 3. check logsdocker logs -f CONTAINER_ID [17/04/07 15:57:13] [info] 当前正在使用 Mojo-Webqq v2.0.8 [17/04/07 15:57:13] [warn] 当前版本与1.x.x版本不兼容，改动详情参见更新日志 [17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::UploadQRcode ] [17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::ShowMsg ] [17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::Openqq ] [17/04/07 15:57:13] [info] Listening at \"http://0.0.0.0:5000\" Server available at http://0.0.0.0:5000 [17/04/07 15:57:13] [info] 初始化 smartqq 客户端参数... [17/04/07 15:57:13] [info] 正在获取登录二维码... [17/04/07 15:57:13] [info] 二维码已下载到本地[ /tmp/mojo_webqq_qrcode_default.png ] [17/04/07 15:57:15] [info] 二维码已上传云存储[ https://ooo.0o0.ooo/2017/04/07/58e7465b89582.png ] [17/04/07 15:57:15] [info] 等待手机QQ扫描二维码... [17/04/07 15:58:01] [info] 手机QQ扫码成功，请在手机上点击[允许登录smartQQ]按钮... [17/04/07 15:59:11] [info] 检查安全代码... [17/04/07 15:59:11] [info] 获取数据验证参数... [17/04/07 15:59:11] [info] 尝试进行登录(2)... [17/04/07 15:59:12] [info] 帐号(xxxxxxxxxxxxx)登录成功 4. scan qrcode/tmp/mojo_webqq_qrcode_default.png 5. sendmessage:curl \"http://YOUR_SERVER_IP:9999/openqq/send_friend_message?uid=friends_qq_num&amp;content=hello\" curl \"http://YOUR_SERVER_IP:9999/openqq/send_group_message?uid=group_qq_number&amp;content=hehe\" 6. other system use:def send_message(number, content): url = \"http://YOUR_SERVER_IP:9999/openqq/send_friend_message\" data = { \"uid\": number, \"content\": content, } requests.get(url=url,data=data) if __name__ == \"__main__\": send_message('2399447849','hello')","categories":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"}],"tags":[],"keywords":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"}]},{"title":"How to Install Gateone(WebSSH)","slug":"how-to-install-gateonewebssh","date":"2017-02-21T15:23:54.000Z","updated":"2025-09-01T01:59:08.959Z","comments":true,"path":"2017/02/21/how-to-install-gateonewebssh/","permalink":"https://blog.sctux.cc/2017/02/21/how-to-install-gateonewebssh/","excerpt":"1.Install dependent package(CentOS7)yum install python-pip 2.Get install source codegit clone https://github.com/liftoff/GateOne.git cd GateOne/ python setup.py install 3. Start this service will Generate the configuration file(/etc/gateone/*)/etc/gateone # GateOne's default install dir service gateone start 4. You will see some messages flash by, wait untill you see something like:GateOne will by default run on 443(https), and will allow access only from the hostname that it autodetects, so make sure that whaterver URL you normally access, this server through is listed in GateOne’s orgin list:Listening on https://*:443/ vim /etc/gateone/conf.d/10server.conf 5.Now run:/etc/init.d/gateone start or service gateone start","text":"1.Install dependent package(CentOS7)yum install python-pip 2.Get install source codegit clone https://github.com/liftoff/GateOne.git cd GateOne/ python setup.py install 3. Start this service will Generate the configuration file(/etc/gateone/*)/etc/gateone # GateOne's default install dir service gateone start 4. You will see some messages flash by, wait untill you see something like:GateOne will by default run on 443(https), and will allow access only from the hostname that it autodetects, so make sure that whaterver URL you normally access, this server through is listed in GateOne’s orgin list:Listening on https://*:443/ vim /etc/gateone/conf.d/10server.conf 5.Now run:/etc/init.d/gateone start or service gateone start 6.Access Webssh:https://YOUR_SERVER_IP like this:￼ Sure ,the GateOne has Docker Version, you can:docker pull gateone docker run -d --name=gateone -p 443:8000 liftoff/gateone or git clone https://github.com/liftoff/GateOne.git cd GateOne/docker docker build -t gateone .","categories":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}],"tags":[{"name":"系统运维","slug":"系统运维","permalink":"https://blog.sctux.cc/tags/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"keywords":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}]},{"title":"\"Devops\" Demo(如果文中图片显示不完整，请多次刷新)","slug":"devops-demo","date":"2016-12-15T23:45:49.000Z","updated":"2025-09-01T01:59:08.981Z","comments":true,"path":"2016/12/16/devops-demo/","permalink":"https://blog.sctux.cc/2016/12/16/devops-demo/","excerpt":"今年断断续续在工作之余学习了一下 Python Web框架,下面简要说下学的东西主要有以下几个： Flask(包括各个子系统组件) + Bootstrap主要是第一个接触的就是Flask这个框架,期初给我的感觉就是开发起来非常的快捷、方便,各个子系统也有单独的学习资料,所以从一开始我也就一直断断续续的在折腾；Bootstrap DiaoBao了. SaltStack API(netapi/rest_cherrypy)这个就不用说了吧，运维人都知道；在掌握了SaltStack在命令行的用法之后就需要去学习、探索更多更便利的方法来实现我们想要的效果； Zabbix API监控利器 zabbix, 更不用说了。由于工作上的所需,增减服务器是常有的事,那怎么快速的去添加/删除主机呢,当然就要让接口帮忙去处理啦； 下面是demo的雏形：登录系统使用的是Flask自带的Flask-Login组件，验证码用的是Google的ReCAPTCHA,当然这个在Flask表单中已经集成了； ￼ SaltStack 命令执行,调用的是SaltApi,这里需要手动填写主机名,是个需要优化的地方.￼￼ 当然有了这个执行命令,哥子就要做坏事了,这怎么行!那就把一些危险的命令添加到block_list当中,避免误操作.￼ (⊙o⊙)… 这个做的不是太美观；数据是从数据库查询出来的;而这些服务器的”静态”数据都是通过SaltStack的grains获取到的,看到旁边那个Collet按钮了么;那个的功能就是一键获取各个minion的grains然后导入到数据库中，每当有新的minion加入时只要在此点击Collet Salt就会马不停蹄的到新minion端拉取相关信息,然后写入数据库, 删除按钮的作用是从数据库当中删除此条记录.￼","text":"今年断断续续在工作之余学习了一下 Python Web框架,下面简要说下学的东西主要有以下几个： Flask(包括各个子系统组件) + Bootstrap主要是第一个接触的就是Flask这个框架,期初给我的感觉就是开发起来非常的快捷、方便,各个子系统也有单独的学习资料,所以从一开始我也就一直断断续续的在折腾；Bootstrap DiaoBao了. SaltStack API(netapi/rest_cherrypy)这个就不用说了吧，运维人都知道；在掌握了SaltStack在命令行的用法之后就需要去学习、探索更多更便利的方法来实现我们想要的效果； Zabbix API监控利器 zabbix, 更不用说了。由于工作上的所需,增减服务器是常有的事,那怎么快速的去添加/删除主机呢,当然就要让接口帮忙去处理啦； 下面是demo的雏形：登录系统使用的是Flask自带的Flask-Login组件，验证码用的是Google的ReCAPTCHA,当然这个在Flask表单中已经集成了； ￼ SaltStack 命令执行,调用的是SaltApi,这里需要手动填写主机名,是个需要优化的地方.￼￼ 当然有了这个执行命令,哥子就要做坏事了,这怎么行!那就把一些危险的命令添加到block_list当中,避免误操作.￼ (⊙o⊙)… 这个做的不是太美观；数据是从数据库查询出来的;而这些服务器的”静态”数据都是通过SaltStack的grains获取到的,看到旁边那个Collet按钮了么;那个的功能就是一键获取各个minion的grains然后导入到数据库中，每当有新的minion加入时只要在此点击Collet Salt就会马不停蹄的到新minion端拉取相关信息,然后写入数据库, 删除按钮的作用是从数据库当中删除此条记录.￼点击上图的详细就可以看到关于这台主机的详细状态了,Status这个是实时抓取的当前这台主机的当前状态,而下面的两张监控图形则是调用了Grafana.本想自己画图的,但是没太多的时间去研究前端方面的东西,况且图形化的东西也蛮多了,就比如zabbix里面的监控图不美观的话加上grafana渲染；再次就是ELK里面的图形,本想去调用ELK API的,发现结合到这里也没啥卵用,要看图直接访问Kibana就行了；￼ 下面两张是关于ZabbixAPI的操作,添加服务器是通过上传一个固定格式的.xls格式文件,然后台去解析文件内容,调用API进行主机的添加,删除主机也同样调用API操作实现.￼￼ 差不多实现的就是上面这些,其他功能还在继续学习、研究中…望各路大神勿喷,希望能得到各位大神的建议,谢谢。￼","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Docker之gitlab-Ce","slug":"docker-zhigitlabce","date":"2016-08-05T03:40:58.000Z","updated":"2025-09-01T01:59:08.983Z","comments":true,"path":"2016/08/05/docker-zhigitlabce/","permalink":"https://blog.sctux.cc/2016/08/05/docker-zhigitlabce/","excerpt":"1、docker pull gitlab-ce 2、mkdir -p /data/gitlab/{config,data,logs} 3、docker run --detach \\ -p 443:443 -p 80:80 -p 2222:22 \\ --name gitlab-ce \\ --restart=always \\ --volume /data/gitlab/config:/etc/gitlab \\ --volume /data/gitlab/logs:/var/log/gitlab \\ --volume /data/gitlab/data:/var/opt/gitlab \\ docker.io/gitlab/gitlab-ce 或者省略第一个步骤直接创建目录，然后run起来；docker run --detach -p 4433:443 -p 880:80 -p 2222:22 --name gitlab-ce --restart=always --volume /data/gitlab/config:/etc/gitlab --volume /data/gitlab/logs:/var/log/gitlab --volume /data/gitlab/data:/var/opt/gitlab docker.io/gitlab/gitlab-ce GitLab 修改主机名与更换 IP 配置vim /data/gitlab/config/gitlab.rb13 liens:添加：[将external_url = 'http://git.example.com'修改为’http://docker宿主机IP/\\]267 lines:gitlab_rails[‘gitlab_shell_ssh_port’] = 2222 #修改使用ssh协议时的端口2222","text":"1、docker pull gitlab-ce 2、mkdir -p /data/gitlab/{config,data,logs} 3、docker run --detach \\ -p 443:443 -p 80:80 -p 2222:22 \\ --name gitlab-ce \\ --restart=always \\ --volume /data/gitlab/config:/etc/gitlab \\ --volume /data/gitlab/logs:/var/log/gitlab \\ --volume /data/gitlab/data:/var/opt/gitlab \\ docker.io/gitlab/gitlab-ce 或者省略第一个步骤直接创建目录，然后run起来；docker run --detach -p 4433:443 -p 880:80 -p 2222:22 --name gitlab-ce --restart=always --volume /data/gitlab/config:/etc/gitlab --volume /data/gitlab/logs:/var/log/gitlab --volume /data/gitlab/data:/var/opt/gitlab docker.io/gitlab/gitlab-ce GitLab 修改主机名与更换 IP 配置vim /data/gitlab/config/gitlab.rb13 liens:添加：[将external_url = 'http://git.example.com'修改为’http://docker宿主机IP/\\]267 lines:gitlab_rails[‘gitlab_shell_ssh_port’] = 2222 #修改使用ssh协议时的端口2222 重读配置:进入容器，然后执行gitlab-ctl reconfigure 访问：由于ssh使用了非22标准段端口，所以在这里使用这样连接gitlab即可.git clone ssh://git@192.168.1.89:2222/guomaoqiu/test.git","categories":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"}],"tags":[],"keywords":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"}]},{"title":"Flask+bootstrap写登录页面","slug":"flaskbootstrap-e5-86-99-e7-99-bb-e5-bd-95-e9-a1-b5-e9-9d-a2","date":"2016-07-14T14:57:56.000Z","updated":"2025-09-01T01:59:08.876Z","comments":true,"path":"2016/07/14/flaskbootstrap-e5-86-99-e7-99-bb-e5-bd-95-e9-a1-b5-e9-9d-a2/","permalink":"https://blog.sctux.cc/2016/07/14/flaskbootstrap-e5-86-99-e7-99-bb-e5-bd-95-e9-a1-b5-e9-9d-a2/","excerpt":"最近有些需求、想法，毕竟devops的风还是很强烈的啊，所以就跟风学学dev方面的东西； flask是一个很小巧很方便的webframe，听朋友说起django非常的重，于是我就在还有点python基础的能力下选择flask学学；准备用这个框架开发新的平台，首先就要有用户登录页面，用flask可以这样实现： 代码结构： flasky├── run.py├── static│&nbsp;&nbsp; ├── av1.jpg│&nbsp;&nbsp; ├── av2.jpg│&nbsp;&nbsp; ├── av3.jpg│&nbsp;&nbsp; ├── bootstrap.min.css│&nbsp;&nbsp; ├── bootstrap.min.js│&nbsp;&nbsp; ├── bootstrap-responsive.min.css│&nbsp;&nbsp; ├── excanvas.min.js│&nbsp;&nbsp; ├── fullcalendar.css│&nbsp;&nbsp; ├── fullcalendar.min.js│&nbsp;&nbsp; ├── glyphicons-halflings.png│&nbsp;&nbsp; ├── GNU-Linux-Logo-Penguin-SVG.png│&nbsp;&nbsp; ├── jquery.flot.min.js│&nbsp;&nbsp; ├── jquery.flot.resize.min.js│&nbsp;&nbsp; ├── jquery.min.js│&nbsp;&nbsp; ├── jquery.peity.min.js│&nbsp;&nbsp; ├── jquery.ui.custom.js│&nbsp;&nbsp; ├── logo.png│&nbsp;&nbsp; ├── Tux.jpg│&nbsp;&nbsp; ├── unicorn.dashboard.js│&nbsp;&nbsp; ├── unicorn.grey.css│&nbsp;&nbsp; ├── unicorn.js│&nbsp;&nbsp; ├── unicorn.login.css│&nbsp;&nbsp; ├── unicorn.login.js│&nbsp;&nbsp; └── unicorn.main.css└── templates ├── base.html ├── check.html ├── index.html ├── login.html └── upload.html &nbsp; 前端就用bootstrap展示，login.html 12345678910111213141516171819202122232425262728293031323334353637383940{% extends \\\"base.html\\\" %}{% block title %} Dachen FTP {% endblock %}&lt;/style&gt;&lt;/head&gt;{% block body %} &lt;body&gt; &lt;div id=\"logo\"&gt; &lt;img src=\"{ {url_for('static',filename='GNU-Linux-Logo-Penguin-SVG.png')}}\" alt=\"\"&gt; &lt;/div&gt; &lt;div id=\"loginbox\"&gt; &lt;form id=\"loginform\" class=\"form-vertical\" action=\"\" method=\"post\"&gt; &lt;p&gt;Sing in to blog.sctux.com&lt;/p&gt; &lt;!--&lt;div class=\"control-group\"&gt; --&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"controls\"&gt; &lt;div class=\"input-prepend\"&gt; &lt;input name=\"username\" value=\"{ {request.form.username}}\" placeholder=\"Username\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!\\-\\- &lt;div class=\"control-group\"&gt; --&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"controls\"&gt; &lt;div class=\"input-prepend\"&gt; &lt;input name=\"pass\" placeholder=\"Password \"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-md-90 col-md-offset-12\"&gt; &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Sign in&lt;/button&gt; &lt;/div&gt; {% if error %} &lt;p&gt;&lt;font color=\"red\"&gt;{ { error }}&lt;/font&gt;&lt;/p&gt; {% endif %} &lt;/div&gt; &lt;/form&gt; {% endblock %}&lt;/body&gt; &nbsp; run.py内容： 12345678910111213141516171819202122232425262728293031323334\\# coding:utf-8from flask.ext.wtf import Formfrom flask import render\\_template, redirect, url\\_forapp = Flask(\\_\\_name\\_\\_)@app.route('/login',methods=\\['POST','GET'\\])def login(): error=False if request.method == 'POST': if request.form\\['username'\\] != 'user1' or request.form\\['pass'\\] != 'user1@1qaz': error=\"username or password error !\" else: return redirect(url_for('index')) return render_template('login.html',error=error)@app.route('/index')def index(): return render_template('index.html',result=result)@app.route('/upload',methods=\\['GET','POST'\\])def upload(): return render_template('upload.html')@app.route('/check',methods=\\['POST','GET'\\])def check(): return render_template('check.html')@app.route('/logout')def logout(): return redirect(url_for('login'))if \\_\\_name\\_\\_==\"\\_\\_main\\_\\_\": app.run(debug=True,host='xxxxxxxxx',port=4000) 启动 python run.py 访问 http://xxxxxxxxx:4000/ 登录主页的效果： 好啦，一个简单的登录页面就写好啦；flask 值得你拥有!","text":"最近有些需求、想法，毕竟devops的风还是很强烈的啊，所以就跟风学学dev方面的东西； flask是一个很小巧很方便的webframe，听朋友说起django非常的重，于是我就在还有点python基础的能力下选择flask学学；准备用这个框架开发新的平台，首先就要有用户登录页面，用flask可以这样实现： 代码结构： flasky├── run.py├── static│&nbsp;&nbsp; ├── av1.jpg│&nbsp;&nbsp; ├── av2.jpg│&nbsp;&nbsp; ├── av3.jpg│&nbsp;&nbsp; ├── bootstrap.min.css│&nbsp;&nbsp; ├── bootstrap.min.js│&nbsp;&nbsp; ├── bootstrap-responsive.min.css│&nbsp;&nbsp; ├── excanvas.min.js│&nbsp;&nbsp; ├── fullcalendar.css│&nbsp;&nbsp; ├── fullcalendar.min.js│&nbsp;&nbsp; ├── glyphicons-halflings.png│&nbsp;&nbsp; ├── GNU-Linux-Logo-Penguin-SVG.png│&nbsp;&nbsp; ├── jquery.flot.min.js│&nbsp;&nbsp; ├── jquery.flot.resize.min.js│&nbsp;&nbsp; ├── jquery.min.js│&nbsp;&nbsp; ├── jquery.peity.min.js│&nbsp;&nbsp; ├── jquery.ui.custom.js│&nbsp;&nbsp; ├── logo.png│&nbsp;&nbsp; ├── Tux.jpg│&nbsp;&nbsp; ├── unicorn.dashboard.js│&nbsp;&nbsp; ├── unicorn.grey.css│&nbsp;&nbsp; ├── unicorn.js│&nbsp;&nbsp; ├── unicorn.login.css│&nbsp;&nbsp; ├── unicorn.login.js│&nbsp;&nbsp; └── unicorn.main.css└── templates ├── base.html ├── check.html ├── index.html ├── login.html └── upload.html &nbsp; 前端就用bootstrap展示，login.html 12345678910111213141516171819202122232425262728293031323334353637383940{% extends \\\"base.html\\\" %}{% block title %} Dachen FTP {% endblock %}&lt;/style&gt;&lt;/head&gt;{% block body %} &lt;body&gt; &lt;div id=\"logo\"&gt; &lt;img src=\"{ {url_for('static',filename='GNU-Linux-Logo-Penguin-SVG.png')}}\" alt=\"\"&gt; &lt;/div&gt; &lt;div id=\"loginbox\"&gt; &lt;form id=\"loginform\" class=\"form-vertical\" action=\"\" method=\"post\"&gt; &lt;p&gt;Sing in to blog.sctux.com&lt;/p&gt; &lt;!--&lt;div class=\"control-group\"&gt; --&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"controls\"&gt; &lt;div class=\"input-prepend\"&gt; &lt;input name=\"username\" value=\"{ {request.form.username}}\" placeholder=\"Username\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!\\-\\- &lt;div class=\"control-group\"&gt; --&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"controls\"&gt; &lt;div class=\"input-prepend\"&gt; &lt;input name=\"pass\" placeholder=\"Password \"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-md-90 col-md-offset-12\"&gt; &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Sign in&lt;/button&gt; &lt;/div&gt; {% if error %} &lt;p&gt;&lt;font color=\"red\"&gt;{ { error }}&lt;/font&gt;&lt;/p&gt; {% endif %} &lt;/div&gt; &lt;/form&gt; {% endblock %}&lt;/body&gt; &nbsp; run.py内容： 12345678910111213141516171819202122232425262728293031323334\\# coding:utf-8from flask.ext.wtf import Formfrom flask import render\\_template, redirect, url\\_forapp = Flask(\\_\\_name\\_\\_)@app.route('/login',methods=\\['POST','GET'\\])def login(): error=False if request.method == 'POST': if request.form\\['username'\\] != 'user1' or request.form\\['pass'\\] != 'user1@1qaz': error=\"username or password error !\" else: return redirect(url_for('index')) return render_template('login.html',error=error)@app.route('/index')def index(): return render_template('index.html',result=result)@app.route('/upload',methods=\\['GET','POST'\\])def upload(): return render_template('upload.html')@app.route('/check',methods=\\['POST','GET'\\])def check(): return render_template('check.html')@app.route('/logout')def logout(): return redirect(url_for('login'))if \\_\\_name\\_\\_==\"\\_\\_main\\_\\_\": app.run(debug=True,host='xxxxxxxxx',port=4000) 启动 python run.py 访问 http://xxxxxxxxx:4000/ 登录主页的效果： 好啦，一个简单的登录页面就写好啦；flask 值得你拥有!","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"},{"name":"脚本编程","slug":"Python/脚本编程","permalink":"https://blog.sctux.cc/categories/Python/%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"bootstrap","slug":"bootstrap","permalink":"https://blog.sctux.cc/tags/bootstrap/"},{"name":"flask","slug":"flask","permalink":"https://blog.sctux.cc/tags/flask/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"},{"name":"脚本编程","slug":"Python/脚本编程","permalink":"https://blog.sctux.cc/categories/Python/%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B/"}]},{"title":"Jq命令-在命令行直接解析Json文档","slug":"jq-ming-ling-zai-ming-ling-xing-zhi-jie-jie-xijson","date":"2016-06-20T05:01:33.000Z","updated":"2025-09-01T01:59:08.884Z","comments":true,"path":"2016/06/20/jq-ming-ling-zai-ming-ling-xing-zhi-jie-jie-xijson/","permalink":"https://blog.sctux.cc/2016/06/20/jq-ming-ling-zai-ming-ling-xing-zhi-jie-jie-xijson/","excerpt":"安装yum install -y libtool &amp;&amp; \\ wget https://github.com/stedolan/jq/releases/download/jq-1.5/jq-1.5.tar.gz &amp;&amp; \\ tar -xf jq-1.5.tar.gz -C /usr/local &amp;&amp; \\ cd /usr/local/jq-1.5 &amp;&amp; \\ ./configure --disable-maintainer-mode &amp;&amp; \\ make LDFLAGS=-all-static &amp;&amp; \\ make install &amp;&amp; \\ cp /usr/local/jq-1.5/jq /sbin/ \\ echo \"程序已安装: `which jq`\" 举个栗子，如：我这里有个API请求，在终端中得到的结果是一串json字符串，但是看起来不是那么规整，一下子还很难判断是什么格式；[root@localhost ~]# curl -s http://SERVER_IP/health/monitor/service {\"data\":{\"serCode\":\"0\",\"serDesc\":\"服务正常\"},\"resultCode\":1} 于是通过jq这个命令我们格式化一下，终端里面看到的结果就是下面这样的啦[root@salt-api ~]# curl -s http://SERVER_IP/health/monitor/service | jq { \"data\": { \"serCode\": \"0\", \"serDesc\": \"服务正常\" }, \"resultCode\": 1 }","text":"安装yum install -y libtool &amp;&amp; \\ wget https://github.com/stedolan/jq/releases/download/jq-1.5/jq-1.5.tar.gz &amp;&amp; \\ tar -xf jq-1.5.tar.gz -C /usr/local &amp;&amp; \\ cd /usr/local/jq-1.5 &amp;&amp; \\ ./configure --disable-maintainer-mode &amp;&amp; \\ make LDFLAGS=-all-static &amp;&amp; \\ make install &amp;&amp; \\ cp /usr/local/jq-1.5/jq /sbin/ \\ echo \"程序已安装: `which jq`\" 举个栗子，如：我这里有个API请求，在终端中得到的结果是一串json字符串，但是看起来不是那么规整，一下子还很难判断是什么格式；[root@localhost ~]# curl -s http://SERVER_IP/health/monitor/service {\"data\":{\"serCode\":\"0\",\"serDesc\":\"服务正常\"},\"resultCode\":1} 于是通过jq这个命令我们格式化一下，终端里面看到的结果就是下面这样的啦[root@salt-api ~]# curl -s http://SERVER_IP/health/monitor/service | jq { \"data\": { \"serCode\": \"0\", \"serDesc\": \"服务正常\" }, \"resultCode\": 1 }","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"jq","slug":"jq","permalink":"https://blog.sctux.cc/tags/jq/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"Flask-Login AttributeError: 'Bool' Object Has No Attribute '__Call__'","slug":"flasklogin-yong-hu-pan-duan-shi-hou-bao-cuo","date":"2016-06-06T11:09:25.000Z","updated":"2025-09-01T01:59:08.983Z","comments":true,"path":"2016/06/06/flasklogin-yong-hu-pan-duan-shi-hou-bao-cuo/","permalink":"https://blog.sctux.cc/2016/06/06/flasklogin-yong-hu-pan-duan-shi-hou-bao-cuo/","excerpt":"今天在使用flask-login添加用户认证的时候服务器出现了报错 而我的 base.html是这样写的： ￼去看官方文档吧，选择对应版本的文档(我这里是0.3.1): ￼再看看其前面的版本是不是这样定义的： ￼ 看吧 版本问题造成。","text":"今天在使用flask-login添加用户认证的时候服务器出现了报错 而我的 base.html是这样写的： ￼去看官方文档吧，选择对应版本的文档(我这里是0.3.1): ￼再看看其前面的版本是不是这样定义的： ￼ 看吧 版本问题造成。","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"千里马常有，而伯乐不常有","slug":"qianlima","date":"2016-05-08T12:42:29.000Z","updated":"2025-09-01T01:59:08.871Z","comments":true,"path":"2016/05/08/qianlima/","permalink":"https://blog.sctux.cc/2016/05/08/qianlima/","excerpt":"世有伯乐，然后有千里马。千里马常有，而伯乐不常有；故虽有名马，祗辱于奴隶人之手，骈死于槽枥之间，不以千里称也。 马之千里者，一食或尽粟一石。食马者不知其能千里而食也；是马也，虽有千里之能，食不饱，力不足，才美不外见，且欲与常马等不可得，安求其能千里也？ 策之不以其道，食之不能尽其材，鸣之而不能通其意，执策而临之曰：“天下无马！”呜呼！其真无马邪？其真不知马也！","text":"世有伯乐，然后有千里马。千里马常有，而伯乐不常有；故虽有名马，祗辱于奴隶人之手，骈死于槽枥之间，不以千里称也。 马之千里者，一食或尽粟一石。食马者不知其能千里而食也；是马也，虽有千里之能，食不饱，力不足，才美不外见，且欲与常马等不可得，安求其能千里也？ 策之不以其道，食之不能尽其材，鸣之而不能通其意，执策而临之曰：“天下无马！”呜呼！其真无马邪？其真不知马也！","categories":[{"name":"心情随笔","slug":"心情随笔","permalink":"https://blog.sctux.cc/categories/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/"}],"tags":[],"keywords":[{"name":"心情随笔","slug":"心情随笔","permalink":"https://blog.sctux.cc/categories/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/"}]},{"title":"MongoDB基本操作","slug":"mongodb-ji-ben-cao-zuo","date":"2016-05-02T06:25:53.000Z","updated":"2025-09-01T01:59:08.870Z","comments":true,"path":"2016/05/02/mongodb-ji-ben-cao-zuo/","permalink":"https://blog.sctux.cc/2016/05/02/mongodb-ji-ben-cao-zuo/","excerpt":"1.删除用户mongo&gt;use db1 mongo&gt;show users; mongo&gt;db.dropUser(\"User_name\") 2.备份mongodump -h 127.0.0.1:27017 -d health_init -u=health_init -p=health_init -o /tmp/ 3.恢复mongorestore -d health_test --drop -u=dachenadm -p=dachen@ad /tmp/health_ini 4.授权：#创建一个角色： use amdin;db.createRole({role:'sysadmin',roles:[],privileges:[{resource:{anyResource:true},actions:['anyAction']}]}) #授权一个用户 use health;db.createUser( { user: \"health\", pwd: \"healthnx\", roles: [{ role: \"sysadmin\", db: \"admin\" }] } 5. mongo远程执行需要认证的服务器的js文件/usr/bin/mongo -h 10.251.225.205:27017/health -u health -p healthnx update.js","text":"1.删除用户mongo&gt;use db1 mongo&gt;show users; mongo&gt;db.dropUser(\"User_name\") 2.备份mongodump -h 127.0.0.1:27017 -d health_init -u=health_init -p=health_init -o /tmp/ 3.恢复mongorestore -d health_test --drop -u=dachenadm -p=dachen@ad /tmp/health_ini 4.授权：#创建一个角色： use amdin;db.createRole({role:'sysadmin',roles:[],privileges:[{resource:{anyResource:true},actions:['anyAction']}]}) #授权一个用户 use health;db.createUser( { user: \"health\", pwd: \"healthnx\", roles: [{ role: \"sysadmin\", db: \"admin\" }] } 5. mongo远程执行需要认证的服务器的js文件/usr/bin/mongo -h 10.251.225.205:27017/health -u health -p healthnx update.js 6.查看collection内容：db.d_role.find() 7.更新操作Mongodb数据更新命令Mongodb更新有两个命令：update、save。update命令格式:db.collection.update(criteria,objNew,upsert,multi) 参数说明：criteria：查询条件objNew：update对象和一些更新操作符upsert：如果不存在update的记录，是否插入objNew这个新的文档，true为插入，默认为false，不插入。multi：默认是false，只更新找到的第一条记录。如果为true，把按条件查询s出来的记录全部更新。8.mongodb的文档导入导出：1. mongoimport -h 127.0.0.1:27017 -u USENAME -p PASSWORD -c COLLECTIONS b_hospitaldept.json 2. mongoexport -h 127.0.0.1:27017 -d DB -u USENAME -p PASSWORD -c COLLECTIONS -o checkin_b_hospitaldept.json 9.对collections操作1. use demodb &nbsp;//使用demodb，以下假设操作的collection是foo 2. db.foo.remove({\"id\":\"bar\"}) &nbsp;//删除一条数据 3. db.foo.remove() //删除foo中的所有记录，但是foo还存在，show collection还可以看到foo 4. db.foo.drop() &nbsp;//删除foo这个collection，（show collection已经看不到foo了）但是查看数据文件发现大小不变，Mongodb不会自动释放文件空间 5. db.repairDatabase() &nbsp;//执行这个命令后，Mongodb会把不需要的空间释放出来","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.sctux.cc/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[],"keywords":[{"name":"数据库","slug":"数据库","permalink":"https://blog.sctux.cc/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"Jenkins配置令牌远程触发项目构建","slug":"jenkins-e9-85-8d-e7-bd-ae-e4-bb-a4-e7-89-8c-e8-bf-9c-e7-a8-8b-e8-a7-a6-e5-8f-91-e9-a1-b9-e7-9b-ae-e6-9e-84-e5-bb-ba","date":"2016-04-18T15:10:01.000Z","updated":"2025-09-01T01:59:08.981Z","comments":true,"path":"2016/04/18/jenkins-e9-85-8d-e7-bd-ae-e4-bb-a4-e7-89-8c-e8-bf-9c-e7-a8-8b-e8-a7-a6-e5-8f-91-e9-a1-b9-e7-9b-ae-e6-9e-84-e5-bb-ba/","permalink":"https://blog.sctux.cc/2016/04/18/jenkins-e9-85-8d-e7-bd-ae-e4-bb-a4-e7-89-8c-e8-bf-9c-e7-a8-8b-e8-a7-a6-e5-8f-91-e9-a1-b9-e7-9b-ae-e6-9e-84-e5-bb-ba/","excerpt":"我们在执行Jenkins的项目构建的时候一般都是通过web管理界面中的”构建”来执行项目构建操作，但是除此之外我们还可以通过项目配置中的”构建触发器”来触发构建操作，其中”构建触发器”有一种方式是通过配置令牌远程触发项目构建；要启用Token(令牌)远程触发项目构建首先要保证Jenkins服务安装了build-token-root&nbsp;插件，并且配置了Jenkins的身份验证(不是必须)。 该如何使用这个插件呢？ 打开一个Jenkins Project –&gt; 配置 然后在我们自己的工作机上编写一条远程执行的命令即可，其实就是一条POST请求，如： 在我们工作机上直接执行该curl请求就可以执行项目的构建啦，而不再需要点击页面中的”构建”，用这种方式主要还是习惯吧，敲命令的感觉还是挺好的；一般为了方便我都是写到一个脚本里面去执行，如果有多个项目也可以通过传参啊，多个参数(项目)一起执行构建；看自己的工作环境而定吧，在解决问题的情况下，加快个人工作效率的情况下还是怎么简单怎么来吧。下图就是我执行了这条命令的结果，跟我们手动在web界面点击”构建”是一样的效果。","text":"我们在执行Jenkins的项目构建的时候一般都是通过web管理界面中的”构建”来执行项目构建操作，但是除此之外我们还可以通过项目配置中的”构建触发器”来触发构建操作，其中”构建触发器”有一种方式是通过配置令牌远程触发项目构建；要启用Token(令牌)远程触发项目构建首先要保证Jenkins服务安装了build-token-root&nbsp;插件，并且配置了Jenkins的身份验证(不是必须)。 该如何使用这个插件呢？ 打开一个Jenkins Project –&gt; 配置 然后在我们自己的工作机上编写一条远程执行的命令即可，其实就是一条POST请求，如： 在我们工作机上直接执行该curl请求就可以执行项目的构建啦，而不再需要点击页面中的”构建”，用这种方式主要还是习惯吧，敲命令的感觉还是挺好的；一般为了方便我都是写到一个脚本里面去执行，如果有多个项目也可以通过传参啊，多个参数(项目)一起执行构建；看自己的工作环境而定吧，在解决问题的情况下，加快个人工作效率的情况下还是怎么简单怎么来吧。下图就是我执行了这条命令的结果，跟我们手动在web界面点击”构建”是一样的效果。","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.sctux.cc/tags/Jenkins/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Lvm的创建","slug":"lvm-chuang-jian","date":"2016-03-28T17:57:13.000Z","updated":"2025-09-01T01:59:08.861Z","comments":true,"path":"2016/03/29/lvm-chuang-jian/","permalink":"https://blog.sctux.cc/2016/03/29/lvm-chuang-jian/","excerpt":"1. 查看磁盘：[root@localhost ~]# fdisk -l ...... 磁盘 /dev/sdb：21.5 GB, 21474836480 字节，41943040 个扇区 Units = 扇区 of 1 * 512 = 512 bytes 扇区大小(逻辑/物理)：512 字节 / 512 字节 I/O 大小(最小/最佳)：512 字节 / 512 字节 磁盘 /dev/sdc：21.5 GB, 21474836480 字节，41943040 个扇区 Units = 扇区 of 1 * 512 = 512 bytes 扇区大小(逻辑/物理)：512 字节 / 512 字节 I/O 大小(最小/最佳)：512 字节 / 512 字节 ...... 2. 对两块磁盘进行lvm格式分区fdisk /dev/sdb/ --&gt; n --&gt; p --&gt; 分区号(默认) --&gt; 起始扇区(默认) --&gt; last扇区(默认) # 修改分区格式 t --&gt; 8e Linux LVM(分区格式为lvm) --&gt; w (保存) # 退出后对磁盘/dev/sdc 执行相同操作即可 3. 创建物理卷(PV)[root@localhost ~]# pvcreate /dev/sdb1 Physical volume \"/dev/sdb1\" successfully created [root@localhost ~]# pvcreate /dev/sdc1 Physical volume \"/dev/sdc1\" successfully created [root@localhost ~]# # pvdisplay 显示详细PV信息 \"/dev/sdb1\" is a new physical volume of \"20.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdb1 VG Name PV Size 20.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID LcLg0U-gofo-MapY-eilj-OpkQ-TYMO-nSDfh3 \"/dev/sdc1\" is a new physical volume of \"20.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdc1 VG Name PV Size 20.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID r9Dp6u-S1hL-EmXb-6M7c-yefR-Bzts-B0Rdtd 4. 将PV加入到卷组中(VG)[root@localhost ~]# vgcreate vg-group /dev/sdb1 /dev/sdc1 Volume group \"vg-group\" successfully created 5. 创建一个10G大小的lvm[root@localhost ~]# vgs VG #PV #LV #SN Attr VSize VFree centos 1 2 0 wz--n- 19.51g 0 vg-group 2 0 0 wz--n- 39.99g 39.99g [root@localhost ~]# lvcreate -L 10G -n data vg-group Logical volume \"data\" created [root@localhost ~]# lvs LV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert root centos -wi-ao---- 17.51g swap centos -wi-ao---- 2.00g data vg-group -wi-a----- 10.00g","text":"1. 查看磁盘：[root@localhost ~]# fdisk -l ...... 磁盘 /dev/sdb：21.5 GB, 21474836480 字节，41943040 个扇区 Units = 扇区 of 1 * 512 = 512 bytes 扇区大小(逻辑/物理)：512 字节 / 512 字节 I/O 大小(最小/最佳)：512 字节 / 512 字节 磁盘 /dev/sdc：21.5 GB, 21474836480 字节，41943040 个扇区 Units = 扇区 of 1 * 512 = 512 bytes 扇区大小(逻辑/物理)：512 字节 / 512 字节 I/O 大小(最小/最佳)：512 字节 / 512 字节 ...... 2. 对两块磁盘进行lvm格式分区fdisk /dev/sdb/ --&gt; n --&gt; p --&gt; 分区号(默认) --&gt; 起始扇区(默认) --&gt; last扇区(默认) # 修改分区格式 t --&gt; 8e Linux LVM(分区格式为lvm) --&gt; w (保存) # 退出后对磁盘/dev/sdc 执行相同操作即可 3. 创建物理卷(PV)[root@localhost ~]# pvcreate /dev/sdb1 Physical volume \"/dev/sdb1\" successfully created [root@localhost ~]# pvcreate /dev/sdc1 Physical volume \"/dev/sdc1\" successfully created [root@localhost ~]# # pvdisplay 显示详细PV信息 \"/dev/sdb1\" is a new physical volume of \"20.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdb1 VG Name PV Size 20.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID LcLg0U-gofo-MapY-eilj-OpkQ-TYMO-nSDfh3 \"/dev/sdc1\" is a new physical volume of \"20.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdc1 VG Name PV Size 20.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID r9Dp6u-S1hL-EmXb-6M7c-yefR-Bzts-B0Rdtd 4. 将PV加入到卷组中(VG)[root@localhost ~]# vgcreate vg-group /dev/sdb1 /dev/sdc1 Volume group \"vg-group\" successfully created 5. 创建一个10G大小的lvm[root@localhost ~]# vgs VG #PV #LV #SN Attr VSize VFree centos 1 2 0 wz--n- 19.51g 0 vg-group 2 0 0 wz--n- 39.99g 39.99g [root@localhost ~]# lvcreate -L 10G -n data vg-group Logical volume \"data\" created [root@localhost ~]# lvs LV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert root centos -wi-ao---- 17.51g swap centos -wi-ao---- 2.00g data vg-group -wi-a----- 10.00g 6. 格式化这个lvm成xfs文件系统[root@localhost ~]# mkfs.xfs /dev/vg-group/data # 注意这个路径是/dev/卷组名/lvm名 meta-data=/dev/vg-group/data isize=256 agcount=4, agsize=655360 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 data = bsize=4096 blocks=2621440, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=0 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@localhost ~]# lvs LV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert root centos -wi-ao---- 17.51g swap centos -wi-ao---- 2.00g data vg-group -wi-a----- 10.00g [root@localhost ~]# 7. 逻辑卷扩容使用 lvextend 命令进行逻辑卷扩容。我把所有剩余空间都分配给了data [root@localhost ~]# lvextend -l +100%FREE /dev/vg-group/data Extending logical volume data to 39.99 GiB Logical volume data successfully resized [root@localhost ~]# lvs LV VG Attr LSize Pool Origin Data% Move Log Cpy%Sync Convert root centos -wi-ao---- 17.51g swap centos -wi-ao---- 2.00g data vg-group -wi-a----- 39.99g [root@localhost ~]# mkdir /data [root@localhost ~]# mount /dev/vg-group/data /data/ [root@localhost ~]# df -hT | grep \"data\" # 可以看到这时候的data 只有10G，可是我们的lvm已经有40G /dev/mapper/vg--group-data xfs 10G 33M 10G 1% /data 8. 使用xfs_growfs命令在线调整xfs格式文件系统大小[root@localhost ~]# xfs_growfs /dev/vg-group/data meta-data=/dev/mapper/vg--group-data isize=256 agcount=4, agsize=655360 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 data = bsize=4096 blocks=2621440, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=0 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 2621440 to 10483712 [root@localhost ~]# df -hT | grep \"data\" /dev/mapper/vg--group-data xfs 40G 33M 40G 1% /data 9 .加入现有的卷组中：(1)# 查看现有卷组 [root@localhost ~]# vgs VG #PV #LV #SN Attr VSize VFree centos 1 2 0 wz--n- 19.51g 0 vg-group 2 1 0 wz--n- 39.99g 0 (2)#查看新加磁盘,还是先分区，然后更改lvm格式(略) [root@localhost ~]# fdisk -l | grep sdd 磁盘 /dev/sdd：21.5 GB, 21474836480 字节，41943040 个扇区 /dev/sdd1 2048 41943039 20970496 8e Linux LVM (3)#使用 vgextend 命令把/dev/sdd1加入到centos [root@localhost ~]# vgextend centos /dev/sdd1 Volume group \"centos\" successfully extended [root@localhost ~]# vgs VG #PV #LV #SN Attr VSize VFree centos 2 2 0 wz--n- 39.50g 20.00g vg-group 2 1 0 wz--n- 39.99g 0 (4)使用 lvextend 命令进行逻辑卷root扩容。我把所有剩余空间都分配给了root [root@localhost ~]# lvextend -l +100%FREE /dev/centos/root Extending logical volume root to 37.50 GiB Logical volume root successfully resized [root@localhost ~]# xfs_growfs /dev/centos/root meta-data=/dev/mapper/centos-root isize=256 agcount=4, agsize=1147392 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 data = bsize=4096 blocks=4589568, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=0 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 4589568 to 9831424 [root@localhost ~]# # 此时我们的根分区就得到了扩容啦； [root@localhost ~]# df -hT | grep \"root\" /dev/mapper/centos-root xfs 38G 929M 37G 3% / 注意：pvs -- vgs -- lvs","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"lvm","slug":"lvm","permalink":"https://blog.sctux.cc/tags/lvm/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"MySQL报错ERROR 1030 (HY000): 解决过程","slug":"mysql-e6-8a-a5-e9-94-99error-1030-hy000-e8-a7-a3-e5-86-b3-e8-bf-87-e7-a8-8b","date":"2016-03-19T10:45:58.000Z","updated":"2025-09-01T01:59:08.941Z","comments":true,"path":"2016/03/19/mysql-e6-8a-a5-e9-94-99error-1030-hy000-e8-a7-a3-e5-86-b3-e8-bf-87-e7-a8-8b/","permalink":"https://blog.sctux.cc/2016/03/19/mysql-e6-8a-a5-e9-94-99error-1030-hy000-e8-a7-a3-e5-86-b3-e8-bf-87-e7-a8-8b/","excerpt":"问题： 今天开发同事在执行jenkins自动化构建时，由于执行一个mysql更新的操作导致构建失败，报错如下： 这个文件主要是开发用于执行构建部署的时候更新数据库的sql文件，里面有大量的插入语句； 登录到测试服务器将jenkins中的语句在命令行执行了一遍还是报同样的错误 看到这个报错貌似与存储引擎有关系，于是登录到mysql中执行了以下操作 解决思路 1.看了下具体错误代码 在tmp下没有这个文件？但是这个文件时临时生成的，本来就应该没有啊！ 那就是创建的时候出错了?! 2.查看了一下磁盘空间，我滴个乖乖，原来第一块磁盘满了 好吧，清理了一些文件 再次执行构建，顺利完成！","text":"问题： 今天开发同事在执行jenkins自动化构建时，由于执行一个mysql更新的操作导致构建失败，报错如下： 这个文件主要是开发用于执行构建部署的时候更新数据库的sql文件，里面有大量的插入语句； 登录到测试服务器将jenkins中的语句在命令行执行了一遍还是报同样的错误 看到这个报错貌似与存储引擎有关系，于是登录到mysql中执行了以下操作 解决思路 1.看了下具体错误代码 在tmp下没有这个文件？但是这个文件时临时生成的，本来就应该没有啊！ 那就是创建的时候出错了?! 2.查看了一下磁盘空间，我滴个乖乖，原来第一块磁盘满了 好吧，清理了一些文件 再次执行构建，顺利完成！","categories":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}],"tags":[{"name":"mysql error","slug":"mysql-error","permalink":"https://blog.sctux.cc/tags/mysql-error/"}],"keywords":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}]},{"title":"我的51CTO博客文章链接汇总","slug":"e6-88-91-e7-9a-8451cto-e5-8d-9a-e5-ae-a2-e6-96-87-e7-ab-a0-e9-93-be-e6-8e-a5-e6-b1-87-e6-80-bb","date":"2016-01-07T13:13:29.000Z","updated":"2025-09-01T01:59:08.870Z","comments":false,"path":"2016/01/07/e6-88-91-e7-9a-8451cto-e5-8d-9a-e5-ae-a2-e6-96-87-e7-ab-a0-e9-93-be-e6-8e-a5-e6-b1-87-e6-80-bb/","permalink":"https://blog.sctux.cc/2016/01/07/e6-88-91-e7-9a-8451cto-e5-8d-9a-e5-ae-a2-e6-96-87-e7-ab-a0-e9-93-be-e6-8e-a5-e6-b1-87-e6-80-bb/","excerpt":"将之前在51CTO的所有博客整理了一番, 点击文章名称就可以跳到51cto读阅啦^o^ 集群/高可用/负载均衡LVS_NAT实现过程… LVS_DR实现过程… Keepalived基础知识 Linux HA集群之DRBD详解 基于keepalived的Haproxy高可用配置 分布式分布式缓存varnish简介 分布式文件系统MogileFS简介 数据库Mysql知识总结（一） Mysql知识总结（二） Mysql知识总结（三） MariaDB/Mysql之主从架构的复制原理及主从/双主配置详解(一) MariaDB/Mysql之主从架构的复制原理及主从/双主配置详解(二) MariaDB之备份恢复准则（三） MariaDB之基于mysqldump与lvm-snapshot备份恢复Databases or Tables（一） MariaDB之基于Percona Xtrabackup备份大数据库【完整备份与增量备份】（二） Linux 剖析Linux系统-小倒腾之Linux DIY定制裁剪(附带简单网络功能)o_o(一) Linux系统-小倒腾之Linux DIY定制裁剪(New kernel+Busybox)o_o（二） Linux系统-小倒腾之Linux DIY定制裁剪(定制Linux+SSH/Nginx)o_o（三） 文件同步","text":"将之前在51CTO的所有博客整理了一番, 点击文章名称就可以跳到51cto读阅啦^o^ 集群/高可用/负载均衡LVS_NAT实现过程… LVS_DR实现过程… Keepalived基础知识 Linux HA集群之DRBD详解 基于keepalived的Haproxy高可用配置 分布式分布式缓存varnish简介 分布式文件系统MogileFS简介 数据库Mysql知识总结（一） Mysql知识总结（二） Mysql知识总结（三） MariaDB/Mysql之主从架构的复制原理及主从/双主配置详解(一) MariaDB/Mysql之主从架构的复制原理及主从/双主配置详解(二) MariaDB之备份恢复准则（三） MariaDB之基于mysqldump与lvm-snapshot备份恢复Databases or Tables（一） MariaDB之基于Percona Xtrabackup备份大数据库【完整备份与增量备份】（二） Linux 剖析Linux系统-小倒腾之Linux DIY定制裁剪(附带简单网络功能)o_o(一) Linux系统-小倒腾之Linux DIY定制裁剪(New kernel+Busybox)o_o（二） Linux系统-小倒腾之Linux DIY定制裁剪(定制Linux+SSH/Nginx)o_o（三） 文件同步LinuxTools—Rsync—原理及其应用(一) LinuxTools—Rsync—原理及其应用(二) 安全管理Linux安全管理-Iptables-NAT技术应用 Linux安全管理-Iptables原理及其应用 Linux ServiceLinux网络服务-Web Service之【HTTP协议简介】(一) Linux网络服务-Web Service之【Apache-Prefork、Worker和Event三种工作模式分析】(二) Linux网络服务-Web Service之【apache的功能、安装、配置文件介绍以及实验实例】(三) Linux网络服务-LAMP之基于NFS+Fastcgi的LAMP搭建 Linux网络服务-LAMP之Php基于Apache的模块实现 Linux网络服务之DNS服务器介绍及配置实例详解 系统管理Linux系统基础-管理之加密、解密、Openssl基本应用及CA实现过程 Linux系统基础-管理之系统启动过程及系统初始化学习总结 Linux系统基础-管理之软件包管理【附http源码安装实例】 Linux系统基础-管理之find命令学习总结 Shell编程入门进阶之Grep命令及正则表达式知识梳理 Linux系统基础-管理之用户、权限管理 Shell编程入门进阶之bash配置文件介绍 Shell编程入门进阶之Bash Shell特性 Linux命令的格式、常用命令汇总以及一些系统基本概念 Linux系统基础-管理之如何在终端上获取Linux命令帮助. Linux系统基础-管理之终端,伪终端概念详解之tty,pty等 Linux系统基础-管理之Linux 目录配置标准：FHS：FileSystem Hierarchy Standard 杂谈假如Linux版本都如女人","categories":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}],"tags":[],"keywords":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}]},{"title":"Zabbix微信报警","slug":"zabbix-e5-be-ae-e4-bf-a1-e6-8a-a5-e8-ad-a6","date":"2015-11-21T07:25:35.000Z","updated":"2025-09-01T01:59:08.921Z","comments":true,"path":"2015/11/21/zabbix-e5-be-ae-e4-bf-a1-e6-8a-a5-e8-ad-a6/","permalink":"https://blog.sctux.cc/2015/11/21/zabbix-e5-be-ae-e4-bf-a1-e6-8a-a5-e8-ad-a6/","excerpt":"一、注册微信公众号首先申请微信公众平台https://mp.weixin.qq.com/一个人最多申请5个公众号，申请完之后就可以根据腾讯的提示使用微信公众号了，然后用你自己的微信扫描关注微信号。 通过扫描过后就可以看到已经有一个用户关注啦；于是我们这里需要查看用户的ID 点击 “用户管理”，然后点击一下用户的头像，这时候我们可以在浏览器的地址栏就可以看到一个这个，其中红色部分就是用户的微信ID啦，先记下这个ID https://mp.weixin.qq.com/cgi-bin/singlesendpage?t=message/send&amp;action=index&amp;tofakeid=250995555&amp;token=94167798&amp;lang=zh_CN &nbsp; 二、下载并配置微信公众平台私有接口1.获取代码 git clone https://github.com/lealife/WeiXin-Private-API 2.修改zabbix配置文件 [root@Control-machine ~]# cp -r WeiXin-Private-API /usr/lib/zabbix/alertscripts/[root@Control-machine ~]# cd /usr/lib/zabbix/alertscripts/[root@Control-machine alertscripts]# chown zabbix:zabbix WeiXin-Private-API[root@Control-machine alertscripts]# #修改test.php文件[root@Control-machine alertscripts]# vim WeiXin-Private-API/test.php&lt;?phprequire “config.php”;require “include/WeiXin.php”;$weiXin = new WeiXin($G_CONFIG[‘weiXin’]);$testFakeId = “$argv[1]“;$msg=”$argv[3]“; print_r($weiXin-&gt;send($testFakeId, “$msg”)); 3.修改config.php文件 [root@Control-machine alertscripts]# vim WeiXin-Private-API/config.php&lt;?php// 全局配置$G_ROOT = dirname(__FILE__);$G_CONFIG[“weiXin”] = array( ‘account’ =&gt; ‘你的微信公众登录号码’, ‘password’ =&gt; ‘你的微信公众登录密码’, ‘cookiePath’ =&gt; $G_ROOT. ‘/cache/cookie’, // cookie缓存文件路径 ‘webTokenPath’ =&gt; $G_ROOT. ‘/cache/webToken’, // webToken缓存文件路径);","text":"一、注册微信公众号首先申请微信公众平台https://mp.weixin.qq.com/一个人最多申请5个公众号，申请完之后就可以根据腾讯的提示使用微信公众号了，然后用你自己的微信扫描关注微信号。 通过扫描过后就可以看到已经有一个用户关注啦；于是我们这里需要查看用户的ID 点击 “用户管理”，然后点击一下用户的头像，这时候我们可以在浏览器的地址栏就可以看到一个这个，其中红色部分就是用户的微信ID啦，先记下这个ID https://mp.weixin.qq.com/cgi-bin/singlesendpage?t=message/send&amp;action=index&amp;tofakeid=250995555&amp;token=94167798&amp;lang=zh_CN &nbsp; 二、下载并配置微信公众平台私有接口1.获取代码 git clone https://github.com/lealife/WeiXin-Private-API 2.修改zabbix配置文件 [root@Control-machine ~]# cp -r WeiXin-Private-API /usr/lib/zabbix/alertscripts/[root@Control-machine ~]# cd /usr/lib/zabbix/alertscripts/[root@Control-machine alertscripts]# chown zabbix:zabbix WeiXin-Private-API[root@Control-machine alertscripts]# #修改test.php文件[root@Control-machine alertscripts]# vim WeiXin-Private-API/test.php&lt;?phprequire “config.php”;require “include/WeiXin.php”;$weiXin = new WeiXin($G_CONFIG[‘weiXin’]);$testFakeId = “$argv[1]“;$msg=”$argv[3]“; print_r($weiXin-&gt;send($testFakeId, “$msg”)); 3.修改config.php文件 [root@Control-machine alertscripts]# vim WeiXin-Private-API/config.php&lt;?php// 全局配置$G_ROOT = dirname(__FILE__);$G_CONFIG[“weiXin”] = array( ‘account’ =&gt; ‘你的微信公众登录号码’, ‘password’ =&gt; ‘你的微信公众登录密码’, ‘cookiePath’ =&gt; $G_ROOT. ‘/cache/cookie’, // cookie缓存文件路径 ‘webTokenPath’ =&gt; $G_ROOT. ‘/cache/webToken’, // webToken缓存文件路径); 4.修改test.php文件 [root@Control-machine alertscripts]# vim WeiXin-Private-API/test.php&lt;?phprequire “config.php”;require “include/WeiXin.php”;$weiXin = new WeiXin($G_CONFIG[‘weiXin’]);$testFakeId = “$argv[1]“;$msg=”$argv[3]“; print_r($weiXin-&gt;send($testFakeId, “$msg”)); 注意这里$msg=”$argv[3]“表示zabbix传入的第三个参数，因为在zabbix报警时会传入三个参数： 一是微信好友ID，二是报警信息的主题，三是报警信息的具体内容，这里跳过了报警信息主题，直接发送报警信息内容 5.创建微信报警脚本 [root@Control-machine alertscripts]# vim weixin/usr/bin/php /usr/lib/zabbix/alertscripts/WeiXin-Private-API/test.php “$1” “$2” “$3”[root@Control-machine alertscripts]# chown zabbix:zabbix weixin[root@Control-machine alertscripts]# chmod +x weixin 6.测试报警 [root@Control-machine alertscripts]# /usr/bin/php /usr/lib/zabbix/alertscripts/WeiXin-Private-API/test.php 250995555 “” “Test”PHP Notice: Undefined index: HTTP_USER_AGENT in /usr/lib/zabbix/alertscripts/WeiXin-Private-API/include/LeaWeiXinClient.php on line 33PHP Notice: curl_setopt(): CURLOPT_SSL_VERIFYHOST no longer accepts the value 1, value 2 will be used instead in /usr/lib/zabbix/alertscripts/WeiXin-Private-API/include/LeaWeiXinClient.php on line 32PHP Notice: Undefined index: HTTP_USER_AGENT in /usr/lib/zabbix/alertscripts/WeiXin-Private-API/include/LeaWeiXinClient.php on line 33stdClass Object( [base_resp] =&gt; stdClass Object ( [ret] =&gt; 0 [err_msg] =&gt; ok #说明已经发送出去啦 ) )[root@Control-machine alertscripts]# #以上PHP的提示信息可以忽略 查看结果 三、配置zabbix1.添加报警媒介 2.添加用户报警媒介，这里使用的是administrator 3.添加报警动作 信息如下 修改操作条件，保持默认的也可以； 保存设置 四、测试我这里将监控mysql主从的脚本手动改一下，让其zabbix检测到并报警 手机微信查看报警信息 但是需要注意的是，如果公众号向我的微信发送消息超过48个小时我没有回复，那么公众号将不会主动发送消息。然后我们在微信，也就是说，我们在收到报警通知后在48个小时之内可以简单的回复一个字符，或者一段话即可 参考文章链接： http://blog.chinaunix.net/uid-30236771-id-5037842.html","categories":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"},{"name":"自动化运维","slug":"Monitor/自动化运维","permalink":"https://blog.sctux.cc/categories/Monitor/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[],"keywords":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"},{"name":"自动化运维","slug":"Monitor/自动化运维","permalink":"https://blog.sctux.cc/categories/Monitor/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Saltstack利用Returners程序保存执行结果到mysql","slug":"saltstack-e5-88-a9-e7-94-a8returners-e7-a8-8b-e5-ba-8f-e4-bf-9d-e5-ad-98-e6-89-a7-e8-a1-8c-e7-bb-93-e6-9e-9c-e5-88-b0mysql","date":"2015-11-19T13:03:02.000Z","updated":"2025-09-01T01:59:08.954Z","comments":true,"path":"2015/11/19/saltstack-e5-88-a9-e7-94-a8returners-e7-a8-8b-e5-ba-8f-e4-bf-9d-e5-ad-98-e6-89-a7-e8-a1-8c-e7-bb-93-e6-9e-9c-e5-88-b0mysql/","permalink":"https://blog.sctux.cc/2015/11/19/saltstack-e5-88-a9-e7-94-a8returners-e7-a8-8b-e5-ba-8f-e4-bf-9d-e5-ad-98-e6-89-a7-e8-a1-8c-e7-bb-93-e6-9e-9c-e5-88-b0mysql/","excerpt":"在我们执行saltstack的时候，minion端会返回一大堆的执行结果显示在master端，那如何将每一次slat执行的结果这些结果保存起来便于日后查询，这里就用到了saltstack的返回程序Returners，可以保存在Redis，Mongodb，MySQL等这些程序当中； 注意这的返回并不是将执行结果返回给master，master再写入到MySQL或者Redis中，而是salt-minion端直接向MySQL或者Redis中写， 下面是操作步骤： 1.安装软件包 master端：yum install -y MySQL-python mysql mysql-serverminion端：yum install -y MySQL-python 2.建立数据库，创建salt所需要的数据库及表结构 CREATE DATABASE `salt` DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; USE `salt`; ---- Table structure for table `jids`DROP TABLE IF EXISTS `jids`;CREATE TABLE `jids` ( `jid` varchar(255) NOT NULL, `load` mediumtext NOT NULL, UNIQUE KEY `jid` (`jid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; ---- Table structure for table `salt_returns`DROP TABLE IF EXISTS `salt_returns`;CREATE TABLE `salt_returns` ( `fun` varchar(50) NOT NULL, `jid` varchar(255) NOT NULL, `return` mediumtext NOT NULL, `id` varchar(255) NOT NULL, `success` varchar(10) NOT NULL, `full_ret` mediumtext NOT NULL, `alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP, KEY `id` (`id`), KEY `jid` (`jid`), KEY `fun` (`fun`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; ---- Table structure for table `salt_events`","text":"在我们执行saltstack的时候，minion端会返回一大堆的执行结果显示在master端，那如何将每一次slat执行的结果这些结果保存起来便于日后查询，这里就用到了saltstack的返回程序Returners，可以保存在Redis，Mongodb，MySQL等这些程序当中； 注意这的返回并不是将执行结果返回给master，master再写入到MySQL或者Redis中，而是salt-minion端直接向MySQL或者Redis中写， 下面是操作步骤： 1.安装软件包 master端：yum install -y MySQL-python mysql mysql-serverminion端：yum install -y MySQL-python 2.建立数据库，创建salt所需要的数据库及表结构 CREATE DATABASE `salt` DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; USE `salt`; ---- Table structure for table `jids`DROP TABLE IF EXISTS `jids`;CREATE TABLE `jids` ( `jid` varchar(255) NOT NULL, `load` mediumtext NOT NULL, UNIQUE KEY `jid` (`jid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; ---- Table structure for table `salt_returns`DROP TABLE IF EXISTS `salt_returns`;CREATE TABLE `salt_returns` ( `fun` varchar(50) NOT NULL, `jid` varchar(255) NOT NULL, `return` mediumtext NOT NULL, `id` varchar(255) NOT NULL, `success` varchar(10) NOT NULL, `full_ret` mediumtext NOT NULL, `alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP, KEY `id` (`id`), KEY `jid` (`jid`), KEY `fun` (`fun`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; ---- Table structure for table `salt_events`DROP TABLE IF EXISTS `salt_events`;CREATE TABLE `salt_events` (`id` BIGINT NOT NULL AUTO_INCREMENT,`tag` varchar(255) NOT NULL,`data` varchar(1024) NOT NULL,`alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,PRIMARY KEY (`id`),KEY `tag` (`tag`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; #检查一下是否创建成功：mysql&gt; use saltDatabase changedmysql&gt; show tables;+—————-+| Tables_in_salt |+—————-+| jids || salt_events || salt_returns |+—————-+3 rows in set (0.00 sec) 3.授权数据库，修改master配置 mysql &gt; grant all on salt.* to salt@’192.168.2.0/255.255.255.0’ identified by ‘salt’;mysql &gt; flush privileges; 在master端或者每个minion端都写如以下配置内容。当然如果写在master端是比较简单的做法，因为只需要写一次就行啦。我这里写在了master端。 [root@saltstack-node1 ~]# vim /etc/salt/masterreturn: mysqlmysql.host: ‘192.168.2.21’#mysql 数据库服务器地址mysql.user: ‘salt’mysql.pass: ‘salt’mysql.db: ‘salt’mysql.port: 3306 4.执行语句测试：","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"ELK+Kafka 企业日志收集平台(二)","slug":"elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-ba-8c","date":"2015-11-14T08:49:57.000Z","updated":"2025-09-01T01:59:08.879Z","comments":true,"path":"2015/11/14/elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-ba-8c/","permalink":"https://blog.sctux.cc/2015/11/14/elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-ba-8c/","excerpt":"上篇博文主要总结了一下elk、基于kafka的zookeeper集群搭建，以及系统日志通过zookeeper集群达到我们集群的整个过程。下面我们接着下面这个未完成的几个主题 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用; &nbsp; Kibana的部署;Kibana的作用，想必大家都知道了就是一个展示工具，报表内容非常的丰富； 下面我们在两台es上面搭建两套kibana 1.获取kibana软件包 [root@es1 ~]# wget https://download.elastic.co/kibana/kibana/kibana-4.1.2-linux-x64.tar.gz[root@es1 ~]# tar -xf kibana-4.2.0-linux-x64.tar.gz -C /usr/local/ 2.修改配置文件 [root@es1 ~]# cd /usr/local/[root@es1 local]# ln -sv kibana-4.1.2-linux-x64 kibana`kibana’ -&gt; `kibana-4.2.0-linux-x64’[root@es1 local]# cd kibana [root@es1 kibana]# vim config/kibana.ymlserver.port: 5601 #默认端口可以修改的server.host: “0.0.0.0” #kibana监听的ipelasticsearch.url: “http://localhost:9200“ #由于es在本地主机上面，所以这个选项打开注释即可 3.提供kibana服务管理脚本，我这里写了个相对简单的脚本 [root@es1 config]# cat /etc/init.d/kibana#!/bin/bash#chkconfig: 2345 55 24#description: kibana service manager KIBBIN=’/usr/local/kibana/bin/kibana’LOCK=’/usr/local/kibana/locks’","text":"上篇博文主要总结了一下elk、基于kafka的zookeeper集群搭建，以及系统日志通过zookeeper集群达到我们集群的整个过程。下面我们接着下面这个未完成的几个主题 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用; &nbsp; Kibana的部署;Kibana的作用，想必大家都知道了就是一个展示工具，报表内容非常的丰富； 下面我们在两台es上面搭建两套kibana 1.获取kibana软件包 [root@es1 ~]# wget https://download.elastic.co/kibana/kibana/kibana-4.1.2-linux-x64.tar.gz[root@es1 ~]# tar -xf kibana-4.2.0-linux-x64.tar.gz -C /usr/local/ 2.修改配置文件 [root@es1 ~]# cd /usr/local/[root@es1 local]# ln -sv kibana-4.1.2-linux-x64 kibana`kibana’ -&gt; `kibana-4.2.0-linux-x64’[root@es1 local]# cd kibana [root@es1 kibana]# vim config/kibana.ymlserver.port: 5601 #默认端口可以修改的server.host: “0.0.0.0” #kibana监听的ipelasticsearch.url: “http://localhost:9200“ #由于es在本地主机上面，所以这个选项打开注释即可 3.提供kibana服务管理脚本，我这里写了个相对简单的脚本 [root@es1 config]# cat /etc/init.d/kibana#!/bin/bash#chkconfig: 2345 55 24#description: kibana service manager KIBBIN=’/usr/local/kibana/bin/kibana’LOCK=’/usr/local/kibana/locks’ START() { if [ -f $LOCK ];then echo -e “kibana is already \\033[32mrunning\\033[0m, do nothing.” else echo -e “Start kibana service.\\033[32mdone\\033[m” cd /usr/local/kibana/bin nohup ./kibana &amp; &gt;/dev/null touch $LOCK fi} STOP() { if [ ! -f $LOCK ];then echo -e “kibana is already stop, do nothing.” else echo -e “Stop kibana serivce \\033[32mdone\\033[m” rm -rf $LOCK ps -ef | grep kibana | grep -v “grep” | awk ‘{print $2}’ | xargs kill -s 9 &gt;/dev/null fi} STATUS() { Port=$(netstat -tunl | grep “:5602”) if [ “$Port” != “” ] &amp;&amp; [ -f $LOCK ];then echo -e “kibana is: \\033[32mrunning\\033[0m…” else echo -e “kibana is: \\033[31mstopped\\033[0m…” fi} case “$1” in start) START ;; stop) STOP ;; status) STATUS ;; restart) STOP sleep 2 START ;; *) echo “Usage: /etc/init.d/kibana (|start|stop|status|restart)” ;;esac 4.启动kibana服务 [root@es1 config]# chkconfig –add kibana[root@es1 config]# service kibana startStart kibana service.done[root@es1 config]# 5.服务检查 [root@es1 config]# ss -tunl | grep “5601”tcp LISTEN 0 511 *:5601 :[root@es1 config]# ok，此时我直接访问es1这台主机的5601端口 ok，能成功的访问5601端口，那我把es1这台的配置放到es2上面去然后启动，效果跟访问es1一样 Nginx负载均衡kibana的请求1.在nginx-proxy上面yum安装nginx yum install -y nignx 2.编写配置文件es.conf [root@saltstack-node1 conf.d]# pwd/etc/nginx/conf.d[root@saltstack-node1 conf.d]# cat es.confupstream es { server 192.168.2.18:5601 max_fails=3 fail_timeout=30s; server 192.168.2.19:5601 max_fails=3 fail_timeout=30s;} server { listen 80; server_name localhost; location / { proxy_pass http://es/; index index.html index.htm; #auth auth_basic \"ELK Private\"; auth\\_basic\\_user_file /etc/nginx/.htpasswd; } } 3.创建认证 [root@saltstack-node1 conf.d]# htpasswd -cm /etc/nginx/.htpasswd elkNew password:Re-type new password:Adding password for user elk-user[root@saltstack-node1 conf.d]# /etc/init.d/nginx restartStopping nginx: [ OK ]Starting nginx: [ OK ][root@saltstack-node1 conf.d]# 4.直接输入认证用户及密码就可访问啦http://192.168.2.21/ Nginx及MySQL慢日志收集首先我们在webserver1上面都分别安装了nginx 及mysql. 1.为了方便nginx日志的统计搜索，这里设置nginx访问日志格式为json (1)修改nginx主配置文件 说明：如果想实现日志的报表展示，最好将业务日志直接以json格式输出，这样可以极大减轻cpu负载，也省得运维需要写负载的filter过滤正则。 [root@webserver1 nginx]# vim nginx.conflog_format json ‘{“@timestamp”:”$time_iso8601”,’ ‘“@version”:”1”,’ ‘“client”:”$remote_addr”,’ ‘“url”:”$uri”,’ ‘“status”:”$status”,’ ‘“domain”:”$host”,’ ‘“host”:”$server_addr”,’ ‘“size”:$body_bytes_sent,’ ‘“responsetime”:$request_time,’ ‘“referer”: “$http_referer”,’ ‘“ua”: “$http_user_agent”‘ ‘}’; access_log /var/log/access_json.log json; (2)收集nginx日志和MySQL日志到消息队列中；这个文件我们是定义在客户端，即生产服务器上面的Logstash文件哦. 注意：这里刚搭建完毕，没有什么数据，为了展示效果，我这里导入了线上的nginx和MySQL慢日志 input { file { #从nginx日志读入 type =&gt; “nginx-access” path =&gt; “/var/log/nginx/access.log” start_position =&gt; “beginning” codec =&gt; “json” #这里指定 codec格式为json } file { #从MySQL慢日志读入 type =&gt; “slow-mysql” path =&gt; “/var/log/mysql/slow-mysql.log” start_position =&gt; “beginning” codec =&gt; multiline { #这里用到了logstash的插件功能，将本来属于一行的多行日志条目整合在一起，让他属于一条 pattern =&gt; “^# User@Host” #用到了正则去匹配 negate =&gt; true what =&gt; “previous” } }} output {# stdout { codec=&gt; rubydebug } if [type] == “nginx-access” { #通过判断input中定义的type，来让它在kafka集群中生成的主题名称 kafka { #输出到kafka集群 bootstrap_servers =&gt; “192.168.2.22:9092,192.168.2.23:9092,192.168.2.24:9092” #生产者们 topic_id =&gt; “nginx-access” #主题名称 compression_type =&gt; “snappy” #压缩类型 } } if [type] == “slow-mysql” { kafka { bootstrap_servers =&gt; “192.168.2.22:9092,192.168.2.23:9092,192.168.2.24:9092” topic_id =&gt; “slow-mysql” compression_type =&gt; “snappy” } }} (3)Logstash 从kafka集群中读取日志存储到es中，这里的定义logstash文件是在三台kafka服务器上面的哦，并且要保持一致，你可以在一台上面修改测试好之后，拷贝至另外两台即可。 input { kafka { zk_connect =&gt; “192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181” type =&gt; “nginx-access” topic_id =&gt; “nginx-access” codec =&gt; plain reset_beginning =&gt; false consumer_threads =&gt; 5 decorate_events =&gt; true } kafka { zk_connect =&gt; “192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181” type =&gt; “slow-mysql” topic_id =&gt; “slow-mysql” codec =&gt; plain reset_beginning =&gt; false consumer_threads =&gt; 5 decorate_events =&gt; true }} output {# stdout { codec=&gt; rubydebug } if [type] == “nginx-access” { elasticsearch { hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”] index =&gt; “nginx-access-%{+YYYY-MM}” } } if [type] == “slow-mysql” { elasticsearch { hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”] index =&gt; “slow-mysql-%{+YYYY-MM}” } }} 通过上图可以看到，nginx日志以及MySQL慢日志已经成功抵达es集群 然后我们在kibana上面创建索引就可以啦 (4)创建nginx-access 日志索引 此时就可以看到索引啦 (5)创建MySQL慢日志索引 p MySQL的索引也出来啦 Kibana报表展示 kibana报表功能非常的强大，也就是可视化；可以制作出下面不同类型的图形 下面就是我简单的一些图形展示 由于篇幅问题，可以看官方介绍。 参考： https://github.com/liquanzhou/ops_doc/tree/master/Service/kafka http://www.lujinhong.com/kafka%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97.html http://www.it165.net/admin/html/201405/3192.html http://blog.csdn.net/lizhitao/article/details/39499283 https://taoistwar.gitbooks.io/spark-operationand-maintenance-management/content/spark_relate_software/zookeeper_install.html","categories":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"},{"name":"Other","slug":"Monitor/Other","permalink":"https://blog.sctux.cc/categories/Monitor/Other/"},{"name":"自动化运维","slug":"Monitor/Other/自动化运维","permalink":"https://blog.sctux.cc/categories/Monitor/Other/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://blog.sctux.cc/tags/ELK/"},{"name":"kafka","slug":"kafka","permalink":"https://blog.sctux.cc/tags/kafka/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://blog.sctux.cc/tags/zookeeper/"}],"keywords":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"},{"name":"Other","slug":"Monitor/Other","permalink":"https://blog.sctux.cc/categories/Monitor/Other/"},{"name":"自动化运维","slug":"Monitor/Other/自动化运维","permalink":"https://blog.sctux.cc/categories/Monitor/Other/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"ELK+Kafka 企业日志收集平台(一)","slug":"elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-b8-80","date":"2015-11-14T08:48:45.000Z","updated":"2025-09-01T01:59:08.877Z","comments":true,"path":"2015/11/14/elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-b8-80/","permalink":"https://blog.sctux.cc/2015/11/14/elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-b8-80/","excerpt":"背景：最近线上上了ELK，但是只用了一台Redis在中间作为消息队列，以减轻前端es集群的压力，Redis的集群解决方案暂时没有接触过，并且Redis作为消息队列并不是它的强项；所以最近将Redis换成了专业的消息信息发布订阅系统Kafka, Kafka的更多介绍大家可以看这里：传送门&nbsp; ,关于ELK的知识网上有很多的哦，&nbsp;此篇博客主要是总结一下目前线上这个平台的实施步骤，ELK是怎么跟Kafka结合起来的。好吧，动手！ ELK架构拓扑：然而我这里的整个日志收集平台就是这样的拓扑： 1，使用一台Nginx代理访问kibana的请求; 2，两台es组成es集群，并且在两台es上面都安装kibana;（以下对elasticsearch简称es） 3，中间三台服务器就是我的kafka(zookeeper)集群啦; 上面写的消费者/生产者这是kafka(zookeeper)中的概念; 4，最后面的就是一大堆的生产服务器啦，上面使用的是logstash，当然除了logstash也可以使用其他的工具来收集你的应用程序的日志，例如：Flume，Scribe，Rsyslog，Scripts…… 角色： 软件选用： elasticsearch-1.7.3.tar.gz #这里需要说明一下，前几天使用了最新的elasticsearch2.0，java-1.8.0报错，目前未找到原因，故这里使用1.7.3版本Logstash-2.0.0.tar.gzkibana-4.1.2-linux-x64.tar.gz以上软件都可以从官网下载:https://www.elastic.co/downloads java-1.8.0，nginx采用yum安装 部署步骤： 1.ES集群安装配置; 2.Logstash客户端配置(直接写入数据到ES集群，写入系统messages日志); 3.Kafka(zookeeper)集群配置;(Logstash写入数据到Kafka消息系统); 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用; ES集群安装配置;es1.example.com: 1.安装java-1.8.0以及依赖包 yum install -y epel-releaseyum install -y java-1.8.0 git wget lrzsz","text":"背景：最近线上上了ELK，但是只用了一台Redis在中间作为消息队列，以减轻前端es集群的压力，Redis的集群解决方案暂时没有接触过，并且Redis作为消息队列并不是它的强项；所以最近将Redis换成了专业的消息信息发布订阅系统Kafka, Kafka的更多介绍大家可以看这里：传送门&nbsp; ,关于ELK的知识网上有很多的哦，&nbsp;此篇博客主要是总结一下目前线上这个平台的实施步骤，ELK是怎么跟Kafka结合起来的。好吧，动手！ ELK架构拓扑：然而我这里的整个日志收集平台就是这样的拓扑： 1，使用一台Nginx代理访问kibana的请求; 2，两台es组成es集群，并且在两台es上面都安装kibana;（以下对elasticsearch简称es） 3，中间三台服务器就是我的kafka(zookeeper)集群啦; 上面写的消费者/生产者这是kafka(zookeeper)中的概念; 4，最后面的就是一大堆的生产服务器啦，上面使用的是logstash，当然除了logstash也可以使用其他的工具来收集你的应用程序的日志，例如：Flume，Scribe，Rsyslog，Scripts…… 角色： 软件选用： elasticsearch-1.7.3.tar.gz #这里需要说明一下，前几天使用了最新的elasticsearch2.0，java-1.8.0报错，目前未找到原因，故这里使用1.7.3版本Logstash-2.0.0.tar.gzkibana-4.1.2-linux-x64.tar.gz以上软件都可以从官网下载:https://www.elastic.co/downloads java-1.8.0，nginx采用yum安装 部署步骤： 1.ES集群安装配置; 2.Logstash客户端配置(直接写入数据到ES集群，写入系统messages日志); 3.Kafka(zookeeper)集群配置;(Logstash写入数据到Kafka消息系统); 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用; ES集群安装配置;es1.example.com: 1.安装java-1.8.0以及依赖包 yum install -y epel-releaseyum install -y java-1.8.0 git wget lrzsz 2.获取es软件包 wget https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.3.tar.gztar -xf elasticsearch-1.7.3.tar.gz -C /usr/localln -sv /usr/local/elasticsearch-1.7.3 /usr/local/elasticsearch 3.修改配置文件 [root@es1 ~]# vim /usr/local/elasticsearch/config/elasticsearch.yml32 cluster.name: es-cluster #组播的名称地址40 node.name: “es-node1 “ #节点名称，不能和其他节点重复47 node.master: true #节点能否被选举为master51 node.data: true #节点是否存储数据107 index.number_of_shards: 5 #索引分片的个数111 index.number_of_replicas: 1 #分片的副本个数145 path.conf: /usr/local/elasticsearch/config/ #配置文件的路径149 path.data: /data/es/data #数据目录路径159 path.work: /data/es/worker #工作目录路径163 path.logs: /usr/local/elasticsearch/logs/ #日志文件路径167 path.plugins: /data/es/plugins #插件路径184 bootstrap.mlockall: true #内存不向swap交换232 http.enabled: true #启用http 4.创建相关目录 mkdir /data/es/{data,worker,plugins} -p 5.获取es服务管理脚本 ​[root@es1 ~]# git clone https://github.com/elastic/elasticsearch-servicewrapper.git[root@es1 ~]# mv elasticsearch-servicewrapper/service /usr/local/elasticsearch/bin/[root@es1 ~]# /usr/local/elasticsearch/bin/service/elasticsearch installDetected RHEL or Fedora:Installing the Elasticsearch daemon..[root@es1 ~]##这时就会在/etc/init.d/目录下安装上es的管理脚本啦 #修改其配置:[root@es1 ~]#set.default.ES_HOME=/usr/local/elasticsearch #安装路径set.default.ES_HEAP_SIZE=1024 #jvm内存大小，根据实际环境调整即可 6.启动es ，并检查其服务是否正常 [root@es1 ~]# netstat -nlpt | grep -E “9200|”9300tcp 0 0 0.0.0.0:9200 0.0.0.0:* LISTEN 1684/javatcp 0 0 0.0.0.0:9300 0.0.0.0:* LISTEN 1684/java 访问http://192.168.2.18:9200/ 如果出现以下提示信息说明安装配置完成啦， 7.es1节点好啦，我们直接把目录复制到es2 [root@es1 local]# scp -r elasticsearch-1.7.3 192.168.12.19:/usr/local/ [root@es2 local]# ln -sv elasticsearch-1.7.3 elasticsearch[root@es2 local]# elasticsearch/bin/service/elasticsearch install #es2只需要修改node.name即可，其他都与es1相同配置 8.安装es的管理插件 es官方提供一个用于管理es的插件，可清晰直观看到es集群的状态，以及对集群的操作管理，安装方法如下： [root@es1 local]# /usr/local/elasticsearch/bin/plugin -i mobz/elasticsearch-head 安装好之后，访问方式为： http://192.168.2.18:9200/_plugin/head，由于集群中现在暂时没有数据，所以显示为空, &nbsp; &nbsp; &nbsp; 此时，es集群的部署完成。 Logstash客户端安装配置;在webserve1上面安装Logstassh 1.downloads &nbsp;软件包 ，这里注意，Logstash是需要依赖java环境的，所以这里还是需要yum install -y java-1.8.0. [root@webserver1 ~]# wget https://download.elastic.co/logstash/logstash/logstash-2.0.0.tar.gz[root@webserver1 ~]# tar -xf logstash-2.0.0.tar.gz -C /usr/local[root@webserver1 ~]# cd /usr/local/[root@webserver1 local]# ln -sv logstash-2.0.0 logstash[root@webserver1 local]# mkdir logs etc 2.提供logstash管理脚本，其中里面的配置路径可根据实际情况修改 #!/bin/bash#chkconfig: 2345 55 24#description: logstash service manager#auto: Maoqiu GuoFILE=’/usr/local/logstash/etc/*.conf’ #logstash配置文件LOGBIN=’/usr/local/logstash/bin/logstash agent –verbose –config’ #指定logstash配置文件的命令LOCK=’/usr/local/logstash/locks’ #用锁文件配合服务启动与关闭LOGLOG=’–log /usr/local/logstash/logs/stdou.log’ #日志 START() { if [ -f $LOCK ];then echo -e “Logstash is already \\033[32mrunning\\033[0m, do nothing.” else echo -e “Start logstash service.\\033[32mdone\\033[m” nohup ${LOGBIN} ${FILE} ${LOGLOG} &amp; touch $LOCK fi} STOP() { if [ ! -f $LOCK ];then echo -e “Logstash is already stop, do nothing.” else echo -e “Stop logstash serivce \\033[32mdone\\033[m” rm -rf $LOCK ps -ef | grep logstash | grep -v “grep” | awk ‘{print $2}’ | xargs kill -s 9 &gt;/dev/null fi} STATUS() { ps aux | grep logstash | grep -v “grep” &gt;/dev/null if [ -f $LOCK ] &amp;&amp; [ $? -eq 0 ]; then echo -e “Logstash is: \\033[32mrunning\\033[0m…” else echo -e “Logstash is: \\033[31mstopped\\033[0m…” fi} TEST(){ ${LOGBIN} ${FILE} –configtest} case “$1” in start) START ;; stop) STOP ;; status) STATUS ;; restart) STOP sleep 2 START ;; test) TEST ;; *) echo “Usage: /etc/init.d/logstash (test|start|stop|status|restart)” ;;esac 3.Logstash 向es集群写数据 (1)编写一个logstash配置文件 [root@webserver1 etc]# cat logstash.confinput { #数据的输入从标准输入 stdin {}} output { #数据的输出我们指向了es集群 elasticsearch { hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”] ＃es主机的ip及端口 }}[root@webserver1 etc]# (2)检查配置文件是否有语法错 [root@webserver1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf –configtest –verboseConfiguration OK[root@webserver1 etc]# (3)既然配置ok我们手动启动它，然后写点东西看能否写到es ok.上图已经看到logstash已经可以正常的工作啦. ４.下面演示一下如何收集系统日志 将之前的配置文件修改如下所示内容，然后启动logstash服务就可以在web页面中看到messages的日志写入es，并且创建了一条索引 [root@webserver1 etc]# cat logstash.confinput { #这里的输入使用的文件，即日志文件messsages file { path =&gt; “/var/log/messages” ＃这是日志文件的绝对路径 start_position =&gt; “beginning” ＃这个表示从messages的第一行读取，即文件开始处 }} output { ＃输出到es elasticsearch { hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”] index =&gt; “system-messages-%{+YYYY-MM}” ＃这里将按照这个索引格式来创建索引 }}[root@webserver1 etc]# 启动logstash后，我们来看head这个插件的web页面 ok，系统日志我们已经成功的收集，并且已经写入到es集群中，那上面的演示是logstash直接将日志写入到es集群中的，这种场合我觉得如果量不是很大的话直接像上面已将将输出output定义到es集群即可，如果量大的话需要加上消息队列来缓解es集群的压力。前面已经提到了我这边之前使用的是单台redis作为消息队列，但是redis不能作为list类型的集群，也就是redis单点的问题没法解决，所以这里我选用了kafka ;下面就在三台server上面安装kafka集群 Kafka集群安装配置;在搭建kafka集群时，需要提前安装zookeeper集群，当然kafka已经自带zookeeper程序只需要解压并且安装配置就行了 kafka1上面的配置： 1.获取软件包.官网：http://kafka.apache.org [root@kafka1 ~]# wget http://mirror.rise.ph/apache/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz[root@kafka1 ~]# tar -xf kafka_2.11-0.8.2.1.tgz -C /usr/local/[root@kafka1 ~]# cd /usr/local/[root@kafka1 local]# ln -sv kafka_2.11-0.8.2.1 kafka 2.配置zookeeper集群，修改配置文件 [root@kafka1 ~]# vim /usr/local/kafka/config/zookeeper.propertiedataDir=/data/zookeeperclientPort=2181tickTime=2000initLimit=20syncLimit=10server.2=192.168.2.22:2888:3888server.3=192.168.2.23:2888:3888server.4=192.168.2.24:2888:3888 ＃说明：tickTime: 这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。2888端口：表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；3888端口：表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 3.创建zookeeper所需要的目录 [root@kafka1 ~]# mkdir /data/zookeeper 4.在/data/zookeeper目录下创建myid文件，里面的内容为数字，用于标识主机，如果这个文件没有的话，zookeeper是没法启动的哦 [root@kafka1 ~]# echo 2 &gt; /data/zookeeper/myid 以上就是zookeeper集群的配置，下面等我配置好kafka之后直接复制到其他两个节点即可 5.kafka配置 [root@kafka1 ~]# vim /usr/local/kafka/config/server.propertiesbroker.id=2 ＃ 唯一，填数字，本文中分别为2/3/4prot=9092 ＃ 这个broker监听的端口 host.name=192.168.2.22 ＃ 唯一，填服务器IPlog.dir=/data/kafka-logs # 该目录可以不用提前创建，在启动时自己会创建zookeeper.connect=192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181 ＃这个就是zookeeper的ip及端口num.partitions=16 # 需要配置较大 分片影响读写速度log.dirs=/data/kafka-logs # 数据目录也要单独配置磁盘较大的地方log.retention.hours=168 # 时间按需求保留过期时间 避免磁盘满 6.将kafka(zookeeper)的程序目录全部拷贝至其他两个节点 [root@kafka1 ~]# scp -r /usr/local/kafka 192.168.2.23:/usr/local/[root@kafka1 ~]# scp -r /usr/local/kafka 192.168.2.24:/usr/local/ 7.修改两个借点的配置，注意这里除了以下两点不同外，都是相同的配置 （1）zookeeper的配置mkdir /data/zookeeperecho “x” &gt; /data/zookeeper/myid（2）kafka的配置broker.id=2host.name=192.168.2.22 8.修改完毕配置之后我们就可以启动了，这里先要启动zookeeper集群，才能启动kafka 我们按照顺序来，kafka1 –&gt; kafka2 –&gt;kafka3 [root@kafka1 ~]# /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties &amp; #zookeeper启动命令[root@kafka1 ~]# /usr/local/kafka/bin/zookeeper-server-stop.sh #zookeeper停止的命令 注意，如果zookeeper有问题 nohup的日志文件会非常大，把磁盘占满，这个zookeeper服务可以通过自己些服务脚本来管理服务的启动与关闭。 后面两台执行相同操作，在启动过程当中会出现以下报错信息 [2015-11-13 19:18:04,225] WARN Cannot open channel to 3 at election address /192.168.2.23:3888 (org.apache.zookeeper.server.quorum.QuorumCnxManager)java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402) at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)[2015-11-13 19:18:04,232] WARN Cannot open channel to 4 at election address /192.168.2.24:3888 (org.apache.zookeeper.server.quorum.QuorumCnxManager)java.net.ConnectException: Connection refused at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368) at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402) at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840) at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)[2015-11-13 19:18:04,233] INFO Notification time out: 6400 (org.apache.zookeeper.server.quorum.FastLeaderElection) 由于zookeeper集群在启动的时候，每个结点都试图去连接集群中的其它结点，先启动的肯定连不上后面还没启动的，所以上面日志前面部分的异常是可以忽略的。通过后面部分可以看到，集群在选出一个Leader后，最后稳定了。 其他节点也可能会出现类似的情况，属于正常。 9.zookeeper服务检查 [root@kafka1~]# netstat -nlpt | grep -E “2181|2888|3888”tcp 0 0 192.168.2.24:3888 0.0.0.0:* LISTEN 1959/javatcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 1959/java [root@kafka2 ~]# netstat -nlpt | grep -E “2181|2888|3888”tcp 0 0 192.168.2.23:3888 0.0.0.0:* LISTEN 1723/javatcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 1723/java [root@kafka3 ~]# netstat -nlpt | grep -E “2181|2888|3888”tcp 0 0 192.168.2.24:3888 0.0.0.0:* LISTEN 950/javatcp 0 0 0.0.0.0:2181 0.0.0.0:* LISTEN 950/javatcp 0 0 192.168.2.24:2888 0.0.0.0:* LISTEN 950/java #可以看出，如果哪台是Leader,那么它就拥有2888这个端口 ok. &nbsp;这时候zookeeper集群已经启动起来了，下面启动kafka，也是依次按照顺序启动 [root@kafka1 ~]# nohup /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &amp; #kafka启动的命令[root@kafka1 ~]# /usr/local/kafka/bin/kafka-server-stop.sh #kafka停止的命令 注意，跟zookeeper服务一样，如果kafka有问题 nohup的日志文件会非常大,把磁盘占满，这个kafka服务同样可以通过自己些服务脚本来管理服务的启动与关闭。 此时三台上面的zookeeper及kafka都已经启动完毕，来检测以下吧 (1)建立一个主题 [root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 3 –partitions 1 –topic summer#注意：factor大小不能超过broker数 (2)查看有哪些主题已经创建 [root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –list –zookeeper 192.168.2.22:2181 #列出集群中所有的topicsummer #已经创建成功 (3)查看summer这个主题的详情 [root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –describe –zookeeper 192.168.2.22:2181 –topic summerTopic:summer PartitionCount:1 ReplicationFactor:3 Configs: Topic: summer Partition: 0 Leader: 2 Replicas: 2,4,3 Isr: 2,4,3 #主题名称：summer#Partition:只有一个，从0开始#leader ：id为2的broker#Replicas 副本存在于broker id为2,3,4的上面#Isr:活跃状态的broker (4)发送消息，这里使用的是生产者角色 [root@kafka1 ~]# /bin/bash /usr/local/kafka/bin/kafka-console-producer.sh –broker-list 192.168.2.22:9092 –topic summerThis is a messageswelcome to kafka (5)接收消息，这里使用的是消费者角色 [root@kafka2 ~]# /usr/local/kafka/bin/kafka-console-consumer.sh –zookeeper 192.168.2.24:2181 –topic summer –from-beginningThis is a messageswelcome to kafka 如果能够像上面一样能够接收到生产者发过来的消息，那说明基于kafka的zookeeper集群就成功啦。 10，下面我们将webserver1上面的logstash的输出改到kafka上面，将数据写入到kafka中 (1)修改webserver1上面的logstash配置，如下所示：各个参数可以到官网查询. root@webserver1 etc]# cat logstash.confinput { #这里的输入还是定义的是从日志文件输入 file { type =&gt; “system-message” path =&gt; “/var/log/messages” start_position =&gt; “beginning” }} output { #stdout { codec =&gt; rubydebug } #这是标准输出到终端，可以用于调试看有没有输出，注意输出的方向可以有多个 kafka { #输出到kafka bootstrap_servers =&gt; “192.168.2.22:9092,192.168.2.23:9092,192.168.2.24:9092” #他们就是生产者 topic_id =&gt; “system-messages” #这个将作为主题的名称，将会自动创建 compression_type =&gt; “snappy” #压缩类型 }}[root@webserver1 etc]# (2)配置检测 [root@webserver1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf –configtest –verboseConfiguration OK[root@webserver1 etc]# (2)启动Logstash，这里我直接在命令行执行即可 [root@webserver1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf (3)验证数据是否写入到kafka，这里我们检查是否生成了一个叫system-messages的主题 [root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –list –zookeeper 192.168.2.22:2181summersystem-messages #可以看到这个主题已经生成了 #再看看这个主题的详情:[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –describe –zookeeper 192.168.2.22:2181 –topic system-messagesTopic:system-messages PartitionCount:16 ReplicationFactor:1 Configs: Topic: system-messages Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: system-messages Partition: 1 Leader: 3 Replicas: 3 Isr: 3 Topic: system-messages Partition: 2 Leader: 4 Replicas: 4 Isr: 4 Topic: system-messages Partition: 3 Leader: 2 Replicas: 2 Isr: 2 Topic: system-messages Partition: 4 Leader: 3 Replicas: 3 Isr: 3 Topic: system-messages Partition: 5 Leader: 4 Replicas: 4 Isr: 4 Topic: system-messages Partition: 6 Leader: 2 Replicas: 2 Isr: 2 Topic: system-messages Partition: 7 Leader: 3 Replicas: 3 Isr: 3 Topic: system-messages Partition: 8 Leader: 4 Replicas: 4 Isr: 4 Topic: system-messages Partition: 9 Leader: 2 Replicas: 2 Isr: 2 Topic: system-messages Partition: 10 Leader: 3 Replicas: 3 Isr: 3 Topic: system-messages Partition: 11 Leader: 4 Replicas: 4 Isr: 4 Topic: system-messages Partition: 12 Leader: 2 Replicas: 2 Isr: 2 Topic: system-messages Partition: 13 Leader: 3 Replicas: 3 Isr: 3 Topic: system-messages Partition: 14 Leader: 4 Replicas: 4 Isr: 4 Topic: system-messages Partition: 15 Leader: 2 Replicas: 2 Isr: 2[root@kafka1 ~]# 可以看出，这个主题生成了16个分区，每个分区都有对应自己的Leader，但是我想要有10个分区，3个副本如何办？还是跟我们上面一样命令行来创建主题就行，当然对于logstash输出的我们也可以提前先定义主题，然后启动logstash 直接往定义好的主题写数据就行啦，命令如下： [root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –create –zookeeper 192.168.2.22:2181 –replication-factor 3 –partitions 10 –topic TOPIC_NAME 好了，我们将logstash收集到的数据写入到了kafka中了，在实验过程中我使用while脚本测试了如果不断的往kafka写数据的同时停掉两个节点，数据写入没有任何问题。 那如何将数据从kafka中读取然后给我们的es集群呢？那下面我们在kafka集群上安装Logstash，安装步骤不再赘述；三台上面的logstash 的配置如下，作用是将kafka集群的数据读取然后转交给es集群，这里为了测试我让他新建一个索引文件，注意这里的输入日志还是messages，主题名称还是“system-messages” [root@kafka1 etc]# more logstash.confinput { kafka { zk_connect =&gt; “192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181” #消费者们 topic_id =&gt; “system-messages” codec =&gt; plain reset_beginning =&gt; false consumer_threads =&gt; 5 decorate_events =&gt; true }} output { elasticsearch { hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”] index =&gt; “test-system-messages-%{+YYYY-MM}” #为了区分之前实验，我这里新生成的所以名字为“test-system-messages-%{+YYYY-MM}” } } 在三台kafka上面启动Logstash，注意我这里是在命令行启动的； [root@kafka1 etc]# pwd/usr/local/logstash/etc[root@kafka1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf[root@kafka2 etc]# pwd/usr/local/logstash/etc[root@kafka2 etc]# /usr/local/logstash/bin/logstash -f logstash.conf[root@kafka3 etc]# pwd/usr/local/logstash/etc[root@kafka3 etc]# /usr/local/logstash/bin/logstash -f logstash.conf 在webserver1上写入测试内容，即webserver1上面利用message这个文件来测试，我先将其清空，然后启动 [root@webserver1 etc]# &gt;/var/log/messages[root@webserver1 etc]# echo “我将通过kafka集群达到es集群哦^0^” &gt;&gt; /var/log/messages#启动logstash,让其读取messages中的内容 下图为我在客户端写入到kafka集群的同时也将其输入到终端，这里写入了三条内容 而下面三张图侧可以看出，三台Logstash 很平均的从kafka集群当中读取出来了日志内容 再来看看我们的es管理界面 ok ,看到了吧， 流程差不多就是下面 酱紫咯 由于篇幅较长，我将 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用; 放到下一篇博客。","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://blog.sctux.cc/tags/ELK/"},{"name":"kafka","slug":"kafka","permalink":"https://blog.sctux.cc/tags/kafka/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://blog.sctux.cc/tags/zookeeper/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"清理Elasticsearch的索引","slug":"e6-b8-85-e7-90-86elasticsearch-e7-9a-84-e7-b4-a2-e5-bc-95","date":"2015-10-23T14:17:33.000Z","updated":"2025-09-01T01:59:08.937Z","comments":true,"path":"2015/10/23/e6-b8-85-e7-90-86elasticsearch-e7-9a-84-e7-b4-a2-e5-bc-95/","permalink":"https://blog.sctux.cc/2015/10/23/e6-b8-85-e7-90-86elasticsearch-e7-9a-84-e7-b4-a2-e5-bc-95/","excerpt":"最近在使用 logstash来做日志收集 并用 elasticsearch来搜索，因为日志没有进行过滤，没几天就发现elasticsearch的索引文件大的吓人，之前还真没清理过。其实要说清理 也简单，直接到 elasticsearch data文件夹里删掉就行了，但怎么也得做的有点技术含量不是？ 上网站看了看文档，其实也挺简单一条命令就行了 1, # curl -XDELETE ‘http://localhost:9200/logstash-*’ 2, 清理掉了所有的索引文件，我发现curl删除比rm删除要快出很多 下面是主页上的详细介绍，其他部分可以自己看， http://www.elasticsearch.org/guide/reference/api/delet","text":"最近在使用 logstash来做日志收集 并用 elasticsearch来搜索，因为日志没有进行过滤，没几天就发现elasticsearch的索引文件大的吓人，之前还真没清理过。其实要说清理 也简单，直接到 elasticsearch data文件夹里删掉就行了，但怎么也得做的有点技术含量不是？ 上网站看了看文档，其实也挺简单一条命令就行了 1, # curl -XDELETE ‘http://localhost:9200/logstash-*’ 2, 清理掉了所有的索引文件，我发现curl删除比rm删除要快出很多 下面是主页上的详细介绍，其他部分可以自己看， http://www.elasticsearch.org/guide/reference/api/delet","categories":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}],"tags":[{"name":"logstash","slug":"logstash","permalink":"https://blog.sctux.cc/tags/logstash/"}],"keywords":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}]},{"title":"运维人,你应该了解的三张武功心法图(转载)","slug":"e8-bf-90-e7-bb-b4-e4-ba-ba-e4-bd-a0-e5-ba-94-e8-af-a5-e4-ba-86-e8-a7-a3-e7-9a-84-e4-b8-89-e5-bc-a0-e6-ad-a6-e5-8a-9f-e5-bf-83-e6-b3-95-e5-9b-be-e8-bd-ac-e8-bd-bd","date":"2015-10-23T07:10:41.000Z","updated":"2025-09-01T01:59:08.932Z","comments":true,"path":"2015/10/23/e8-bf-90-e7-bb-b4-e4-ba-ba-e4-bd-a0-e5-ba-94-e8-af-a5-e4-ba-86-e8-a7-a3-e7-9a-84-e4-b8-89-e5-bc-a0-e6-ad-a6-e5-8a-9f-e5-bf-83-e6-b3-95-e5-9b-be-e8-bd-ac-e8-bd-bd/","permalink":"https://blog.sctux.cc/2015/10/23/e8-bf-90-e7-bb-b4-e4-ba-ba-e4-bd-a0-e5-ba-94-e8-af-a5-e4-ba-86-e8-a7-a3-e7-9a-84-e4-b8-89-e5-bc-a0-e6-ad-a6-e5-8a-9f-e5-bf-83-e6-b3-95-e5-9b-be-e8-bd-ac-e8-bd-bd/","excerpt":"一、运维技能图 做为一个运维工程师，你知道你应该学习什么？怎么学习吗？朝哪个方向发展吗？下面一张运维工程师技能图，让你了解！ 图片链接，点我^_^ 二、自动化运维路线图 运维自动化在国内已经声名远躁了，随着互联网快速的发展，运维不单单是几个脚本，几个文档可以胜任的！DevOps在国内很受热捧，但是真正的自动化之路，你走到了哪？你知道该怎么走吗？下面的武功心法图告诉你该怎么走！ 图片链接，点我^_^ 三、云计算知识大宝典 从2013年开始，我国云计算持续快速发展，产业规模不断扩大，产业链日趋完善，产业环境不断优化。在这种情况下，不少创业者看到了市场，不少云计算公司崛起。但是人才在哪里，哪些是真正的云计算人才？云计算人才他应该会什么，下面Cloud computing image告诉你 图片链接，点我^_^ 由于原作者制作的图片较大，这里不能正常的显示，点击链接然后扩大可清晰看到！ 天下武功唯快不攻，这句话运用到互联网，就是你最快、最好、最早的掌握了互联网的最新技术，你就是比较吃香的人才！就像最新比较人们的容器Docker技术一样，如果你是先行者，你现在至少是一家Docker生态技术创业服务的合伙人！革命还未胜利，同志还需努力，各位苦逼的互联网工作者，加油！","text":"一、运维技能图 做为一个运维工程师，你知道你应该学习什么？怎么学习吗？朝哪个方向发展吗？下面一张运维工程师技能图，让你了解！ 图片链接，点我^_^ 二、自动化运维路线图 运维自动化在国内已经声名远躁了，随着互联网快速的发展，运维不单单是几个脚本，几个文档可以胜任的！DevOps在国内很受热捧，但是真正的自动化之路，你走到了哪？你知道该怎么走吗？下面的武功心法图告诉你该怎么走！ 图片链接，点我^_^ 三、云计算知识大宝典 从2013年开始，我国云计算持续快速发展，产业规模不断扩大，产业链日趋完善，产业环境不断优化。在这种情况下，不少创业者看到了市场，不少云计算公司崛起。但是人才在哪里，哪些是真正的云计算人才？云计算人才他应该会什么，下面Cloud computing image告诉你 图片链接，点我^_^ 由于原作者制作的图片较大，这里不能正常的显示，点击链接然后扩大可清晰看到！ 天下武功唯快不攻，这句话运用到互联网，就是你最快、最好、最早的掌握了互联网的最新技术，你就是比较吃香的人才！就像最新比较人们的容器Docker技术一样，如果你是先行者，你现在至少是一家Docker生态技术创业服务的合伙人！革命还未胜利，同志还需努力，各位苦逼的互联网工作者，加油！","categories":[{"name":"心情随笔","slug":"心情随笔","permalink":"https://blog.sctux.cc/categories/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/"}],"tags":[],"keywords":[{"name":"心情随笔","slug":"心情随笔","permalink":"https://blog.sctux.cc/categories/%E5%BF%83%E6%83%85%E9%9A%8F%E7%AC%94/"}]},{"title":"Zabbix 监控Mysql状态以及mysql主从","slug":"zabbix-e7-9b-91-e6-8e-a7mysql-e7-8a-b6-e6-80-81-e4-bb-a5-e5-8f-8amysql-e4-b8-bb-e4-bb-8e","date":"2015-10-23T06:48:07.000Z","updated":"2025-09-01T01:59:08.958Z","comments":true,"path":"2015/10/23/zabbix-e7-9b-91-e6-8e-a7mysql-e7-8a-b6-e6-80-81-e4-bb-a5-e5-8f-8amysql-e4-b8-bb-e4-bb-8e/","permalink":"https://blog.sctux.cc/2015/10/23/zabbix-e7-9b-91-e6-8e-a7mysql-e7-8a-b6-e6-80-81-e4-bb-a5-e5-8f-8amysql-e4-b8-bb-e4-bb-8e/","excerpt":"一，利用zabbix自带模板监控mysql状态： 1，在从的mysql服务器上面创建一个用于zabbix监控的用户 grant replication client on . to zabbix@’localhost’ &nbsp;IDENTIFIED BY ‘PASSWORD’; 2，根据zabbix监控mysql的key改写脚本 #/bin/bashDEF=”–defaults-file=/etc/zabbix/my.conf”MYSQL=’/usr/local/webservers/mysql-5.6.19/bin/mysqladmin’ARGS=1if [ $# -ne “$ARGS” ];then echo “Please input one arguement:”ficase $1 in Uptime) result=${MYSQL} $DEF status|cut -f2 -d\":\"|cut -f1 -d\"T\" echo $result ;; Com_update) result=${MYSQL} $DEF extended-status |grep -w \"Com_update\"|cut -d\"|\" -f3 echo $result ;; Slow_queries) result=${MYSQL} $DEF status |cut -f5 -d\":\"|cut -f1 -d\"O\" echo $result ;; Com_select) result=${MYSQL} $DEF extended-status |grep -w \"Com_select\"|cut -d\"|\" -f3 echo $result ;; Com_rollback) result=${MYSQL} $DEF extended-status |grep -w \"Com_rollback\"|cut -d\"|\" -f3 echo $result ;; Questions) result=${MYSQL} $DEF status|cut -f4 -d\":\"|cut -f1 -d\"S\" echo $result ;; Com_insert) result=${MYSQL} $DEF extended-status |grep -w \"Com_insert\"|cut -d\"|\" -f3 echo $result ;; Com_delete) result=${MYSQL} $DEF extended-status |grep -w \"Com_delete\"|cut -d\"|\" -f3 echo $result ;; Com_commit) result=${MYSQL} $DEF extended-status |grep -w \"Com_commit\"|cut -d\"|\" -f3 echo $result ;; Bytes_sent) result=${MYSQL} $DEF extended-status |grep -w \"Bytes_sent\" |cut -d\"|\" -f3 echo $result ;; Bytes_received) result=${MYSQL} $DEF extended-status |grep -w \"Bytes_received\" |cut -d\"|\" -f3 echo $result ;; Com_begin) result=${MYSQL} $DEF extended-status |grep -w \"Com_begin\"|cut -d\"|\" -f3 echo $result ;; *) echo \"Usage:$0(Uptime|Com\\_update|Slow\\_queries|Com\\_select|Com\\_rollback|Questions)\" ;; esac 上面的脚本中： /etc/zabbix/my.conf 这个文件定义的是zabbix这个mysql用户的信息，然后直接指定这个文件，如果我们在命令行直接输入zabbix的用户密码的话会一行提示，影响我们zabbix取值，例如： 如果我们直接指定了这个文件，那么那行提示就不会出现 /etc/zabbix/my.conf的内容： [client]host=localhostuser=zabbixpassword=’PASSWORD’socket = /data/mysql/mysql.sock 3，设置zabbix_agent端 首先启用自定义的key 去掉zabbix_agent配置文件的259行: Include=/usr/local/etc/zabbix_agentd.conf.d/*.conf 然后自定义一个mysql-status.conf 在这个目录下，内容为： UserParameter=mysql.version,/usr/local/webservers/mysql-5.6.19/bin/mysql -VUserParameter=mysql.ping,/usr/local/webservers/mysql-5.6.19/bin/mysqladmin –defaults-file=/etc/zabbix/my.conf ping | grep -c aliveUserParameter=mysql.status[*],/home/shell/mysql-status.sh $1#注意：这里自定义的key 建议使用命令的绝对路径 ok 上面的内容定义好之后重启zabbix_agent服务，然后就可以在zabbix_server端看一下能不能获取到key值 好啦，以上内容中可以看到取值正常，我们在zabbix dashboard中查看一下吧； 至此已经监控到mysql的状态啦；这里一定要注意mysql用户取获取mysql服务的状态时候的权限； 二、zabbix监控mysql主从状态 跟上面的步骤差不多，这里使用的mysql用户我还是使用的zabbix用户 那这里直接就自定义key 啦，大家都知道mysql主从状态我们一般通过在mysql slaves上面的show slave status 然后看 Slave_IO_Running: YesSlave_SQL_Running: Yes","text":"一，利用zabbix自带模板监控mysql状态： 1，在从的mysql服务器上面创建一个用于zabbix监控的用户 grant replication client on . to zabbix@’localhost’ &nbsp;IDENTIFIED BY ‘PASSWORD’; 2，根据zabbix监控mysql的key改写脚本 #/bin/bashDEF=”–defaults-file=/etc/zabbix/my.conf”MYSQL=’/usr/local/webservers/mysql-5.6.19/bin/mysqladmin’ARGS=1if [ $# -ne “$ARGS” ];then echo “Please input one arguement:”ficase $1 in Uptime) result=${MYSQL} $DEF status|cut -f2 -d\":\"|cut -f1 -d\"T\" echo $result ;; Com_update) result=${MYSQL} $DEF extended-status |grep -w \"Com_update\"|cut -d\"|\" -f3 echo $result ;; Slow_queries) result=${MYSQL} $DEF status |cut -f5 -d\":\"|cut -f1 -d\"O\" echo $result ;; Com_select) result=${MYSQL} $DEF extended-status |grep -w \"Com_select\"|cut -d\"|\" -f3 echo $result ;; Com_rollback) result=${MYSQL} $DEF extended-status |grep -w \"Com_rollback\"|cut -d\"|\" -f3 echo $result ;; Questions) result=${MYSQL} $DEF status|cut -f4 -d\":\"|cut -f1 -d\"S\" echo $result ;; Com_insert) result=${MYSQL} $DEF extended-status |grep -w \"Com_insert\"|cut -d\"|\" -f3 echo $result ;; Com_delete) result=${MYSQL} $DEF extended-status |grep -w \"Com_delete\"|cut -d\"|\" -f3 echo $result ;; Com_commit) result=${MYSQL} $DEF extended-status |grep -w \"Com_commit\"|cut -d\"|\" -f3 echo $result ;; Bytes_sent) result=${MYSQL} $DEF extended-status |grep -w \"Bytes_sent\" |cut -d\"|\" -f3 echo $result ;; Bytes_received) result=${MYSQL} $DEF extended-status |grep -w \"Bytes_received\" |cut -d\"|\" -f3 echo $result ;; Com_begin) result=${MYSQL} $DEF extended-status |grep -w \"Com_begin\"|cut -d\"|\" -f3 echo $result ;; *) echo \"Usage:$0(Uptime|Com\\_update|Slow\\_queries|Com\\_select|Com\\_rollback|Questions)\" ;; esac 上面的脚本中： /etc/zabbix/my.conf 这个文件定义的是zabbix这个mysql用户的信息，然后直接指定这个文件，如果我们在命令行直接输入zabbix的用户密码的话会一行提示，影响我们zabbix取值，例如： 如果我们直接指定了这个文件，那么那行提示就不会出现 /etc/zabbix/my.conf的内容： [client]host=localhostuser=zabbixpassword=’PASSWORD’socket = /data/mysql/mysql.sock 3，设置zabbix_agent端 首先启用自定义的key 去掉zabbix_agent配置文件的259行: Include=/usr/local/etc/zabbix_agentd.conf.d/*.conf 然后自定义一个mysql-status.conf 在这个目录下，内容为： UserParameter=mysql.version,/usr/local/webservers/mysql-5.6.19/bin/mysql -VUserParameter=mysql.ping,/usr/local/webservers/mysql-5.6.19/bin/mysqladmin –defaults-file=/etc/zabbix/my.conf ping | grep -c aliveUserParameter=mysql.status[*],/home/shell/mysql-status.sh $1#注意：这里自定义的key 建议使用命令的绝对路径 ok 上面的内容定义好之后重启zabbix_agent服务，然后就可以在zabbix_server端看一下能不能获取到key值 好啦，以上内容中可以看到取值正常，我们在zabbix dashboard中查看一下吧； 至此已经监控到mysql的状态啦；这里一定要注意mysql用户取获取mysql服务的状态时候的权限； 二、zabbix监控mysql主从状态 跟上面的步骤差不多，这里使用的mysql用户我还是使用的zabbix用户 那这里直接就自定义key 啦，大家都知道mysql主从状态我们一般通过在mysql slaves上面的show slave status 然后看 Slave_IO_Running: YesSlave_SQL_Running: Yes 这两个值，如果其中一个不为Yes那说明主从同步是有问题的，那我们用zabbix这个的身份来获取这个值 编写脚本：mysql_replication.sh #!/bin/bash/usr/local/webservers/mysql-5.6.19/bin/mysql –defaults-file=/etc/zabbix/my.conf -e ‘show slave status\\G’ | grep -E “Slave_IO_Running:|Slave_SQL_Running:” | awk ‘{print $2}’ | grep -c Yes 这个脚本的用途是，用zabbix用户身份执行”show slave status\\G”这个命令，然后截取Yes这个关键字的行数是2 或者非 2 在zabbix_server 端/usr/local/etc/zabbix_agentd.conf.d/目录下新建文件：mysql-replication.conf，内容为： UserParameter=mysql.replication,/home/shell/mysql-replication.sh 写好之后，重启zabbix_agent 然后在zabbix_server端测试一下看能否获取到这个值 zabbix_server端能获取，现在在zabbix dashboard添加这个item吧 Configuration–&gt;Host–&gt;database-node2–&gt;Items–&gt;Create item 这里我们自定义的key 需要手动输入key名称。我这里定义过了，直接点击“Add”就行了 然后我们需要定义一个Triggers Triggers–&gt;Create triggers 检测它最后一次的取值是不是小于2，定义N的值为2，如果取得的值小于2就说明有问题啦， 定义一下报警： Configuration–&gt;Action–&gt;Create action 邮件通知内容：MySQL.RepliactionERROR—MySQL master-slave –&gt;{ITEM.VALUE1}MySQL 主从出现问题，请检测主从状态！！！告警主机 : {HOSTNAME1}告警时间 : {EVENT.DATE} {EVENT.TIME}告警等级 : {TRIGGER.SEVERITY}告警信息 : {TRIGGER.NAME}告警项目 : {TRIGGER.KEY1}问题详情 : {ITEM.NAME}:{ITEM.VALUE}当前状态 : {TRIGGER.STATUS}:{ITEM.VALUE1}事件ID : {EVENT.ID} 恢复后的回复:OK—MySQL master-slave –&gt;{ITEM.VALUE1}MySQL 主从问题恢复，请确认主从状态！！！告警主机 : {HOSTNAME1}告警时间 : {EVENT.DATE} {EVENT.TIME}告警等级 : {TRIGGER.SEVERITY}告警信息 : {TRIGGER.NAME}告警项目 : {TRIGGER.KEY1}问题详情 : {ITEM.NAME}:{ITEM.VALUE}当前状态 : {TRIGGER.STATUS}:{ITEM.VALUE1}事件ID : {EVENT.ID} 条件： zabbix自定义脚本发送邮件； 首先我们要在zabbix_server端启用自定义脚本发送邮件，在zabbix_server主配置文件中修改为 AlertScriptsPath=/usr/lib/zabbix/alertscripts &nbsp;脚本存放的目录 然后我们自定义一个脚本mail.py #!/usr/bin/python#coding:utf-8 import smtplibfrom email.mime.text import MIMETextimport sys #邮箱服务器地址mail_host = ‘smtp.qq.com’#邮箱用户名mail_user = ‘xxxxxxx’#邮箱密码mail_pass = ‘xxxxxxxx’mail_postfix = ‘qq.com’ def send_mail(to_list,subject,content): me = mail_user+”&lt;”+mail_user+”@”+mail_postfix+”&gt;” msg = MIMEText(content,_charset=’utf-8’) msg[‘Subject’] = subject msg[‘From’] = me msg[‘to’] = to_list try: s = smtplib.SMTP() s.connect(mail_host) s.login(mail\\_user,mail\\_pass) s.sendmail(me,to\\_list,msg.as\\_string()) s.close() return True except Exception,e: print str(e) return False if __name__ == “__main__“: send_mail(sys.argv[1], sys.argv[2], sys.argv[3]) 记得加上执行权限； 然后在zabbix dashboard上面定义用户的media加上用户的邮箱即可 以上就可以实现报警啦 测试一下，将mysql-replication.sh这个脚本的值自定义一个输出值为0或者1 查看邮件： 如果恢复后的回复 ok 已经成功能看到这个监控没问题啦。 下面来看一下zabbix + grafana ,这个是作为页面展示比较华丽的效果 只要是在zabbix 中有的 items都可以在grafana中展示。","categories":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.sctux.cc/tags/zabbix/"},{"name":"grafana","slug":"grafana","permalink":"https://blog.sctux.cc/tags/grafana/"}],"keywords":[{"name":"Monitor","slug":"Monitor","permalink":"https://blog.sctux.cc/categories/Monitor/"}]},{"title":"检查文件时间戳，对比时间的Shell脚本","slug":"e6-a3-80-e6-9f-a5-e6-96-87-e4-bb-b6-e6-97-b6-e9-97-b4-e6-88-b3-ef-bc-8c-e5-af-b9-e6-af-94-e6-97-b6-e9-97-b4-e7-9a-84shell-e8-84-9a-e6-9c-ac","date":"2015-10-16T14:30:11.000Z","updated":"2025-09-01T01:59:08.861Z","comments":true,"path":"2015/10/16/e6-a3-80-e6-9f-a5-e6-96-87-e4-bb-b6-e6-97-b6-e9-97-b4-e6-88-b3-ef-bc-8c-e5-af-b9-e6-af-94-e6-97-b6-e9-97-b4-e7-9a-84shell-e8-84-9a-e6-9c-ac/","permalink":"https://blog.sctux.cc/2015/10/16/e6-a3-80-e6-9f-a5-e6-96-87-e4-bb-b6-e6-97-b6-e9-97-b4-e6-88-b3-ef-bc-8c-e5-af-b9-e6-af-94-e6-97-b6-e9-97-b4-e7-9a-84shell-e8-84-9a-e6-9c-ac/","excerpt":"应开发需求，有些锁文件生成之后不会在固定时间内删除，造成程序的计划任务会卡死，于是需要一个检查对比locks文件的脚本来实时检测这个目录下的*.locks文件 12345678910111213141516171819202122232425262728293031#!/bin/bashwhile truedo#获取当前时间curren_time=\\`date +%H:%M:%S\\` #--time-stype=FORMATls -l --time-style=+%H:%M:%S /xxxxxx/*.locks &gt;/tmp/locks 2&gt;/dev/nullwhile read -r linedo#文件内容file_time=\\`echo $line | awk -F ' ' '{print $6}'\\` #文件绝对路径及名称File=\\`echo $line | awk -F ' ' '{print $7}'\\`#转换成Unix时间戳date1=\\`date -d \"$curren_time\" +%s\\`date2=\\`date -d \"$file_time\" +%s\\`#当前时间减去文件生产时间的差值进行比较(shell 的运算用expr关键字)DD=$(expr $date1 - $date2)if \\[ $DD -gt 120 \\];then echo \"$curren_time --- Greater than 2 minutes, $File will be delete.\" sleep 2 rm -rf $Filefidone &lt; /tmp/locksdonechmod +x /home/shell/check\\_locks\\_file.shnohup /bin/bash /home/shell/check\\_locks\\_file.sh &gt;/var/log/check_locks.log &amp; 两个关键点1、ls命令的用法 ： –time-type=FORMAT 参数2、将常规的时间转换成Unix时间戳","text":"应开发需求，有些锁文件生成之后不会在固定时间内删除，造成程序的计划任务会卡死，于是需要一个检查对比locks文件的脚本来实时检测这个目录下的*.locks文件 12345678910111213141516171819202122232425262728293031#!/bin/bashwhile truedo#获取当前时间curren_time=\\`date +%H:%M:%S\\` #--time-stype=FORMATls -l --time-style=+%H:%M:%S /xxxxxx/*.locks &gt;/tmp/locks 2&gt;/dev/nullwhile read -r linedo#文件内容file_time=\\`echo $line | awk -F ' ' '{print $6}'\\` #文件绝对路径及名称File=\\`echo $line | awk -F ' ' '{print $7}'\\`#转换成Unix时间戳date1=\\`date -d \"$curren_time\" +%s\\`date2=\\`date -d \"$file_time\" +%s\\`#当前时间减去文件生产时间的差值进行比较(shell 的运算用expr关键字)DD=$(expr $date1 - $date2)if \\[ $DD -gt 120 \\];then echo \"$curren_time --- Greater than 2 minutes, $File will be delete.\" sleep 2 rm -rf $Filefidone &lt; /tmp/locksdonechmod +x /home/shell/check\\_locks\\_file.shnohup /bin/bash /home/shell/check\\_locks\\_file.sh &gt;/var/log/check_locks.log &amp; 两个关键点1、ls命令的用法 ： –time-type=FORMAT 参数2、将常规的时间转换成Unix时间戳","categories":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"},{"name":"脚本编程","slug":"Shell/脚本编程","permalink":"https://blog.sctux.cc/categories/Shell/%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"expr","slug":"expr","permalink":"https://blog.sctux.cc/tags/expr/"}],"keywords":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"},{"name":"脚本编程","slug":"Shell/脚本编程","permalink":"https://blog.sctux.cc/categories/Shell/%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B/"}]},{"title":"基于CentOS6.5 X86_64 源码搭建GitLab","slug":"e5-9f-ba-e4-ba-8ecentos6-5-x86-64-e6-ba-90-e7-a0-81-e6-90-ad-e5-bb-bagitlab","date":"2015-09-30T07:31:43.000Z","updated":"2025-09-01T01:59:08.937Z","comments":true,"path":"2015/09/30/e5-9f-ba-e4-ba-8ecentos6-5-x86-64-e6-ba-90-e7-a0-81-e6-90-ad-e5-bb-bagitlab/","permalink":"https://blog.sctux.cc/2015/09/30/e5-9f-ba-e4-ba-8ecentos6-5-x86-64-e6-ba-90-e7-a0-81-e6-90-ad-e5-bb-bagitlab/","excerpt":"系统：CentOS6.5 X86_64 已完成初始化：防火墙、SELinux 关闭、不必要服务停止，不必要用户删除……… 1.添加epel源 wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 https://www.fedoraproject.org/static/0608B895.txt &amp;&amp; rpm –import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 #查看Key是否安装成功rpm -qa gpg* &nbsp; 2.安装epel源 rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm &nbsp; 3.添加PUIAS源 wget -O /etc/yum.repos.d/PUIAS_6_computational.repo https://gitlab.com/gitlab-org/gitlab-recipes/raw/master/install/centos/PUIAS\\_6\\_computational.repo &nbsp; 4.下载并安装gpg-key wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-puias http://springdale.math.ias.edu/data/puias/6/x86_64/os/RPM-GPG-KEY-puias &amp;&amp; rpm –import /etc/pki/rpm-gpg/RPM-GPG-KEY-puias #查看源是否添加成功[如果不成功，执行：yum-config-manager –enable epel –enable PUIAS_6_computational]yum repolist","text":"系统：CentOS6.5 X86_64 已完成初始化：防火墙、SELinux 关闭、不必要服务停止，不必要用户删除……… 1.添加epel源 wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 https://www.fedoraproject.org/static/0608B895.txt &amp;&amp; rpm –import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 #查看Key是否安装成功rpm -qa gpg* &nbsp; 2.安装epel源 rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm &nbsp; 3.添加PUIAS源 wget -O /etc/yum.repos.d/PUIAS_6_computational.repo https://gitlab.com/gitlab-org/gitlab-recipes/raw/master/install/centos/PUIAS\\_6\\_computational.repo &nbsp; 4.下载并安装gpg-key wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-puias http://springdale.math.ias.edu/data/puias/6/x86_64/os/RPM-GPG-KEY-puias &amp;&amp; rpm –import /etc/pki/rpm-gpg/RPM-GPG-KEY-puias #查看源是否添加成功[如果不成功，执行：yum-config-manager –enable epel –enable PUIAS_6_computational]yum repolist &nbsp; 5.安装整个搭建gitlab的相关的依赖包 yum -y groupinstall ‘Development Tools’yum -y install readline readline-devel ncurses-devel gdbm-devel glibc-devel tcl-devel openssl-devel curl-devel expat-devel db4-devel byacc sqlite-devel libyaml libyaml-devel libffi libffi-devel libxml2 libxml2-devel libxslt libxslt-devel libicu libicu-devel system-config-firewall-tui redis sudo wget crontabs logwatch logrotate perl-Time-HiRes git cmake libcom_err-devel.i686 libcom_err-devel.x86_64 nodejs &nbsp; 6.配置默认编辑器 yum -y install vim-enhancedupdate-alternatives –set editor /usr/bin/vim.basicln -s /usr/bin/vim /usr/bin/edito #reStructuredText markup语法支持，需要安装依赖包：yum install -y python-docutils &nbsp; 7.安装git(git&gt;=1.7.10 如果低于我们可以从新编译安装) yum install zlib-devel perl-CPAN gettext curl-devel expat-devel gettext-devel openssl-develmkdir /tmp/git &amp;&amp; cd /tmp/gitcurl –progress https://www.kernel.org/pub/software/scm/git/git-2.1.3.tar.gz | tar xzcd git-2.1.3/ &amp;&amp; ./configure –prefix=/usr/local/git &amp;&amp; make &amp;&amp; make install &nbsp; 8.安装ruby(如果系统中Ruby的版本是2.0以前的那么请移除，GitLab只支持Ruby 2.0+版本) yum remove ruby mkdir /tmp/ruby &amp;&amp; cd /tmp/rubycurl –progress ftp://ftp.ruby-lang.org/pub/ruby/2.1/ruby-2.1.2.tar.gz | tar xzcd ruby-2.1.2./configure –prefix=/usr/local/ –disable-install-rdocmakemake install#记得检查ruby的版本是否是最新ruby –version &nbsp; 9.安装Bundler Gem(由于`http://rubygems.org\\`已被墙，这里替换源为\\`https://ruby.taobao.org\\`) # 添加淘宝源并且移除官方源gem sources -a https://ruby.taobao.orggem sources -r https://rubygems.org/ &nbsp; 10.安装Bundler gem install bundler -v’1.5.2’ –no-doc &nbsp; 11.System Users为GitLab创建用户 adduser –system –shell /bin/bash –comment ‘GitLab’ –create-home –home-dir /home/git/ git &nbsp; 12.编辑sudoers文件，将ruby和git的程序路径添加到PATH中，使git用户作为root来使用gem命令 sed -ie ‘s@Defaults\\ secure_path\\ \\=\\ \\/sbin\\:\\/bin\\:\\/usr\\/sbin\\:\\/usr\\/bin@Defaults\\ secure_path\\ \\=\\ \\/sbin\\:\\/bin\\:\\/usr\\/sbin\\:\\/usr\\/bin\\:/usr\\/local\\/bin@g’ /etc/sudoers &nbsp; 13.安装mysql yum install -y mysql mysql-devel mysql-serverservice mysqld start #为gitlab创建数据库#创建用户mysql -e “CREATE USER ‘git‘@’localhost’ IDENTIFIED BY ‘xxxxx’;” #创建数据库mysql -e “CREATE DATABASE IF NOT EXISTS \\`gitlabhq_production\\` DEFAULT CHARACTER SET \\`utf8\\` COLLATE \\`utf8_unicode_ci\\`;” #授权mysql -e “GRANT SELECT, LOCK TABLES, INSERT, UPDATE, DELETE, CREATE, DROP, INDEX, ALTER ON \\`gitlabhq_production\\`.* TO ‘git‘@’localhost’;” #用git用户尝试登陆验证是否成功sudo -u git -H mysql -u git -pxxxxxx -e ‘show databases;’ #记得要给root用户密码哦。 &nbsp; 14.安装redis(已经在前面的依赖包安装时安装了) chkconfig redis on #配置Redis使用socketscp /etc/redis.conf /etc/redis.conf.orig # 关闭Redis 监听于TCPsed ‘s/^port .*/port 0/‘ /etc/redis.conf.orig | sudo tee /etc/redis.conf #启用Redis socketecho ‘unixsocket /var/run/redis/redis.sock’ | sudo tee -a /etc/redis.confecho -e ‘unixsocketperm 0770’ | sudo tee -a /etc/redis.conf #设置socket目录属主（组）为redischown redis:redis /var/run/redischmod 755 /var/run/redis #将git用户添加至redis组usermod -aG redis git #启动redis服务service redis start &nbsp; 15.安装gitlab，以及相关配置 cd /home/git/sudo -u git -H git clone http://git.oschina.net/Yxnt/gitlab # 配置gitlabcd /home/git/gitlab &amp;&amp; sudo -u git -H cp config/gitlab.yml.example config/gitlab.yml #访问地址为本机IPsudo -u git -H sed -ie “s/host: .*/host: `hostname –all-ip-addresses`/g” config/gitlab.yml #创建satellites目录sudo -u git -H mkdir /home/git/gitlab-satellites &amp;&amp; chmod u+rwx,g=rx,o-rwx /home/git/gitlab-satellites #确认gitlab可以写入tmp/pids、tmp/sockets以及public/uploads/目录chmod -R u+rwX tmp/pids/ &amp;&amp; chmod -R u+rwX tmp/sockets/ &amp;&amp; chmod -R u+rwX public/uploads #复制Unicorn配置文件sudo -u git -H cp config/unicorn.rb.example config/unicorn.rb #复制Rack attack配置文件sudo -u git -H cp config/initializers/rack_attack.rb.example config/initializers/rack_attack.rb #配置Git 全局设置sudo -u git -H git config –global user.name “GitLab”sudo -u git -H git config –global user.email “example@example.com“sudo -u git -H git config –global core.autocrlf input #配置Redis连接设置sudo -u git -H cp config/resque.yml.example config/resque.ymlsudo -u git -H sed -ie “s/develo.*/development:\\ unix:\\/var\\/run\\/redis\\/redis.sock/g” config/resque.yml #配置Gitlab 数据库设置（注意这里的socket文件位置，不同版本的mysql位置不一样)sudo -u git cp config/database.yml.mysql config/database.ymlsudo -u git sed -ie “12s/password: \\“secure password\\“/password: $MYSQL_PASS/g” config/database.ymlsudo -u git sed -ie “13,14s/#\\ //g” config/database.ymlsudo -u git sed -ie “14s@socket: \\/tmp\\/mysql.sock@socket: \\/var\\/lib/mysql\\/mysql.sock@g” config/database.yml #确定database.yml文件只为git用户可读sudo -u git -H chmod o-rwx config/database.yml &nbsp; 16.安装Gems cd /home/git/gitlab #更换配置文件中的ruby源sudo -u git -H sed -ie “s@source\\ \\“http:\\/\\/rubygems.org\\“@source\\ \\“http:\\/\\/ruby.taobao.org\\“@g” ../gitlab-shell/Gemfile #安装sudo -u git -H bundle install –deployment –without development test postgres aws &nbsp; 17.安装,配置gitlab-shell cd /home/git/gitlab/sudo -u git -H bundle exec rake gitlab🐚install[v2.6.3] REDIS_URL=unix:/var/run/redis/redis.sock RAILS_ENV=production #配置(在安装过程中其实gitlab-shell配置已经自动生成了，但是有必要检查核实一下里面的配置，尤其是gitlab_url)cp /home/git/gitlab-shell/config.yml /home/git/gitlab-shell/config.yml.bak &nbsp; 18.初始化数据并且激活高级特性 #这里会提示输入yes/no，输入yes即可（我这里直接传递一个yes过去）echo yes| sudo -u git -H bundle exec rake gitlab:setup RAILS_ENV=production#这里会生成一个账户和密码：#login………root#password……5iveL!fe 这个密码可以更改： sudo -u git -H bundle exec rake gitlab:setup RAILS_ENV=production GITLAB_ROOT_PASSWORD=YOUR_NETPASSWORD #生成js.css等文件（如果没有这一步的话，你在访问时网页css等是乱的）cd /home/git/gitlabbundle exec rake assets:precompile RAILS_ENV=production &nbsp; 19.安装gitlab启动管理脚本 wget -O /etc/init.d/gitlab https://gitlab.com/gitlab-org/gitlab-recipes/raw/master/init/sysvinit/centos/gitlab-unicornchmod +x /etc/init.d/gitlabchkconfig –add gitlabchkconfig gitlab on #设置gitlab服务的日志滚动cp lib/support/logrotate/gitlab /etc/logrotate.d/gitlab &nbsp; 20.检查应用程序状态 sudo -u git -H bundle exec rake gitlab:env:info RAILS_ENV=production#执行该命令 如果顺利的话，会将关于gitlab的 相关详细信息展示出来 和下图差不多 &nbsp; 21.启动gitlab #启动gitlabservice gitlab start &nbsp; 22.安装nginx ，将访问请求反代到后端的8080端口 yum install -y nginx &amp;&amp; chkconfig nginx on #删除nginx中的这个默认配置文件rm -rf /etc/nginx/conf.d/default.conf #获取gitlab的nginx模板配置文件wget -O /etc/nginx/conf.d/gitlab.conf https://gitlab.com/gitlab-org/gitlab-ce/raw/master/lib/support/nginx/gitlab #对模板文件的一些修改sed -ie “38s/.*/server\\ localhost:8080;/g” /etc/nginx/conf.d/gitlab.confsed -ie “s/YOUR_SERVER_FQDN/`hostname`/g” /etc/nginx/conf.d/gitlab.confsed -ie “52d” /etc/nginx/conf.d/gitlab.conf #将用户nginx加入到git组(这步非常的关键)usermod -a -G git nginxchmod g+rx /home/git/ #启动nginxservice nginx start &nbsp; ok ，至此gitlab的安装已经结束。我们通过http://YOUR-SERVER-IP/ 就可以访问到你的gitlab平台了","categories":[{"name":"Web相关","slug":"Web相关","permalink":"https://blog.sctux.cc/categories/Web%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"git","slug":"git","permalink":"https://blog.sctux.cc/tags/git/"},{"name":"gitlab","slug":"gitlab","permalink":"https://blog.sctux.cc/tags/gitlab/"}],"keywords":[{"name":"Web相关","slug":"Web相关","permalink":"https://blog.sctux.cc/categories/Web%E7%9B%B8%E5%85%B3/"}]},{"title":"通过Inode删除linux下的文件","slug":"e9-80-9a-e8-bf-87inode-e5-88-a0-e9-99-a4linux-e4-b8-8b-e7-9a-84-e6-96-87-e4-bb-b6","date":"2015-09-19T05:38:20.000Z","updated":"2025-09-01T01:59:08.958Z","comments":true,"path":"2015/09/19/e9-80-9a-e8-bf-87inode-e5-88-a0-e9-99-a4linux-e4-b8-8b-e7-9a-84-e6-96-87-e4-bb-b6/","permalink":"https://blog.sctux.cc/2015/09/19/e9-80-9a-e8-bf-87inode-e5-88-a0-e9-99-a4linux-e4-b8-8b-e7-9a-84-e6-96-87-e4-bb-b6/","excerpt":"由于某些原因在我们linux系统上面总会出现一些乱码文件，或者不能正常输入的文件名，当遇到这些无法正常输入的文件名要删除的时候就需要使用文件对应的inode号对文件进行删除。 inode的原理这里就不再说了，具体说明参见：http://www.ruanyifeng.com/blog/2011/12/inode.html 下面这个目录下的文件是我在网上下载的一个网页模板，里面包含了一个不能rm 的文件： 那从何得知 -?+?.txt 这个文件的inode号呢，ls&nbsp; 命令有个参数 -i -i, –inode print the index number of each file 上图中：291606&nbsp; 这个号码就是 这个文件的inode 号啦。然后我们结合find命令就可以将它删除啦","text":"由于某些原因在我们linux系统上面总会出现一些乱码文件，或者不能正常输入的文件名，当遇到这些无法正常输入的文件名要删除的时候就需要使用文件对应的inode号对文件进行删除。 inode的原理这里就不再说了，具体说明参见：http://www.ruanyifeng.com/blog/2011/12/inode.html 下面这个目录下的文件是我在网上下载的一个网页模板，里面包含了一个不能rm 的文件： 那从何得知 -?+?.txt 这个文件的inode号呢，ls&nbsp; 命令有个参数 -i -i, –inode print the index number of each file 上图中：291606&nbsp; 这个号码就是 这个文件的inode 号啦。然后我们结合find命令就可以将它删除啦","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"inode","slug":"inode","permalink":"https://blog.sctux.cc/tags/inode/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"Expect详解(ssh自动登录)","slug":"expect-e8-af-a6-e8-a7-a3ssh-e8-87-aa-e5-8a-a8-e7-99-bb-e5-bd-95","date":"2015-09-16T06:31:02.000Z","updated":"2025-09-01T01:59:08.958Z","comments":true,"path":"2015/09/16/expect-e8-af-a6-e8-a7-a3ssh-e8-87-aa-e5-8a-a8-e7-99-bb-e5-bd-95/","permalink":"https://blog.sctux.cc/2015/09/16/expect-e8-af-a6-e8-a7-a3ssh-e8-87-aa-e5-8a-a8-e7-99-bb-e5-bd-95/","excerpt":"Expect是一个用来处理交互的命令。借助Expect，我们可以将交互过程写在一个脚本上，使之自动化完成。形象的说，ssh登录，ftp登录等都符合交互的定义。下文我们首先提出一个问题，然后介绍基础知四个命令，最后提出解决方法。 Expect中最关键的四个命令是send,expect,spawn,interact。 send：用于向进程发送字符串expect：从进程接收字符串spawn：启动新的进程interact：允许用户交互 1. send命令send命令接收一个字符串参数，并将该参数发送到进程。 expect1.1&gt; send “hello world\\n”hello world 2. expect命令(1)基础知识expect命令和send命令正好相反，expect通常是用来等待一个进程的反馈。expect可以接收一个字符串参数，也可以接收正则表达式参数。和上文的send命令结合，现在我们可以看一个最简单的交互式的例子： expect “hi\\n”send “hello there!\\n” 这两行代码的意思是：从标准输入中等到hi和换行键后，向标准输出输出hello there。","text":"Expect是一个用来处理交互的命令。借助Expect，我们可以将交互过程写在一个脚本上，使之自动化完成。形象的说，ssh登录，ftp登录等都符合交互的定义。下文我们首先提出一个问题，然后介绍基础知四个命令，最后提出解决方法。 Expect中最关键的四个命令是send,expect,spawn,interact。 send：用于向进程发送字符串expect：从进程接收字符串spawn：启动新的进程interact：允许用户交互 1. send命令send命令接收一个字符串参数，并将该参数发送到进程。 expect1.1&gt; send “hello world\\n”hello world 2. expect命令(1)基础知识expect命令和send命令正好相反，expect通常是用来等待一个进程的反馈。expect可以接收一个字符串参数，也可以接收正则表达式参数。和上文的send命令结合，现在我们可以看一个最简单的交互式的例子： expect “hi\\n”send “hello there!\\n” 这两行代码的意思是：从标准输入中等到hi和换行键后，向标准输出输出hello there。 tips： $expect_out(buffer)存储了所有对expect的输入，&lt;$expect_out(0,string)&gt;存储了匹配到expect参数的输入。 比如如下程序： expect “hi\\n”send “you typed &lt;$expect_out(buffer)&gt;”send “but I only expected &lt;$expect_out(0,string)&gt;” 当在标准输入中输入 testhi 是，运行结果如下 you typed: testhiI only expect: hi (2)模式-动作expect最常用的语法是来自tcl语言的模式-动作。这种语法极其灵活，下面我们就各种语法分别说明。 单一分支模式语法： expect “hi” {send “You said hi”} 匹配到hi后，会输出”you said hi” 多分支模式语法： expect “hi” { send “You said hi\\n” } “hello” { send “Hello yourself\\n” } “bye” { send “That was unexpected\\n” } 匹配到hi,hello,bye任意一个字符串时，执行相应的输出。等同于如下写法： expect {“hi” { send “You said hi\\n”}“hello” { send “Hello yourself\\n”}“bye” { send “That was unexpected\\n”}} 3. spawn命令上文的所有demo都是和标准输入输出进行交互，但是我们跟希望他可以和某一个进程进行交互。spawm命令就是用来启动新的进程的。spawn后 的send和expect命令都是和spawn打开的进程进行交互的。结合上文的send和expect命令我们可以看一下更复杂的程序段了。 set timeout -1spawn ftp ftp.test.com //打开新的进程，该进程用户连接远程ftp服务器expect “Name” //进程返回Name时send “user\\r” //向进程输入anonymous\\rexpect “Password:” //进程返回Password:时send “123456\\r” //向进程输入don@libes.com\\rexpect “ftp&gt; “ //进程返回ftp&gt;时send “binary\\r” //向进程输入binary\\rexpect “ftp&gt; “ //进程返回ftp&gt;时send “get test.tar.gz\\r” //向进程输入get test.tar.gz\\r 这段代码的作用是登录到ftp服务器ftp ftp.uu.net上，并以二进制的方式下载服务器上的文件test.tar.gz。程序中有详细的注释。 4.interact到现在为止，我们已经可以结合spawn、expect、send自动化的完成很多任务了。但是，如何让人在适当的时候干预这个过程了。比如下载完 ftp文件时，仍然可以停留在ftp命令行状态，以便手动的执行后续命令。interact可以达到这些目的。下面的demo在自动登录ftp后，允许用 户交互。 spawn ftp ftp.test.comexpect “Name”send “user\\r”expect “Password:”send “123456\\r”interact 如何实现expect ssh自动登录？ yum install -y expect vim loging_myvps.sh#!/usr/bin/expect -fset ip xxx.xxx.xxx.xxxset password **********set timeout 10spawn ssh USERNAME@$ipexpect {“*yes/no” { send “yes\\r”; exp_continue}“*password:” { send “$password\\r” }}interact ./login_myvps.sh 即可登录","categories":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"},{"name":"脚本编程","slug":"Shell/脚本编程","permalink":"https://blog.sctux.cc/categories/Shell/%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"expect","slug":"expect","permalink":"https://blog.sctux.cc/tags/expect/"}],"keywords":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"},{"name":"脚本编程","slug":"Shell/脚本编程","permalink":"https://blog.sctux.cc/categories/Shell/%E8%84%9A%E6%9C%AC%E7%BC%96%E7%A8%8B/"}]},{"title":"Python os.system的结果不能赋值到变量","slug":"python-os-system-e7-9a-84-e7-bb-93-e6-9e-9c-e4-b8-8d-e8-83-bd-e8-b5-8b-e5-80-bc-e5-88-b0-e5-8f-98-e9-87-8f","date":"2015-09-15T14:52:05.000Z","updated":"2025-09-01T01:59:08.860Z","comments":true,"path":"2015/09/15/python-os-system-e7-9a-84-e7-bb-93-e6-9e-9c-e4-b8-8d-e8-83-bd-e8-b5-8b-e5-80-bc-e5-88-b0-e5-8f-98-e9-87-8f/","permalink":"https://blog.sctux.cc/2015/09/15/python-os-system-e7-9a-84-e7-bb-93-e6-9e-9c-e4-b8-8d-e8-83-bd-e8-b5-8b-e5-80-bc-e5-88-b0-e5-8f-98-e9-87-8f/","excerpt":"今天在学习python os模块的system方法时，发现不能赋值给变量，具体操作如下 &nbsp; 后来查询得知有更新的模块，如下： os.systemos.spawn*os.popen*popen2.*commands.* 重新使用a = os.popen(‘df -hT’).read()&nbsp; 就能获取到啦。","text":"今天在学习python os模块的system方法时，发现不能赋值给变量，具体操作如下 &nbsp; 后来查询得知有更新的模块，如下： os.systemos.spawn*os.popen*popen2.*commands.* 重新使用a = os.popen(‘df -hT’).read()&nbsp; 就能获取到啦。","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"Git 代码拉取、同步、通知","slug":"git-e4-bb-a3-e7-a0-81-e6-8b-89-e5-8f-96-e3-80-81-e5-90-8c-e6-ad-a5-e3-80-81-e9-80-9a-e7-9f-a5","date":"2015-09-14T06:10:14.000Z","updated":"2025-09-01T01:59:08.958Z","comments":true,"path":"2015/09/14/git-e4-bb-a3-e7-a0-81-e6-8b-89-e5-8f-96-e3-80-81-e5-90-8c-e6-ad-a5-e3-80-81-e9-80-9a-e7-9f-a5/","permalink":"https://blog.sctux.cc/2015/09/14/git-e4-bb-a3-e7-a0-81-e6-8b-89-e5-8f-96-e3-80-81-e5-90-8c-e6-ad-a5-e3-80-81-e9-80-9a-e7-9f-a5/","excerpt":"代码托管到gtihub，开发人员进行本地修改push到仓库后，这里使用pull在web服务器上定期拉去代码至开发开发服务器 more code_sync_template.sh#!/bin/shupdate_dir=”/home/demo/“ #every time upload code main dirwebroot=”/var/www/template”#bak_dir=”/codedata/bak/deploy” #backup file main dirgibin=”/usr/bin/git” #git bin pathrsync_bin=”/usr/bin/rsync” #rsync path#这个变量定义的是代码拉取到本地之后同步到web目录时过排除一些特定的文件#rsync_exclude=”.svn|.gz|.cache|.txt|.git|_tmp|temp|pma|dsn.php|gem*|global_config.php“Name=”template”LOG=”/var/log/code_sync_template.log”TIME=`date +%F\\ %T`OLD_IFS=”$IFS” #backup IFSCOMMAND=”sed -n ‘/$TIME start/,\\$p’ $LOG &gt; /tmp/template.log” #get gitos code to localpull_code(){if [ -d $update_dir/$Name ];then if cd $update_dir/$Name &amp;&amp; $gibin pull git@这里是你的代码仓库地址 &amp;&gt;&gt; $LOG ;then echo -e “\\033[32m `date` git pull code success \\033[m” else echo -e “\\033[31m `date` git pull code error \\033[m” &amp;&amp; exit 1 fielse echo -e “\\033[32m Clone code to local. \\033[m” cd $update_dir &amp;&amp; $gibin clone git@这里是你的代码仓库地址fi} #sync local code to webroot#定义的是将最新拉取到的代码同步到web目录sync_code(){ #$rsync_bin -uavz $update_dir/$Name $webroot/$Nameif [ “$rsync_exclude” ];thenIFS=”|” exclude_arr=($rsync_exclude) for e_arr in ${exclude_arr[@]};do echo $e_arr &gt;&gt;/tmp/code_exclude_list done $rsync_bin -avz –exclude-from=”/tmp/code_exclude_list” $update_dir/$Name/* $webroot/else $rsync_bin -avz $update_dir/$Name/* $webroot/fiIFS=”$OLD_IFS”} Notice(){ #这个是通过一个eval命令来执行一段命令，这个命令描述的是从上次脚本执行记录日志后开始到最后的日志信息eval $COMMAND #这里是在过滤此次同步是记录的日志中pull有没有文件更新grep -E “files changed|Fast-forward” /tmp/template.logif [ $? -eq 0 ];then #这里使用一个python脚本来获取pull时更新了的具体文件有哪些，然后将内容通过邮件发送到指定邮箱 python /home/demo/mail-template.pyelse exit 5fi} echo ‘ ‘ &gt;&gt; $LOGecho -e “\\033[42;37m ############ Template $TIME start ############ \\033[m” &gt;&gt; $LOGecho -e “\\033[47;30m Template Pull remote code to local. \\033[0m” &gt;&gt; $LOGecho -e “\\033[47;30m Template Pull remote code to local. \\033[0m”pull_code &gt;&gt; $LOGecho -e “\\033[47;30m Template Sync local code to wwwroot.\\033[0m” &gt;&gt; $LOGecho -e “\\033[47;30m Template Sync local code to wwwroot.\\033[0m”sync_code &gt;&gt; $LOGecho -e “\\033[46;37m ############ Template $TIME end ############ \\033[m” &gt;&gt; $LOGecho ‘ ‘ &gt;&gt; $LOGNotice &nbsp; 邮件通知脚本，能够在自定义的时间周期内获取到最新代码时 能发送邮件到指定的邮箱，能够自动提醒管理人员知悉这一动作。 #!/usr/bin/env python#-*- coding: UTF-8 -*-import smtplibimport os,sysfrom email.mime.text import MIMEText","text":"代码托管到gtihub，开发人员进行本地修改push到仓库后，这里使用pull在web服务器上定期拉去代码至开发开发服务器 more code_sync_template.sh#!/bin/shupdate_dir=”/home/demo/“ #every time upload code main dirwebroot=”/var/www/template”#bak_dir=”/codedata/bak/deploy” #backup file main dirgibin=”/usr/bin/git” #git bin pathrsync_bin=”/usr/bin/rsync” #rsync path#这个变量定义的是代码拉取到本地之后同步到web目录时过排除一些特定的文件#rsync_exclude=”.svn|.gz|.cache|.txt|.git|_tmp|temp|pma|dsn.php|gem*|global_config.php“Name=”template”LOG=”/var/log/code_sync_template.log”TIME=`date +%F\\ %T`OLD_IFS=”$IFS” #backup IFSCOMMAND=”sed -n ‘/$TIME start/,\\$p’ $LOG &gt; /tmp/template.log” #get gitos code to localpull_code(){if [ -d $update_dir/$Name ];then if cd $update_dir/$Name &amp;&amp; $gibin pull git@这里是你的代码仓库地址 &amp;&gt;&gt; $LOG ;then echo -e “\\033[32m `date` git pull code success \\033[m” else echo -e “\\033[31m `date` git pull code error \\033[m” &amp;&amp; exit 1 fielse echo -e “\\033[32m Clone code to local. \\033[m” cd $update_dir &amp;&amp; $gibin clone git@这里是你的代码仓库地址fi} #sync local code to webroot#定义的是将最新拉取到的代码同步到web目录sync_code(){ #$rsync_bin -uavz $update_dir/$Name $webroot/$Nameif [ “$rsync_exclude” ];thenIFS=”|” exclude_arr=($rsync_exclude) for e_arr in ${exclude_arr[@]};do echo $e_arr &gt;&gt;/tmp/code_exclude_list done $rsync_bin -avz –exclude-from=”/tmp/code_exclude_list” $update_dir/$Name/* $webroot/else $rsync_bin -avz $update_dir/$Name/* $webroot/fiIFS=”$OLD_IFS”} Notice(){ #这个是通过一个eval命令来执行一段命令，这个命令描述的是从上次脚本执行记录日志后开始到最后的日志信息eval $COMMAND #这里是在过滤此次同步是记录的日志中pull有没有文件更新grep -E “files changed|Fast-forward” /tmp/template.logif [ $? -eq 0 ];then #这里使用一个python脚本来获取pull时更新了的具体文件有哪些，然后将内容通过邮件发送到指定邮箱 python /home/demo/mail-template.pyelse exit 5fi} echo ‘ ‘ &gt;&gt; $LOGecho -e “\\033[42;37m ############ Template $TIME start ############ \\033[m” &gt;&gt; $LOGecho -e “\\033[47;30m Template Pull remote code to local. \\033[0m” &gt;&gt; $LOGecho -e “\\033[47;30m Template Pull remote code to local. \\033[0m”pull_code &gt;&gt; $LOGecho -e “\\033[47;30m Template Sync local code to wwwroot.\\033[0m” &gt;&gt; $LOGecho -e “\\033[47;30m Template Sync local code to wwwroot.\\033[0m”sync_code &gt;&gt; $LOGecho -e “\\033[46;37m ############ Template $TIME end ############ \\033[m” &gt;&gt; $LOGecho ‘ ‘ &gt;&gt; $LOGNotice &nbsp; 邮件通知脚本，能够在自定义的时间周期内获取到最新代码时 能发送邮件到指定的邮箱，能够自动提醒管理人员知悉这一动作。 #!/usr/bin/env python#-*- coding: UTF-8 -*-import smtplibimport os,sysfrom email.mime.text import MIMEText mailto_list=[‘guomaoqiu@gmail.com‘]mail_host=’smtp.qq.com’mail_port=’25’mail_user=’2399447849’mail_pass=’xxxxxx’mail_postfix=’qq.com’ filename = “/tmp/template.log” fo = open(filename,”rb”)filecon = fo.read();str1 = “{0}“.format(filecon) def send_mail(to_list,sub,content): me=”Code Sync Notice【Template】”+”&lt;”+mail_user+”@”+mail_postfix+”&gt;” msg = MIMEText(content,_subtype=’html’,_charset=’utt-8’) msg[‘Subject’] = sub msg[‘From’]=me msg[‘to’]=”;”.join(to_list) try: s = smtplib.SMTP() s.connect(mail_host) s.login(mail_user,mail_pass) s.sendmail(me,to_list,msg.as_string()) s.close() return True except Exception, e: print str(e) return Falseif __name__==’__main__‘: if send_mail(mailto_list,”New Files Are Added”,str1): print “发送成功” else: print “发送失败”","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"},{"name":"Shell","slug":"Python/Shell","permalink":"https://blog.sctux.cc/categories/Python/Shell/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"},{"name":"Shell","slug":"Python/Shell","permalink":"https://blog.sctux.cc/categories/Python/Shell/"}]},{"title":"Python发送邮件(邮件内容从文件读入)","slug":"python-e5-8f-91-e9-80-81-e9-82-ae-e4-bb-b6-e9-82-ae-e4-bb-b6-e5-86-85-e5-ae-b9-e4-bb-8e-e6-96-87-e4-bb-b6-e8-af-bb-e5-85-a5","date":"2015-09-10T14:50:49.000Z","updated":"2025-09-01T01:59:08.877Z","comments":true,"path":"2015/09/10/python-e5-8f-91-e9-80-81-e9-82-ae-e4-bb-b6-e9-82-ae-e4-bb-b6-e5-86-85-e5-ae-b9-e4-bb-8e-e6-96-87-e4-bb-b6-e8-af-bb-e5-85-a5/","permalink":"https://blog.sctux.cc/2015/09/10/python-e5-8f-91-e9-80-81-e9-82-ae-e4-bb-b6-e9-82-ae-e4-bb-b6-e5-86-85-e5-ae-b9-e4-bb-8e-e6-96-87-e4-bb-b6-e8-af-bb-e5-85-a5/","excerpt":"有个需求，利用python脚本发出来的邮件的内容是从文件读取的。并且保持这个文件原有的格式。 #!/usr/bin/env python#-*- coding: UTF-8 -*-import smtplib,os,sysfrom email.mime.text import MIMEText mailto_list=[‘guomaoqiu@gmail.com‘]mail_host=’smtp.qq.com’mail_port=’25’mail_user=’2399447849’mail_pass=’xxxxxxxxx’ #-&gt;你懂的…mail_postfix=’qq.com’ filename = “/tmp/test.log” #-&gt;将要读取作为邮件内容的文件 fo = open(filename,”rb”)filecon = fo.read();str1 = “{0}“.format(filecon) def send_mail(to_list,sub,content): me=”Code Sync Notice”+”&lt;”+mail_user+”@”+mail_postfix+”&gt;” msg = MIMEText(content,_subtype=’html’,_charset=’utf-8’) msg[‘Subject’] = sub msg[‘From’]=me msg[‘to’]=”;”.join(to_list) try: s = smtplib.SMTP() s.connect(mail_host) s.login(mail_user,mail_pass) s.sendmail(me,to_list,msg.as_string()) s.close() return True except Exception, e: print str(e) return Falseif __name__==’__main__‘: if send_mail(mailto_list,”New Flies Are Added”,str1): print “发送成功” else: print “发送失败” 执行结果： 之前出现一个问题，就是我在文件读取之后写成这样的，未对读入的字符串格式格式化。 fo = open(filename,”rb”)filecon = fo.read();str1 = filecon","text":"有个需求，利用python脚本发出来的邮件的内容是从文件读取的。并且保持这个文件原有的格式。 #!/usr/bin/env python#-*- coding: UTF-8 -*-import smtplib,os,sysfrom email.mime.text import MIMEText mailto_list=[‘guomaoqiu@gmail.com‘]mail_host=’smtp.qq.com’mail_port=’25’mail_user=’2399447849’mail_pass=’xxxxxxxxx’ #-&gt;你懂的…mail_postfix=’qq.com’ filename = “/tmp/test.log” #-&gt;将要读取作为邮件内容的文件 fo = open(filename,”rb”)filecon = fo.read();str1 = “{0}“.format(filecon) def send_mail(to_list,sub,content): me=”Code Sync Notice”+”&lt;”+mail_user+”@”+mail_postfix+”&gt;” msg = MIMEText(content,_subtype=’html’,_charset=’utf-8’) msg[‘Subject’] = sub msg[‘From’]=me msg[‘to’]=”;”.join(to_list) try: s = smtplib.SMTP() s.connect(mail_host) s.login(mail_user,mail_pass) s.sendmail(me,to_list,msg.as_string()) s.close() return True except Exception, e: print str(e) return Falseif __name__==’__main__‘: if send_mail(mailto_list,”New Flies Are Added”,str1): print “发送成功” else: print “发送失败” 执行结果： 之前出现一个问题，就是我在文件读取之后写成这样的，未对读入的字符串格式格式化。 fo = open(filename,”rb”)filecon = fo.read();str1 = filecon 而这样发出来的邮件结果却是这样的 发送 HTML 形式的邮件，需要 email.mime.text 中的 MIMEText 的 _subtype 设置为 html，并且 _text 的内容应该为 HTML 形式 可参考：http://m.blog.chinaunix.net/uid-23802873-id-4477364.html","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"}]},{"title":"Python链接mysql数据库的一些操作","slug":"python-e9-93-be-e6-8e-a5mysql-e6-95-b0-e6-8d-ae-e5-ba-93-e7-9a-84-e4-b8-80-e4-ba-9b-e6-93-8d-e4-bd-9c","date":"2015-09-10T14:01:18.000Z","updated":"2025-09-01T01:59:08.932Z","comments":true,"path":"2015/09/10/python-e9-93-be-e6-8e-a5mysql-e6-95-b0-e6-8d-ae-e5-ba-93-e7-9a-84-e4-b8-80-e4-ba-9b-e6-93-8d-e4-bd-9c/","permalink":"https://blog.sctux.cc/2015/09/10/python-e9-93-be-e6-8e-a5mysql-e6-95-b0-e6-8d-ae-e5-ba-93-e7-9a-84-e4-b8-80-e4-ba-9b-e6-93-8d-e4-bd-9c/","excerpt":"一、实验规划 1、首先实验之前，我们需要提前创建一个数据库TESTDB，然后授一个用户test管理该数据库,这里我设置的密码是test123; 2、要用python链接数据库需要用到第三方法模块MySQLdb; 3、利用python脚本对数据库做一些简单操作; 二、安装第三方模块MySQLdb. 1、获取模块 wget https://pypi.python.org/packages/source/M/MySQL-python/MySQL-python-1.2.5.zip 2、解压，安装 unzip MySQL-python-1.2.5.zipcd MySQL-python-1.2.5python setup.py buildpython setup.py install 3、检查安装情况，进入python命令行执行 import，如果不报出任何信息说明成功安装并且导入成功 demo@demo:~$ pythonPython 2.7.9 (default, Apr 2 2015, 15:33:21)[GCC 4.9.2] on linux2Type “help”, “copyright”, “credits” or “license” for more information. &gt; import MySQLdb 三、对mysql数据库的一些操作 1、连接数据库TESTDB并查看数据库的版本 #!/usr/bin/env python#-*- coding: UTF-8 -*-import MySQLdb db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”)","text":"一、实验规划 1、首先实验之前，我们需要提前创建一个数据库TESTDB，然后授一个用户test管理该数据库,这里我设置的密码是test123; 2、要用python链接数据库需要用到第三方法模块MySQLdb; 3、利用python脚本对数据库做一些简单操作; 二、安装第三方模块MySQLdb. 1、获取模块 wget https://pypi.python.org/packages/source/M/MySQL-python/MySQL-python-1.2.5.zip 2、解压，安装 unzip MySQL-python-1.2.5.zipcd MySQL-python-1.2.5python setup.py buildpython setup.py install 3、检查安装情况，进入python命令行执行 import，如果不报出任何信息说明成功安装并且导入成功 demo@demo:~$ pythonPython 2.7.9 (default, Apr 2 2015, 15:33:21)[GCC 4.9.2] on linux2Type “help”, “copyright”, “credits” or “license” for more information. &gt; import MySQLdb 三、对mysql数据库的一些操作 1、连接数据库TESTDB并查看数据库的版本 #!/usr/bin/env python#-*- coding: UTF-8 -*-import MySQLdb db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”) cursor = db.cursor() cursor.execute(“SELECT VERSION()”) data = cursor.fetchone() print “Database version: %s “ % data db.close() 执行结果如下： demo@demo:/python_learn$ python mysql_connect.pyDatabase version: 5.6.25-0ubuntu0.15.04.1demo@demo:/python_learn$ 2、链接到数据库TESTDB并且创建一个张表 #!/usr/bin/env python#-*- coding: UTF-8 -*-import MySQLdb db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”) cursor = db.cursor() cursor.execute(“DROP TABLE IF EXISTS EMPLOYEE”) sql = “””CREATE TABLE `EMPLOYEE` ( `FIRST_NAME` varchar(20) DEFAULT NULL, `LAST_NAME` varchar(20) DEFAULT NULL, `AGE` int(11) DEFAULT NULL, `SEX` varchar(20) DEFAULT NULL, `INCOME` varchar(20) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=latin1 “”” cursor.execute(sql) db.close() 执行结果我们通过链接到数据库去查看 mysql&gt; use TESTDB;Database changedmysql&gt; SHOW TABLES;+——————+| Tables_in_TESTDB |+——————+| EMPLOYEE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+——————+1 row in set (0.00 sec) mysql&gt;mysql&gt; DESC EMPLOYEE;+————+————-+——+—–+———+——-+| Field&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Null | Key | Default | Extra |+————+————-+——+—–+———+——-+| FIRST_NAME | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || LAST_NAME&nbsp; | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || AGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | int(11)&nbsp;&nbsp;&nbsp;&nbsp; | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || SEX&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; || INCOME&nbsp;&nbsp;&nbsp;&nbsp; | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |+————+————-+——+—–+———+——-+5 rows in set (0.00 sec) mysql&gt; select * from EMPLOYEE; #目前该表没有任何数据Empty set (0.00 sec) mysql&gt; 3、连接到数据库，并在EMPLOYEE表中插入数据 #!/usr/bin/env python#-*- coding: UTF-8 -*-import MySQLdb db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”) cursor = db.cursor() sql = “””INSERT INTO EMPLOYEE(FIRST_NAME, LAST_NAME,AGE,SEX,INCOME) VALUES (‘Guo’,’Maoqiuo’,’24’,’M’,’1991’)””” try: cursor.execute(sql) db.commit()except: db.rollback() db.close() 执行结果如下 mysql&gt; select * from EMPLOYEE;+————+———–+——+——+——–+| FIRST_NAME | LAST_NAME | AGE | SEX | INCOME |+————+———–+——+——+——–+| Guo | Maoqiuo | 24 | M | 1991 |+————+———–+——+——+——–+1 row in set (0.00 sec) mysql&gt; 4、链接数据库使用select语句进行查询，这里实验使用条件查询，那我需要多一点数据才行，那我修改上面的那个脚本然后多执行几遍即可，看到的结果如下 mysql&gt; select * from EMPLOYEE;+————+———–+——+——+——–+| FIRST_NAME | LAST_NAME | AGE | SEX | INCOME |+————+———–+——+——+——–+| Guo | Maoqiuo | 24 | M | 1991 || Bruce | Li | 25 | M | 1989 || Fan | bingbing | 28 | F | 1988 || Jack | cheng | 45 | M | 1968 || joy | jiang | 26 | F | 1977 |+————+———–+——+——+——–+5 rows in set (0.00 sec) 使用条件查询 #!/usr/bin/env python#-*- coding: UTF-8 -*-import MySQLdb db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”) cursor = db.cursor() sql = “SELECT * FROM EMPLOYEE WHERE INCOME &gt; ‘%d’” % (1988) try: cursor.execute(sql) results = cursor.fetchall() for row in results: fname = row\\[0\\] lname = row\\[1\\] age = row\\[2\\] sex = row\\[3\\] income = row\\[4\\] print \"Fname=%s, Lanme=%s, Age=%d, Sex=%s, Income=%s\\\\n\" % (fname,lname,age,sex,income) except: print “Error: unable to fecth data” db.close() 执行结果如下 deamon@deamon:~/python_learn$ python mysql_select.pyFname=Guo, Lanme=Maoqiuo, Age=24, Sex=M, Income=1991 Fname=Bruce, Lanme=Li, Age=25, Sex=M, Income=1989 4、连接到数据库，并更新在EMPLOYEE表的数据，将性别为M的人员在年龄上加2 #!/usr/bin/env python#-*- coding: UTF-8 -*-import MySQLdb db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”) cursor = db.cursor() sql = “UPDATE EMPLOYEE SET AGE = AGE +2 WHERE SEX = ‘%c’ “ % (‘M’) try: cursor.execute(sql) db.commit()except: db.rollback() print “Error” db.close() 执行后的结果 mysql&gt; select * from TESTDB.EMPLOYEE;+————+———–+——+——+——–+| FIRST_NAME | LAST_NAME | AGE | SEX | INCOME |+————+———–+——+——+——–+| Guo | Maoqiuo | 26 | M | 1991 || Bruce | Li | 27 | M | 1989 || Fan | bingbing | 28 | F | 1988 || Jack | cheng | 47 | M | 1968 || joy | jiang | 26 | F | 1977 |+————+———–+——+——+——–+5 rows in set (0.00 sec) mysql&gt;","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"}],"tags":[{"name":"mysqldb","slug":"mysqldb","permalink":"https://blog.sctux.cc/tags/mysqldb/"},{"name":"python-mysql","slug":"python-mysql","permalink":"https://blog.sctux.cc/tags/python-mysql/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://blog.sctux.cc/categories/Python/"}]},{"title":"Seafile-个人/团队/公司专属私有文件同步服务 (云存储网盘)","slug":"seafile-e4-b8-aa-e4-ba-ba-e5-9b-a2-e9-98-9f-e5-85-ac-e5-8f-b8-e4-b8-93-e5-b1-9e-e7-a7-81-e6-9c-89-e6-96-87-e4-bb-b6-e5-90-8c-e6-ad-a5-e6-9c-8d-e5-8a-a1-e4-ba-91-e5-ad-98-e5-82-a8-e7-bd-91","date":"2015-09-08T15:38:37.000Z","updated":"2025-09-01T01:59:08.922Z","comments":true,"path":"2015/09/08/seafile-e4-b8-aa-e4-ba-ba-e5-9b-a2-e9-98-9f-e5-85-ac-e5-8f-b8-e4-b8-93-e5-b1-9e-e7-a7-81-e6-9c-89-e6-96-87-e4-bb-b6-e5-90-8c-e6-ad-a5-e6-9c-8d-e5-8a-a1-e4-ba-91-e5-ad-98-e5-82-a8-e7-bd-91/","permalink":"https://blog.sctux.cc/2015/09/08/seafile-e4-b8-aa-e4-ba-ba-e5-9b-a2-e9-98-9f-e5-85-ac-e5-8f-b8-e4-b8-93-e5-b1-9e-e7-a7-81-e6-9c-89-e6-96-87-e4-bb-b6-e5-90-8c-e6-ad-a5-e6-9c-8d-e5-8a-a1-e4-ba-91-e5-ad-98-e5-82-a8-e7-bd-91/","excerpt":"一、简介： seafile 是由国内团队开发的一个国际化的开源云存储软件项目，目前据说已有10万左右的用户，典型的机构用户包括比利时的皇家自然科学博物馆、德国的 Wuppertal 气候、能源研究所等等。Seafile 同时提供了客户端和服务器端软件免费下载，任何个人或公司都能搭建属于自己的私有文件同步服务。Seafile 的服务器端支持 Linux&nbsp;、Windows 以及树莓派平台，客户端除了网页版之外，还支持 Mac、Linux、Windows 三个桌面平台以及 Android 和 iOS 两个移动平台。你可以利用局域网里的一台电脑作为服务器，搭建一个仅局域网内部能访问的专有云存储服务，也能将 Seafile 部署到互联网上的诸如阿里云、Linode 或任何 VPS、独立服务器上，实现一个私人的在线云存储服务。 同时，Seafile 支持用户同时使用多个同步服务器，而且能够在不同服务器之间切换。比如，用户可以用公司服务器来同步工作文件，用个人服务器与朋友共享私人文件，两者互不干扰，私密性也可保证。而且，由于 Seafile 是开源的项目，因此相对来说数据的私密性还是有保障的，起码不必担心有什么看不见的后门。具体介绍可以参见seafile官方文档介绍。 下面我们就开始在局域网内搭建一台私有的云存储。 二、安装seafile服务器 1、安装前准备： 请确保服务器 上面安装了以下模块(这款软件是用Django+Python2.7所开发的，所以要保证服务器上面的python版本) python 2.7 python-setuptools python-imaging (这个是python的一个库，网上不好找到，下载地址http://www.pythonware.com/products/pil/) python-mysqldb 2、获取服务端软件包 wget https://bintray.com/artifact/download/seafile-org/seafile/seafile-server\\_4.3.2\\_x86-64.tar.gz 3、解压安装 tar -xf seafile-server_4.3.2_x86-64.tar.gzmkdir /home/seafilemv seafile-server-4.3.2 /home/seafile/seafile-servercd /home/seafile/seafile-server ./setup-seafile-mysql.sh #运行安装脚本并回答预设问题 如果你的系统中没有安装上面的某个软件，那么 Seafile初始化脚本会提醒你安装相应的软件包. ./setup-seafile-mysql.shChecking python on this machine … #-&gt;执行这个脚本之后会去检查之前说的那些依赖包，如果安装包不完整将会提示你某个软件包未安装 Checking python module: setuptools … Done. Checking python module: python-imaging … Done. Checking python module: python-mysqldb … Done. -----------------------------------------------------------------This script will guide you to setup your seafile server using MySQL.Make sure you have read seafile server manual at https://github.com/haiwen/seafile/wiki Press ENTER to continue #-&gt;这里我们需要一下回车，再继续-----------------------------------------------------------------","text":"一、简介： seafile 是由国内团队开发的一个国际化的开源云存储软件项目，目前据说已有10万左右的用户，典型的机构用户包括比利时的皇家自然科学博物馆、德国的 Wuppertal 气候、能源研究所等等。Seafile 同时提供了客户端和服务器端软件免费下载，任何个人或公司都能搭建属于自己的私有文件同步服务。Seafile 的服务器端支持 Linux&nbsp;、Windows 以及树莓派平台，客户端除了网页版之外，还支持 Mac、Linux、Windows 三个桌面平台以及 Android 和 iOS 两个移动平台。你可以利用局域网里的一台电脑作为服务器，搭建一个仅局域网内部能访问的专有云存储服务，也能将 Seafile 部署到互联网上的诸如阿里云、Linode 或任何 VPS、独立服务器上，实现一个私人的在线云存储服务。 同时，Seafile 支持用户同时使用多个同步服务器，而且能够在不同服务器之间切换。比如，用户可以用公司服务器来同步工作文件，用个人服务器与朋友共享私人文件，两者互不干扰，私密性也可保证。而且，由于 Seafile 是开源的项目，因此相对来说数据的私密性还是有保障的，起码不必担心有什么看不见的后门。具体介绍可以参见seafile官方文档介绍。 下面我们就开始在局域网内搭建一台私有的云存储。 二、安装seafile服务器 1、安装前准备： 请确保服务器 上面安装了以下模块(这款软件是用Django+Python2.7所开发的，所以要保证服务器上面的python版本) python 2.7 python-setuptools python-imaging (这个是python的一个库，网上不好找到，下载地址http://www.pythonware.com/products/pil/) python-mysqldb 2、获取服务端软件包 wget https://bintray.com/artifact/download/seafile-org/seafile/seafile-server\\_4.3.2\\_x86-64.tar.gz 3、解压安装 tar -xf seafile-server_4.3.2_x86-64.tar.gzmkdir /home/seafilemv seafile-server-4.3.2 /home/seafile/seafile-servercd /home/seafile/seafile-server ./setup-seafile-mysql.sh #运行安装脚本并回答预设问题 如果你的系统中没有安装上面的某个软件，那么 Seafile初始化脚本会提醒你安装相应的软件包. ./setup-seafile-mysql.shChecking python on this machine … #-&gt;执行这个脚本之后会去检查之前说的那些依赖包，如果安装包不完整将会提示你某个软件包未安装 Checking python module: setuptools … Done. Checking python module: python-imaging … Done. Checking python module: python-mysqldb … Done. -----------------------------------------------------------------This script will guide you to setup your seafile server using MySQL.Make sure you have read seafile server manual at https://github.com/haiwen/seafile/wiki Press ENTER to continue #-&gt;这里我们需要一下回车，再继续----------------------------------------------------------------- What is the name of the server? It will be displayed on the client.3 - 15 letters or digits[ server name ] seafile-server #-&gt;设置我们的服务器名称 What is the ip or domain of the server?For example: www.mycompany.com, 192.168.1.101 #-&gt;服务器的IP[ This server’s ip or domain ] 192.168.2.108 Where do you want to put your seafile data?Please use a volume with enough free space[ default “/home/seafile/seafile-data” ] /data/seafile #-&gt;存储的位置我这里选择的是/data/seafile Which port do you want to use for the seafile fileserver?[ default “8082” ] #-&gt;默认的工作端口 -------------------------------------------------------Please choose a way to initialize seafile databases:-------------------------------------------------------#-&gt;如果选择1, 你需要提供根密码. 脚本程序会创建数据库和用户。#-&gt;如果选择2, ccnet/seafile/seahub 数据库应该已经被你（或者其他人）提前创建。[1] Create new ccnet/seafile/seahub databases[2] Use existing ccnet/seafile/seahub databases [ 1 or 2 ] 1 What is the host of mysql server?[ default “localhost” ] What is the port of mysql server?[ default “3306” ] What is the password of the mysql root user?[ root password ] #-&gt;这里需要mysql的root权限进行创建库的操作 verifying password of user root … done Enter the name for mysql user of seafile. It would be created if not exists.[ default “root” ] Enter the database name for ccnet-server:[ default “ccnet-db” ] Enter the database name for seafile-server:[ default “seafile-db” ] Enter the database name for seahub:[ default “seahub-db” ] #-&gt;以上三个库名都用默认的 以上步骤完成后将会出现一下提示信息，说明我们安装就成功啦 三、启动seafile服务和seahub网站 1、在/home/seafile/seafile-server目录下执行 #-&gt;启动seafile./seafile.sh start # 启动 Seafile 服务 #-&gt;启动seahub./seahub.sh start # 启动 Seahub 网站 （默认运行在8000端口上） 注意：你第一次启动 seahub 时，seahub.sh 脚本会提示你创建一个 seafile 管理员帐号，就像下面这样 这个管理账号必须是你自己取注册的任意邮箱地址，登陆管理使用这个地址，我这里用的是gmail. 服务启动后, 打开浏览器并输入这个地址 http://192.168.2.108:8000 输入账号密码就会被重定向到登陆页面. 输入你在安装 Seafile 时提供的用户名和密码后，你会进入 Myhome 页面，新建资料库. &nbsp; 至此，seafile私有与存储共享平台已经部署完毕了。 下面我们可以去下载一个客户端安装上 我这里使用的Ubuntu下的客户端 &nbsp; 因为之前就把我的库下载到了本地，你可以在本地库添加文件，然后点击同步就会同步到服务器端啦，就像这样(如果你不使用客户端的话，可以使用网页版) 当然他的客户端不只是在linux(ubuntu) 上面才有哦， 客户端在： 移动端有：Android，Ios 桌面端有：windows、Linux、Mac 服务端在： Windows、Linux、树莓派 任何平台的浏览器。 下面是我手机端的截图，服务器跟手机的wifi是在一个局域网内的 &nbsp; 既然说了是团队、企业或者个人使用，那每个人都要有个账号，那我们需要新建一个账号才行，因为我使用的seafile server是社区版，有许多的功能都不能使用，但是我觉得作为一个小团队，或者个人使用再适合不过了。 下面我们创建一个账号，在win上面登陆，必须要让你都服务器连接网络，并且注册使用seafile的用户必须使用email地址作为登陆账号，这个账号必须是存在可登录的email 我们只需要将这个资料库下载到本地就可了，添加文件，然后右击“我的资料库”就可以看到同步到服务器了。 然后我们可以在本地资料库中指定某个文件生成一个下载链接，可以分享给我的小伙伴下载。 我看了一下服务器上面的数据存储目录，数据传入到服务器端就类似于Map Reduce这种软件架构将数据切成了很多很多份，然后创建索引保存到数据库，获取数据时拿到索引，最后根据索引重组数据，再返回结果给客户端。 上图中0211e*********这个目录是用户seafileshare的资料库，文件上传到服务器是它就将一个完整的文件拆散了，在服务端我们找不到一个完整的软件包。这种存储方式跟mogilefs有些类似。 感觉社区版还是有些局限性，但是对于个人，一个团队我个人觉得完全足够啦。 好了如果需要了解更多，可以到官网了解使用：http://www.seafile.com","categories":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}],"tags":[],"keywords":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}]},{"title":"学习SaltStack小记—第三章《Pillar使用方法》","slug":"saltstack-pillar-de-shi-xian-fang-shi","date":"2015-08-18T15:00:00.000Z","updated":"2025-09-01T01:59:08.873Z","comments":true,"path":"2015/08/18/saltstack-pillar-de-shi-xian-fang-shi/","permalink":"https://blog.sctux.cc/2015/08/18/saltstack-pillar-de-shi-xian-fang-shi/","excerpt":"1.要启用pillar，首先要修改master中的配置[root@salt-master pillar]# vim /etc/salt/master pillar_roots: &nbsp;base： &nbsp; &nbsp;- /salt/pillar 2.重启master[root@salt-master pillar]# systemctl restar salt-master 3.是存放在master端，默认位置/srv/pillar，需要新建目录。和salt sls类似，也是需要top.slsmkdir /srv/pillar 4.在/srv/pillar目录中创建一个top.sls文件[root@salt-master pillar]# vim top.sls #内容如下： base: &nbsp;'node2.example.com': &nbsp;#指定的主机 &nbsp; &nbsp;- sc #调用sc模版中的值 5.在/srv/pillar中创建一个sc.sls文件[root@salt-master pillar]# vim sc.sls #内容为键值对,k, v格式的 name: guomaoqiu age: 22 language: chineses","text":"1.要启用pillar，首先要修改master中的配置[root@salt-master pillar]# vim /etc/salt/master pillar_roots: &nbsp;base： &nbsp; &nbsp;- /salt/pillar 2.重启master[root@salt-master pillar]# systemctl restar salt-master 3.是存放在master端，默认位置/srv/pillar，需要新建目录。和salt sls类似，也是需要top.slsmkdir /srv/pillar 4.在/srv/pillar目录中创建一个top.sls文件[root@salt-master pillar]# vim top.sls #内容如下： base: &nbsp;'node2.example.com': &nbsp;#指定的主机 &nbsp; &nbsp;- sc #调用sc模版中的值 5.在/srv/pillar中创建一个sc.sls文件[root@salt-master pillar]# vim sc.sls #内容为键值对,k, v格式的 name: guomaoqiu age: 22 language: chineses 好啦，此时pillar定义好了 6.执行saltutil.sync_grains #刷新pillar的值[root@salt-master pillar]# salt 'node2.exmaple.com' saltutil.refresh_pillar #看下结果： [root@salt-master pillar]# salt \"node2.example.com\" pillar.data node2.example.com: &nbsp; &nbsp;---------- &nbsp; &nbsp;age: &nbsp; &nbsp; &nbsp; &nbsp;22 &nbsp; &nbsp;language: &nbsp; &nbsp; &nbsp; &nbsp;chineses &nbsp; &nbsp;name: &nbsp; &nbsp; &nbsp; &nbsp;guomaoqiu 7.由上可见在node2这台主机上已经有pillar值啦，只是这个值是保存在master端的；那问题来了,如何使用jinja模板来调用grains或者是pillar的值呢？看下面：[root@salt-master salt]# pwd /srv/salt [root@salt-master salt]# cat top.sls base: &nbsp;'node2.example.com': &nbsp; &nbsp;- test.test [root@salt-master salt]# cat test/test.sls /tmp/test1.conf: &nbsp;file.managed: &nbsp; &nbsp;- source: salt://test/test1.conf.jinja &nbsp; &nbsp;- template: jinja &nbsp;#调用jinja模板 看一下模板文件：test1.conf.jinja ￼ 该模板文件都调用了grains及pillar的值 8.推到node2上面去：￼ 9.在minion端查看￼ 看到了红色框住的就是通过调用grains,pillar值生成的。","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"学习SaltStack小记—第二章《编写配置及应用》","slug":"e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-ba-8c-e7-ab-a0-e3-80-8a-e7-bc-96-e5-86-99-e9-85-8d-e7-bd-ae-e5-8f-8a-e5-ba-94-e7-94-a8-e3-80-8b","date":"2015-08-13T12:17:38.000Z","updated":"2025-09-01T01:59:08.980Z","comments":true,"path":"2015/08/13/e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-ba-8c-e7-ab-a0-e3-80-8a-e7-bc-96-e5-86-99-e9-85-8d-e7-bd-ae-e5-8f-8a-e5-ba-94-e7-94-a8-e3-80-8b/","permalink":"https://blog.sctux.cc/2015/08/13/e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-ba-8c-e7-ab-a0-e3-80-8a-e7-bc-96-e5-86-99-e9-85-8d-e7-bd-ae-e5-8f-8a-e5-ba-94-e7-94-a8-e3-80-8b/","excerpt":"上次记录了一下salt的安装和配置，下面记录一下如何去编写一个配置并且应用到minion. SaltStack默认的配置文件路径在/srv/slat下，如果没有这个目录就新建个。如果不确定可以打开Master的主配置文件看下。 在master主配置文件中，打开一下三行的注释，这就是salt的默认配置路径。 这里默认配置文件是注释了的，如果需要直接取消注释就行了。 切记，每次修改了配置文件都要重启服务 下面以两个需求为例子进行学习: a.在minion上面安装httpd服务，并且启动之 b.自建index.html文件，在master端应用配置完成后直接访问minion的http服务，在浏览器中显示我自定的index.html文件内容 额，刚开始学习这个的时候配置文件编写很是一件头疼的事情； 一、配置文件说明 1.引导配置文件 top.sls saltstack的第一个配置文件默认为top.sls，是位于/srv/salt/目录下的，当然这不是绝对的，可以修改配置文件的哦。这个文件是必须要有的。 例如： [root@saltstack-node1 salt]# more top.slsbase: ###可以理解为仓库 ‘node0.example.com’: ###对象名，就是针对那些minion，这里可以支持组什么的，就是各种匹配吧 - apache ###资源名(自定义) 说明：资源名需要在/srv/salt目录新建文件为apache.sls，注意：所有生效的配置文件都是以sls结尾的。 2.资源配置文件 /srv/salt/apache.sls 这个就是上面top中指定的资源名 [root@saltstack-node1 salt]# cat apache.slsapache-service: ###ID,自定义 pkg.installed: ###使用包管理的insalled方法 - names: ###名称 - httpd ###软件包名 - httpd-devel service.running: ###模块名称，安装好之后要启动它 - name: httpd ###启动什么 - enable: True ###开机自启动 - reload: True ###监视这个文件，如果有变动，我们要重载服务 - watch: - file: /var/www/html/index.html ###文件路径 - require: ###应用前提，httpd这个软件包安装好之后，才执行service这个模块 - pkg: httpd file.managed: ###模块名称，使用file模块的managed这个方法，目的，文件管理 - name: /var/www/html/index.html ###minion端的文件具体路径 - source: salt://index.html ###源文件 - user: apache ###这个文件的一些属性 - group: root - mode: 644 - backup: minion ###改变之前备份 - require: ###同上，执行完上面的，在执行这个 - pkg: httpd 说明：源文件的位置就是在/srv/salt目录下新建一个文件index.html。 我这里内容自定义： echo “SaltStack“ &gt; index.html 配置编写完毕最后的目录结构如下： 二、应用于客户端 经过上面Master端编写配置文件后，此时已经生效。Minions只要同步资源即可实现上面的两个需求。那么就是如何来同步？ 这里有两种方式： 第一种手动在Master上执行推送命令，第二种是在Minion设置时间间隔自动想Master端同步最新的资源。 1、Master手动同步 在Master上执行# salt ‘*’ state.highstate ###意思是向所有Minions推送最新资源 2、Minion自动同步 在/etc/salt/minion配置文件中增加 schedule:highstate:function: state.highstateseconds: 30 ###意思是每30秒向Master同步一次最新资源，也可设置分-mintus、小时-hours","text":"上次记录了一下salt的安装和配置，下面记录一下如何去编写一个配置并且应用到minion. SaltStack默认的配置文件路径在/srv/slat下，如果没有这个目录就新建个。如果不确定可以打开Master的主配置文件看下。 在master主配置文件中，打开一下三行的注释，这就是salt的默认配置路径。 这里默认配置文件是注释了的，如果需要直接取消注释就行了。 切记，每次修改了配置文件都要重启服务 下面以两个需求为例子进行学习: a.在minion上面安装httpd服务，并且启动之 b.自建index.html文件，在master端应用配置完成后直接访问minion的http服务，在浏览器中显示我自定的index.html文件内容 额，刚开始学习这个的时候配置文件编写很是一件头疼的事情； 一、配置文件说明 1.引导配置文件 top.sls saltstack的第一个配置文件默认为top.sls，是位于/srv/salt/目录下的，当然这不是绝对的，可以修改配置文件的哦。这个文件是必须要有的。 例如： [root@saltstack-node1 salt]# more top.slsbase: ###可以理解为仓库 ‘node0.example.com’: ###对象名，就是针对那些minion，这里可以支持组什么的，就是各种匹配吧 - apache ###资源名(自定义) 说明：资源名需要在/srv/salt目录新建文件为apache.sls，注意：所有生效的配置文件都是以sls结尾的。 2.资源配置文件 /srv/salt/apache.sls 这个就是上面top中指定的资源名 [root@saltstack-node1 salt]# cat apache.slsapache-service: ###ID,自定义 pkg.installed: ###使用包管理的insalled方法 - names: ###名称 - httpd ###软件包名 - httpd-devel service.running: ###模块名称，安装好之后要启动它 - name: httpd ###启动什么 - enable: True ###开机自启动 - reload: True ###监视这个文件，如果有变动，我们要重载服务 - watch: - file: /var/www/html/index.html ###文件路径 - require: ###应用前提，httpd这个软件包安装好之后，才执行service这个模块 - pkg: httpd file.managed: ###模块名称，使用file模块的managed这个方法，目的，文件管理 - name: /var/www/html/index.html ###minion端的文件具体路径 - source: salt://index.html ###源文件 - user: apache ###这个文件的一些属性 - group: root - mode: 644 - backup: minion ###改变之前备份 - require: ###同上，执行完上面的，在执行这个 - pkg: httpd 说明：源文件的位置就是在/srv/salt目录下新建一个文件index.html。 我这里内容自定义： echo “SaltStack“ &gt; index.html 配置编写完毕最后的目录结构如下： 二、应用于客户端 经过上面Master端编写配置文件后，此时已经生效。Minions只要同步资源即可实现上面的两个需求。那么就是如何来同步？ 这里有两种方式： 第一种手动在Master上执行推送命令，第二种是在Minion设置时间间隔自动想Master端同步最新的资源。 1、Master手动同步 在Master上执行# salt ‘*’ state.highstate ###意思是向所有Minions推送最新资源 2、Minion自动同步 在/etc/salt/minion配置文件中增加 schedule:highstate:function: state.highstateseconds: 30 ###意思是每30秒向Master同步一次最新资源，也可设置分-mintus、小时-hours 下面是在master端执行的结果： 当然在推送资源时我们可以先测试一下，命令： salt ‘node0.example.com’ state.highstate test=True 上面的推送已经完成，下面验证结果： 至此上面两个需求已经实现了。这也是salt最基本的用法及功能啦。后面继续学习。 看来搞个lamp或者lnmp平台也可以轻而易举的实现了。","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"学习SaltStack小记---第一章《安装及简单测试》","slug":"e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-b8-80-e7-ab-a0-e3-80-8a-e5-ae-89-e8-a3-85-e5-8f-8a-e7-ae-80-e5-8d-95-e6-b5-8b-e8-af-95-e3-80-8b","date":"2015-08-12T08:22:39.000Z","updated":"2025-09-01T01:59:08.931Z","comments":true,"path":"2015/08/12/e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-b8-80-e7-ab-a0-e3-80-8a-e5-ae-89-e8-a3-85-e5-8f-8a-e7-ae-80-e5-8d-95-e6-b5-8b-e8-af-95-e3-80-8b/","permalink":"https://blog.sctux.cc/2015/08/12/e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-b8-80-e7-ab-a0-e3-80-8a-e5-ae-89-e8-a3-85-e5-8f-8a-e7-ae-80-e5-8d-95-e6-b5-8b-e8-af-95-e3-80-8b/","excerpt":"一、什么是saltstack Salt，,一种全新的基础设施管理方式，部署轻松，在几分钟内可运行起来，扩展性好，很容易管理上万台服务器，速度够快，服务器之间秒级通讯。salt底层采用动态的连接总线, 使其可以用于编配, 远程执行, 配置管理等等. SaltStack 是继 Puppet、Chef 之后新出现的配置管理及远程执行工具， 目前，SaltStack 正得到越来越多的瞩目。与 Puppet 相比，SaltStack 没有那么笨重，感觉较为轻量；不像 Puppet 有 一套自己的 DSL 用来写配置，SaltStack 使用 YAML 作为配置文件格式，写 起来既简单又容易，同时也便于动态生成；此外，SaltStack 在远程执行命令 时的速度非常快，也包含丰富的模块。 官方站点：http://www.saltstack.com/ 官方文档：http://docs.saltstack.com/ 中文站点：http://www.saltstack.cn/ 中文手册：http://docs.saltstack.cn/ 中文wiki：http://wiki.saltstack.cn/doc &nbsp; 二、安装 这里采用的是从EPEL源直接yum安装，当然我们要更新epel源： # rpm -Uvh http://dl.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm SaltStack-Master（主服务器） # yum -y install salt-master SaltStack-Minion（从服务器） # yum -y install salt-minion &nbsp; 三、简单配置及使用 1、基本信息： Master端：192.168.1.21 saltstack-node1.example.comMinion端：192.168.1.22 saltstack-node2.example.com 2、启动命令： Master端：/etc/init.d/salt-master {start|stop|status|restart|condrestart|reload}Minion端：/etc/init.d/salt-minion {start|stop|status|restart|condrestart|reload} 3.主配置文件： Master端：/etc/salt/masterMinion端：/etc/salt/master 4.配置Master端 修改监听地址默认为监听所有 sed -ie ‘s/^#.*interface:.*/\\ interface: 192.168.1.111/g’ /etc/salt/master###注：如果使用主机名，请绑定hosts","text":"一、什么是saltstack Salt，,一种全新的基础设施管理方式，部署轻松，在几分钟内可运行起来，扩展性好，很容易管理上万台服务器，速度够快，服务器之间秒级通讯。salt底层采用动态的连接总线, 使其可以用于编配, 远程执行, 配置管理等等. SaltStack 是继 Puppet、Chef 之后新出现的配置管理及远程执行工具， 目前，SaltStack 正得到越来越多的瞩目。与 Puppet 相比，SaltStack 没有那么笨重，感觉较为轻量；不像 Puppet 有 一套自己的 DSL 用来写配置，SaltStack 使用 YAML 作为配置文件格式，写 起来既简单又容易，同时也便于动态生成；此外，SaltStack 在远程执行命令 时的速度非常快，也包含丰富的模块。 官方站点：http://www.saltstack.com/ 官方文档：http://docs.saltstack.com/ 中文站点：http://www.saltstack.cn/ 中文手册：http://docs.saltstack.cn/ 中文wiki：http://wiki.saltstack.cn/doc &nbsp; 二、安装 这里采用的是从EPEL源直接yum安装，当然我们要更新epel源： # rpm -Uvh http://dl.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm SaltStack-Master（主服务器） # yum -y install salt-master SaltStack-Minion（从服务器） # yum -y install salt-minion &nbsp; 三、简单配置及使用 1、基本信息： Master端：192.168.1.21 saltstack-node1.example.comMinion端：192.168.1.22 saltstack-node2.example.com 2、启动命令： Master端：/etc/init.d/salt-master {start|stop|status|restart|condrestart|reload}Minion端：/etc/init.d/salt-minion {start|stop|status|restart|condrestart|reload} 3.主配置文件： Master端：/etc/salt/masterMinion端：/etc/salt/master 4.配置Master端 修改监听地址默认为监听所有 sed -ie ‘s/^#.*interface:.*/\\ interface: 192.168.1.111/g’ /etc/salt/master###注：如果使用主机名，请绑定hosts 5.配置minion端 客户端minion的配置，vim /etc/salt/minion,添加master IP地址和minion ID号，ID建议用主机名来配置,然后开启日志功能，为了不重复操作我下面写了一个脚本，当我们配置minion时能快速搞定： #/bin/bash#desc : salt client setttingsread -p “Input Mster IP: “ MIP ###指定masterIPCOMM1=”sed -i \\“s/#master: salt/master: $MIP/\\“ /etc/salt/minion”eval $COMM1 echo ‘ ‘read -p “Input Minion ID: “ MID ###修改minion IDCOMM2=”sed -i \\“s/#id:.*/id: $MID/\\“ /etc/salt/minion”eval $COMM2### 开启日志sed -i “s@#log_file: /var/log/salt/minion@log_file: /var/log/salt/minion@1” /etc/salt/minionsed -i ‘488d’ /etc/salt/minion sed -i “s@#key_logfile: /var/log/salt/key@key_logfile: /var/log/salt/key@” /etc/salt/minion echo ‘ ‘echo -e “\\033[42;37m Salt-minion Information: \\033[0m”echo ‘##############################’sed -e ‘/^#/d;/^$/d’ /etc/salt/minionecho ‘##############################’echo ‘ ‘###启动服务/etc/init.d/salt-minion start###只要有新的minion客户端添加进来我们运行这个脚本就可快速完成minion端的配置啦. 6.修改完毕我们就可以重启服务啦. &nbsp; 四、开始和Master端通信 1.Minion第一次与Master端通信会向其申请签发证书。在Master端执行# salt-key 即可看到已经签发、待签发以及拒绝的Minion列表。上面那台Minion刚刚申请，所以现在在Master上执行命令后SaltStack- Minion01将会出现在待签发的列表中。 对于批量管理，首先建议的就是打开Master端的自动签发证书，要不然就得在服务器上执行命令# salt-key -a Minion-ID或者# salt-key -A，但相比都比较麻烦,于是我们可执行以下命令将其自动签发 sed -ie ‘s/^#auto_accept:.*/\\auto_accept: True/g’ /etc/salt/master/etc/init.d/salt-master restart 此时再次执行# salt-key 将会看到saltstack-node2.example.com出现在已签发的列表中。 &nbsp; 2.简单测试其功能： 目前为止，Master端已经可以和Minion正常通信。 首先测试下，向saltstack-node2.example.com执行一条查看内存的命令 这里就先不解释命令的含义啦,后面在细说。 至此，简单测试完毕 当然SaltStack的功能不止于此，下篇正式看下他是如何工作以及各种配置的编写.","categories":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"saltstack","slug":"saltstack","permalink":"https://blog.sctux.cc/tags/saltstack/"}],"keywords":[{"name":"自动化运维","slug":"自动化运维","permalink":"https://blog.sctux.cc/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"}]},{"title":"Linux系统初始化脚本","slug":"linux-e7-b3-bb-e7-bb-9f-e5-88-9d-e5-a7-8b-e5-8c-96","date":"2015-06-19T12:06:31.000Z","updated":"2025-09-01T01:59:08.921Z","comments":true,"path":"2015/06/19/linux-e7-b3-bb-e7-bb-9f-e5-88-9d-e5-a7-8b-e5-8c-96/","permalink":"https://blog.sctux.cc/2015/06/19/linux-e7-b3-bb-e7-bb-9f-e5-88-9d-e5-a7-8b-e5-8c-96/","excerpt":"我们在安装好操作系统后一般都需要对系统做一些针对于自己环境的情况做一下系统初始化，这个我们一般用shell脚本来跑一遍，把相关的参数、配置调整一下就好了。使用下面这个脚本就可以初始化我们系统啦。 #!/bin/bash#Version 1.9#Auth: guomaoqiu#For CentOS_mini#Made on 2015-06-19 echo “ “echo “#############################################################################”echo “# Initialize for the CentOS 6.4/6.5 mini_installed. #”echo “# #”echo “# Please affirm this OS connected net already before running this script ! #”echo “# #”echo “# must first connect to the Internet. because yum need it. #”echo “#############################################################################”echo “ “ format() { #echo -e “\\033[42;37m ########### Finished ########### \\033[0m” sleep 5 echo -e “\\033[42;37m ########### Finished ########### \\033[0m” echo “ “ } ########################################################################### Set time 时区/时间同步设置echo “Set time.” /bin/cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&gt; /dev/nullyum -y install ntpdate &amp;&gt; /dev/nullntpdate 0.centos.pool.ntp.org &amp;&gt; /dev/nullhwclock -wformat #####################################################################################Set network 网卡开机自启动#sed -i “s/ONBOOT=no/ONBOOT=yes/g” /etc/sysconfig/network-scripts/ifcfg-eth0#sed -i “s/NM_CONTROLLED=no/NM_CONTROLLED=no/g”/etc/sysconfig/network-scripts/ifcfg-eth0#/etc/init.d/network restart &amp;&gt;/dev/null#echo “==================================” &gt;&gt; $LOG#format ########################################################################### Create Log 创建该脚本运行记录日志echo “Create log file.”DATE1=`date +”%F %H:%M”`LOG=/var/log/sysinitinfo.logecho $DATE1 &gt;&gt; $LOGecho “——————————————“ &gt;&gt; $LOGecho “For CentOS_mini” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGecho “Set timezone is Shanghai” &gt;&gt; $LOGecho “Finished ntpdate” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Disabled Selinux 禁用Selinuxecho “Disabled SELinux.”sed -i ‘s/^SELINUX=enforcing/SELINUX=disabled/‘ /etc/sysconfig/selinuxecho “==================================================”echo “Disabled SELinux.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Stop iptables 禁用iptablesecho “Stop iptables.”service iptables stop &amp;&gt; /dev/nullchkconfig –level 35 iptables offecho “Stop iptables.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Disable ipv6 禁用IPV6echo “Disable ipv6.”echo “alias net-pf-10 off” &gt;&gt; /etc/modprobe.confecho “alias ipv6 off” &gt;&gt; /etc/modprobe.confchkconfig –level 35 ip6tables offecho “Disable ipv6.”&gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat","text":"我们在安装好操作系统后一般都需要对系统做一些针对于自己环境的情况做一下系统初始化，这个我们一般用shell脚本来跑一遍，把相关的参数、配置调整一下就好了。使用下面这个脚本就可以初始化我们系统啦。 #!/bin/bash#Version 1.9#Auth: guomaoqiu#For CentOS_mini#Made on 2015-06-19 echo “ “echo “#############################################################################”echo “# Initialize for the CentOS 6.4/6.5 mini_installed. #”echo “# #”echo “# Please affirm this OS connected net already before running this script ! #”echo “# #”echo “# must first connect to the Internet. because yum need it. #”echo “#############################################################################”echo “ “ format() { #echo -e “\\033[42;37m ########### Finished ########### \\033[0m” sleep 5 echo -e “\\033[42;37m ########### Finished ########### \\033[0m” echo “ “ } ########################################################################### Set time 时区/时间同步设置echo “Set time.” /bin/cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&gt; /dev/nullyum -y install ntpdate &amp;&gt; /dev/nullntpdate 0.centos.pool.ntp.org &amp;&gt; /dev/nullhwclock -wformat #####################################################################################Set network 网卡开机自启动#sed -i “s/ONBOOT=no/ONBOOT=yes/g” /etc/sysconfig/network-scripts/ifcfg-eth0#sed -i “s/NM_CONTROLLED=no/NM_CONTROLLED=no/g”/etc/sysconfig/network-scripts/ifcfg-eth0#/etc/init.d/network restart &amp;&gt;/dev/null#echo “==================================” &gt;&gt; $LOG#format ########################################################################### Create Log 创建该脚本运行记录日志echo “Create log file.”DATE1=`date +”%F %H:%M”`LOG=/var/log/sysinitinfo.logecho $DATE1 &gt;&gt; $LOGecho “——————————————“ &gt;&gt; $LOGecho “For CentOS_mini” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGecho “Set timezone is Shanghai” &gt;&gt; $LOGecho “Finished ntpdate” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Disabled Selinux 禁用Selinuxecho “Disabled SELinux.”sed -i ‘s/^SELINUX=enforcing/SELINUX=disabled/‘ /etc/sysconfig/selinuxecho “==================================================”echo “Disabled SELinux.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Stop iptables 禁用iptablesecho “Stop iptables.”service iptables stop &amp;&gt; /dev/nullchkconfig –level 35 iptables offecho “Stop iptables.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Disable ipv6 禁用IPV6echo “Disable ipv6.”echo “alias net-pf-10 off” &gt;&gt; /etc/modprobe.confecho “alias ipv6 off” &gt;&gt; /etc/modprobe.confchkconfig –level 35 ip6tables offecho “Disable ipv6.”&gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ###########################################################################Set history commands 设置命令历史记录参数echo “Set history commands.”echo “HISTFILESIZE=4000” &gt;&gt; /etc/bashrcecho “HISTSIZE=4000” &gt;&gt; /etc/bashrcecho “HISTTIMEFORMAT=’%F/%T’” &gt;&gt; /etc/bashrcsource /etc/bashrcecho “==================================” &gt;&gt; $LOGformat ########################################################################### Epel 升级epel源echo “Install epel”rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm &amp;&gt; /dev/nullsed -i “s/^#base/base/g” /etc/yum.repos.d/epel.reposed -i “s/^mirr/#mirr/g” /etc/yum.repos.d/epel.repoecho “==================================================”echo “Install epel” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ###########################################################################Yum install Development tools 安装开发包组及必备软件echo “Install Development tools(It will be a moment,wait……)”yum groupinstall -y “Development tools” &amp;&gt; /dev/nullyum groupinstall -y “Server Platform Development” &amp;&gt; /dev/nullyum groupinstall -y “Desktop Platform Development” &amp;&gt; /dev/nullyum groupinstall -y “chinese-support” &amp;&gt;/dev/nullfor I in bind-utils lrzsz wget gcc gcc-c++ vim htop ;do yum install -y $Idoneecho “==================================================”echo “Install Development tools” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ########################################################################### Yum update bash and openssl 升级bash/opensslecho “Update bash and openssl”yum -y update bash &amp;&gt; /dev/nullyum -y update openssl &amp;&gt; /dev/nullecho “==================================================”echo “Update bash and openssl” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Set ssh 设置ssh登录策略echo “Disabled EmptyPassword.”echo “Disabled SSH-DNS.”echo “Set timeout is 6m.”sed -i “s/^#PermitEmptyPasswords/PermitEmptyPasswords/“ /etc/ssh/sshd_configsed -i “s/^#LoginGraceTime 2m/LoginGraceTime 6m/“ /etc/ssh/sshd_configecho “UseDNS no” &gt;&gt; /etc/ssh/sshd_configecho “==================================================”echo “Disabled EmptyPassword.” &gt;&gt; $LOGecho “Disabled SSH-DNS.” &gt;&gt; $LOGecho “Set timeout is 6m.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Set default init 3 设置系统默认初始化echo “Default init 3.”sed -i ‘s/^id:5:initdefault:/id:3:initdefault:/‘ /etc/inittabecho “==================================================”echo “Default init 3.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Stop Service 关闭不必要的服务echo “Some services are turned off now.”for SER in rpcbind postfix portreserve certmonger mdmonitor blk-availability lvm2-monitor udev-post cups dhcpd firstboot gpm haldaemon hidd ip6tables ipsec isdn kudzu lpd mcstrans messagebus microcode_ctl netfs nfs nfslock nscd acpid anacron apmd atd auditd autofs avahi-daemon avahi-dnsconfd bluetooth cpuspeed pcscd portmap readahead_early restorecond rpcgssd rpcidmapd rstatd sendmail setroubleshoot snmpd sysstat xfs xinetd yppasswdd ypserv yum-updatesd do /sbin/chkconfig –list $SER &amp;&gt; /dev/null if [ $? -eq 0 ] then chkconfig –level 35 $SER off echo “$SER” &gt;&gt; $LOG fi doneecho “==================================================”echo “Some services are turned off now:” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Del unnecessary users 删除不必要的用户echo “Del unnecessary users.”for USERS in adm lp sync shutdown halt mail news uucp operator games gopher do grep $USERS /etc/passwd &amp;&gt;/dev/null if [ $? -eq 0 ] then userdel $USERS &amp;&gt; /dev/null echo $USERS &gt;&gt; $LOG fi doneecho “==================================================”echo “Del unnecessary users.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Del unnecessary groups 删除不必要的用户组echo “Del unnecessary groups.”for GRP in adm lp mail news uucp games gopher mailnull floppy dip pppusers popusers slipusers daemon do grep $GRP /etc/group &amp;&gt; /dev/null if [ $? -eq 0 ] then groupdel $GRP &amp;&gt; /dev/null echo $GRP &gt;&gt; $LOG fi doneecho “==================================================”echo “Del unnecessary groups.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Disabled reboot by keys ctlaltdelete 禁用ctlaltdelete重启功能echo “Disabled reboot by keys ctlaltdelete”sed -i ‘s/^exec/#exec/‘ /etc/init/control-alt-delete.confecho “==================================================”echo “Disabled reboot by keys ctlaltdelete” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Set ulimit 设置文件句柄数echo “Set ulimit 1000000”echo “* soft nofile 1000000” &gt;&gt; /etc/security/limits.confecho “* hard nofile 1000000” &gt;&gt; /etc/security/limits.confecho “* soft nproc 102400” &gt;&gt; /etc/security/limits.confecho “* hard nproc 102400” &gt;&gt; /etc/security/limits.confsed -i ‘s/102400/1000000/‘ /etc/security/limits.d/90-nproc.confecho “==================================================”echo “Set ulimit 1000000” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Set login message 设置登录时显示的信息echo “Set login message.”echo “This is not a public Server” &gt; /etc/issueecho “This is not a public Server” &gt; /etc/redhat-releaseecho “==================================================”echo “Set login message.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Record SUID and SGID filesDATE2=`date +”%F”`echo “Record SUID and SGID files.”echo “SUID — “ &gt; /var/log/SuSg_”$DATE2”.logfind / -path ‘/proc’ -prune -o -perm -4000 &gt;&gt; /var/log/SuSg_”$DATE2”.logecho “—————————————————— “ &gt;&gt; /var/log/SuSg_”$DATE2”.logecho “SGID — “ &gt;&gt; /var/log/SuSg_”$DATE2”.logfind / -path ‘/proc’ -prune -o -perm -2000 &gt;&gt; /var/log/SuSg_”$DATE2”.logecho “==================================================”echo “Record SUID and SGID.” &gt;&gt; $LOGecho “Record is in /var/log/SuSg_”$DATE2”.log” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Disabled crontab send mail 禁用执行任务计划时向root发送邮件echo “Disable crontab send mail.”sed -i ‘s/^MAILTO=root/MAILTO=””/‘ /etc/crontabsed -i ‘s/^mail\\.\\*/mail\\.err/‘ /etc/rsyslog.confecho “==================================================”echo “Disable crontab send mail.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Set ntp client 设置时间服务客户端echo “Set ntp client.”SED() { cp -p /etc/ntp.conf /etc/ntp.conf.bak sed -i ‘/^server/d’ /etc/ntp.conf sed -i ‘/^includefile/ i\\server 0.centos.pool.ntp.org iburst’ /etc/ntp.conf sed -i ‘/0.centos.pool.ntp.org/ a\\server 1.centos.pool.ntp.org iburst’ /etc/ntp.conf sed -i ‘/1.centos.pool.ntp.org/ a\\server 2.centos.pool.ntp.org iburst’ /etc/ntp.conf sed -i ‘/2.centos.pool.ntp.org/ a\\server 3.centos.pool.ntp.org iburst’ /etc/ntp.conf chkconfig –level 35 ntpd on &amp;&gt; /dev/null echo “==================================================”}rpm -q ntp &amp;&gt; /dev/nullif [ $? -eq 0 ] then SED else yum -y install ntp &amp;&gt; /dev/null SEDfiecho “Set ntp client.” &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat ############################################################################ Set sysctl.conf 设置内核参数echo “Set sysctl.conf”#web应用中listen函数的backlog默认会将内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认是511，所以必须调整,一般调整为2048echo “net.core.somaxconn = 2048” &gt;&gt; /etc/sysctl.conf echo “net.core.rmem_default = 262144” &gt;&gt; /etc/sysctl.confecho “net.core.wmem_default = 262144” &gt;&gt; /etc/sysctl.confecho “net.core.rmem_max = 16777216” &gt;&gt; /etc/sysctl.confecho “net.core.wmem_max = 16777216” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_rmem = 4096 4096 16777216” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_wmem = 4096 4096 16777216” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_mem = 786432 2097152 3145728” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_max_syn_backlog = 16384” &gt;&gt; /etc/sysctl.confecho “net.core.netdev_max_backlog = 20000” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_fin_timeout = 15” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_tw_reuse = 1” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_tw_recycle = 1” &gt;&gt; /etc/sysctl.confecho “net.ipv4.tcp_max_orphans = 131072” &gt;&gt; /etc/sysctl.confecho “net.ipv4.ip_local_port_range = 1024 65535” &gt;&gt; /etc/sysctl.confecho “Set sysctl.conf —- “ &gt;&gt; $LOG/sbin/sysctl -p &gt;&gt; $LOGecho “==================================” &gt;&gt; $LOGformat############################################################################ Doneecho “Finished,You can check infomations in $LOG .”#echo “System will reboot in 60s.”#shutdown -r 1 如果有不对的地方，各位大婶可以提出来，大家一起进步哈；谢谢您的意见。 Download Script: initsystem.sh","categories":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"}],"tags":[],"keywords":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"}]},{"title":"通过 Ulimit 改善系统性能","slug":"e9-80-9a-e8-bf-87-ulimit-e6-94-b9-e5-96-84-e7-b3-bb-e7-bb-9f-e6-80-a7-e8-83-bd","date":"2015-04-23T12:29:29.000Z","updated":"2025-09-01T01:59:08.872Z","comments":true,"path":"2015/04/23/e9-80-9a-e8-bf-87-ulimit-e6-94-b9-e5-96-84-e7-b3-bb-e7-bb-9f-e6-80-a7-e8-83-bd/","permalink":"https://blog.sctux.cc/2015/04/23/e9-80-9a-e8-bf-87-ulimit-e6-94-b9-e5-96-84-e7-b3-bb-e7-bb-9f-e6-80-a7-e8-83-bd/","excerpt":"概述 系统性能一直是一个受关注的话题，如何通过最简单的设置来实现最有效的性能调优，如何在有限资源的条件下保证程序的运作，ulimit 是我们在处理这些问题时，经常使用的一种简单手段。ulimit 是一种 linux 系统的内键功能，它具有一套参数集，用于为由它生成的 shell 进程及其子进程的资源使用设置限制。本文将在后面的章节中详细说明 ulimit 的功能，使用以及它的影响，并以具体的例子来详细地阐述它在限制资源使用方面的影响。 ulimit 的功能和用法 ulimit 功能简述 假设有这样一种情况，当一台 Linux 主机上同时登陆了 10 个人，在系统资源无限制的情况下，这 10 个用户同时打开了 500 个文档，而假设每个文档的大小有 10M，这时系统的内存资源就会受到巨大的挑战。 而实际应用的环境要比这种假设复杂的多，例如在一个嵌入式开发环境中，各方面的资源都是非常紧缺的，对于开启文件描述符的数量，分配堆栈的大小，CPU 时间，虚拟内存大小，等等，都有非常严格的要求。资源的合理限制和分配，不仅仅是保证系统可用性的必要条件，也与系统上软件运行的性能有着密不可分的联系。这时，ulimit 可以起到很大的作用，它是一种简单并且有效的实现资源限制的方式。 ulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制：所创建的内核文件的大小、进程数据块的大小、Shell 进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell 进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。 作为临时限制，ulimit 可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。而对于长期的固定限制，ulimit 命令语句又可以被添加到由登录 shell 读取的文件中，作用于特定的 shell 用户。 在下面的章节中，将详细介绍如何使用 ulimit 做相应的资源限制。 如何使用 ulimit ulimit 通过一些参数选项来管理不同种类的系统资源。在本节，我们将讲解这些参数的使用。 ulimit 命令的格式为：ulimit [options] [limit] 具体的 options 含义以及简单示例可以参考以下表格。 表 1. ulimit 参数说明 选项 [options] 含义 例子 -H 设置硬资源限制，一旦设置不能增加。 ulimit – Hs 64；限制硬资源，线程栈大小为 64K。 -S 设置软资源限制，设置后可以增加，但是不能超过硬资源设置。","text":"概述 系统性能一直是一个受关注的话题，如何通过最简单的设置来实现最有效的性能调优，如何在有限资源的条件下保证程序的运作，ulimit 是我们在处理这些问题时，经常使用的一种简单手段。ulimit 是一种 linux 系统的内键功能，它具有一套参数集，用于为由它生成的 shell 进程及其子进程的资源使用设置限制。本文将在后面的章节中详细说明 ulimit 的功能，使用以及它的影响，并以具体的例子来详细地阐述它在限制资源使用方面的影响。 ulimit 的功能和用法 ulimit 功能简述 假设有这样一种情况，当一台 Linux 主机上同时登陆了 10 个人，在系统资源无限制的情况下，这 10 个用户同时打开了 500 个文档，而假设每个文档的大小有 10M，这时系统的内存资源就会受到巨大的挑战。 而实际应用的环境要比这种假设复杂的多，例如在一个嵌入式开发环境中，各方面的资源都是非常紧缺的，对于开启文件描述符的数量，分配堆栈的大小，CPU 时间，虚拟内存大小，等等，都有非常严格的要求。资源的合理限制和分配，不仅仅是保证系统可用性的必要条件，也与系统上软件运行的性能有着密不可分的联系。这时，ulimit 可以起到很大的作用，它是一种简单并且有效的实现资源限制的方式。 ulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制：所创建的内核文件的大小、进程数据块的大小、Shell 进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell 进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。 作为临时限制，ulimit 可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。而对于长期的固定限制，ulimit 命令语句又可以被添加到由登录 shell 读取的文件中，作用于特定的 shell 用户。 在下面的章节中，将详细介绍如何使用 ulimit 做相应的资源限制。 如何使用 ulimit ulimit 通过一些参数选项来管理不同种类的系统资源。在本节，我们将讲解这些参数的使用。 ulimit 命令的格式为：ulimit [options] [limit] 具体的 options 含义以及简单示例可以参考以下表格。 表 1. ulimit 参数说明 选项 [options] 含义 例子 -H 设置硬资源限制，一旦设置不能增加。 ulimit – Hs 64；限制硬资源，线程栈大小为 64K。 -S 设置软资源限制，设置后可以增加，但是不能超过硬资源设置。 ulimit – Sn 32；限制软资源，32 个文件描述符。 -a 显示当前所有的 limit 信息。 ulimit – a；显示当前所有的 limit 信息。 -c 最大的 core 文件的大小， 以 blocks 为单位。 ulimit – c unlimited； 对生成的 core 文件的大小不进行限制。 -d 进程最大的数据段的大小，以 Kbytes 为单位。 ulimit -d unlimited；对进程的数据段大小不进行限制。 -f 进程可以创建文件的最大值，以 blocks 为单位。 ulimit – f 2048；限制进程可以创建的最大文件大小为 2048 blocks。 -l 最大可加锁内存大小，以 Kbytes 为单位。 ulimit – l 32；限制最大可加锁内存大小为 32 Kbytes。 -m 最大内存大小，以 Kbytes 为单位。 ulimit – m unlimited；对最大内存不进行限制。 -n 可以打开最大文件描述符的数量。 ulimit – n 128；限制最大可以使用 128 个文件描述符。 -p 管道缓冲区的大小，以 Kbytes 为单位。 ulimit – p 512；限制管道缓冲区的大小为 512 Kbytes。 -s 线程栈大小，以 Kbytes 为单位。 ulimit – s 512；限制线程栈的大小为 512 Kbytes。 -t 最大的 CPU 占用时间，以秒为单位。 ulimit – t unlimited；对最大的 CPU 占用时间不进行限制。 -u 用户最大可用的进程数。 ulimit – u 64；限制用户最多可以使用 64 个进程。 -v 进程最大可用的虚拟内存，以 Kbytes 为单位。 ulimit – v 200000；限制最大可用的虚拟内存为 200000 Kbytes。 新装的linux默认只有1024，当作负载较大的服务器时，很容易遇到error: too many open files。因此，需要将其改大。 使用 ulimit -n 65535 可即时修改，但重启后就无效了。（注ulimit -SHn 65535 等效 ulimit -n 65535，-S指soft，-H指hard) 有如下三种修改方式： 1.在/etc/rc.local 中增加一行 ulimit -SHn 65535 2.在/etc/profile 中增加一行 ulimit -SHn 65535 3.在/etc/security/limits.conf最后增加如下两行记录 * soft nofile 65535 * hard nofile 65535","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"ulimit","slug":"ulimit","permalink":"https://blog.sctux.cc/tags/ulimit/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"数据库链接超时，可能原因由于空间不足造成","slug":"e6-95-b0-e6-8d-ae-e5-ba-93-e9-93-be-e6-8e-a5-e8-b6-85-e6-97-b6-ef-bc-8c-e5-8f-af-e8-83-bd-e5-8e-9f-e5-9b-a0-e7-94-b1-e4-ba-8e-e7-a9-ba-e9-97-b4-e4-b8-8d-e8-b6-b3-e9-80-a0-e6-88-90","date":"2015-04-15T11:41:42.000Z","updated":"2025-09-01T01:59:08.871Z","comments":true,"path":"2015/04/15/e6-95-b0-e6-8d-ae-e5-ba-93-e9-93-be-e6-8e-a5-e8-b6-85-e6-97-b6-ef-bc-8c-e5-8f-af-e8-83-bd-e5-8e-9f-e5-9b-a0-e7-94-b1-e4-ba-8e-e7-a9-ba-e9-97-b4-e4-b8-8d-e8-b6-b3-e9-80-a0-e6-88-90/","permalink":"https://blog.sctux.cc/2015/04/15/e6-95-b0-e6-8d-ae-e5-ba-93-e9-93-be-e6-8e-a5-e8-b6-85-e6-97-b6-ef-bc-8c-e5-8f-af-e8-83-bd-e5-8e-9f-e5-9b-a0-e7-94-b1-e4-ba-8e-e7-a9-ba-e9-97-b4-e4-b8-8d-e8-b6-b3-e9-80-a0-e6-88-90/","excerpt":"今天开发人员告知，测试服务器数据库链接超时 我登录测试服务器1、检查mysql运行状态，结果：正常 通过执行ss -tunl | grep “3306” 以及 ps aux | grep “mysqld” 来判定2、检查mysql日志，结果：正常 通过查看mysql的错误日志3、由于系统用的是CentOS7 于是乎通过另外一种方式查看mysql的状态systemctl status mysqld可以看出来，虽然连接数据库时出现了点问题，但是mysql 并未停止运行。 解决办法：清理磁盘空间，为mysql数据 所在分区腾出空间。","text":"今天开发人员告知，测试服务器数据库链接超时 我登录测试服务器1、检查mysql运行状态，结果：正常 通过执行ss -tunl | grep “3306” 以及 ps aux | grep “mysqld” 来判定2、检查mysql日志，结果：正常 通过查看mysql的错误日志3、由于系统用的是CentOS7 于是乎通过另外一种方式查看mysql的状态systemctl status mysqld可以看出来，虽然连接数据库时出现了点问题，但是mysql 并未停止运行。 解决办法：清理磁盘空间，为mysql数据 所在分区腾出空间。","categories":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://blog.sctux.cc/tags/mysql/"},{"name":"nospace","slug":"nospace","permalink":"https://blog.sctux.cc/tags/nospace/"}],"keywords":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}]},{"title":"Vsftp-虚拟用户设置不同权限","slug":"vsftp-xu-ni-yong-hu-she-zhi-bu-tong-quan-xian","date":"2015-04-01T22:38:50.000Z","updated":"2025-09-01T01:59:08.872Z","comments":true,"path":"2015/04/02/vsftp-xu-ni-yong-hu-she-zhi-bu-tong-quan-xian/","permalink":"https://blog.sctux.cc/2015/04/02/vsftp-xu-ni-yong-hu-she-zhi-bu-tong-quan-xian/","excerpt":"1.system versionCentOS Linux release 7.2.1511 (Core) 2.install vsftpdyum install -y vsftpd 3.create rootdirmkdir /data/program/ftpdata &amp;&amp; chmod a-w /data/program/ftpdata/ 4.create local user(for map to virtualuser)useradd -s /sbin/nologin -d /data/program/ftpdata ftpuser 5.install db(for auth)yum install -y db*","text":"1.system versionCentOS Linux release 7.2.1511 (Core) 2.install vsftpdyum install -y vsftpd 3.create rootdirmkdir /data/program/ftpdata &amp;&amp; chmod a-w /data/program/ftpdata/ 4.create local user(for map to virtualuser)useradd -s /sbin/nologin -d /data/program/ftpdata ftpuser 5.install db(for auth)yum install -y db* 6.create virtual userscd /etc/vsftpd/ vim virtualuser.txt user1 # this username 1234qwer # this password user2 # this username 123.com # this password 7.create dbdb_load -T -t hash -f virtualuser.txt /etc/vsftpd/virtualuser.db 8.create pam auth filevim /etc/pam.d/vsftpd.vu auth required /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser account required /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser # if your system is centos6.x release so use this configure: auth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser account sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser 9.configure vsftpd.confvim /etc/vsftpd.conf anonymous_enable=NO #allow_writeable_chroot # if your vsftpd version is 3.x local_enable=YES write_enable=YES local_umask=022 dirmessage_enable=YES xferlog_enable=YES connect_from_port_20=YES xferlog_std_format=YES chroot_local_user=YES chroot_list_enable=YES chroot_list_file=/etc/vsftpd/chroot_list listen=NO listen_ipv6=YES pam_service_name=ftpuser userlist_enable=YES tcp_wrappers=YES guest_enable=YES guest_username=ftp01 pam_service_name=vsftpd.vu user_config_dir=/etc/vsftpd/user_conf 10.create each user config dircd /etc/vsftpd/ &amp;&amp; mkdir user_conf &amp;&amp; cd user_conf # for user1 config # this user can upload、wirte、download vim user1 anon_world_readable_only=NO anon_upload_enable=YES anon_mkdir_write_enable=YES anon_other_write_enable=YES /data/program/ftpdata/user1_home/ # virtual user's datadir # for user2 config # this user only can read、download, can't upload vim user2 anon_world_readable_only=NO anon_upload_enable=NO anon_mkdir_write_enable=NO anon_other_write_enable=NO /data/program/ftpdata/user2_home # virtual user's datadir","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"ftp","slug":"ftp","permalink":"https://blog.sctux.cc/tags/ftp/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"Vim Nginx配置文件语法高亮","slug":"vim-nginx-e9-85-8d-e7-bd-ae-e6-96-87-e4-bb-b6-e8-af-ad-e6-b3-95-e9-ab-98-e4-ba-ae","date":"2015-03-09T06:49:06.000Z","updated":"2025-09-01T01:59:08.936Z","comments":true,"path":"2015/03/09/vim-nginx-e9-85-8d-e7-bd-ae-e6-96-87-e4-bb-b6-e8-af-ad-e6-b3-95-e9-ab-98-e4-ba-ae/","permalink":"https://blog.sctux.cc/2015/03/09/vim-nginx-e9-85-8d-e7-bd-ae-e6-96-87-e4-bb-b6-e8-af-ad-e6-b3-95-e9-ab-98-e4-ba-ae/","excerpt":"我们在编辑配置nginx的配置文件时，由于他没有高亮的功能，但是nginx官方是支持这个功能的；要想在编辑配置nginx配置文件的时候高亮语法以降低配置的错误发生率可移执行这个小脚本而到达目的： #!/bin/bashmkdir -p ~/.vim/syntax &amp;&amp; cd ~/.vim/syntaxwget http://www.vim.org/scripts/download\\_script.php?src\\_id=14376 -O nginx.vim &gt;/dev/nullecho “au BufRead,BufNewFile /usr/local/webserver/nginx/conf/* set ft=nginx” &gt; ~/.vim/filetype.vim#其中路径/usr/local/webserver/nginx/conf/*为你的nginx.conf文件路径","text":"我们在编辑配置nginx的配置文件时，由于他没有高亮的功能，但是nginx官方是支持这个功能的；要想在编辑配置nginx配置文件的时候高亮语法以降低配置的错误发生率可移执行这个小脚本而到达目的： #!/bin/bashmkdir -p ~/.vim/syntax &amp;&amp; cd ~/.vim/syntaxwget http://www.vim.org/scripts/download\\_script.php?src\\_id=14376 -O nginx.vim &gt;/dev/nullecho “au BufRead,BufNewFile /usr/local/webserver/nginx/conf/* set ft=nginx” &gt; ~/.vim/filetype.vim#其中路径/usr/local/webserver/nginx/conf/*为你的nginx.conf文件路径","categories":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"}],"tags":[],"keywords":[{"name":"Shell","slug":"Shell","permalink":"https://blog.sctux.cc/categories/Shell/"}]},{"title":"Zabbix 清理过久的历史信息","slug":"zabbix-e6-b8-85-e7-90-86-e8-bf-87-e4-b9-85-e7-9a-84-e5-8e-86-e5-8f-b2-e4-bf-a1-e6-81-af","date":"2014-11-04T08:17:57.000Z","updated":"2025-09-01T01:59:08.954Z","comments":true,"path":"2014/11/04/zabbix-e6-b8-85-e7-90-86-e8-bf-87-e4-b9-85-e7-9a-84-e5-8e-86-e5-8f-b2-e4-bf-a1-e6-81-af/","permalink":"https://blog.sctux.cc/2014/11/04/zabbix-e6-b8-85-e7-90-86-e8-bf-87-e4-b9-85-e7-9a-84-e5-8e-86-e5-8f-b2-e4-bf-a1-e6-81-af/","excerpt":"当我们的zabbix运行时间久了，监控的节点多了，数据信息会增长的很快，想备份里面的数据库时，要浪费大量的时间，zabbix里面最大的表就是历史记录的表了，网上很多人都是写全部清空这些表的数据，其实我们可以按时间来删除里面的历史记录； 里面最大的表是 “history” 和 “history_uint”两个表； zabbix里面的时间是用的时间戳方式记录，我们可以转换一下，然后根据时间戳来删除； 比如要删除2012年的11月25号以前的数据 1、先将标准时间转换为时间戳 # date +%s -d “2012-11-25 00:00:00”1353772800 2、mysql清理数据 mysql&gt; use zabbix;mysql&gt; DELETE FROM `history_uint` WHERE `clock` &lt; 1327939201;mysql&gt; optimize table history_uint; 注：执行过第二行命令之后可能会需要很长的一段时间，中间不要K掉了，否则容易丢失数据的。","text":"当我们的zabbix运行时间久了，监控的节点多了，数据信息会增长的很快，想备份里面的数据库时，要浪费大量的时间，zabbix里面最大的表就是历史记录的表了，网上很多人都是写全部清空这些表的数据，其实我们可以按时间来删除里面的历史记录； 里面最大的表是 “history” 和 “history_uint”两个表； zabbix里面的时间是用的时间戳方式记录，我们可以转换一下，然后根据时间戳来删除； 比如要删除2012年的11月25号以前的数据 1、先将标准时间转换为时间戳 # date +%s -d “2012-11-25 00:00:00”1353772800 2、mysql清理数据 mysql&gt; use zabbix;mysql&gt; DELETE FROM `history_uint` WHERE `clock` &lt; 1327939201;mysql&gt; optimize table history_uint; 注：执行过第二行命令之后可能会需要很长的一段时间，中间不要K掉了，否则容易丢失数据的。","categories":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}],"tags":[],"keywords":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}]},{"title":"LVS+Keepalived","slug":"lvskeepalived","date":"2014-10-31T02:54:30.000Z","updated":"2025-09-01T01:59:08.872Z","comments":true,"path":"2014/10/31/lvskeepalived/","permalink":"https://blog.sctux.cc/2014/10/31/lvskeepalived/","excerpt":"说明： 本文档仅围绕lvs+keepalived如何实现负载均衡、故障剔除、后端realserver健康监测、主备切换邮件通知;而防火墙、网络(路由交换)、后端数据存储、内外网暂未考虑; 一、环境准备： 1.操作系统 CentOS6.4-x86_64 2.软件版本： ipvsadm-1.25-10.el6.x86_64keepalived-1.2.7-3.el6.x86_64httpd-2.2.15-26.el6.centos.x86_64 3.实验拓扑： 4.时间同步： 123456789101112node1:\\[root@node1 ~\\]# ntpdate 203.117.180.36\\[root@node1 ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontabnode2:\\[root@node2 ~\\]# ntpdate 203.117.180.36\\[root@node1 ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontabmaster:\\[root@master ~\\]# ntpdate 203.117.180.36\\[root@master ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontabSlave:\\[root@slave ~\\]# ntpdate 203.117.180.36\\[root@slave ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontab 5.主机名相互解析： 123456789101112node1:\\[root@node1 ~\\]# cat /etc/hosts127.0.0.1&nbsp;&nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4&nbsp;::1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6&nbsp;192.168.254.201&nbsp;&nbsp;&nbsp; node1.test.com&nbsp;&nbsp;&nbsp; node1&nbsp;192.168.254.202&nbsp;&nbsp;&nbsp; node2.test.com&nbsp;&nbsp;&nbsp; node2node2:\\[root@node2 ~\\]# cat /etc/hosts127.0.0.1&nbsp;&nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4&nbsp;::1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6&nbsp;192.168.254.201&nbsp;&nbsp;&nbsp; node1.test.com&nbsp;&nbsp;&nbsp; node1&nbsp;192.168.254.202&nbsp;&nbsp;&nbsp; node2.test.com&nbsp;&nbsp;&nbsp; node2 6.安装yum源：(其他三台主机上面同样执行以下两条命令即可，前提:能上网) 1234node1:\\[root@node1~\\]#rpm -ivh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\\[root@node1 ~\\]# rpm -ivh http://elrepo.org/elrepo-release-6-5.el6.elrepo.noarch.rpm","text":"说明： 本文档仅围绕lvs+keepalived如何实现负载均衡、故障剔除、后端realserver健康监测、主备切换邮件通知;而防火墙、网络(路由交换)、后端数据存储、内外网暂未考虑; 一、环境准备： 1.操作系统 CentOS6.4-x86_64 2.软件版本： ipvsadm-1.25-10.el6.x86_64keepalived-1.2.7-3.el6.x86_64httpd-2.2.15-26.el6.centos.x86_64 3.实验拓扑： 4.时间同步： 123456789101112node1:\\[root@node1 ~\\]# ntpdate 203.117.180.36\\[root@node1 ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontabnode2:\\[root@node2 ~\\]# ntpdate 203.117.180.36\\[root@node1 ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontabmaster:\\[root@master ~\\]# ntpdate 203.117.180.36\\[root@master ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontabSlave:\\[root@slave ~\\]# ntpdate 203.117.180.36\\[root@slave ~\\]# echo \"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36\" &gt;&gt; /etc/crontab 5.主机名相互解析： 123456789101112node1:\\[root@node1 ~\\]# cat /etc/hosts127.0.0.1&nbsp;&nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4&nbsp;::1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6&nbsp;192.168.254.201&nbsp;&nbsp;&nbsp; node1.test.com&nbsp;&nbsp;&nbsp; node1&nbsp;192.168.254.202&nbsp;&nbsp;&nbsp; node2.test.com&nbsp;&nbsp;&nbsp; node2node2:\\[root@node2 ~\\]# cat /etc/hosts127.0.0.1&nbsp;&nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4&nbsp;::1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6&nbsp;192.168.254.201&nbsp;&nbsp;&nbsp; node1.test.com&nbsp;&nbsp;&nbsp; node1&nbsp;192.168.254.202&nbsp;&nbsp;&nbsp; node2.test.com&nbsp;&nbsp;&nbsp; node2 6.安装yum源：(其他三台主机上面同样执行以下两条命令即可，前提:能上网) 1234node1:\\[root@node1~\\]#rpm -ivh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\\[root@node1 ~\\]# rpm -ivh http://elrepo.org/elrepo-release-6-5.el6.elrepo.noarch.rpm *二、web节点安装配置：* 安装web服务并执行realserver.sh,为lo:0绑定VIP地址192.168.254.200，抑制ARP广播; node1/node2配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859\\[root@node1 ~\\]# yum install -y httpd&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;\\[root@node1 ~\\]# echo \"&lt;h1&gt;Rs1.test.com&lt;/h1&gt;\" &gt; /var/www/html/index.html\\[root@node1 ~\\]# service httpd start\\[root@node1 ~\\]# chkconfig httpd on\\[root@node1 ~\\]# mkdir script\\[root@node1 ~\\]# cd script/\\[root@node1 ~\\]# vim script/realserver.sh#-&gt;LVS客户端配置脚本realserver.sh：#!/bin/bash&nbsp;\\# Script to start LVS DR real server.&nbsp;&nbsp;\\# description: LVS DR real server&nbsp;&nbsp;.&nbsp; /etc/rc.d/init.d/functionsVIP=192.168.254.200&nbsp;host=`/bin/hostname`case \"$1\" in&nbsp;start)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Start LVS-DR real server on this machine.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /sbin/ifconfig lo down&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /sbin/ifconfig lo up&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 1 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 2 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /sbin/ifconfig lo:0 $VIP broadcast $VIP netmask 255.255.255.255 up&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /sbin/route add -host $VIP dev lo:0;;&nbsp;stop)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Stop LVS-DR real server loopback device(s).&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /sbin/ifconfig lo:0 down&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_announce;;&nbsp;status)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Status of LVS-DR real server.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; islothere=`/sbin/ifconfig lo:0 | grep $VIP`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; isrothere=\\`netstat -rn | grep \"lo:0\" | grep $VIP\\`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if \\[ ! \"$islothere\" -o ! \"isrothere\" \\];then&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Either the route or the lo:0 device&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # not found.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo \"LVS-DR real server Stopped.\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo \"LVS-DR real server Running.\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fi&nbsp;&nbsp;;;&nbsp;&nbsp;*)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Invalid entry.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo \"$0: Usage: $0 {start|status|stop}\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exit 1&nbsp;&nbsp;;;&nbsp;&nbsp;esac\\[root@node1 ~\\]# chmod +x script/realserver.sh\\[root@node1 ~\\]# ./script/realserver.sh start #-&gt;将该脚本scp到node2节点执行./script/realserver.sh start即可 #-&gt;如果服务器重启，那还需要手动的去执行这个脚本，服务便不可使用了，所以可以将realserver.sh加入到开机启动项中; 1234567\\[root@node1 ~\\]# vim /etc/rc.local/bin/bash /root/script/realserver.sh start\\[root@node1 ~\\]# scp /script/realserver 192.168.254.46:/root/script#-&gt;效果如下：[![2](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/2.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/2.png) [![3](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/3.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/3.png)#-&gt;客户端访问测试：[![4](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/4.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/4.png) *三、LVS-DR-Master/Slave安装配置：* 3.1.master/slave都安装keepalived和ipvsadm : 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677master:\\[root@master ~\\]# yum install -y ipvsadm keepalived\\[root@master ~\\]# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak&nbsp; #-&gt;修改配置文件之前最好将该配置文件备份。避免后续出现问题Slave:\\[root@slave ~\\]# yum install -y ipvsadm keepalived\\[root@slave ~\\]# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak**3.2.****在****Lvs-DR-Master****上面的配置：**\\[root@master ~\\]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {&nbsp;&nbsp; notification_email {&nbsp;&nbsp;&nbsp; 2399447849@qq.com&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#-&gt;#设置报警邮件地址，可以设置多个，每行一个&nbsp;&nbsp; }&nbsp;&nbsp; notification\\_email\\_from root@localhost.localdomain&nbsp;&nbsp; smtp_server 127.0.0.1&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#-&gt;设置smtp server的地址&nbsp;&nbsp; smtp\\_connect\\_timeout 30 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#-&gt;设置连接smtp server的超时时间&nbsp;&nbsp; router\\_id LVS\\_DEVEL&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#-&gt;表示运行keepalived服务器的一个标识。发邮件时显示在邮件主题的信息}vrrp\\_instance VI\\_1 {&nbsp;&nbsp;&nbsp; state MASTER&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;指定keepalived的角色，MASTER表示此主机是主服务器，BACKUP表示此主机是备用服务器&nbsp;&nbsp;&nbsp; interface eth0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;指定HA监测网络的接口&nbsp;&nbsp;&nbsp; virtual\\_router\\_id 60 &nbsp;#-&gt;虚拟路由标识，这个标识是一个数字,同一个vrrp实例使用唯一的标识。即同一vrrp_instance下,MASTER和BACKUP&gt;必须是一致的&nbsp;&nbsp;&nbsp; priority 101&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;定义优先级，数字越大，优先级越高，在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级&nbsp;&nbsp;&nbsp; advert_int 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;设定MASTER与BACKUP负载均衡器之间同步检查的时间间隔，单位是秒&nbsp;&nbsp;&nbsp; authentication {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;设置验证类型和密码&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; auth_type PASS&nbsp;&nbsp;&nbsp; #-&gt;设置验证类型，主要有PASS和AH两种&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; auth\\_pass 1111&nbsp;&nbsp;&nbsp; #-&gt;设置验证密码，在同一个vrrp\\_instance下，MASTER与BACKUP必须使用相同的密码才能正常通信&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; virtual_ipaddress {&nbsp;&nbsp; #-&gt;设置虚拟IP地址，可以设置多个虚拟IP地址，每行一个&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 192.168.254.200&nbsp;&nbsp; #-&gt;客户端通过访问的就是该IP地址&nbsp;&nbsp;&nbsp; }}virtual_server 192.168.254.200 80 {&nbsp; #-&gt;设置虚拟服务器，需要指定虚拟IP地址和服务端口，IP与端口之间用空格隔开&nbsp;&nbsp;&nbsp; delay_loop 6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;设置运行情况检查时间，单位是秒&nbsp;&nbsp;&nbsp; lb_algo rr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;设置负载调度算法，这里设置为rr，即轮询算法&nbsp;&nbsp;&nbsp; lb_kind DR&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;设置LVS实现负载均衡的机制，有NAT、TUN、DR三个模式可选nat_mask 255.255.255.0#persistence_timeout 50&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;会话保持时间，单位是秒。这个选项对动态网页是非常有用的，为集群系统中的session共享提供了一个很好&gt;的解决方案。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;有了这个会话保持功能，用户的请求会被一直分发到某个服务节点，直到超过这个会话的保持时间。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;需要注意的是，这个会话保持时间是最大无响应超时时间，也就是说，用户在操作动态页面时，如果50秒内没有执行任何操作，&nbsp;&nbsp;&nbsp; protocol TCP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;指定转发协议类型，有TCP和UDP两种&nbsp;&nbsp;&nbsp; real_server 192.168.254.45 80&nbsp; {&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;配置服务节点1，需要指定real server的真实IP地址和端口，IP与端口之间用空格隔开&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; weight 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;配置服务节点的权值，权值大小用数字表示，数字越大，权值越高，设置权值大小可以为不同性能的服务器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;分配不同的负载，可以为性能高的服务器设置较高的权值，而为性能较低的服务器设置相对较低的权值，这样才能合理地利用和分配系统资源&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HTTP_GET {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;realserver的状态检测设置部分，单位是秒&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; url {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path /&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; status_code 200&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;状态码定义&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; connect_timeout 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;表示3秒无响应超时&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nb\\_get\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;表示重试次数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delay\\_before\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;表示重试间隔&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; real_server 192.168.254.46 80 {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; weight 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HTTP_GET {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; url {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path /&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; status_code 200&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; connect_timeout 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nb\\_get\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delay\\_before\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; }} #-&gt;根据实验的拓扑可知后端的realserver有两台，所以上面定义了两个realserver的配置 3.3在Lvs-DR-Slave****上面的配置： [root@master ~]# scp /etc/keepalived/keepalived.conf 192.168.254.48:/etc/keepalived/&nbsp; #-&gt;将master上面的配置复制至slave,然后稍作修改：[root@slave ~]# vim /etc/keepalived/keepalived.conf#-&gt;配置文件各个参数上面以解释，这里只对需要修改的作说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566! Configuration File for keepalivedglobal_defs {&nbsp;&nbsp; notification_email {&nbsp;&nbsp;&nbsp; 2399447849@qq.com&nbsp;&nbsp; }&nbsp;&nbsp; notification\\_email\\_from root&nbsp;&nbsp; smtp_server 127.0.0.1&nbsp;&nbsp; smtp\\_connect\\_timeout 30&nbsp;&nbsp; router\\_id LVS\\_DEVEL}&nbsp;vrrp\\_instance VI\\_1 {&nbsp;&nbsp;&nbsp; state BACKUP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;指定该服务器的keepalived角色为BACKUP(备用服务器)&nbsp;&nbsp; interface eth0&nbsp;&nbsp; virtual\\_router\\_id 60&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; priority 100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级&nbsp;&nbsp;&nbsp; advert_int 1&nbsp;&nbsp;&nbsp; authentication {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; auth_type PASS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; auth_pass 1111&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; virtual_ipaddress {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 192.168.254.200&nbsp;&nbsp;&nbsp; }}&nbsp;virtual_server 192.168.254.200 80 {&nbsp;&nbsp;&nbsp; delay_loop 6&nbsp;&nbsp;&nbsp; lb_algo rr&nbsp;&nbsp;&nbsp; lb_kind DR&nbsp;&nbsp; nat_mask 255.255.255.0&nbsp;&nbsp;&nbsp; #persistence_timeout 50protocol TCP&nbsp;&nbsp; real_server 192.168.254.45 80&nbsp; {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; weight 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HTTP_GET {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; url {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path /&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; status_code 200&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; connect_timeout 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nb\\_get\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delay\\_before\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; real_server 192.168.254.46 80 {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; weight 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HTTP_GET {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; url {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path /&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; status_code 200&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; connect_timeout 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nb\\_get\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delay\\_before\\_retry 3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; }} *3.4.启动keepalived (启动过程中观察日志)* 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641master:\\[root@master ~\\]# service keepalived start &amp;&amp; tail -f /var/log/messages......................Oct 29 21:42:14 master Keepalived_healthcheckers\\[31358\\]: Opening file '/etc/keepalived/keepalived.conf'.Oct 29 21:42:14 master Keepalived_healthcheckers\\[31358\\]: Configuration is using : 16384 BytesOct 29 21:42:14 master Keepalived_healthcheckers\\[31358\\]: Using LinkWatch kernel netlink reflector...Oct 29 21:42:14 master Keepalived_healthcheckers\\[31358\\]: Activating healthchecker for service \\[192.168.254.45\\]:80Oct 29 21:42:14 master Keepalived_healthcheckers\\[31358\\]: Activating healthchecker for service \\[192.168.254.46\\]:80Oct 29 21:42:15 master Keepalived\\_vrrp\\[31359\\]: VRRP\\_Instance(VI_1) Transition to MASTER STATEOct 29 21:42:16 master Keepalived\\_vrrp\\[31359\\]: VRRP\\_Instance(VI_1) Entering MASTER STATE&nbsp; #-&gt;主服务器状态Oct 29 21:42:16 master Keepalived\\_vrrp\\[31359\\]: VRRP\\_Instance(VI_1) setting protocol VIPs.Oct 29 21:42:16 master Keepalived\\_vrrp\\[31359\\]: VRRP\\_Instance(VI_1) Sending gratuitous ARPs on eth0 for 192.168.254.200Oct 29 21:42:16 master Keepalived_healthcheckers\\[31358\\]: Netlink reflector reports IP 192.168.254.200 addedOct 29 21:42:21 master Keepalived\\_vrrp\\[31359\\]: VRRP\\_Instance(VI_1) Sending gratuitous ARPs on eth0 for 192.168.254.200......................\\[root@master ~\\]# ipvsadm -L –n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;LVS状态IP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags&nbsp; -\\&gt; RemoteAddress:Port&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Forward Weight ActiveConn InActConnTCP&nbsp; 192.168.254.200:80 rr&nbsp; -\\&gt; 192.168.254.45:80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -\\&gt; 192.168.254.46:80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\[root@master ~\\]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN&nbsp;&nbsp;&nbsp; link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&nbsp;&nbsp;&nbsp; inet 127.0.0.1/8 scope host lo&nbsp;&nbsp;&nbsp; inet6 ::1/128 scope host&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; valid\\_lft forever preferred\\_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER\\_UP&gt; mtu 1500 qdisc pfifo\\_fast state UP qlen 1000&nbsp;&nbsp; link/ether 00:0c:29:5d:7d:94 brd ff:ff:ff:ff:ff:ff&nbsp;&nbsp;&nbsp; inet 192.168.254.47/24 brd 192.168.254.255 scope global eth0&nbsp;&nbsp;&nbsp; inet 192.168.254.200/32 scope global eth0&nbsp;&nbsp;&nbsp; #-&gt;此时VIP在master上面&nbsp;&nbsp;&nbsp; inet6 fe80::20c:29ff:fe5d:7d94/64 scope link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; valid\\_lft forever preferred\\_lft forever\\[root@master ~\\]#Slave:\\[root@slave ~\\]# service keepalived start &amp;&amp; tail -f /var/log/messages......................Oct 29 21:42:34 slave Keepalived_vrrp\\[31389\\]: Opening file '/etc/keepalived/keepalived.conf'.Oct 29 21:42:34 slave Keepalived_vrrp\\[31389\\]: Configuration is using : 62845 BytesOct 29 21:42:34 slave Keepalived_vrrp\\[31389\\]: Using LinkWatch kernel netlink reflector...Oct 29 21:42:34 slave Keepalived\\_vrrp\\[31389\\]: VRRP\\_Instance(VI_1) Entering BACKUP STATE&nbsp; #-&gt;备用服务器状态Oct 29 21:42:34 slave Keepalived_vrrp\\[31389\\]: VRRP sockpool: \\[ifindex(2), proto(112), fd(10,11)\\]Oct 29 21:42:34 slave Keepalived_healthcheckers\\[31388\\]: Opening file '/etc/keepalived/keepalived.conf'.Oct 29 21:42:34 slave Keepalived_healthcheckers\\[31388\\]: Configuration is using : 16384 BytesOct 29 21:42:34 slave Keepalived_healthcheckers\\[31388\\]: Using LinkWatch kernel netlink reflector...Oct 29 21:42:34 slave Keepalived_healthcheckers\\[31388\\]: Activating healthchecker for service \\[192.168.254.45\\]:80Oct 29 21:42:34 slave Keepalived_healthcheckers\\[31388\\]: Activating healthchecker for service \\[192.168.254.46\\]:80......................**3.5.****测试：** [![5](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/5.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/5.png) **3.6.****模拟故障**： (1)停掉node1节点的web服务：\\[root@node1 ~\\]# service httpd stop停止 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@node1 ~\\]#(2)查看一下报警邮件： [![6](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/6.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/6.png) (3)再在前端调度器上查看一下LVS状态： [![7](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/7.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/7.png) 很明显那台出现问题的realserver条目已经被剔除了 (4)恢复node1节点上的web服务：\\[root@node1 ~\\]# service httpd start启动 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@node1 ~\\]#(5) 查看一下报警邮件： [![8](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/8.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/8.png) (6)关闭master上面的keepalived：\\[root@master ~\\]# service keepalived stop停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@master ~\\]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags&nbsp; -\\&gt; RemoteAddress:Port&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Forward Weight ActiveConn InActConn\\[root@master ~\\]#(7)查看slave状态：\\[root@slave ~\\]# ip a..............&nbsp;&nbsp;&nbsp; inet 192.168.254.200/32 scope global eth0&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;可见VIP已经转移到了slave上面；并且通过客户端访问仍然正常！..............&nbsp;\\[root@slave ~\\]# ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags&nbsp; -\\&gt; RemoteAddress:Port&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Forward Weight ActiveConn InActConnTCP&nbsp; 192.168.254.200:80 rr&nbsp; -\\&gt; 192.168.254.45:80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -\\&gt; 192.168.254.46:80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\[root@slave ~\\]#**通过上面的演示现在的****LVS****的高可用即前端负载均衡调度器的高可用，同时实现了对后端****realserver****监控，也实现了后端****realserver****宕机时会给管理员发送邮件；但是目前还面临几个问题：**1. **如果所有的****realserver****都宕机，如何处理，用户打不开就等它打不开，还是友善的提示一下？**2. **怎么完成维护模式****keepalived****切换？**3. **如何在****keepalived****主备切换时向管理员发送邮件？****四、****LVS+Keepalived****后续延伸：** **4.1.****所有****realserver****都宕机如何处理？** 在集群中如果所有real server全部宕机了，客户端访问时就会出现错误页面，这样是很不友好的，我们得提供一个维护页面来提醒用户，服务器正在维护，什么时间可以访问等，下面就来解决一下这个问题。解决方案有两种，一种是提供一台备用的real server当所有的服务器宕机时，提供维护页面，但这样做有点浪费服务器。另一种就是在负载均衡器上提供维护页面，这样是比较靠谱的，也比较常用。下面就来具体操作一下。 (1)在master和slave上面安装httpd\\[root@master ~\\]# yum install -y httpd\\[root@slave ~\\]# yum install -y httpd(2)提供维护页面文件\\[root@master ~\\]# echo \"Oops ... you visit the page does not exist, the server may be maintained?\" &gt; /var/www/html/index.html\\[root@master ~\\]# service httpd start启动 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@slave ~\\]# echo \"Oops ... you visit the page does not exist, the server may be maintained?\" &gt; /var/www/html/index.html\\[root@slave ~\\]# service httpd start启动 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\](3)测试： [![9](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/9.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/9.png) (4)修改master/slave的keepalived配置文件：\\[root@master ~\\]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {notification_email {2399447849@qq.com}notification\\_email\\_from rootsmtp_server 127.0.0.1smtp\\_connect\\_timeout 30router\\_id LVS\\_DEVEL}vrrp\\_instance VI\\_1 {state MASTERinterface eth0 virtual\\_router\\_id 60 priority 101 advert_int 1 authentication { auth_type PASS auth_pass 1111}virtual_ipaddress { 192.168.254.200 }}virtual_server 192.168.254.200 80 {delay_loop 6lb_algo rrlb_kind DRnat_mask 255.255.255.0#persistence_timeout 50protocol TCPreal_server 192.168.254.45 80&amp;nbsp; {weight 1HTTP_GET { url { path / status_code 200}connect_timeout 3nb\\_get\\_retry 3delay\\_before\\_retry 3 }}real_server 192.168.254.46 80 {weight 1HTTP_GET {url {path /status_code 200}connect_timeout 3nb\\_get\\_retry 3delay\\_before\\_retry 3}}sorry_server 127.0.0.1&amp;nbsp;&amp;nbsp; #-&gt;增加该配置参数，slave上面也需要添加，此处略。}(5)关闭所有的realserver web服务,重新启动master/slave 的keepalived: &nbsp;node1:\\[root@node1 ~\\]# service httpd stop停止 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@node1 ~\\]#node2:\\[root@node2 ~\\]# service httpd stop停止 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@node2 ~\\]#master:\\[root@master ~\\]# service keepalived restart停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@master ~\\]#slave:\\[root@slave ~\\]# service keepalived restart停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@slave ~\\]#(6)查看一下LVS状态： [![10](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/10.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/10.png) (7)访问测试： [![11](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/11.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/11.png) **4.2.****如何完成维护模式****keepalived** **切换？** 一般我们在测试主从切换的过程当中要么是手动停止keepalived服务，要么是手动关闭网卡，那还有其他方法实现维护模式的切换，这就是vrrp_script功能； (1)master/slave配置：**(****注****:****这里演示主服务器的配置，添加上去的在****slave****上面也需要添加以红色标注内容****)**\\[root@master ~\\]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {notification_email {2399447849@qq.com}notification\\_email\\_from rootsmtp_server 127.0.0.1smtp\\_connect\\_timeout 30router\\_id LVS\\_DEVEL}vrrp\\_script chk\\_schedown {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;定义vrrp执行脚本&nbsp;&nbsp; script \"\\[ -e /etc/keepalived/down \\] &amp;&amp; exit 1 || exit 0\" &nbsp;#-&gt;查看是否有down文件，有就进入维护模式&nbsp;&nbsp; interval 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;监控间隔时间&nbsp;&nbsp; weight -5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;降低优先级,即priority参数&nbsp;&nbsp; fall 2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;失败次数&nbsp;&nbsp; rise 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #-&gt;成功次数}vrrp\\_instance VI\\_1 {state MASTERinterface eth0virtual\\_router\\_id 60priority 101advert_int 1authentication {auth_type PASSauth_pass 1111}virtual_ipaddress {192.168.254.200}track_script {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#-&gt;脚本追踪&nbsp;&nbsp;&nbsp; chk_schedown&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;#-&gt;上面自定义的vrrp脚本名称 }}virtual_server 192.168.254.200 80 {delay_loop 6lb_algo rrlb_kind DRnat_mask 255.255.255.0#persistence_timeout 50protocol TCP&nbsp;real_server 192.168.254.45 80&nbsp; {weight 1HTTP_GET {url {path /status_code 200}connect_timeout 3nb\\_get\\_retry 3delay\\_before\\_retry 3}}real_server 192.168.254.46 80 {weight 1HTTP_GET {url {path /status_code 200}connect_timeout 3nb\\_get\\_retry 3delay\\_before\\_retry 3}}sorry_server 127.0.0.1}(2)测试： master:\\[root@master keepalived\\]# touch down &nbsp;#-&gt;新建一个down文件，进入维护模式\\[root@master keepalived\\]# ll总用量 4-rw-r--r--. 1 root root&nbsp;&nbsp;&nbsp; 0 10月 30 00:16 down-rw-r--r--. 1 root root 1513 10月 30 00:08 keepalived.conf\\[root@master keepalived\\]# tail -f /var/log/messages..............Oct 30 00:16:43 node3 Keepalived\\_vrrp\\[31993\\]: VRRP\\_Script(chk_schedown) failedOct 30 00:16:44 node3 Keepalived\\_vrrp\\[31993\\]: VRRP\\_Instance(VI_1) Received higher prio advertOct 30 00:16:44 node3 Keepalived\\_vrrp\\[31993\\]: VRRP\\_Instance(VI_1) Entering BACKUP STATEOct 30 00:16:44 node3 Keepalived\\_vrrp\\[31993\\]: VRRP\\_Instance(VI_1) removing protocol VIPs.Oct 30 00:16:44 node3 Keepalived_healthcheckers\\[31992\\]: Netlink reflector reports IP 192.168.254.200 removed&nbsp; #-&gt;该VIP已转移到slave...............\\[root@master keepalived\\]# ip a&nbsp;&nbsp; #-&gt;VIP 已转移到slave1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWNlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00inet 127.0.0.1/8 scope host loinet6 ::1/128 scope hostvalid\\_lft forever preferred\\_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER\\_UP&gt; mtu 1500 qdisc pfifo\\_fast state UP qlen 1000link/ether 00:0c:29:5d:7d:94 brd ff:ff:ff:ff:ff:ffinet 192.168.254.47/24 brd 192.168.254.255 scope global eth0inet6 fe80::20c:29ff:fe5d:7d94/64 scope linkvalid\\_lft forever preferred\\_lft forever\\[root@master keepalived\\]#slave:\\[root@slave keepalived\\]# ip a..............inet 192.168.254.200/32 scope global eth0&nbsp;&nbsp; #-&gt;VIP 已转移过来..............\\[root@slave keepalived\\]#&nbsp; **至此自写监测脚本，完成维护模式切换，已经完成；下面来解决最后一个问题：** **4.3.****如何在****Keepalived****主从切换时向管理员发送通知邮件？** (1)Keepalived通知脚本进阶示例：下面的脚本可以接受选项，其中：-s, --service SERVICE,...：指定服务脚本名称，当状态切换时可自动启动、重启或关闭此服务；-a, --address VIP: 指定相关虚拟路由器的VIP地址；-m, --mode {mm|mb}：指定虚拟路由的模型，mm表示主主，mb表示主备；它们表示相对于同一种服务而方，其VIP的工作类型；-n, --notify {master|backup|fault}：指定通知的类型，即vrrp角色切换的目标角色；-h, --help：获取脚本的使用帮助；#!/bin/bash\\# Author: Tux\\# description: An example of notify script\\# Usage: notify.sh -m|--mode {mm|mb} -s|--service SERVICE1,... -a|--address VIP&amp;nbsp; -n|--notify {master|backup|falut} -h|--help&amp;nbsp;helpflag=0serviceflag=0modeflag=0addressflag=0notifyflag=0contact='2399447849@qq.com'&amp;nbsp;&amp;nbsp; #-&gt;指定联系人;可以有多个，用”,”分隔开来&lt;br&gt;Usage() { echo \"Usage: notify.sh \\[-m|--mode {mm|mb}\\] \\[-s|--service SERVICE1,...\\] &lt;-a|--address VIP&gt; &lt;-n|--notify {master|backup|falut}&gt;\" echo \"Usage: notify.sh -h|--help\"}########################################################################################ParseOptions() { local I=1; if \\[ $# -gt 0 \\]; then while \\[ $I -le $# \\]; do case $1 in -s|--service) \\[ $# -lt 2 \\] &amp;&amp; return 3 serviceflag=1 services=(\\`echo $2|awk -F\",\" '{for(i=1;i&lt;=NF;i++) print $i}'\\`) shift 2 ;; -h|--help) helpflag=1 return 0 shift ;; -a|--address) \\[ $# -lt 2 \\] &amp;&amp; return 3 addressflag=1 vip=$2 shift 2 ;; -m|--mode) \\[ $# -lt 2 \\] &amp;&amp; return 3 mode=$2 shift 2 ;; -n|--notify) \\[ $# -lt 2 \\] &amp;&amp; return 3 notifyflag=1 notify=$2 shift 2 ;; *) echo \"Wrong options...\" Usage return 7 ;; esac done return 0 fi}#workspace=$(dirname $0)RestartService() { if \\[ ${#@} -gt 0 \\]; then for I in $@; do if \\[ -x /etc/rc.d/init.d/$I \\]; then /etc/rc.d/init.d/$I restart else echo \"$I is not a valid service...\" fi done fi}StopService() { if \\[ ${#@} -gt 0 \\]; then for I in $@; do if \\[ -x /etc/rc.d/init.d/$I \\]; then /etc/rc.d/init.d/$I stop else echo \"$I is not a valid service...\" fi done fi}Notify() { mailsubject=\"\\`hostname\\` to be $1: $vip floating\" mailbody=\"\\`date '+%F %H:%M:%S'\\`, vrrp transition, \\`hostname\\` changed to be $1.\" echo $mailbody | mail -s \"$mailsubject\" $contact}\\# Main FunctionParseOptions $@\\[ $? -ne 0 \\] &amp;&amp; Usage &amp;&amp; exit 5\\[ $helpflag -eq 1 \\] &amp;&amp; Usage &amp;&amp; exit 0if \\[ $addressflag -ne 1 -o $notifyflag -ne 1 \\]; then Usage exit 2fimode=${mode:-mb}case $notify in'master') if \\[ $serviceflag -eq 1 \\]; then RestartService ${services\\[*\\]} fi Notify master ;;'backup') if \\[ $serviceflag -eq 1 \\]; then if \\[ \"$mode\" == 'mb' \\]; then StopService ${services\\[*\\]} else RestartService ${services\\[*\\]} fi fi Notify backup ;;'fault') Notify fault ;;*) Usage exit 4 ;;esac(2) 在keepalived.conf配置文件中，其调用方法如下所示：notify\\_master \"/etc/keepalived/notify.sh -n master -a VIP\\_address\"notify\\_backup \"/etc/keepalived/notify.sh -n backup -a VIP\\_address\"notify\\_fault \"/etc/keepalived/notify.sh -n fault -a VIP\\_address\"(3)修改master/slave 的keepalived配置文件：、 &nbsp;master:\\[root@master ~\\]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs {notification_email {2399447849@qq.com}notification\\_email\\_from rootsmtp_server 127.0.0.1smtp\\_connect\\_timeout 30router\\_id LVS\\_DEVEL}&nbsp;vrrp\\_script chk\\_schedown {script \"\\[ -e /etc/keepalived/down \\] &amp;&amp; exit 1 || exit 0\"interval 1weight -5fall 2rise 1}vrrp\\_instance VI\\_1 {state MASTERinterface eth0virtual\\_router\\_id 60priority 101advert_int 1authentication {auth_type PASSauth_pass 1111}virtual_ipaddress {192.168.254.200}track_script { chk_schedown}#-&gt;增加以下三行(注：在slave上面也一样添加这三行，此处略)&nbsp;&nbsp;&nbsp; notify_master \"/etc/keepalived/notify.sh -n master -a 192.168.254.200\"&nbsp;&nbsp;&nbsp; notify_backup \"/etc/keepalived/notify.sh -n backup -a 192.168.254.200\"&nbsp;&nbsp;&nbsp; notify_fault \"/etc/keepalived/notify.sh -n fault -a 192.168.254.200\"}&nbsp;virtual_server 192.168.254.200 80 {delay_loop 6lb_algo rrlb_kind DRnat_mask 255.255.255.0#persistence_timeout 50protocol TCP&nbsp;real_server 192.168.254.45 80&nbsp; {weight 1HTTP_GET { url { path / status_code 200}connect_timeout 3nb\\_get\\_retry 3delay\\_before\\_retry 3}}real_server 192.168.254.46 80 {weight 1HTTP_GET {url {path /status_code 200}connect_timeout 3nb\\_get\\_retry 3delay\\_before\\_retry 3}}sorry_server 127.0.0.1 80}&nbsp; (4)添加脚本： 讲上述的脚本添加至master和slave的/etc/keepalived/目录下(注意权限)：\\[root@master ~\\]# ll /etc/keepalived总用量 8-rw-r--r--. 1 root root 1748 10月 30 17:08 keepalived.conf-rwxr-xr-x. 1 root root 2380 10月 30 00:57 notify.sh\\[root@master ~\\]#&nbsp;#-&gt;复制至slave\\[root@master ~\\]# scp /etc/keepalived/notify.sh 192.168.254.48:/etc/keepalived/(5)测试一下脚本可用性：\\[root@slave keepalived\\]# ./notify.sh --helpUsage: notify.sh \\[-m|--mode {mm|mb}\\] \\[-s|--service SERVICE1,...\\] &lt;-a|--address VIP&gt;&nbsp; &lt;-n|--notify {master|backup|falut}&gt;Usage: notify.sh -h|--help\\[root@slave keepalived\\]# ./notify.sh -m mb -a 2.2.2.2 -n master\\[root@slave keepalived\\]#查看邮件: [![12](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/12.png)](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/12.png) 在模拟故障时重启一下keepalived,以免前面的实验造成影响。 注：现在已经可以成功收到邮件，通知脚本可用； (6)故障模拟： &lt;1&gt;先重启主备keepalived服务master:\\[root@master ~\\]# service keepalived restart停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@master ~\\]#slave:\\[root@slave ~\\]# service keepalived restart停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\[确定\\]正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \\[确定\\]\\[root@slave ~\\]#&lt;2&gt;正常情况下此时VIP在master上面master:\\[root@master ~\\]# ip a..............inet 192.168.254.200/32 scope global eth0..............\\[root@master ~\\]#&lt;3&gt;在master的/etc/keepalived目录下 touch一个文件”down”master:\\[root@master keepalived\\]# touch down&lt;4&gt;观察VIP 转移情况slave:\\[root@slave ~\\]# ip a..............inet 192.168.254.200/32 scope global eth0..............\\[root@slave ~\\]# &lt;5&gt;结果查看—&gt;邮件收取 &lt;6&gt;Client访问测试 从上可以看到，在keepalived主备切换时，不仅能够发送邮件，而且访问服务也没有问题； 至此Lvs+Keepalived的基本应用实验演示完毕！","categories":[{"name":"负载均衡","slug":"负载均衡","permalink":"https://blog.sctux.cc/categories/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"}],"tags":[{"name":"Floating","slug":"Floating","permalink":"https://blog.sctux.cc/tags/Floating/"},{"name":"keepalived","slug":"keepalived","permalink":"https://blog.sctux.cc/tags/keepalived/"},{"name":"lvs","slug":"lvs","permalink":"https://blog.sctux.cc/tags/lvs/"}],"keywords":[{"name":"负载均衡","slug":"负载均衡","permalink":"https://blog.sctux.cc/categories/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"}]},{"title":"Tomcat环境搭建及jsp站点实现","slug":"tomcat-e7-8e-af-e5-a2-83-e6-90-ad-e5-bb-ba-e5-8f-8ajsp-e7-ab-99-e7-82-b9-e5-ae-9e-e7-8e-b0","date":"2014-08-28T03:31:53.000Z","updated":"2025-09-01T01:59:08.877Z","comments":true,"path":"2014/08/28/tomcat-e7-8e-af-e5-a2-83-e6-90-ad-e5-bb-ba-e5-8f-8ajsp-e7-ab-99-e7-82-b9-e5-ae-9e-e7-8e-b0/","permalink":"https://blog.sctux.cc/2014/08/28/tomcat-e7-8e-af-e5-a2-83-e6-90-ad-e5-bb-ba-e5-8f-8ajsp-e7-ab-99-e7-82-b9-e5-ae-9e-e7-8e-b0/","excerpt":"&nbsp; 安装JDK(配置java环境)： 环境：CentOS6.5_x86-64bit 一、先安装JVM 1.使用7版本的jdk(到官网下载相应的软件包到本地http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jre-7u9-oth-JPR) [root@localhost ~]# rpm -ivh jdk-7u9-linux-x64.rpmPreparing… ########################################### [100%]1:jdk ########################################### [100%]Unpacking JAR files…rt.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/rt.packjsse.jar..Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/jsse.packcharsets.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/charsets.packtools.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/lib/tools.packlocaledata.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/ext/localedata.pack#-&gt;以上那些错误可以忽略，不影响jdk到安装和使用 2.安装后生成的文件： [root@localhost ~]# cd /usr/java/[root@localhost java]# lltotal 4lrwxrwxrwx 1 root root 16 May 6 10:58 default -&gt; /usr/java/latestdrwxr-xr-x 10 root root 4096 May 6 10:58 jdk1.7.0_09lrwxrwxrwx 1 root root 21 May 6 10:58 latest -&gt; /usr/java/jdk1.7.0_09 3.修改java环境变量： [root@localhost ~]# vim /etc/profile.d/java.shexport JAVA_HOME=/usr/java/latestexport PATH=$JAVA_HOME/bin:$PATH[root@localhost ~]# . /etc/profile.d/java.sh 4.测试java环境是否可用. [root@localhost java]# java -versionjava version “1.7.0_09”Java(TM) SE Runtime Environment (build 1. 7.0_09-b05)Java HotSpot(TM) 64-Bit Server VM (build 23.5-b02, mixed mode)[root@localhost java]# 此时只是部署好了java环境,而并非运行任何的JAVA程序,而Tomcat就是运行在这个环境上面的JAVA程序 二、安装配置tomcat 1.获取软件包并解压安装： [root@localhost ~]# wget http://apache.fayea.com/apache-mirror/tomcat/tomcat-7/v7.0.55/bin/apache-tomcat-7.0.55.tar.gz[root@localhost ~]# tar -xf apache-tomcat-7.0.55.tar.gz -C /usr/local/[root@localhost ~]# cd /usr/local/[root@localhost local]# ln -sv apache-tomcat-7.0.5 tomcat`tomcat’ -&gt; `apache-tomcat-7.0.55’[root@localhost local]# cd tomcat/[root@localhost tomcat]#","text":"&nbsp; 安装JDK(配置java环境)： 环境：CentOS6.5_x86-64bit 一、先安装JVM 1.使用7版本的jdk(到官网下载相应的软件包到本地http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jre-7u9-oth-JPR) [root@localhost ~]# rpm -ivh jdk-7u9-linux-x64.rpmPreparing… ########################################### [100%]1:jdk ########################################### [100%]Unpacking JAR files…rt.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/rt.packjsse.jar..Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/jsse.packcharsets.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/charsets.packtools.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/lib/tools.packlocaledata.jar…Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/ext/localedata.pack#-&gt;以上那些错误可以忽略，不影响jdk到安装和使用 2.安装后生成的文件： [root@localhost ~]# cd /usr/java/[root@localhost java]# lltotal 4lrwxrwxrwx 1 root root 16 May 6 10:58 default -&gt; /usr/java/latestdrwxr-xr-x 10 root root 4096 May 6 10:58 jdk1.7.0_09lrwxrwxrwx 1 root root 21 May 6 10:58 latest -&gt; /usr/java/jdk1.7.0_09 3.修改java环境变量： [root@localhost ~]# vim /etc/profile.d/java.shexport JAVA_HOME=/usr/java/latestexport PATH=$JAVA_HOME/bin:$PATH[root@localhost ~]# . /etc/profile.d/java.sh 4.测试java环境是否可用. [root@localhost java]# java -versionjava version “1.7.0_09”Java(TM) SE Runtime Environment (build 1. 7.0_09-b05)Java HotSpot(TM) 64-Bit Server VM (build 23.5-b02, mixed mode)[root@localhost java]# 此时只是部署好了java环境,而并非运行任何的JAVA程序,而Tomcat就是运行在这个环境上面的JAVA程序 二、安装配置tomcat 1.获取软件包并解压安装： [root@localhost ~]# wget http://apache.fayea.com/apache-mirror/tomcat/tomcat-7/v7.0.55/bin/apache-tomcat-7.0.55.tar.gz[root@localhost ~]# tar -xf apache-tomcat-7.0.55.tar.gz -C /usr/local/[root@localhost ~]# cd /usr/local/[root@localhost local]# ln -sv apache-tomcat-7.0.5 tomcat`tomcat’ -&gt; `apache-tomcat-7.0.55’[root@localhost local]# cd tomcat/[root@localhost tomcat]# 2.更改tomcat环境变量： [root@localhost ~]# vim /etc/profile.d/tomcat.shexport CATALINA_HOME=/usr/local/tomcatexport PATH=$CATALINA_HOME/bin/:$PATH[root@localhost ~]# . /etc/profile.d/tomcat 3.启动tomcat [root@localhost tomcat]# catalina.sh start 4.验证是否启动 [root@localhost tomcat]# ss -tunlp | grep javatcp LISTEN 0 100 :::8080 :::* users:((“java”,25953,42))tcp LISTEN 0 1 ::ffff:127.0.0.1:8005 :::* users:((“java”,25953,46))tcp LISTEN 0 100 :::8009 :::* users:((“java”,25953,43))#—&gt;注意三个端口，每一个单独的java程序都是由一个jvm来运行的.[root@localhost tomcat]# jps25953 Bootstrap #—&gt;这个进程是tomcat启动时为了实现其加速的一个程序.有这个进程则表示启动成功.25995 Jps[root@localhost tomcat]# 5.网页访问测试：http://IP:8080 现在tomcat已经启动了,注意上图中的三个按钮,后续说. 6.为了方便管理tomcat，为其提供SysV管理脚本. [root@localhost ~]# vim /etc/rc.d/init.d/tomcat#!/bin/sh# Tomcat init script for Linux. # chkconfig: 2345 96 14# description: The Apache Tomcat servlet/JSP container.JAVA_HOME=/usr/java/latestCATALINA_HOME=/usr/local/tomcatexport JAVA_HOME CATALINA_HOME. /etc/init.d/functions case $1 instart) $CATALINA_HOME/bin/catalina.sh start &amp;&gt;/dev/null;;stop) $CATALINA_HOME/bin/catalina.sh stop &amp;&gt;/dev/null;; restart)$CATALINA_HOME/bin/catalina.sh stop &amp;&gt;/dev/null sleep 2 $CATALINA_HOME/bin/catalina.sh start &amp;&gt;/dev/null;;*) echo “Usage:`basename $0` {start|stop|restart}”exit 1;;esac[root@localhost ~]# chmod +X /etc/rc.d/init.d/tomcat[root@localhost ~]# chkconfig –add tomcat#-&gt;通过上面这个服务控制脚本就可以控制tomcat的启动、关闭等操作了; 7.Tomcat 配置文件配置层次： Tomcat是一个基于组件的服务器，它的构成组件都是可配置的，其中最外层的给件是CATALINA SERVLET容器，其他的组件按照一定的格式要求配置在这个顶层容器中。Tomcat的各个组件是server.xml文件中配置的，Tomcat服务器默认情况下对各种组件都有默认的实现，下面通过分析server.xml文件来理解Tomcat的各个组件是如何组织的。 #—&gt;顶层组件，代表一个服务器 #—&gt;顶层组件，是Connector的集合，将连接器关联至engine；因此一个service内部可以有多个connector，但只能有一个engine #—&gt;连接器类组件，代表通信接口 #—&gt;容器类组件，为特定的Service组件处理所有客户请求，可包含多个Host #—&gt;为特定的虚拟主机处理所有客户请求 #—&gt;为特定的WEB应用处理所有客户请求 详述这些组件： **顶级组件：**位于整个配置的顶层； **容器类：**可以包含其它组件的组件；&nbsp; &nbsp; &nbsp;engine: 核心容器，catalina引擎，负责通过connector接收用户请求,在一个engin内部可以有多个host&nbsp; &nbsp; &nbsp;host: 类似于httpd中的虚拟主机；支持基于FQDN的虚拟主机&nbsp;&nbsp;&nbsp;&nbsp; context: 最内层的容器类组件，一个context代表一个web应用程序；配置context的主要目的，指定对应的webapp的根目录；还能为webapp指定额外的属性，如部署方式等； **连接器组件：**连接用户请求至tomcat； 两类连接器： 1.基于HTTP 协议 2.基于AJP 二进制协议,使用httpd反向代理用户请求至tomcat时,在httpd和tomcat之间使用.被嵌套类的组件：位于一个容器当中，不能包含其它组件； **被嵌套类的组件：**位于一个容器当中，不能包含其它组件； valve: 拦截请求并在将其转至对应的webapp之前进行某种处理操作；可以用于任何容器中；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; access log valve:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; remote address filter value: 基于IP做访问控制&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger: 日志记录器，用于记录组件 内部的状态信息(可用于除context之外的任何容器中)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; realm: 可以用于任何容器类的组件中，关联一个用户认证库，实现认证和授权；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UserDatabaseRealm: 使用JNDI自定义的用户认证库；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRealm: tomcat-users.xml中&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; JDBCRealm: 基于JDBC连接至数据库中查找用户； **部署:**使用类加载器,为webapp准备好其依赖所有类. 8.服务端口说明： 在我们启动之后有3个端口：8005，8009，8080 8005 &nbsp;==&gt;位于server顶级组件当中,它监听于本地&lt;127.0.0.1&gt; 通过这个port我们可以直接在本地关闭tomcat应用程序,以及java进程. 如下： (1)在本机安装telnet ;&nbsp; [root@localhost ~]# yum install -y telnet (2)连接本机8005port,发送”SHUTDOWN”指令 [root@localhost ~]# telnet localhost 8005 #–&gt;telnet本地8005portTrying ::1…telnet: connect to address ::1: Connection refusedTrying 127.0.0.1…Connected to localhost.Escape character is ‘^]‘.SHUTDOWN #–&gt;发送”SHUTDOWN”指令！Connection closed by foreign host.[root@node1 ~]# ss -tunlp | grep java #–&gt;此时再去检查端口,显示没有,说明我们通过telnet已经发送指令将其关闭成功啦,危险系数较高,配置服务安全性时需注意！[root@node1 ~]# service tomcat start #–&gt;再次启动,检查端口正常否.[root@node1 ~]# ss -tunlp | grep javatcp LISTEN 0 100 :::8080 :::* users:((“java”,6430,42))tcp LISTEN 0 1 ::ffff:127.0.0.1:8005 :::* users:((“java”,6430,46))tcp LISTEN 0 100 :::8009 :::* users:((“java”,6430,43))[root@node1 ~]# **&nbsp;8009，8080 &nbsp;==&gt;**前面已说到了在连接器组件当中有个,他们在这个组件当中分别对应了两种协议. 8009 &nbsp;&nbsp;基于AJD协议 8080 &nbsp;基于http协议,当然在中可以添加各种参数,例如”address”等,默认配置文件中没有指定ip地址,默认就是监听所有.这个端口一般修改为80. 三、虚拟主机定义： (1)创建网页目录： [root@node1 ~]# mkdir /www/webapps/ROOT (2)创建网页测试文件: [root@localhost ~]# vim /www/webapps/ROOT/index.jsp&lt;%@ page language=”java” %&gt;&lt;%@ page import=”java.util.*” %&gt; JSP test page. &lt;% out.println(\"hello,world!\"); %&gt; (3)修改配置文件如下： [root@node1 ~]# vim /usr/local/tomcat/conf/server.xml………… …..….. &lt;Valve className=”org.apache.catalina.valves.AccessLogValve” directory=”logs” #-&gt;日志保存格式 prefix=”www.aaa.www_log.” suffix=”.txt” pattern=”%h %l %u %t \"%r\" %s %b” /&gt;………… (4)重启服务测试： [root@localhost ~]# service tomcat restart 注意:此处我已经对该服务器做了解析. (5)虚拟主机的别名机制： [root@node1 ~]# vim /usr/local/tomcat/conf/server.xml………… …..….. #—&gt;在原有虚拟主机中添加一个Context &lt;Valve className=”org.apache.catalina.valves.AccessLogValve” directory=”logs” #-&gt;日志保存格式 prefix=”www.aaa.www_log.” suffix=”.txt” pattern=”%h %l %u %t \"%r\" %s %b” /&gt;…… #—&gt;创建目录bbsapp[root@localhost ~]# mkdir /www/webapps/bbsapp#—&gt;创建测试文件[root@localhost ~]# cp /www/webapps/ROOT/index.jsp /www/webapps/bbsapp/index.jsp#—&gt;编辑测试文件,与上面的不同即可[root@localhost ~]# vim /www/webapps/bbsapp/index.jsp#—&gt;重启服务测试[root@localhost ~]# service tomcat restart[root@localhost ~]#…… 四、服务状态信息&amp;Dashboard: 在最前面的那种图片当中我们看到了tomcat服务的默认界面有三个按钮： &nbsp; 当我们点击”Server Status”时,需要身份验证： 点击”取消”,网页会提示我们如何去修改,并能够登录查看状态等信息. &nbsp;那就去修改这个配置文件吧： [root@localhost ~]# vim /usr/local/tomcat/conf/tomcat-users.xml…………………… #—&gt;重启服务[root@varnish ~]# service tomcat restart#—&gt;#测试：(这里只是状态界面,如果想看其他请看帮助信息)","categories":[{"name":"Web相关","slug":"Web相关","permalink":"https://blog.sctux.cc/categories/Web%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.sctux.cc/tags/Java/"},{"name":"jdk","slug":"jdk","permalink":"https://blog.sctux.cc/tags/jdk/"},{"name":"tomcat","slug":"tomcat","permalink":"https://blog.sctux.cc/tags/tomcat/"}],"keywords":[{"name":"Web相关","slug":"Web相关","permalink":"https://blog.sctux.cc/categories/Web%E7%9B%B8%E5%85%B3/"}]},{"title":"Snmp协议详解","slug":"snmp-e5-8d-8f-e8-ae-ae-e8-af-a6-e8-a7-a3","date":"2014-08-27T03:16:07.000Z","updated":"2025-09-01T01:59:08.967Z","comments":true,"path":"2014/08/27/snmp-e5-8d-8f-e8-ae-ae-e8-af-a6-e8-a7-a3/","permalink":"https://blog.sctux.cc/2014/08/27/snmp-e5-8d-8f-e8-ae-ae-e8-af-a6-e8-a7-a3/","excerpt":"监控主机的方式： 1.基于通行的snmp 2.基于专用agent 3.基于ssh(shell) 对于不同的监控指标，最后实现的功能也有所不同.下面是本人通过学习总结关于与snmp协议的一些解释以及运用 SNMP 主要几个简单的操作就可以实现对远程设备上的服务、资源等各种系统状态信息的获取,不仅仅是获取信息这么简单,还能在达到我们设定值或状态状态发生转换时能向对方发送控制指令; SNMP的工作机制： 监控端/一个或多个被监控端 监控端:NMS,这个平台会为管理员提供命令行接口,可以发送snmp指令到任何一个被监控主机,而在被监控端安装了一个服务进程,它用于接收来自监控端的查询请求, 并且能够解析对方的查询请求,并将查询数据返回给对方,所以这个服务进程可以称之为是一个agent,它只是接收一些查询或者是管理指令而存在的； 监控端发送请求—&gt;agent接受请求(根据查询请求内容)—&gt;agent返回查询请求到监控端. 但是上面有个弊端,也就是说任何主机连接到我们的agent就可以发送指令,这样一来安全方面就有了很大的隐患,于是在此基础上面就引入了communities在监控端和agent之间建立通信的;相当于两个端之间发送认证信息,但是这个认证信息是明文的; SNMP的协议 v1: 所有的安全机制都是基于communities机制来实现,snmp的所有操作在服务器端、客户端也是基于communities机制来实现,在此版本中communities有三种: read-only:只读,监控端只能到agent端获取信息; read-write:读写,可发送管理指令;有权限操作agent端 trap:agent端主动联系监控端；引起监控端注意的一种机制； v2: 对于v2来说只是对v1做了强化,此版本是基于community-string-based,这个版本也叫做v2c； v3: 由于前面两个版本过于简化,认证机制较为薄弱,故此增强了其认证机制,数据传输方面也有了安全性.但是在三个版本中用得最多的还是v1版本,原因是简单; snmp(NMS)只是提供了命令接口去完成采集数据而已,对于采集到的数据保存在哪里它不会关心； snmp这种机制虽然实现了网络管理功能，但是它只是一种协议,把这种协议予以实现的软件的功能也是各不相同的; MIBs管理信息库. OID&lt;对象标识符&gt; 将大多数监控对象它的名称和ID号之间的对应关系; 比如在agent端,它不仅仅要收集主机的信息外还需要维持一个MIB,请求达到agent端后它要知道请求的内容是什么,那么在agent端就会记录下监控对象和ID之间的映射关系;所以一个标准的agent要提供至少一个基本的MIB,以维持监控对象;也叫做MIB-II,根据主机不同的需要,MIB的内容也有所不同； 大多数能够基于snmp的管理功能都属于mgmt节点,而在mgmt下面的这个mib下面则有众多的监控对象，1.3.6.1.2.1是每一个主机或设备默认提供的MIB的标识：","text":"监控主机的方式： 1.基于通行的snmp 2.基于专用agent 3.基于ssh(shell) 对于不同的监控指标，最后实现的功能也有所不同.下面是本人通过学习总结关于与snmp协议的一些解释以及运用 SNMP 主要几个简单的操作就可以实现对远程设备上的服务、资源等各种系统状态信息的获取,不仅仅是获取信息这么简单,还能在达到我们设定值或状态状态发生转换时能向对方发送控制指令; SNMP的工作机制： 监控端/一个或多个被监控端 监控端:NMS,这个平台会为管理员提供命令行接口,可以发送snmp指令到任何一个被监控主机,而在被监控端安装了一个服务进程,它用于接收来自监控端的查询请求, 并且能够解析对方的查询请求,并将查询数据返回给对方,所以这个服务进程可以称之为是一个agent,它只是接收一些查询或者是管理指令而存在的； 监控端发送请求—&gt;agent接受请求(根据查询请求内容)—&gt;agent返回查询请求到监控端. 但是上面有个弊端,也就是说任何主机连接到我们的agent就可以发送指令,这样一来安全方面就有了很大的隐患,于是在此基础上面就引入了communities在监控端和agent之间建立通信的;相当于两个端之间发送认证信息,但是这个认证信息是明文的; SNMP的协议 v1: 所有的安全机制都是基于communities机制来实现,snmp的所有操作在服务器端、客户端也是基于communities机制来实现,在此版本中communities有三种: read-only:只读,监控端只能到agent端获取信息; read-write:读写,可发送管理指令;有权限操作agent端 trap:agent端主动联系监控端；引起监控端注意的一种机制； v2: 对于v2来说只是对v1做了强化,此版本是基于community-string-based,这个版本也叫做v2c； v3: 由于前面两个版本过于简化,认证机制较为薄弱,故此增强了其认证机制,数据传输方面也有了安全性.但是在三个版本中用得最多的还是v1版本,原因是简单; snmp(NMS)只是提供了命令接口去完成采集数据而已,对于采集到的数据保存在哪里它不会关心； snmp这种机制虽然实现了网络管理功能，但是它只是一种协议,把这种协议予以实现的软件的功能也是各不相同的; MIBs管理信息库. OID&lt;对象标识符&gt; 将大多数监控对象它的名称和ID号之间的对应关系; 比如在agent端,它不仅仅要收集主机的信息外还需要维持一个MIB,请求达到agent端后它要知道请求的内容是什么,那么在agent端就会记录下监控对象和ID之间的映射关系;所以一个标准的agent要提供至少一个基本的MIB,以维持监控对象;也叫做MIB-II,根据主机不同的需要,MIB的内容也有所不同； 大多数能够基于snmp的管理功能都属于mgmt节点,而在mgmt下面的这个mib下面则有众多的监控对象，1.3.6.1.2.1是每一个主机或设备默认提供的MIB的标识：","categories":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}],"tags":[],"keywords":[{"name":"Other","slug":"Other","permalink":"https://blog.sctux.cc/categories/Other/"}]},{"title":"Perl: Warning: Falling Back to the Standard Locale (“C”)","slug":"perl-warning-falling-back-to-the-standard-locale-c","date":"2014-08-21T07:56:27.000Z","updated":"2025-09-01T01:59:08.984Z","comments":true,"path":"2014/08/21/perl-warning-falling-back-to-the-standard-locale-c/","permalink":"https://blog.sctux.cc/2014/08/21/perl-warning-falling-back-to-the-standard-locale-c/","excerpt":"今天在安装memcache的php的扩展时遇到的错误： 1.将该扩展包解压 2.使用/usr/bin/phpize(rpm包安装后的位置)命令来准备 PHP 外挂模块的编译环境(如果找不到该命令则需要安装，这个命令有php-devel这个包生成，并且该包位于DVD2中) 上面1、2步成功 3.使用/usr/bin/phpize时报以下错误: [root@node1 memcache-2.2.5]# /usr/bin/phpizeConfiguring for:PHP Api Version: 20090626Zend Module Api No: 20090626Zend Extension Api No: 220090626perl: warning: Setting locale failed.perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LANG = “en” are supported and installed on your system.perl: warning: Falling back to the standard locale (“C”).perl: warning: Setting locale failed.perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LANG = “en” are supported and installed on your system.perl: warning: Falling back to the standard locale (“C”). 在使用其它的某些命令时(如：mysqlhotcopy)也会也出现类似的提示。 搜索了好一段时间，并试了几次，找到一个解决此问题的简单方法，如下： [root@node1 memcache-2.2.5]#vim /root/.bashrc#-&gt;再最底部加上export LC_ALL=C#-&gt;或者直接运行[root@node1 memcache-2.2.5]# echo “export LC_ALL=C” &gt;&gt; /root/.bashrc#-&gt;然后执行一下：[root@node1 memcache-2.2.5]# source /root/.bashrc","text":"今天在安装memcache的php的扩展时遇到的错误： 1.将该扩展包解压 2.使用/usr/bin/phpize(rpm包安装后的位置)命令来准备 PHP 外挂模块的编译环境(如果找不到该命令则需要安装，这个命令有php-devel这个包生成，并且该包位于DVD2中) 上面1、2步成功 3.使用/usr/bin/phpize时报以下错误: [root@node1 memcache-2.2.5]# /usr/bin/phpizeConfiguring for:PHP Api Version: 20090626Zend Module Api No: 20090626Zend Extension Api No: 220090626perl: warning: Setting locale failed.perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LANG = “en” are supported and installed on your system.perl: warning: Falling back to the standard locale (“C”).perl: warning: Setting locale failed.perl: warning: Please check that your locale settings: LANGUAGE = (unset), LC_ALL = (unset), LANG = “en” are supported and installed on your system.perl: warning: Falling back to the standard locale (“C”). 在使用其它的某些命令时(如：mysqlhotcopy)也会也出现类似的提示。 搜索了好一段时间，并试了几次，找到一个解决此问题的简单方法，如下： [root@node1 memcache-2.2.5]#vim /root/.bashrc#-&gt;再最底部加上export LC_ALL=C#-&gt;或者直接运行[root@node1 memcache-2.2.5]# echo “export LC_ALL=C” &gt;&gt; /root/.bashrc#-&gt;然后执行一下：[root@node1 memcache-2.2.5]# source /root/.bashrc","categories":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}],"tags":[{"name":"perl","slug":"perl","permalink":"https://blog.sctux.cc/tags/perl/"},{"name":"phpize","slug":"phpize","permalink":"https://blog.sctux.cc/tags/phpize/"}],"keywords":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}]},{"title":"Vmware Clone陷阱","slug":"vmware-clone-e9-99-b7-e9-98-b1","date":"2014-07-19T14:59:06.000Z","updated":"2025-09-01T01:59:08.876Z","comments":true,"path":"2014/07/19/vmware-clone-e9-99-b7-e9-98-b1/","permalink":"https://blog.sctux.cc/2014/07/19/vmware-clone-e9-99-b7-e9-98-b1/","excerpt":"在我们平时使用vmware workstation做实验时，会遇到主机不够用的情况，那此时我们的解决办法一般都是要么从新装一台新的，要么就是通过vmware workstation强大的克隆功能克隆出我们需要的虚拟机；显然后者的优势比前者大，毕竟你从新装一台的话占用你的物理硬盘空间，其次就是浪费Your Time. SO,就选择Clone吧. 我现在有一台刚装好的虚拟机(母机)，由于做实验我需要多台主机；我将这台主机命名为node1，目的是通过这台母机克隆一台虚拟机node2； 在Clone Type页中，单击Create a linked clone(创建一个克隆链接)。如果选择第二项Create a full clone，则创建一个完整的克隆。这两个区别在于：第一项创建的虚拟机将依赖于源虚拟机的存在，使用这项创建的虚拟机占用较少的硬盘空间；第二项创建的 虚拟机是一个独立的虚拟机，但占用较多的硬盘空间。我这里选择的是Create a linked clone，具体的步骤我就不再给出； node2通过node1不到10s就克隆好啦，但是待我查看克隆出来的两台机子的网络信息时居然不是默认的eth0网卡，怎么变成eth1了呢，对于我这种学习强迫症的来说，这种事情真不能发生，还有的同学遇到了通过clone这种机制在操作时网卡冲突、无法启动网卡、或者是配置了IP也不济于是；于是通过google了一下： 原因如下： Centos或RedHat使用udev动态管理设备文件，并根据设备的信息对其进行持久化命名。udev会在系统引导的过程中识别网卡，将mac地址和网卡名称对应起来记录在udev的规则脚本中。而对于新的虚拟机，VMware会自动为虚拟机的网卡生成MAC地址，当你克隆或者重装虚拟机软件时，由于你使用的是以前系统虚拟硬盘的信息，而该系统中已经有eth0的信息，对于新增的网卡，udev会自动将其命名为eth1（累加的原则），所以在你的系统启动后，你使用ifconfig看到的网卡名为eth1。这时候在/etc/sysconfig/network-script/下依然是eth0的配置文件，自然他会识别出eth1了噻。 解决办法： 1. 将node2这台主机的/etc/udev/rules.d/70-persistent-net.rules 中 2. 将/etc/sysconfig/network-script/ifcfg-eth0 中关于mac信息删掉； 3. 改完后reboot或者重启网卡：service network restart.完成后你会发现与网卡编号与你的正常逻辑中的一样啦.","text":"在我们平时使用vmware workstation做实验时，会遇到主机不够用的情况，那此时我们的解决办法一般都是要么从新装一台新的，要么就是通过vmware workstation强大的克隆功能克隆出我们需要的虚拟机；显然后者的优势比前者大，毕竟你从新装一台的话占用你的物理硬盘空间，其次就是浪费Your Time. SO,就选择Clone吧. 我现在有一台刚装好的虚拟机(母机)，由于做实验我需要多台主机；我将这台主机命名为node1，目的是通过这台母机克隆一台虚拟机node2； 在Clone Type页中，单击Create a linked clone(创建一个克隆链接)。如果选择第二项Create a full clone，则创建一个完整的克隆。这两个区别在于：第一项创建的虚拟机将依赖于源虚拟机的存在，使用这项创建的虚拟机占用较少的硬盘空间；第二项创建的 虚拟机是一个独立的虚拟机，但占用较多的硬盘空间。我这里选择的是Create a linked clone，具体的步骤我就不再给出； node2通过node1不到10s就克隆好啦，但是待我查看克隆出来的两台机子的网络信息时居然不是默认的eth0网卡，怎么变成eth1了呢，对于我这种学习强迫症的来说，这种事情真不能发生，还有的同学遇到了通过clone这种机制在操作时网卡冲突、无法启动网卡、或者是配置了IP也不济于是；于是通过google了一下： 原因如下： Centos或RedHat使用udev动态管理设备文件，并根据设备的信息对其进行持久化命名。udev会在系统引导的过程中识别网卡，将mac地址和网卡名称对应起来记录在udev的规则脚本中。而对于新的虚拟机，VMware会自动为虚拟机的网卡生成MAC地址，当你克隆或者重装虚拟机软件时，由于你使用的是以前系统虚拟硬盘的信息，而该系统中已经有eth0的信息，对于新增的网卡，udev会自动将其命名为eth1（累加的原则），所以在你的系统启动后，你使用ifconfig看到的网卡名为eth1。这时候在/etc/sysconfig/network-script/下依然是eth0的配置文件，自然他会识别出eth1了噻。 解决办法： 1. 将node2这台主机的/etc/udev/rules.d/70-persistent-net.rules 中 2. 将/etc/sysconfig/network-script/ifcfg-eth0 中关于mac信息删掉； 3. 改完后reboot或者重启网卡：service network restart.完成后你会发现与网卡编号与你的正常逻辑中的一样啦.","categories":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}],"tags":[{"name":"vmware workstation","slug":"vmware-workstation","permalink":"https://blog.sctux.cc/tags/vmware-workstation/"}],"keywords":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}]},{"title":"获取Linux服务器硬件信息","slug":"e8-8e-b7-e5-8f-96linux-e6-9c-8d-e5-8a-a1-e5-99-a8-e7-a1-ac-e4-bb-b6-e4-bf-a1-e6-81-af","date":"2014-06-29T10:13:58.000Z","updated":"2025-09-01T01:59:08.885Z","comments":true,"path":"2014/06/29/e8-8e-b7-e5-8f-96linux-e6-9c-8d-e5-8a-a1-e5-99-a8-e7-a1-ac-e4-bb-b6-e4-bf-a1-e6-81-af/","permalink":"https://blog.sctux.cc/2014/06/29/e8-8e-b7-e5-8f-96linux-e6-9c-8d-e5-8a-a1-e5-99-a8-e7-a1-ac-e4-bb-b6-e4-bf-a1-e6-81-af/","excerpt":"1.查看服务器型号、序列号： [root@localhots ~]#dmidecode|grep “System Information” -A9|egrep “Manufacturer|Product|Serial” Manufacturer: HP Product Name: ProLiant DL360 G6 Serial Number: JPT0012J2W 2.Linux 查看内存的插槽数,已经使用多少插槽.每条内存多大: [root@localhost ~]#dmidecode|grep -A5 “Memory Device”|grep Size|grep -v Range Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: No Module Installed Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: No Module Installed 3.Linux 查看内存的频率: [root@localhost ~]#dmidecode|grep -A16 “Memory Device”|grep ‘Speed’ Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: Unknown Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: Unknown 4.查看cpu的统计信息: [root@localhost ~]# lscpu #-&gt;如该命令找不到,请安装软件包util-linux-ngArchitecture: x86_64 #-&gt;cpu架构CPU op-mode(s): 32-bit, 64-bitByte Order: Little Endian #-&gt;小尾序CPU(s): 8 #-&gt;总共有8核On-line CPU(s) list: 0-7Thread(s) per core: 1 #-&gt;每个cpu核，只能支持一个线程，即不支持超线程Core(s) per socket: 4 #-&gt;每个cpu，有4个核Socket(s): 2 #-&gt;总共有2一个cpuNUMA node(s): 2Vendor ID: GenuineIntel #-&gt;cpu产商 intelCPU family: 6Model: 26Stepping: 5CPU MHz: 2266.877BogoMIPS: 4532.68Virtualization: VT-x #-&gt;支持cpu虚拟化技术L1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 8192KNUMA node0 CPU(s): 0,2,4,6NUMA node1 CPU(s): 1,3,5,7 5.查看/proc/cpuinfo,可以知道每个cpu信息，如每个CPU的型号，主频等: [root@localhost ~]#cat /proc/cpuinfoprocessor : 0vendor_id : GenuineIntelcpu family : 6model : 26model name : Intel(R) Xeon(R) CPU E5520 @ 2.27GHz…..…..…..#-&gt;上面输出的是第一个cpu部分信息，还有7个cpu信息省略了","text":"1.查看服务器型号、序列号： [root@localhots ~]#dmidecode|grep “System Information” -A9|egrep “Manufacturer|Product|Serial” Manufacturer: HP Product Name: ProLiant DL360 G6 Serial Number: JPT0012J2W 2.Linux 查看内存的插槽数,已经使用多少插槽.每条内存多大: [root@localhost ~]#dmidecode|grep -A5 “Memory Device”|grep Size|grep -v Range Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: No Module Installed Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: 4096 MB Size: No Module Installed Size: No Module Installed Size: No Module Installed 3.Linux 查看内存的频率: [root@localhost ~]#dmidecode|grep -A16 “Memory Device”|grep ‘Speed’ Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: Unknown Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: 1067 MHz Speed: Unknown Speed: Unknown Speed: Unknown 4.查看cpu的统计信息: [root@localhost ~]# lscpu #-&gt;如该命令找不到,请安装软件包util-linux-ngArchitecture: x86_64 #-&gt;cpu架构CPU op-mode(s): 32-bit, 64-bitByte Order: Little Endian #-&gt;小尾序CPU(s): 8 #-&gt;总共有8核On-line CPU(s) list: 0-7Thread(s) per core: 1 #-&gt;每个cpu核，只能支持一个线程，即不支持超线程Core(s) per socket: 4 #-&gt;每个cpu，有4个核Socket(s): 2 #-&gt;总共有2一个cpuNUMA node(s): 2Vendor ID: GenuineIntel #-&gt;cpu产商 intelCPU family: 6Model: 26Stepping: 5CPU MHz: 2266.877BogoMIPS: 4532.68Virtualization: VT-x #-&gt;支持cpu虚拟化技术L1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 8192KNUMA node0 CPU(s): 0,2,4,6NUMA node1 CPU(s): 1,3,5,7 5.查看/proc/cpuinfo,可以知道每个cpu信息，如每个CPU的型号，主频等: [root@localhost ~]#cat /proc/cpuinfoprocessor : 0vendor_id : GenuineIntelcpu family : 6model : 26model name : Intel(R) Xeon(R) CPU E5520 @ 2.27GHz…..…..…..#-&gt;上面输出的是第一个cpu部分信息，还有7个cpu信息省略了 6.查看物理CPU个数: [root@localhost ~]# grep “physical id” /proc/cpuinfo | sort -uphysical id : 0physical id : 1 7.查看CPU核心数: [root@localhost ~]# grep ‘core id’ /proc/cpuinfo | sort -u | wc -l4 8.查看CPU线程数: [root@localhost ~]# grep ‘processor’ /proc/cpuinfo | sort -u | wc -l8","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"cpu","slug":"cpu","permalink":"https://blog.sctux.cc/tags/cpu/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"如何让计划任务实现秒级执行","slug":"e5-a6-82-e4-bd-95-e8-ae-a9-e8-ae-a1-e5-88-92-e4-bb-bb-e5-8a-a1-e5-ae-9e-e7-8e-b0-e7-a7-92-e7-ba-a7-e6-89-a7-e8-a1-8c","date":"2014-06-16T14:37:51.000Z","updated":"2025-09-01T01:59:08.877Z","comments":true,"path":"2014/06/16/e5-a6-82-e4-bd-95-e8-ae-a9-e8-ae-a1-e5-88-92-e4-bb-bb-e5-8a-a1-e5-ae-9e-e7-8e-b0-e7-a7-92-e7-ba-a7-e6-89-a7-e8-a1-8c/","permalink":"https://blog.sctux.cc/2014/06/16/e5-a6-82-e4-bd-95-e8-ae-a9-e8-ae-a1-e5-88-92-e4-bb-bb-e5-8a-a1-e5-ae-9e-e7-8e-b0-e7-a7-92-e7-ba-a7-e6-89-a7-e8-a1-8c/","excerpt":"最近有个应用需求，根据实际要求最好是每3秒执行一次，但是crond只能支持到分。这该如何是好？ 第一种方法： 首先想到的是通过一个触发的脚本，然后在脚本中使用死循环来解决此问题，如： cat test.sh----------------#!/bin/bashwhile : ;do /home/script/test.sh 2&gt;/dev/null &amp; sleep 3done---------------- 注意第一次运行时请不要使用sh test.sh&nbsp;&amp; 这种后台运行的方式，它会僵死的。 可以把它放到计划任务使其运行，然后将计划任务中的此条目删除即可。最后把这个脚本放到/etc/rc.local让它每次开机都可以被运行。 第二种方法： 和第二种方法类似，但是比起来更便捷一些。 cat cron-seconds.sh----------------#!/bin/bash#For excuting the scripts every 3 seconds in crond. for((i=1;i&lt;=20;i++));do /home/script/test.sh 2&gt;/dev/null &amp; sleep 3 done---------------- 然后写入的crontab里每分钟执行一次，如下: crontab -e----------------* * * * * /bin/bash /home/somedir/cron-seconds.sh---------------- 第三种方法： 那么如何使用计划任务来直接实现呢？ 最后解决方案如下，虽然条目看起来很多，但是经验证，脚本运行非常稳定。","text":"最近有个应用需求，根据实际要求最好是每3秒执行一次，但是crond只能支持到分。这该如何是好？ 第一种方法： 首先想到的是通过一个触发的脚本，然后在脚本中使用死循环来解决此问题，如： cat test.sh----------------#!/bin/bashwhile : ;do /home/script/test.sh 2&gt;/dev/null &amp; sleep 3done---------------- 注意第一次运行时请不要使用sh test.sh&nbsp;&amp; 这种后台运行的方式，它会僵死的。 可以把它放到计划任务使其运行，然后将计划任务中的此条目删除即可。最后把这个脚本放到/etc/rc.local让它每次开机都可以被运行。 第二种方法： 和第二种方法类似，但是比起来更便捷一些。 cat cron-seconds.sh----------------#!/bin/bash#For excuting the scripts every 3 seconds in crond. for((i=1;i&lt;=20;i++));do /home/script/test.sh 2&gt;/dev/null &amp; sleep 3 done---------------- 然后写入的crontab里每分钟执行一次，如下: crontab -e----------------* * * * * /bin/bash /home/somedir/cron-seconds.sh---------------- 第三种方法： 那么如何使用计划任务来直接实现呢？ 最后解决方案如下，虽然条目看起来很多，但是经验证，脚本运行非常稳定。 crontab -e#—————————————————————–## For excuting test.sh.sh every 3 seconds* * * * * /home/script/test.sh.sh* * * * * sleep 3 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 6 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 9 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 12 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 15 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 18 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 21 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 24 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 27 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 30 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 33 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 36 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 39 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 42 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 45 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 48 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 51 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 54 &amp;&amp; /home/script/test.sh.sh* * * * * sleep 57 &amp;&amp; /home/script/test.sh.sh#—————————————————————– &nbsp; 个人还是比较倾向于第三种方法，毕竟你执行test.sh也是需要时间的。但是如果对于时间精度不是很高的，推荐使用第二种方法。","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"crond","slug":"crond","permalink":"https://blog.sctux.cc/tags/crond/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"Zabbix Server Is Not Running:错误的解决","slug":"zabbix-server-is-not-running-e9-94-99-e8-af-af-e7-9a-84-e8-a7-a3-e5-86-b3","date":"2014-06-16T02:08:22.000Z","updated":"2025-09-01T01:59:08.957Z","comments":true,"path":"2014/06/16/zabbix-server-is-not-running-e9-94-99-e8-af-af-e7-9a-84-e8-a7-a3-e5-86-b3/","permalink":"https://blog.sctux.cc/2014/06/16/zabbix-server-is-not-running-e9-94-99-e8-af-af-e7-9a-84-e8-a7-a3-e5-86-b3/","excerpt":"今天在安装了zabbix-server端后 &nbsp;在zabbix的dashboard中中出现下面的信息: &nbsp; 随即检查了zabbix-server的运行状态和mysql的运行状态都是正常的；困扰有半刻钟左右，将所有的配置文件检查了一遍 发现在/etc/zabbix/web/zabbix.conf.php 中有这么一行信息： 解决： 我将”ZABBIX-SERVER” 改成了我的zabbix-server的IP.保存退出、重启zabbix-server、刷新dashboard;恢复正常啦！ 问题原因： 我从新安装了一次,发现在dashboard界面安装过程中,我将host的名称给自定义了, 它默认是”localhost”,这里根本就不需要修改,默认就行啦！ &nbsp; “Name”倒是怎么方便怎么来。 比较重要的文件就是/etc/zabbix/中的那一堆了;日志文件的话都在/var/log/zabbixsrv/下面. 还有一些错误倒是能根据一些提示解决,比如/etc/php.ini中的好几个参数需要修改，按照提示修改完，重启一下apache就ok!","text":"今天在安装了zabbix-server端后 &nbsp;在zabbix的dashboard中中出现下面的信息: &nbsp; 随即检查了zabbix-server的运行状态和mysql的运行状态都是正常的；困扰有半刻钟左右，将所有的配置文件检查了一遍 发现在/etc/zabbix/web/zabbix.conf.php 中有这么一行信息： 解决： 我将”ZABBIX-SERVER” 改成了我的zabbix-server的IP.保存退出、重启zabbix-server、刷新dashboard;恢复正常啦！ 问题原因： 我从新安装了一次,发现在dashboard界面安装过程中,我将host的名称给自定义了, 它默认是”localhost”,这里根本就不需要修改,默认就行啦！ &nbsp; “Name”倒是怎么方便怎么来。 比较重要的文件就是/etc/zabbix/中的那一堆了;日志文件的话都在/var/log/zabbixsrv/下面. 还有一些错误倒是能根据一些提示解决,比如/etc/php.ini中的好几个参数需要修改，按照提示修改完，重启一下apache就ok!","categories":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}],"tags":[{"name":"zabbix","slug":"zabbix","permalink":"https://blog.sctux.cc/tags/zabbix/"}],"keywords":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}]},{"title":"Linux上ip归属查看神器","slug":"linux-e4-b8-8aip-e5-bd-92-e5-b1-9e-e6-9f-a5-e7-9c-8b-e7-a5-9e-e5-99-a8","date":"2014-04-12T11:23:15.000Z","updated":"2025-09-01T01:59:08.958Z","comments":true,"path":"2014/04/12/linux-e4-b8-8aip-e5-bd-92-e5-b1-9e-e6-9f-a5-e7-9c-8b-e7-a5-9e-e5-99-a8/","permalink":"https://blog.sctux.cc/2014/04/12/linux-e4-b8-8aip-e5-bd-92-e5-b1-9e-e6-9f-a5-e7-9c-8b-e7-a5-9e-e5-99-a8/","excerpt":"nali取名”哪里”的拼音。 nali包含一组命令行程序，其主要功能就是把一些网络工具的输出的IP字符串，附加上地理位置信息(使用纯真数据库) 例如218.65.137.1会变成218.65.137.1[广西南宁市 电信]。 查询是在本地进行，并不会进行联网查询，所以效率方面不会有什么影响。 目前包含以下几个命令： nali nali-dig nali-nslookup nali-traceroute nali-tracepath nali-ping 使用这些命令的前提是，他们对应的命令必须存在。例如你要用nali-dig，必须保证dig是存在的。他们的用法和原始命令是一样的。例如nali-dig，用法就和dig一样。 大家可能注意到了nali这个命令，它可以对标准输出的IP串附加上地理信息。nali-*系列工具都是基于这个来实现的。 下载 wget http://www.sctux.com/package/nali-0.1.tar.gz 安装 ./configure –prefix=/usr &amp;&amp; make &amp;&amp; make instal 使用 1.统计nginx的访问记录 [root@sctux ~]# awk -F ‘ ‘ ‘{print $1}’ /var/log/access_sctux.log | sort -n | uniq -cd | sort -rn | nali 结果如下： 2.使用traceroute 也就是说，nali这个命令，可以对标准输出的ip，附加上地理信息。同理，如果你不喜欢用nali-dig，那么也可以用dig ip|nali这样的命令。 如果你觉得输入nali-xxx麻烦，那么可以做一些alias，例如： vim /root/.bashrcalias traceroute=’nali-traceroute’alias dig=’nali-dig’","text":"nali取名”哪里”的拼音。 nali包含一组命令行程序，其主要功能就是把一些网络工具的输出的IP字符串，附加上地理位置信息(使用纯真数据库) 例如218.65.137.1会变成218.65.137.1[广西南宁市 电信]。 查询是在本地进行，并不会进行联网查询，所以效率方面不会有什么影响。 目前包含以下几个命令： nali nali-dig nali-nslookup nali-traceroute nali-tracepath nali-ping 使用这些命令的前提是，他们对应的命令必须存在。例如你要用nali-dig，必须保证dig是存在的。他们的用法和原始命令是一样的。例如nali-dig，用法就和dig一样。 大家可能注意到了nali这个命令，它可以对标准输出的IP串附加上地理信息。nali-*系列工具都是基于这个来实现的。 下载 wget http://www.sctux.com/package/nali-0.1.tar.gz 安装 ./configure –prefix=/usr &amp;&amp; make &amp;&amp; make instal 使用 1.统计nginx的访问记录 [root@sctux ~]# awk -F ‘ ‘ ‘{print $1}’ /var/log/access_sctux.log | sort -n | uniq -cd | sort -rn | nali 结果如下： 2.使用traceroute 也就是说，nali这个命令，可以对标准输出的ip，附加上地理信息。同理，如果你不喜欢用nali-dig，那么也可以用dig ip|nali这样的命令。 如果你觉得输入nali-xxx麻烦，那么可以做一些alias，例如： vim /root/.bashrcalias traceroute=’nali-traceroute’alias dig=’nali-dig’","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"Nagios-NRPE脚本返回值","slug":"nagios-nrpe-e8-84-9a-e6-9c-ac-e8-bf-94-e5-9b-9e-e5-80-bc","date":"2014-02-08T07:34:42.000Z","updated":"2025-09-01T01:59:08.884Z","comments":true,"path":"2014/02/08/nagios-nrpe-e8-84-9a-e6-9c-ac-e8-bf-94-e5-9b-9e-e5-80-bc/","permalink":"https://blog.sctux.cc/2014/02/08/nagios-nrpe-e8-84-9a-e6-9c-ac-e8-bf-94-e5-9b-9e-e5-80-bc/","excerpt":"自定义Nagios NRPE脚本EXIT退出值和nagios状态都应关系： 状态 EXIT退出值 输出 例子 OK 0 echo “OK - it’s ok.” echo “OK - it’s ok.” exit 0 WARNING","text":"自定义Nagios NRPE脚本EXIT退出值和nagios状态都应关系： 状态 EXIT退出值 输出 例子 OK 0 echo “OK - it’s ok.” echo “OK - it’s ok.” exit 0 WARNING 1 echo “WARNING - it’s warning.” echo “WARNING - it’s warning.” exit 1 CRITICAL 2 echo “CRITICAL - it’s critical.” echo “CRITICAL - it’s critical.” exit 2 UNKNOWN 3 echo “UNKNOWN - it’s unknown.” echo “UNKNOWN - it’s unknown.” exit 3 错误的例子： shell脚本中echo和退出值： echo “OK - it’s ok.” exit 1 此时，Nagios会显示： 这条服务对应的状态是”WARNING“，但是输出的信息是”OK - it’s ok.”","categories":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"nrpe","slug":"nrpe","permalink":"https://blog.sctux.cc/tags/nrpe/"}],"keywords":[{"name":"必备知识","slug":"必备知识","permalink":"https://blog.sctux.cc/categories/%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"}]},{"title":"Vsftp 开启 PASV模式","slug":"vsftp-e5-bc-80-e5-90-af-pasv-e6-a8-a1-e5-bc-8f","date":"2013-06-19T06:01:31.000Z","updated":"2025-09-01T01:59:08.958Z","comments":true,"path":"2013/06/19/vsftp-e5-bc-80-e5-90-af-pasv-e6-a8-a1-e5-bc-8f/","permalink":"https://blog.sctux.cc/2013/06/19/vsftp-e5-bc-80-e5-90-af-pasv-e6-a8-a1-e5-bc-8f/","excerpt":"今天应老大要求要在一台在公网的机器配置一个vsftp服务，配置好之后居然不能用，甚是纳闷，具体报错如下图： 1、我在本机 ftp localhost 却一点没有问题？ 2、为什么一出外网就有问题？ 3、这台服务器在公网，在防火墙后面，本机iptables是停了的； 4、以前都是在内网，甚至是同一个网段搭建vsftp来使用； 5、vsftp的配置非常的简单，难道还有些地方有疏漏？ 解决方法： 1、在/etc/vsftpd/vsftpd.conf末尾添加： #YES，允许数据传输时使用PASV模式。NO，不允许使用PASV模式。默认值为YES。pasv_enable=YES #设定在PASV模式下，建立数据传输所可以使用port范围的下界和上界，0 表示任意。默认值为0。把端口范围设在比较高的一段范围内，比如50000-60000，将有助于安全性的提高.pasv_min_port=30000pasv_max_port=30005 #此选项为一个数字IP地址，作为PASV命令的响应。默认值为none，即地址是从呼入的连接套接字(incoming connectd socket)中获取。pasv_address=xxxxxxxxx #我这里配置的是这台服务器的外网IP #此选项激活时，将关闭PASV模式的安全检查。该检查确保数据连接和控制连接是来自同一个IP地址。小心打开此选项。此选项唯一合理的用法是存在于由安全隧道方案构成的组织中。默认值为NO#pasv_promiscuous=NO 2、在防火墙上开启30000-30005端口。 3、重启vsftpd服务 /etc/init.d/vsftpd restart 客户端再次访问就正常啦：","text":"今天应老大要求要在一台在公网的机器配置一个vsftp服务，配置好之后居然不能用，甚是纳闷，具体报错如下图： 1、我在本机 ftp localhost 却一点没有问题？ 2、为什么一出外网就有问题？ 3、这台服务器在公网，在防火墙后面，本机iptables是停了的； 4、以前都是在内网，甚至是同一个网段搭建vsftp来使用； 5、vsftp的配置非常的简单，难道还有些地方有疏漏？ 解决方法： 1、在/etc/vsftpd/vsftpd.conf末尾添加： #YES，允许数据传输时使用PASV模式。NO，不允许使用PASV模式。默认值为YES。pasv_enable=YES #设定在PASV模式下，建立数据传输所可以使用port范围的下界和上界，0 表示任意。默认值为0。把端口范围设在比较高的一段范围内，比如50000-60000，将有助于安全性的提高.pasv_min_port=30000pasv_max_port=30005 #此选项为一个数字IP地址，作为PASV命令的响应。默认值为none，即地址是从呼入的连接套接字(incoming connectd socket)中获取。pasv_address=xxxxxxxxx #我这里配置的是这台服务器的外网IP #此选项激活时，将关闭PASV模式的安全检查。该检查确保数据连接和控制连接是来自同一个IP地址。小心打开此选项。此选项唯一合理的用法是存在于由安全隧道方案构成的组织中。默认值为NO#pasv_promiscuous=NO 2、在防火墙上开启30000-30005端口。 3、重启vsftpd服务 /etc/init.d/vsftpd restart 客户端再次访问就正常啦：","categories":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}],"tags":[],"keywords":[{"name":"故障处理","slug":"故障处理","permalink":"https://blog.sctux.cc/categories/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"}]}]}