<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>01-K8s集群搭建---系统初始化配置</title>
    <url>/2018/05/30/01k8s-ji-qun-da-jian-yixi-tong-chu-shi-hua-pei-zhi/</url>
    <content><![CDATA[<h2 id="1-架构说明"><a href="#1-架构说明" class="headerlink" title="1.架构说明:"></a>1.架构说明:</h2><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/06/15278627011918.jpg">￼</p>
<h2 id="2-版本信息"><a href="#2-版本信息" class="headerlink" title="2.版本信息:"></a>2.版本信息:</h2><ul>
<li>系统版本CentOS7.x</li>
<li>Kubernets: v1.10</li>
<li>Etcd: v3.3.1</li>
<li>Docker: 18.03.1-ce</li>
<li>Flannel: v1.10</li>
<li>CNI-Plugins: v0.7.0 建议部署节点：最少三个节点，请配置好主机名解析（必备）</li>
</ul>
<h2 id="3-系统初始化-三台服务器分别执行即可，以node1为例"><a href="#3-系统初始化-三台服务器分别执行即可，以node1为例" class="headerlink" title="3.系统初始化(三台服务器分别执行即可，以node1为例):"></a>3.系统初始化(三台服务器分别执行即可，以node1为例):</h2><p>a. 主机名配置</p>
<pre><code>node1:
echo "linux-node1.example.com" &gt; /etc/hostname
</code></pre>
<p>b. 设置/etc/hosts保证主机名能够解析</p>
<pre><code>node1:
echo "192.168.56.11 linux-node1 linux-node1.example.com" &gt;&gt; /etc/hosts
</code></pre>
<p>c. 关闭SELinux及防火墙</p>
<pre><code>node1:
systemctl disable firewalld; systemctl stop firewalld 
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
</code></pre>
<p>d. 环境变量配置(后续k8s相关命令都会放到/opt/kubernetes/bin目录下)</p>
<pre><code>PATH=$PATH:$HOME/bin:/opt/kubernetes/bin
source ~/.bash_profile
</code></pre>
<h2 id="4-安装Docker"><a href="#4-安装Docker" class="headerlink" title="4.安装Docker"></a>4.安装Docker</h2><p>a：使用国内Docker源</p>
<pre><code>[root@linux-node1 ~]# cd /etc/yum.repos.d/
[root@linux-node1 yum.repos.d]# wget \
https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</code></pre>
<p>b：Docker安装：</p>
<pre><code>[root@linux-node1 ~]# yum install -y docker-ce
</code></pre>
<p>c：启动后台进程：</p>
<pre><code>[root@linux-node1 ~]# systemctl start docker
</code></pre>
<h2 id="5-准备部署目录"><a href="#5-准备部署目录" class="headerlink" title="5.准备部署目录"></a>5.准备部署目录</h2><pre><code>[root@linux-node1 ~]#  mkdir -p /opt/kubernetes/{cfg,bin,ssl,log}
# 目录结构, 所有文件均存放在/opt/kubernetes目录下：
[root@linux-node1 ~]# tree -L 1 /opt/kubernetes/
/opt/kubernetes/
├── bin   #二进制文件
├── cfg   #配置文件
├── log   #日志文件
└── ssl   #证书文件
</code></pre>
<h2 id="6-准备软件包"><a href="#6-准备软件包" class="headerlink" title="6.准备软件包"></a>6.准备软件包</h2><p>百度网盘下载地址：<br><a href="https://pan.baidu.com/s/1bQo7PEfKJAAhk4KzQgLrMQ">https://pan.baidu.com/s/1bQo7PEfKJAAhk4KzQgLrMQ</a> 密码:gm3k</p>
<h2 id="7-解压软件包"><a href="#7-解压软件包" class="headerlink" title="7.解压软件包"></a>7.解压软件包</h2><pre><code>[root@linux-node1 ~]# cd /tmp/ &amp;&amp; unzip k8s-v1.10.1-manual.zip &amp;&amp; mv k8s-v1.10.1-manual/k8s-v1.10.1/* /usr/local/src/
[root@linux-node1 ~]# cd /usr/local/src
[root@linux-node1 ~]# tar zxf kubernetes.tar.gz 
[root@linux-node1 ~]# tar zxf kubernetes-server-linux-amd64.tar.gz 
[root@linux-node1 ~]# tar zxf kubernetes-client-linux-amd64.tar.gz
[root@linux-node1 ~]# tar zxf kubernetes-node-linux-amd64.tar.gz
</code></pre>
<h2 id="8-做好master节点跟其他node节点的ssh互信-便于搭建"><a href="#8-做好master节点跟其他node节点的ssh互信-便于搭建" class="headerlink" title="8.做好master节点跟其他node节点的ssh互信,便于搭建"></a>8.做好master节点跟其他node节点的ssh互信,便于搭建</h2><pre><code>[root@linux-node1 ~]# ssh-key -t rsa ""
......
......
......
[root@linux-node1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.56.11
[root@linux-node1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.56.12
</code></pre>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>02-K8s集群搭建---CA证书生成</title>
    <url>/2018/06/01/02k8s-ji-qun-da-jianca-zheng-shu-sheng-cheng/</url>
    <content><![CDATA[<p>kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书；</p>
<p>使用证书的组件如下：</p>
<ul>
<li>etcd：使用 ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem；</li>
<li>kubelet：使用 ca.pem；</li>
<li>kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem；</li>
<li>kubectl：使用 ca.pem、admin-key.pem、admin.pem；</li>
<li>kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。</li>
<li>(后面的操作我们做到哪一步再去做对应组建的证书，以免出现差错</li>
</ul>
<h2 id="1-安装-CFSSL-二进制方式"><a href="#1-安装-CFSSL-二进制方式" class="headerlink" title="1. 安装 CFSSL(二进制方式)"></a>1. 安装 CFSSL(二进制方式)</h2><pre><code>[root@linux-node1 ~]# cd /usr/local/src
[root@linux-node1 src]# mv cfssl-certinfo_linux-amd64 /opt/kubernetes/bin/cfssl-certinfo
[root@linux-node1 src]# mv cfssljson_linux-amd64  /opt/kubernetes/bin/cfssljson
[root@linux-node1 src]# mv cfssl_linux-amd64  /opt/kubernetes/bin/cfssl
[root@linux-node1 src]# chmod +x  /opt/kubernetes/bin/cf*
复制cfssl命令文件到node2和node3节点。如果实际中多个节点，就都需要同步复制。
[root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.12: /opt/kubernetes/bin
[root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.13: /opt/kubernetes/bin
</code></pre>
<h2 id="2-初始化cfssl"><a href="#2-初始化cfssl" class="headerlink" title="2.初始化cfssl"></a>2.初始化cfssl</h2><pre><code>[root@linux-node1 src]# mkdir ssl &amp;&amp; cd ssl
[root@linux-node1 ssl]# cfssl print-defaults config &gt; config.json
[root@linux-node1 ssl]# cfssl print-defaults csr &gt; csr.json
</code></pre>
<h2 id="3-创建用来生成-CA-文件的-JSON-配置文件"><a href="#3-创建用来生成-CA-文件的-JSON-配置文件" class="headerlink" title="3.创建用来生成 CA 文件的 JSON 配置文件"></a>3.创建用来生成 CA 文件的 JSON 配置文件</h2><pre><code>[root@linux-node1 ssl]# cat &gt; ca-config.json &lt;&lt;EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "8760h"
      }
    }
  }
}
EOF
</code></pre>
<h2 id="创建用来生成-CA-证书签名请求（CSR）的-JSON-配置文件"><a href="#创建用来生成-CA-证书签名请求（CSR）的-JSON-配置文件" class="headerlink" title="创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件"></a>创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件</h2><pre><code>[root@linux-node1 ssl]# cat &gt; ca-csr.json &lt;&lt;EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
</code></pre>
<h2 id="5-生成CA证书（ca-pem）和密钥（ca-key-pem）"><a href="#5-生成CA证书（ca-pem）和密钥（ca-key-pem）" class="headerlink" title="5.生成CA证书（ca.pem）和密钥（ca-key.pem）"></a>5.生成CA证书（ca.pem）和密钥（ca-key.pem）</h2><pre><code>[root@linux-node1 ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
[root@linux-node1 ssl]# ls -l ca*
-rw-r--r-- 1 root root  290 May 28 15:25 ca-config.json
-rw-r--r-- 1 root root 1001 May 28 15:30 ca.csr
-rw-r--r-- 1 root root  208 May 28 15:30 ca-csr.json
-rw------- 1 root root 1675 May 28 15:30 ca-key.pem
-rw-r--r-- 1 root root 1359 May 28 15:30 ca.pem
</code></pre>
<h2 id="6-分发证书"><a href="#6-分发证书" class="headerlink" title="6.分发证书"></a>6.分发证书</h2><pre><code>[root@ linux-node1 ssl]# cp ca.csr ca.pem ca-key.pem ca-config.json /opt/kubernetes/ssl
scp证书到node2和node3节点
[root@ linux-node1 ssl]# scp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.12:/opt/kubernetes/ssl 
[root@ linux-node1 ssl]# scp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.13:/opt/kubernetes/ssl
</code></pre>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>03-K8s集群搭建---部署ETCD集群</title>
    <url>/2018/06/02/03k8s-ji-qun-da-jian-sanbu-shuetcd-ji-qun/</url>
    <content><![CDATA[<h2 id="0-解压安装分发etcd二进制包"><a href="#0-解压安装分发etcd二进制包" class="headerlink" title="0.解压安装分发etcd二进制包:"></a>0.解压安装分发etcd二进制包:</h2><pre><code>[root@linux-node1 src]# tar zxf etcd-v3.2.18-linux-amd64.tar.gz
[root@linux-node1 src]# cd etcd-v3.2.18-linux-amd64
[root@linux-node1 etcd-v3.2.18-linux-amd64]# chmod +x etcdctl 
[root@linux-node1 etcd-v3.2.18-linux-amd64]# cp etcd etcdctl /opt/kubernetes/bin/ 
[root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.12:/opt/kubernetes/bin/
[root@linux-node1 etcd-v3.2.18-linux-amd64]# scp etcd etcdctl 192.168.56.13:/opt/kubernetes/bin/
</code></pre>
<h2 id="1-创建-etcd-证书签名请求："><a href="#1-创建-etcd-证书签名请求：" class="headerlink" title="1.创建 etcd 证书签名请求："></a>1.创建 etcd 证书签名请求：</h2><pre><code>[root@linux-node1 ~]# cat &gt; etcd-csr.json &lt;&lt; EOF
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
"192.168.56.11",
"192.168.56.12",
"192.168.56.13"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
</code></pre>
<h2 id="2-生成-etcd-证书和私钥："><a href="#2-生成-etcd-证书和私钥：" class="headerlink" title="2.生成 etcd 证书和私钥："></a>2.生成 etcd 证书和私钥：</h2><pre><code>[root@linux-node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \
  -ca-key=/opt/kubernetes/ssl/ca-key.pem \
  -config=/opt/kubernetes/ssl/ca-config.json \
  -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
# 会生成以下证书文件
[root@linux-node1 ssl]# ll etc*
-rw-r--r-- 1 root root 1062 May 28 15:39 etcd.csr
-rw-r--r-- 1 root root  287 May 28 15:38 etcd-csr.json
-rw------- 1 root root 1679 May 28 15:39 etcd-key.pem
-rw-r--r-- 1 root root 1436 May 28 15:39 etcd.pem
</code></pre>
<h2 id="3-将证书移动到-opt-kubernetes-ssl目录下"><a href="#3-将证书移动到-opt-kubernetes-ssl目录下" class="headerlink" title="3.将证书移动到/opt/kubernetes/ssl目录下:"></a>3.将证书移动到/opt/kubernetes/ssl目录下:</h2><pre><code>[root@linux-node1 ~]# cp etcd*.pem /opt/kubernetes/ssl
[root@linux-node1 ~]# scp etcd*.pem 192.168.56.12:/opt/kubernetes/ssl
[root@linux-node1 ~]# scp etcd*.pem 192.168.56.13:/opt/kubernetes/ssl
[root@linux-node1 ~]# rm -f etcd.csr etcd-csr.json
</code></pre>
<h2 id="4-增加etcd配置文件"><a href="#4-增加etcd配置文件" class="headerlink" title="4.增加etcd配置文件:"></a>4.增加etcd配置文件:</h2><pre><code>!!!!! 注意这里需要修改相关对应服务器的IP地址 !!!!!
[root@linux-node1 ~]# vim /opt/kubernetes/cfg/etcd.conf
#[member]
ETCD_NAME="etcd-node1" # 修改节点对应名称
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
#ETCD_SNAPSHOT_COUNTER="10000"
#ETCD_HEARTBEAT_INTERVAL="100"
#ETCD_ELECTION_TIMEOUT="1000"
ETCD_LISTEN_PEER_URLS="https://192.168.56.11:2380" # 修改对应服务器IP
ETCD_LISTEN_CLIENT_URLS="https://192.168.56.11:2379,https://127.0.0.1:2379"
#ETCD_MAX_SNAPSHOTS="5"
#ETCD_MAX_WALS="5"
#ETCD_CORS=""
#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.56.11:2380" # 修改对应服务器IP
# if you use different ETCD_NAME (e.g. test),
# set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."
ETCD_INITIAL_CLUSTER="etcd-node1=https://192.168.56.11:2380,etcd-node2=https://192.168.56.12:2380,etcd-node3=https://192.168.56.13:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="k8s-etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.56.11:2379" # 修改对应服务器IP
#[security]
CLIENT_CERT_AUTH="true"
ETCD_CA_FILE="/opt/kubernetes/ssl/ca.pem"
ETCD_CERT_FILE="/opt/kubernetes/ssl/etcd.pem"
ETCD_KEY_FILE="/opt/kubernetes/ssl/etcd-key.pem"
PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_CA_FILE="/opt/kubernetes/ssl/ca.pem"
ETCD_PEER_CERT_FILE="/opt/kubernetes/ssl/etcd.pem"
ETCD_PEER_KEY_FILE="/opt/kubernetes/ssl/etcd-key.pem"
</code></pre>
<h2 id="5-增加etcd系统服务"><a href="#5-增加etcd系统服务" class="headerlink" title="5.增加etcd系统服务"></a>5.增加etcd系统服务</h2><pre><code>[root@linux-node1 ~]# cat  &gt; /etc/systemd/system/etcd.service &lt;&lt; EOF
[Unit]
Description=Etcd Server
After=network.target
[Service]
Type=simple
WorkingDirectory=/var/lib/etcd
EnvironmentFile=-/opt/kubernetes/cfg/etcd.conf
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /opt/kubernetes/bin/etcd"
Type=notify
[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h2 id="6-重新加载系统服务"><a href="#6-重新加载系统服务" class="headerlink" title="6.重新加载系统服务"></a>6.重新加载系统服务</h2><pre><code>[root@linux-node1 ~]# systemctl daemon-reload
[root@linux-node1 ~]# systemctl enable etcd

[root@linux-node1 ~]# scp /opt/kubernetes/cfg/etcd.conf 192.168.56.12:/opt/kubernetes/cfg/
[root@linux-node1 ~]# scp /etc/systemd/system/etcd.service 192.168.56.12:/etc/systemd/system/
[root@linux-node1 ~]# scp /opt/kubernetes/cfg/etcd.conf 192.168.56.13:/opt/kubernetes/cfg/
[root@linux-node1 ~]# scp /etc/systemd/system/etcd.service 192.168.56.13:/etc/systemd/system/
#在所有节点上创建etcd存储目录并启动etcd(注意这里需要三台尽量同时启动)
[root@linux-node1 ~]# mkdir /var/lib/etcd
[root@linux-node1 ~]# systemctl start etcd
[root@linux-node1 ~]# systemctl status etcd
# 其他节点重复以上操作，直到所有机器的 etcd 服务都已启动。
</code></pre>
<h2 id="7-验证ETCD集群"><a href="#7-验证ETCD集群" class="headerlink" title="7.验证ETCD集群"></a>7.验证ETCD集群</h2><pre><code>[root@linux-node1 ssl]# etcdctl --endpoints=https://192.168.56.11:2379 \
&gt;   --ca-file=/opt/kubernetes/ssl/ca.pem \
&gt;   --cert-file=/opt/kubernetes/ssl/etcd.pem \
&gt;   --key-file=/opt/kubernetes/ssl/etcd-key.pem cluster-health
member 6566e06d7343e1bb is healthy: got healthy result from https://192.168.56.11:2379
member 435fb0a8da627a4c is healthy: got healthy result from https://192.168.56.12:2379
member ce7b884e428b6c8c is healthy: got healthy result from https://192.168.56.13:2379
cluster is healthy  # 集群可用
</code></pre>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>04-K8s集群搭建---Master节点部署</title>
    <url>/2018/06/02/04k8s-ji-qun-da-jianmaster-jie-dian-bu-shu/</url>
    <content><![CDATA[<p>master节点涉及服务组件<code>ApiServer</code> , <code>Scheduler</code> , <code>ControllerManager</code>, <code>kubectl</code></p>
<h2 id="1-软件包准备"><a href="#1-软件包准备" class="headerlink" title="1.软件包准备:"></a>1.软件包准备:</h2><pre><code>[root@linux-node1 ~]# cd /usr/local/src/kubernetes
[root@linux-node1 kubernetes]# cp server/bin/kube-apiserver /opt/kubernetes/bin/
[root@linux-node1 kubernetes]# cp server/bin/kube-controller-manager /opt/kubernetes/bin/
[root@linux-node1 kubernetes]# cp server/bin/kube-scheduler /opt/kubernetes/bin/
</code></pre>
<h2 id="2-创建生成CSR的-JSON-配置文件"><a href="#2-创建生成CSR的-JSON-配置文件" class="headerlink" title="2.创建生成CSR的 JSON 配置文件:"></a>2.创建生成CSR的 JSON 配置文件:</h2><pre><code>[root@linux-node1 src]# cat &gt; kubernetes-csr.json &lt;&lt; EOF
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "192.168.56.11",
    "10.1.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
</code></pre>
<h2 id="3-生成-kubernetes-证书和私钥"><a href="#3-生成-kubernetes-证书和私钥" class="headerlink" title="3.生成 kubernetes 证书和私钥:"></a>3.生成 kubernetes 证书和私钥:</h2><pre><code>[root@linux-node1 src]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \
   -ca-key=/opt/kubernetes/ssl/ca-key.pem \
   -config=/opt/kubernetes/ssl/ca-config.json \
   -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
[root@linux-node1 src]# cp kubernetes*.pem /opt/kubernetes/ssl/
[root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.12:/opt/kubernetes/ssl/
[root@linux-node1 ~]# scp kubernetes*.pem 192.168.56.13:/opt/kubernetes/ssl/
</code></pre>
<h2 id="4-创建-kube-apiserver-使用的客户端-token-文件"><a href="#4-创建-kube-apiserver-使用的客户端-token-文件" class="headerlink" title="4.创建 kube-apiserver 使用的客户端 token 文件"></a>4.创建 kube-apiserver 使用的客户端 token 文件</h2><pre><code>[root@linux-node1 ~]#  head -c 16 /dev/urandom | od -An -t x | tr -d ' '
ad6d5bb607a186796d8861557df0d17f 
[root@linux-node1 ~]# vim /opt/kubernetes/ssl/ bootstrap-token.csv
ad6d5bb607a186796d8861557df0d17f,kubelet-bootstrap,10001,"system:kubelet-bootstrap"
</code></pre>
<h2 id="5-创建基础用户名-密码认证配置"><a href="#5-创建基础用户名-密码认证配置" class="headerlink" title="5.创建基础用户名/密码认证配置"></a>5.创建基础用户名/密码认证配置</h2><pre><code>[root@linux-node1 ~]# vim /opt/kubernetes/ssl/basic-auth.csv
admin,admin,1
readonly,readonly,2
</code></pre>
<h2 id="6-部署K8S-ApiServer服务"><a href="#6-部署K8S-ApiServer服务" class="headerlink" title="6.部署K8S ApiServer服务"></a>6.部署K8S ApiServer服务</h2><h3 id="a-将ApiServer配置为系统服务"><a href="#a-将ApiServer配置为系统服务" class="headerlink" title="a.将ApiServer配置为系统服务"></a>a.将ApiServer配置为系统服务</h3><pre><code>[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/opt/kubernetes/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \
  --bind-address=192.168.56.11 \
  --insecure-bind-address=127.0.0.1 \
  --authorization-mode=Node,RBAC \
  --runtime-config=rbac.authorization.k8s.io/v1 \
  --kubelet-https=true \
  --anonymous-auth=false \
  --basic-auth-file=/opt/kubernetes/ssl/basic-auth.csv \
  --enable-bootstrap-token-auth \
  --token-auth-file=/opt/kubernetes/ssl/bootstrap-token.csv \
  --service-cluster-ip-range=10.1.0.0/16 \
  --service-node-port-range=20000-40000 \
  --tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \
  --tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \
  --client-ca-file=/opt/kubernetes/ssl/ca.pem \
  --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \
  --etcd-cafile=/opt/kubernetes/ssl/ca.pem \
  --etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \
  --etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem \
  --etcd-servers=https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/opt/kubernetes/log/api-audit.log \
  --event-ttl=1h \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<h3 id="b-启动API-Server服务"><a href="#b-启动API-Server服务" class="headerlink" title="b.启动API Server服务"></a>b.启动API Server服务</h3><pre><code>[root@linux-node1 ~]# systemctl daemon-reload
[root@linux-node1 ~]# systemctl enable kube-apiserver
[root@linux-node1 ~]# systemctl start kube-apiserver
#查看API Server服务状态
[root@linux-node1 ~]# systemctl status kube-apiserver
</code></pre>
<h2 id="7-部署K8S-ControllerManager服务"><a href="#7-部署K8S-ControllerManager服务" class="headerlink" title="7.部署K8S ControllerManager服务"></a>7.部署K8S ControllerManager服务</h2><h3 id="a-将ControllerManager配置为系统服务"><a href="#a-将ControllerManager配置为系统服务" class="headerlink" title="a.将ControllerManager配置为系统服务"></a>a.将ControllerManager配置为系统服务</h3><pre><code>[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --master=http://127.0.0.1:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.1.0.0/16 \
  --cluster-cidr=10.2.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \
  --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \
  --root-ca-file=/opt/kubernetes/ssl/ca.pem \
  --leader-elect=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre>
<h3 id="b-启动ControllerManager服务"><a href="#b-启动ControllerManager服务" class="headerlink" title="b.启动ControllerManager服务"></a>b.启动ControllerManager服务</h3><pre><code>[root@linux-node1 ~]# systemctl daemon-reload
[root@linux-node1 scripts]# systemctl enable kube-controller-manager
[root@linux-node1 scripts]# systemctl start kube-controller-manager
# 查看ControllerManager服务状态
[root@linux-node1 scripts]# systemctl status kube-controller-manager
</code></pre>
<h2 id="部署K8S-Scheduler服务"><a href="#部署K8S-Scheduler服务" class="headerlink" title="部署K8S Scheduler服务"></a>部署K8S Scheduler服务</h2><h3 id="a-将Scheduler配置为系统服务"><a href="#a-将Scheduler配置为系统服务" class="headerlink" title="a.将Scheduler配置为系统服务"></a>a.将Scheduler配置为系统服务</h3><pre><code>[root@linux-node1 ~]# vim /usr/lib/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-scheduler \
  --address=127.0.0.1 \
  --master=http://127.0.0.1:8080 \
  --leader-elect=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre>
<h3 id="b-启动Scheduler服务"><a href="#b-启动Scheduler服务" class="headerlink" title="b.启动Scheduler服务"></a>b.启动Scheduler服务</h3><pre><code>[root@linux-node1 ~]# systemctl daemon-reload
[root@linux-node1 scripts]# systemctl enable kube-scheduler
[root@linux-node1 scripts]# systemctl start kube-scheduler
# 查看Scheduler服务状态
[root@linux-node1 scripts]# systemctl status kube-scheduler
</code></pre>
<h2 id="部署kubectl-命令行工具"><a href="#部署kubectl-命令行工具" class="headerlink" title="部署kubectl 命令行工具"></a>部署kubectl 命令行工具</h2><h3 id="1-准备二进制命令包"><a href="#1-准备二进制命令包" class="headerlink" title="1.准备二进制命令包"></a>1.准备二进制命令包</h3><pre><code>[root@linux-node1 ~]# cd /usr/local/src/kubernetes/client/bin
[root@linux-node1 bin]# cp kubectl /opt/kubernetes/bin/
</code></pre>
<h3 id="2-创建-admin-证书签名请求fds"><a href="#2-创建-admin-证书签名请求fds" class="headerlink" title="2.创建 admin 证书签名请求fds"></a>2.创建 admin 证书签名请求fds</h3><pre><code>[root@linux-node1 ~]# cd /usr/local/src/ssl/
[root@linux-node1 ssl]# vim admin-csr.json
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
</code></pre>
<h3 id="3-生成-admin-证书和私钥："><a href="#3-生成-admin-证书和私钥：" class="headerlink" title="3.生成 admin 证书和私钥："></a>3.生成 admin 证书和私钥：</h3><pre><code>[root@linux-node1 ssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \
   -ca-key=/opt/kubernetes/ssl/ca-key.pem \
   -config=/opt/kubernetes/ssl/ca-config.json \
   -profile=kubernetes admin-csr.json | cfssljson -bare admin
[root@linux-node1 ssl]# ls -l admin*
-rw-r--r-- 1 root root 1009 Mar  5 12:29 admin.csr
-rw-r--r-- 1 root root  229 Mar  5 12:28 admin-csr.json
-rw------- 1 root root 1675 Mar  5 12:29 admin-key.pem
-rw-r--r-- 1 root root 1399 Mar  5 12:29 admin.pem

[root@linux-node1 src]# mv admin*.pem /opt/kubernetes/ssl/
</code></pre>
<h3 id="4-设置集群参数"><a href="#4-设置集群参数" class="headerlink" title="4.设置集群参数"></a>4.设置集群参数</h3><pre><code>[root@linux-node1 src]# kubectl config set-cluster kubernetes \
   --certificate-authority=/opt/kubernetes/ssl/ca.pem \
   --embed-certs=true \
   --server=https://192.168.56.11:6443
Cluster "kubernetes" set.
</code></pre>
<h3 id="5-设置客户端认证参数"><a href="#5-设置客户端认证参数" class="headerlink" title="5.设置客户端认证参数"></a>5.设置客户端认证参数</h3><pre><code>[root@linux-node1 src]# kubectl config set-credentials admin \
   --client-certificate=/opt/kubernetes/ssl/admin.pem \
   --embed-certs=true \
   --client-key=/opt/kubernetes/ssl/admin-key.pem
User "admin" set.
</code></pre>
<h3 id="6-设置上下文参数"><a href="#6-设置上下文参数" class="headerlink" title="6.设置上下文参数"></a>6.设置上下文参数</h3><pre><code>[root@linux-node1 src]# kubectl config set-context kubernetes \
   --cluster=kubernetes \
   --user=admin
Context "kubernetes" created.
</code></pre>
<h3 id="7-设置默认上下文"><a href="#7-设置默认上下文" class="headerlink" title="7.设置默认上下文"></a>7.设置默认上下文</h3><pre><code>[root@linux-node1 src]# kubectl config use-context kubernetes
Switched to context "kubernetes".
</code></pre>
<h3 id="8-使用kubectl工具"><a href="#8-使用kubectl工具" class="headerlink" title="8.使用kubectl工具"></a>8.使用kubectl工具</h3><pre><code>[root@linux-node1 ~]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-1               Healthy   {"health":"true"}   
etcd-2               Healthy   {"health":"true"}   
etcd-0               Healthy   {"health":"true"} 
</code></pre>
<p>至此 k8sMaster端以及etcd集群已经搭建完毕</p>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>05-K8s集群搭建---Node节点部署</title>
    <url>/2018/06/02/05k8s-ji-qun-da-jiannode-jie-dian-bu-shu/</url>
    <content><![CDATA[<h2 id="部署kubelet"><a href="#部署kubelet" class="headerlink" title="部署kubelet"></a>部署kubelet</h2><h3 id="1-软件包准备"><a href="#1-软件包准备" class="headerlink" title="1.软件包准备"></a>1.软件包准备</h3><pre><code>[root@linux-node1 ~]# cd /usr/local/src/kubernetes/server/bin/
[root@linux-node1 bin]# cp kubelet kube-proxy /opt/kubernetes/bin/
[root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.12:/opt/kubernetes/bin/
[root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.13:/opt/kubernetes/bin/
</code></pre>
<h3 id="2-创建角色绑定"><a href="#2-创建角色绑定" class="headerlink" title="2.创建角色绑定"></a>2.创建角色绑定</h3><p>kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests):</p>
<pre><code>[root@linux-node1 ~]# cd /usr/local/src/ssl
[root@linux-node1 ~]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
clusterrolebinding "kubelet-bootstrap" created
</code></pre>
<p><code>--user=kubelet-bootstrap</code> 是在 /opt/kubernetes/ssl/bootstrap-token.csv 文件中指定的用户名，同时也写入了bootstrap.kubeconfig 文件；</p>
<h3 id="3-创建-kubelet-bootstrapping-kubeconfig-文件-设置集群参数"><a href="#3-创建-kubelet-bootstrapping-kubeconfig-文件-设置集群参数" class="headerlink" title="3.创建 kubelet bootstrapping kubeconfig 文件 设置集群参数"></a>3.创建 kubelet bootstrapping kubeconfig 文件 设置集群参数</h3><pre><code>[root@linux-node1 ~]# kubectl config set-cluster kubernetes \
   --certificate-authority=/opt/kubernetes/ssl/ca.pem \
   --embed-certs=true \
   --server=https://192.168.56.11:6443 \
   --kubeconfig=bootstrap.kubeconfig
Cluster "kubernetes" set.

# 设置客户端认证参数
[root@linux-node1 ~]# kubectl config set-credentials kubelet-bootstrap \
   --token=ad6d5bb607a186796d8861557df0d17f \
   --kubeconfig=bootstrap.kubeconfig   
User "kubelet-bootstrap" set.

# 设置上下文参数
[root@linux-node1 ~]# kubectl config set-context default \
   --cluster=kubernetes \
   --user=kubelet-bootstrap \
   --kubeconfig=bootstrap.kubeconfig
Context "default" created.

#选择默认上下文
[root@linux-node1 ~]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
Switched to context "default".
[root@linux-node1 kubernetes]# cp bootstrap.kubeconfig /opt/kubernetes/cfg
[root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.12:/opt/kubernetes/cfg
[root@linux-node1 kubernetes]# scp bootstrap.kubeconfig 192.168.56.13:/opt/kubernetes/cfg
</code></pre>
<h3 id="4-配置CNI支持"><a href="#4-配置CNI支持" class="headerlink" title="4.配置CNI支持"></a>4.配置CNI支持</h3><pre><code>[root@linux-node2 ~]# mkdir -p /etc/cni/net.d
[root@linux-node2 ~]# vim /etc/cni/net.d/10-default.conf
{
        "name": "flannel",
        "type": "flannel",
        "delegate": {
            "bridge": "docker0",
            "isDefaultGateway": true,
            "mtu": 1400
        }
}
</code></pre>
<h3 id="5-创建kubelet所需目录"><a href="#5-创建kubelet所需目录" class="headerlink" title="5.创建kubelet所需目录"></a>5.创建kubelet所需目录</h3><pre><code>[root@linux-node2 ~]# mkdir /var/lib/kubelet
</code></pre>
<h3 id="6-创建kubelet系统服务"><a href="#6-创建kubelet系统服务" class="headerlink" title="6.创建kubelet系统服务"></a>6.创建kubelet系统服务</h3><pre><code>[root@k8s-node2 ~]# cat &gt; /usr/lib/systemd/system/kubelet.service  &lt;&lt; EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/opt/kubernetes/bin/kubelet \
  --address=192.168.56.12 \
  --hostname-override=192.168.56.12 \
  --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \
  --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \
  --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \
  --cert-dir=/opt/kubernetes/ssl \
  --network-plugin=cni \
  --cni-conf-dir=/etc/cni/net.d \
  --cni-bin-dir=/opt/kubernetes/bin/cni \
  --cluster-dns=10.1.0.2 \
  --cluster-domain=cluster.local. \
  --hairpin-mode hairpin-veth \
  --allow-privileged=true \
  --fail-swap-on=false \
  --logtostderr=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="7-启动并查看服务状态："><a href="#7-启动并查看服务状态：" class="headerlink" title="7.启动并查看服务状态："></a>7.启动并查看服务状态：</h3><pre><code>[root@linux-node2 ~]# systemctl daemon-reload
[root@linux-node2 ~]# systemctl enable kubelet
[root@linux-node2 ~]# systemctl start kubelet
[root@linux-node2 kubernetes]# systemctl status kubelet
</code></pre>
<h3 id="8-查看csr请求-注意是在linux-node1上执行。"><a href="#8-查看csr请求-注意是在linux-node1上执行。" class="headerlink" title="8.查看csr请求 注意是在linux-node1上执行。"></a>8.查看csr请求 注意是在linux-node1上执行。</h3><pre><code>[root@linux-node1 ~]# kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-0_w5F1FM_la_SeGiu3Y5xELRpYUjjT2icIFk9gO9KOU   1m        kubelet-bootstrap   Pending
</code></pre>
<h3 id="9-批准kubelet-的-TLS-证书请求"><a href="#9-批准kubelet-的-TLS-证书请求" class="headerlink" title="9.批准kubelet 的 TLS 证书请求"></a>9.批准kubelet 的 TLS 证书请求</h3><pre><code>[root@linux-node1 ~]# kubectl get csr|grep 'Pending' | awk 'NR&gt;0{print $1}'| xargs kubectl certificate approve

#执行完毕后，查看节点状态已经是Ready的状态了
[root@linux-node1 ssl]# kubectl get nodes
NAME            STATUS    ROLES     AGE       VERSION
192.168.56.12   Ready     &lt;none&gt;    2m        v1.10.1
</code></pre>
<h2 id="部署Kubernetes-Proxy"><a href="#部署Kubernetes-Proxy" class="headerlink" title="部署Kubernetes Proxy"></a>部署Kubernetes Proxy</h2><h3 id="1-配置kube-proxy使用LVS"><a href="#1-配置kube-proxy使用LVS" class="headerlink" title="1.配置kube-proxy使用LVS"></a>1.配置kube-proxy使用LVS</h3><pre><code>[root@linux-node2 ~]# yum install -y ipvsadm ipset conntrack
</code></pre>
<h3 id="2-创建-kube-proxy-证书请求"><a href="#2-创建-kube-proxy-证书请求" class="headerlink" title="2.创建 kube-proxy 证书请求"></a>2.创建 kube-proxy 证书请求</h3><pre><code>[root@linux-node1 ~]# cd /usr/local/src/ssl/
[root@linux-node1 ~]# cat &gt; kube-proxy-csr.json &lt;&lt; EOF
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
</code></pre>
<h3 id="3-生成证书"><a href="#3-生成证书" class="headerlink" title="3.生成证书"></a>3.生成证书</h3><pre><code>[root@linux-node1~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \
   -ca-key=/opt/kubernetes/ssl/ca-key.pem \
   -config=/opt/kubernetes/ssl/ca-config.json \
   -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
</code></pre>
<h3 id="4-分发证书到所有Node节点"><a href="#4-分发证书到所有Node节点" class="headerlink" title="4.分发证书到所有Node节点"></a>4.分发证书到所有Node节点</h3><pre><code>[root@linux-node1 ssl]# cp kube-proxy*.pem /opt/kubernetes/ssl/
[root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.12:/opt/kubernetes/ssl/
[root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.12:/opt/kubernetes/ssl/
</code></pre>
<h3 id="5-创建kube-proxy配置文件"><a href="#5-创建kube-proxy配置文件" class="headerlink" title="5.创建kube-proxy配置文件"></a>5.创建kube-proxy配置文件</h3><pre><code># 设置集群参数
[root@linux-node1 ssl]# kubectl config set-cluster kubernetes \
   --certificate-authority=/opt/kubernetes/ssl/ca.pem \
   --embed-certs=true \
   --server=https://192.168.56.11:6443 \
   --kubeconfig=kube-proxy.kubeconfig
Cluster "kubernetes" set.

# 设置客户端认证参数
[root@linux-node1 ssl]# kubectl config set-credentials kube-proxy \
   --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \
   --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \
   --embed-certs=true \
   --kubeconfig=kube-proxy.kubeconfig
User "kube-proxy" set.

# 设置上下文参数
[root@linux-node1 ssl]# kubectl config set-context default \
   --cluster=kubernetes \
   --user=kube-proxy \
   --kubeconfig=kube-proxy.kubeconfig
Context "default" created.

# 设置默认上下文
[root@linux-node1 ssl]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
Switched to context "default".
</code></pre>
<h3 id="6-分发kubeconfig配置文件"><a href="#6-分发kubeconfig配置文件" class="headerlink" title="6.分发kubeconfig配置文件"></a>6.分发kubeconfig配置文件</h3><pre><code>[root@linux-node1 ssl]# cp kube-proxy.kubeconfig /opt/kubernetes/cfg/
[root@linux-node1 ssl]# scp kube-proxy.kubeconfig 192.168.56.12:/opt/kubernetes/cfg/
[root@linux-node1 ssl]# scp kube-proxy.kubeconfig 192.168.56.13:/opt/kubernetes/cfg/
</code></pre>
<h3 id="7-创建kube-proxy系统服务"><a href="#7-创建kube-proxy系统服务" class="headerlink" title="7.创建kube-proxy系统服务"></a>7.创建kube-proxy系统服务</h3><pre><code>[root@linux-node2 bin]# mkdir /var/lib/kube-proxy

[root@k8s-node2 ~]# vim /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/opt/kubernetes/bin/kube-proxy \
  --bind-address=192.168.56.12 \
  --hostname-override=192.168.56.12 \
  --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig \
  --masquerade-all \
  --feature-gates=SupportIPVSProxyMode=true \
  --proxy-mode=ipvs \
  --ipvs-min-sync-period=5s \
  --ipvs-sync-period=5s \
  --ipvs-scheduler=rr \
  --logtostderr=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log

Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre>
<h3 id="8-启动Kubernetes-Proxy并查看状态"><a href="#8-启动Kubernetes-Proxy并查看状态" class="headerlink" title="8.启动Kubernetes Proxy并查看状态:"></a>8.启动Kubernetes Proxy并查看状态:</h3><pre><code>systemctl status kube-proxy
[root@linux-node2 ~]# systemctl daemon-reload
[root@linux-node2 ~]# systemctl enable kube-proxy
[root@linux-node2 ~]# systemctl start kube-proxy
[root@linux-node2 ~]# systemctl status kube-proxy

#检查LVS状态
[root@linux-node2 ~]# ipvsadm -L -n
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.1.0.1:443 rr persistent 10800
  -&gt; 192.168.56.11:6443           Masq    1      0          0        
</code></pre>
<h3 id="9-验证node节点是否正常部署-node3按照以上步骤启动服务即可1，7，8"><a href="#9-验证node节点是否正常部署-node3按照以上步骤启动服务即可1，7，8" class="headerlink" title="9.验证node节点是否正常部署(node3按照以上步骤启动服务即可1，7，8)"></a>9.验证node节点是否正常部署(node3按照以上步骤启动服务即可1，7，8)</h3><pre><code>[root@linux-node1 ssl]# kubectl get nodes
NAME            STATUS    ROLES     AGE       VERSION
192.168.56.12   Ready     &lt;none&gt;    12m       v1.10.1
192.168.56.13   Ready     &lt;none&gt;    2d        v1.10.1
</code></pre>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>06-K8s集群搭建---Flannel网络部署</title>
    <url>/2018/06/02/06k8s-ji-qun-da-jianflannel-wang-luo-bu-shu/</url>
    <content><![CDATA[<h3 id="1-为Flannel生成证书"><a href="#1-为Flannel生成证书" class="headerlink" title="1.为Flannel生成证书"></a>1.为Flannel生成证书</h3><pre><code>[root@linux-node1 ~]# vim flanneld-csr.json
{
  "CN": "flanneld",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
</code></pre>
<h3 id="2-生成证书"><a href="#2-生成证书" class="headerlink" title="2.生成证书"></a>2.生成证书</h3><pre><code>[root@linux-node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \
   -ca-key=/opt/kubernetes/ssl/ca-key.pem \
   -config=/opt/kubernetes/ssl/ca-config.json \
   -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
</code></pre>
<h3 id="3-分发证书"><a href="#3-分发证书" class="headerlink" title="3.分发证书"></a>3.分发证书</h3><pre><code>[root@linux-node1 ~]# cp flanneld*.pem /opt/kubernetes/ssl/
[root@linux-node1 ~]# scp flanneld*.pem 192.168.56.12:/opt/kubernetes/ssl/
[root@linux-node1 ~]# scp flanneld*.pem 192.168.56.13:/opt/kubernetes/ssl/
</code></pre>
<h3 id="4-下载Flannel软件包"><a href="#4-下载Flannel软件包" class="headerlink" title="4.下载Flannel软件包"></a>4.下载Flannel软件包</h3><pre><code>[root@linux-node1 ~]# cd /usr/local/src
# wget
 https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz
[root@linux-node1 src]# tar zxf flannel-v0.10.0-linux-amd64.tar.gz
[root@linux-node1 src]# cp flanneld mk-docker-opts.sh /opt/kubernetes/bin/
复制到linux-node2节点
[root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.12:/opt/kubernetes/bin/
[root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.13:/opt/kubernetes/bin/
复制对应脚本到/opt/kubernetes/bin目录下。
[root@linux-node1 ~]# cd /usr/local/src/kubernetes/cluster/centos/node/bin/
[root@linux-node1 bin]# cp remove-docker0.sh /opt/kubernetes/bin/
[root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.12:/opt/kubernetes/bin/
[root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.13:/opt/kubernetes/bin/
</code></pre>
<h3 id="5-配置Flannel"><a href="#5-配置Flannel" class="headerlink" title="5.配置Flannel"></a>5.配置Flannel</h3><pre><code>[root@linux-node1 ~]# vim /opt/kubernetes/cfg/flannel
FLANNEL_ETCD="-etcd-endpoints=https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379"
FLANNEL_ETCD_KEY="-etcd-prefix=/kubernetes/network"
FLANNEL_ETCD_CAFILE="--etcd-cafile=/opt/kubernetes/ssl/ca.pem"
FLANNEL_ETCD_CERTFILE="--etcd-certfile=/opt/kubernetes/ssl/flanneld.pem"
FLANNEL_ETCD_KEYFILE="--etcd-keyfile=/opt/kubernetes/ssl/flanneld-key.pem"
复制配置到其它节点上
[root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.12:/opt/kubernetes/cfg/
[root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.13:/opt/kubernetes/cfg/
</code></pre>
<h3 id="6-设置Flannel系统服务"><a href="#6-设置Flannel系统服务" class="headerlink" title="6.设置Flannel系统服务"></a>6.设置Flannel系统服务</h3><pre><code>[root@linux-node1 ~]# vim /usr/lib/systemd/system/flannel.service
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
Before=docker.service

[Service]
EnvironmentFile=-/opt/kubernetes/cfg/flannel
ExecStartPre=/opt/kubernetes/bin/remove-docker0.sh
ExecStart=/opt/kubernetes/bin/flanneld --ip-masq ${FLANNEL_ETCD} ${FLANNEL_ETCD_KEY} ${FLANNEL_ETCD_CAFILE} ${FLANNEL_ETCD_CERTFILE} ${FLANNEL_ETCD_KEYFILE}
ExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/docker

Type=notify

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
复制系统服务脚本到其它节点上
# scp /usr/lib/systemd/system/flannel.service 192.168.56.12:/usr/lib/systemd/system/
# scp /usr/lib/systemd/system/flannel.service 192.168.56.13:/usr/lib/systemd/system/
</code></pre>
<h2 id="Flannel-CNI集成"><a href="#Flannel-CNI集成" class="headerlink" title="Flannel CNI集成"></a>Flannel CNI集成</h2><h3 id="1-下载CNI插件"><a href="#1-下载CNI插件" class="headerlink" title="1.下载CNI插件"></a>1.下载CNI插件</h3><pre><code>https://github.com/containernetworking/plugins/releases
wget https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz
[root@linux-node1 ~]# mkdir /opt/kubernetes/bin/cni
[root@linux-node1 src]# tar zxf cni-plugins-amd64-v0.7.1.tgz -C /opt/kubernetes/bin/cni
# scp -r /opt/kubernetes/bin/cni/* 192.168.56.12:/opt/kubernetes/bin/cni/
# scp -r /opt/kubernetes/bin/cni/* 192.168.56.13:/opt/kubernetes/bin/cni/
</code></pre>
<h3 id="2-创建Etcd的key"><a href="#2-创建Etcd的key" class="headerlink" title="2.创建Etcd的key"></a>2.创建Etcd的key</h3><pre><code>/opt/kubernetes/bin/etcdctl --ca-file /opt/kubernetes/ssl/ca.pem --cert-file /opt/kubernetes/ssl/flanneld.pem --key-file /opt/kubernetes/ssl/flanneld-key.pem \
      --no-sync -C https://192.168.56.11:2379,https://192.168.56.12:2379,https://192.168.56.13:2379 \
mk /kubernetes/network/config '{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 1 }}' &gt;/dev/null 2&gt;&amp;1
</code></pre>
<h3 id="3-启动flannel"><a href="#3-启动flannel" class="headerlink" title="3.启动flannel"></a>3.启动flannel</h3><pre><code>[root@linux-node1 ~]# systemctl daemon-reload
[root@linux-node1 ~]# systemctl enable flannel
[root@linux-node1 ~]# chmod +x /opt/kubernetes/bin/*
[root@linux-node1 ~]# systemctl start flannel
</code></pre>
<h3 id="4-查看服务状态"><a href="#4-查看服务状态" class="headerlink" title="4.查看服务状态"></a>4.查看服务状态</h3><pre><code>[root@linux-node1 ~]# systemctl status flannel
</code></pre>
<h2 id="配置Docker使用Flannel"><a href="#配置Docker使用Flannel" class="headerlink" title="配置Docker使用Flannel"></a>配置Docker使用Flannel</h2><pre><code>[root@linux-node1 ~]# vim /usr/lib/systemd/system/docker.service
[Unit] #在Unit下面修改After和增加Requires
After=network-online.target firewalld.service flannel.service
Wants=network-online.target
Requires=flannel.service

[Service] #增加EnvironmentFile=-/run/flannel/docker
Type=not
EnvironmentFile=-/run/flannel/docker
ExecStart=/usr/bin/dockerd $DOCKER_OPTS
</code></pre>
<h3 id="1-将配置复制到另外的节点"><a href="#1-将配置复制到另外的节点" class="headerlink" title="1.将配置复制到另外的节点"></a>1.将配置复制到另外的节点</h3><pre><code>[root@linux-node1 ~]# scp /usr/lib/systemd/system/docker.service 192.168.56.12:/usr/lib/systemd/system/
[root@linux-node1 ~]# scp /usr/lib/systemd/system/docker.service 192.168.56.13:/usr/lib/systemd/system/
</code></pre>
<h3 id="2-重启Docker"><a href="#2-重启Docker" class="headerlink" title="2.重启Docker"></a>2.重启Docker</h3><pre><code>[root@linux-node1 ~]# systemctl daemon-reload
[root@linux-node1 ~]# systemctl restart docker

# 此时就可以看到docker0网卡的IP 跟flaneel是一个网段的了：
[root@linux-node2 ~]# ip a | egrep "flannel|docker0"
6: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN
    inet 10.2.70.0/32 scope global flannel.1
7: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1400 qdisc noqueue state DOWN
    inet 10.2.70.1/24 brd 10.2.70.255 scope global docker0
</code></pre>
]]></content>
      <categories>
        <category>虚拟化&amp;amp;云计算&amp;amp;大数据</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>07-K8s集群搭建---创建K8s应用</title>
    <url>/2018/06/03/07k8s-ji-qun-da-jianchuang-jiank8s-ying-yong/</url>
    <content><![CDATA[<h3 id="1-创建一个测试用的deployment"><a href="#1-创建一个测试用的deployment" class="headerlink" title="1.创建一个测试用的deployment"></a>1.创建一个测试用的deployment</h3><pre><code>[root@linux-node1 ~]# kubectl run net-test --image=alpine --replicas=3 sleep 360000
</code></pre>
<h3 id="2-查看创建情况"><a href="#2-查看创建情况" class="headerlink" title="2.查看创建情况"></a>2.查看创建情况</h3><pre><code>[root@linux-node1 ~]# kubectl get pods -o wide
NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
net-test-5767cb94df-6crmr   1/1       Running   0          17s       10.2.97.176   192.168.56.13
net-test-5767cb94df-9mpmb   1/1       Running   0          17s       10.2.70.187   192.168.56.12
net-test-5767cb94df-x8l44   1/1       Running   0          17s       10.2.97.175   192.168.56.13

# 使用kubectl创建了一个名为net-test 的deployment, 镜像是alpine 副本数为3，
[root@linux-node1 ~]# kubectl get deployments
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
net-test   3         3         3            3           7m
</code></pre>
<h3 id="3-测试联通性"><a href="#3-测试联通性" class="headerlink" title="3.测试联通性"></a>3.测试联通性</h3><pre><code>ping 10.2.97.176
</code></pre>
<h2 id="通过yaml文件部署应用"><a href="#通过yaml文件部署应用" class="headerlink" title="通过yaml文件部署应用"></a>通过yaml文件部署应用</h2><h3 id="1-编写一个nginx-deployment-yaml文件"><a href="#1-编写一个nginx-deployment-yaml文件" class="headerlink" title="1.编写一个nginx-deployment.yaml文件:"></a>1.编写一个nginx-deployment.yaml文件:</h3><pre><code>[root@linux-node1 ~]# cat &gt;  nginx-deployment.yaml  &lt;&lt; EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.10.3
        ports:
        - containerPort: 80
   EOF
</code></pre>
<h3 id="2-执行创建"><a href="#2-执行创建" class="headerlink" title="2.执行创建:"></a>2.执行创建:</h3><pre><code>[root@linux-node1 ~]# kubectl create -f nginx-deployment.yaml
deployment.apps "nginx-deployment" created

# 查看deployment
[root@linux-node1 ~]# kubectl get deployment/nginx-deployment
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   10        10        10           10          1m

# 查看deployment 详情
[root@linux-node1 ~]# kubectl describe deployment/nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 31 May 2018 20:20:17 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=1
Selector:               app=nginx
Replicas:               10 desired | 10 updated | 10 total | 10 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.10.3
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-75d56bb955 (10/10 replicas created)
Events:
  Type    Reason             Age               From                   Message
  ----    ------             ----              ----                   -------
  Normal  ScalingReplicaSet  5m                deployment-controller  Scaled up replica set nginx-deployment-75d56bb955 to 3
  Normal  ScalingReplicaSet  1m (x2 over 3m)   deployment-controller  Scaled down replica set nginx-deployment-75d56bb955 to 3
  Normal  ScalingReplicaSet  57s (x3 over 5m)  deployment-controller  Scaled up replica set nginx-deployment-75d56bb955 to 10
  
# 查看pod ,可以看出按照我们的描述文件里面的副本数已经有10个POD通过Schedule到了不同的node节点上面
[root@linux-node1 ~]# kubectl get pod -o wide
NAME                                READY     STATUS    RESTARTS   AGE       IP            NODE
net-test-5767cb94df-6crmr           1/1       Running   0          40m       10.2.97.178   192.168.56.13
net-test-5767cb94df-9mpmb           1/1       Running   0          40m       10.2.70.187   192.168.56.12
net-test-5767cb94df-x8l44           1/1       Running   0          40m       10.2.97.177   192.168.56.13
nginx-deployment-75d56bb955-255w2   1/1       Running   0          3m        10.2.70.196   192.168.56.12
nginx-deployment-75d56bb955-5ckvs   1/1       Running   0          8m        10.2.70.188   192.168.56.12
nginx-deployment-75d56bb955-7dq87   1/1       Running   0          3m        10.2.70.198   192.168.56.12
nginx-deployment-75d56bb955-8vl7p   1/1       Running   0          3m        10.2.97.191   192.168.56.13
nginx-deployment-75d56bb955-9f9ms   1/1       Running   0          3m        10.2.97.189   192.168.56.13
nginx-deployment-75d56bb955-9g9bk   1/1       Running   0          3m        10.2.97.188   192.168.56.13
nginx-deployment-75d56bb955-9kz2r   1/1       Running   0          3m        10.2.70.197   192.168.56.12
nginx-deployment-75d56bb955-s78gs   1/1       Running   0          8m        10.2.70.189   192.168.56.12
nginx-deployment-75d56bb955-v8n5v   1/1       Running   0          3m        10.2.97.190   192.168.56.13
nginx-deployment-75d56bb955-zfb4m   1/1       Running   0          8m        10.2.97.179   192.168.56.13

# 查看某个pod详情
[root@linux-node1 ~]# kubectl describe pod/nginx-deployment-75d56bb955-255w2
Name:           nginx-deployment-75d56bb955-255w2
Namespace:      default
Node:           192.168.56.12/192.168.56.12
Start Time:     Thu, 31 May 2018 20:24:31 +0800
Labels:         app=nginx
                pod-template-hash=3181266511
Annotations:    &lt;none&gt;
Status:         Running
IP:             10.2.70.196
Controlled By:  ReplicaSet/nginx-deployment-75d56bb955
Containers:
  nginx:
    Container ID:   docker://48ad80730efd6d744ac6d31b34778eb8e75b0f4c6698e1c1fc8f6046a6a77b2b
    Image:          nginx:1.10.3
    Image ID:       docker-pullable://nginx@sha256:6202beb06ea61f44179e02ca965e8e13b961d12640101fca213efbfd145d7575
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 31 May 2018 20:24:33 +0800
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5htws (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  default-token-5htws:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-5htws
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     &lt;none&gt;
Events:
  Type    Reason                 Age   From                    Message
  ----    ------                 ----  ----                    -------
  Normal  SuccessfulMountVolume  3m    kubelet, 192.168.56.12  MountVolume.SetUp succeeded for volume "default-token-5htws"
  Normal  Pulled                 3m    kubelet, 192.168.56.12  Container image "nginx:1.10.3" already present on machine
  Normal  Created                3m    kubelet, 192.168.56.12  Created container
  Normal  Started                3m    kubelet, 192.168.56.12  Started container
  Normal  Scheduled              2m    default-scheduler       Successfully assigned nginx-deployment-75d56bb955-255w2 to 192.168.56.12


# 访问Nginx pod
[root@linux-node1 ~]# curl -I http://10.2.70.196
HTTP/1.1 200 OK
Server: nginx/1.10.3
Date: Thu, 31 May 2018 12:30:45 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 31 Jan 2017 15:01:11 GMT
Connection: keep-alive
ETag: "5890a6b7-264"
Accept-Ranges: byte

# 更新deployment
[root@linux-node1 ~]# kubectl set image  deployment/nginx-deployment nginx=nginx:1.12.2 --record
deployment.apps "nginx-deployment" image updated

# 可以看到更新已经成功(滚动更新)
[root@linux-node1 ~]# kubectl get deployment/nginx-deployment -o wide
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR
nginx-deployment   10        10        10           10          17m       nginx        nginx:1.12.2   app=nginx

[root@linux-node1 ~]# curl -I 10.2.97.192
HTTP/1.1 200 OK
Server: nginx/1.12.2   # 版本已经更新成功
Date: Thu, 31 May 2018 12:35:54 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 11 Jul 2017 13:29:18 GMT
Connection: keep-alive
ETag: "5964d2ae-264"
Accept-Ranges: bytes

# 查看deployment更新历史
[root@linux-node1 ~]# kubectl rollout history deployment/nginx-deployment
deployments "nginx-deployment"
REVISION  CHANGE-CAUSE
1         &lt;none&gt; # 用于在之前在创建创建nginx deployment的时候 未加参数 --record 所以这个信息为none
2         kubectl set image deployment/nginx-deployment nginx=nginx:1.12.2 --record=true

# 查看版本详情(--revison=VERSION_NUM)
[root@linux-node1 ~]# kubectl rollout history deployment/nginx-deployment --revision=1
deployments "nginx-deployment" with revision #1
Pod Template:
  Labels:   app=nginx
    pod-template-hash=3181266511
  Containers:
   nginx:
    Image:  nginx:1.10.3
    Port:   80/TCP
    Host Port:  0/TCP
    Environment:    &lt;none&gt;
    Mounts: &lt;none&gt;
  Volumes:  &lt;none&gt;
   
# 版本回滚至上一个版本:
[root@linux-node1 ~]# kubectl rollout undo deployment/nginx-deployment
deployment.apps "nginx-deployment"

[root@linux-node1 ~]# kubectl get deployment/nginx-deployment -o wide
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR
nginx-deployment   10        10        10           10          26m       nginx        nginx:1.10.3   app=nginx

# 扩容pod数量
[root@linux-node1 ~]# kubectl scale --replicas=20 deployment nginx-deployment
deployment.extensions "nginx-deployment" scaled

[root@linux-node1 ~]# kubectl get deployment/nginx-deployment -o wide
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES         SELECTOR
nginx-deployment   20        20        20           20          28m       nginx        nginx:1.10.3   app=nginx
</code></pre>
<p>那问题来了:</p>
<ol>
<li>每次收到获取podIP太扯了，总不能每次都要手动改程序或者配置才能访问服务吧，要怎么提前知道podIP呢？</li>
<li>Pod在运行中可能会重建，IP变了怎么解？</li>
<li>如何在多个Pod中实现负载均衡嘞？</li>
</ol>
<p>这些问题使用k8s Service就可以解决。</p>
<p>Service可以将pod IP封装起来，即使Pod发生重建，依然可以通过Service来访问Pod提供的服务。此外，Service还解决了负载均衡的问题，大家可以多访问几次Service，然后通过kubectl logs 来查看Nginx Pod的访问日志来确认</p>
<h3 id="创建一个对于nginx-deployment的一个service"><a href="#创建一个对于nginx-deployment的一个service" class="headerlink" title="创建一个对于nginx deployment的一个service"></a>创建一个对于nginx deployment的一个service</h3><h4 id="1-编写nginx-service-yaml文件"><a href="#1-编写nginx-service-yaml文件" class="headerlink" title="1.编写nginx-service.yaml文件"></a>1.编写nginx-service.yaml文件</h4><pre><code>[root@linux-node1 ~]# cat &gt; nginx-service.yaml &lt;&lt; EOF
kind: Service 
apiVersion: v1
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
EOF
</code></pre>
<h4 id="2-执行创建操作"><a href="#2-执行创建操作" class="headerlink" title="2.执行创建操作:"></a>2.执行创建操作:</h4><pre><code>[root@linux-node1 ~]# kubectl create -f nginx-service.yaml
service "nginx-service" created

[root@linux-node1 ~]# kubectl get service -o wide
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE       SELECTOR
kubernetes      ClusterIP   10.1.0.1       &lt;none&gt;        443/TCP   3d        &lt;none&gt;
nginx-service   ClusterIP   10.1.146.223   &lt;none&gt;        80/TCP    30s       app=nginx

# 访问VIP
[root@linux-node1 ~]# curl -I http://10.1.146.223
HTTP/1.1 200 OK
Server: nginx/1.10.3
Date: Thu, 31 May 2018 12:54:54 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 31 Jan 2017 15:01:11 GMT
Connection: keep-alive
ETag: "5890a6b7-264"
Accept-Ranges: bytes

# 在节点2上查看LVS，通过k8s封装LVS来实现了负载均衡
[root@linux-node2 ~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.1.0.1:443 rr persistent 10800
  -&gt; 192.168.56.11:6443           Masq    1      1          0
TCP  10.1.146.223:80 rr
  -&gt; 10.2.70.204:80               Masq    1      0          0
  -&gt; 10.2.70.205:80               Masq    1      0          1
  -&gt; 10.2.70.206:80               Masq    1      0          0
  -&gt; 10.2.70.207:80               Masq    1      0          0
  -&gt; 10.2.70.208:80               Masq    1      0          0
  -&gt; 10.2.70.209:80               Masq    1      0          0
  -&gt; 10.2.70.210:80               Masq    1      0          0
  -&gt; 10.2.70.211:80               Masq    1      0          0
  -&gt; 10.2.70.212:80               Masq    1      0          0
  -&gt; 10.2.70.213:80               Masq    1      0          0
  -&gt; 10.2.97.197:80               Masq    1      0          0
  -&gt; 10.2.97.198:80               Masq    1      0          0
  -&gt; 10.2.97.199:80               Masq    1      0          1
  -&gt; 10.2.97.200:80               Masq    1      0          0
  -&gt; 10.2.97.201:80               Masq    1      0          0
  -&gt; 10.2.97.202:80               Masq    1      0          1
  -&gt; 10.2.97.203:80               Masq    1      0          1
  -&gt; 10.2.97.204:80               Masq    1      0          0
  -&gt; 10.2.97.205:80               Masq    1      0          0
</code></pre>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>08-K8s集群搭建---CoreDNS创建&amp;DashBoard</title>
    <url>/2018/06/03/08k8s-ji-qun-da-jiancoredns-chuang-jiandashboard/</url>
    <content><![CDATA[<p>由于k8s集群内部的服务发现都是通过DNS来发现并实现的，所以需要依赖DNS服务</p>
<h2 id="1-创建coredns-yaml文件"><a href="#1-创建coredns-yaml文件" class="headerlink" title="1.创建coredns.yaml文件:"></a>1.创建coredns.yaml文件:</h2><pre><code>[root@linux-node1 ~]# cat &gt; coredns.yaml &lt;&lt; EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
      addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local. in-addr.arpa ip6.arpa {
            pods insecure
            upstream
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
    }
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: coredns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: coredns
  template:
    metadata:
      labels:
        k8s-app: coredns
    spec:
      serviceAccountName: coredns
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      containers:
      - name: coredns
        image: coredns/coredns:1.0.6
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: coredns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: coredns
  clusterIP: 10.1.0.2  #!!!!!!!关键点!!!!!!!
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
    
EOF
# 注意 cluster ip 需要跟service ip是在一个网段
</code></pre>
<h3 id="2-运行创建"><a href="#2-运行创建" class="headerlink" title="2.运行创建:"></a>2.运行创建:</h3><pre><code>[root@linux-node1 ~]# kubectl create -f coredns.yaml
deployment.extensions "coredns" created
Error from server (AlreadyExists): error when creating "coredns.yaml": serviceaccounts "coredns" already exists
Error from server (AlreadyExists): error when creating "coredns.yaml": clusterroles.rbac.authorization.k8s.io "system:coredns" already exists
Error from server (AlreadyExists): error when creating "coredns.yaml": clusterrolebindings.rbac.authorization.k8s.io "system:coredns" already exists
Error from server (AlreadyExists): error when creating "coredns.yaml": configmaps "coredns" already exists
Error from server (Invalid): error when creating "coredns.yaml": Service "coredns" is invalid: spec.clusterIP: Invalid value: "10.1.0.2": provided IP is already allocated

# 忽略以上错误，由于我已经安装过了。
</code></pre>
<h3 id="3-查看创建结果"><a href="#3-查看创建结果" class="headerlink" title="3.查看创建结果:"></a>3.查看创建结果:</h3><pre><code># 注意: 与k8s相关的组件或者服务都是在kube-system这个命名空间 所以这里 -n 指定了命名空间为kube-system
[root@linux-node1 ~]# kubectl get deployment -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
coredns                2         2         2            2           12m


# 查看service
[root@linux-node1 ~]# kubectl get service -n kube-system
NAME                   TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
coredns                ClusterIP   10.1.0.2      &lt;none&gt;        53/UDP,53/TCP   32m

# 通过LVS查看这个cluster的信息
[root@linux-node2 ~]# ipvsadm -Ln | grep -v ":80"
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.1.0.1:443 rr persistent 10800
  -&gt; 192.168.56.11:6443           Masq    1      2          0  
TCP  10.1.0.2:53 rr
  -&gt; 10.2.70.216:53               Masq    1      0          0
  -&gt; 10.2.97.209:53               Masq    1      0          0
UDP  10.1.0.2:53 rr
  -&gt; 10.2.70.216:53               Masq    1      0          0
  -&gt; 10.2.97.209:53               Masq    1      0          0
  
# 可以看到通过LVS转向了两个Pod, 查看pod:
[root@linux-node1 ~]# kubectl get pod -n kube-system -o wide
NAME                                    READY     STATUS    RESTARTS   AGE       IP            NODE
coredns-77c989547b-55kjb                1/1       Running   0          19m       10.2.97.209   192.168.56.13
coredns-77c989547b-qvfw4                1/1       Running   0          19m       10.2.70.216   192.168.56.12
</code></pre>
<h3 id="4-验证创建结果"><a href="#4-验证创建结果" class="headerlink" title="4. 验证创建结果:"></a>4. 验证创建结果:</h3><p>启动一个容器</p>
<pre><code># 可以看出来已经获取到了dns服务的VIP地址
[root@linux-node1 ~]# kubectl exec -it nginx-deployment-75d56bb955-b4z4k /bin/sh
# cat /etc/resolv.conf
nameserver 10.1.0.2 
search default.svc.cluster.localping . svc.cluster.local. cluster.local. example.com
options ndots:5
</code></pre>
<p>梳理主要内容：</p>
<ul>
<li>在k8s集群中，服务是运行在Pod中的，Pod的发现和副本间负载均衡是我们面临的问题。</li>
<li>通过Service可以解决这两个问题，但访问Service也需要对应的IP，因此又引入了Service发现的问题。</li>
<li>得益于coredns插件，我们可以通过域名来访问集群内的Service，解决了Service发现的问题。</li>
<li>为了让Pod中的容器可以使用coredns来解析域名，k8s会修改容器的/etc/resolv.conf配置。</li>
</ul>
<p>有了以上机制的保证，就可以在Pod中通过Service名称和namespace非常方便地访问对应的服务了。</p>
<h2 id="DashBoard"><a href="#DashBoard" class="headerlink" title="DashBoard"></a>DashBoard</h2><h3 id="部署dashboard"><a href="#部署dashboard" class="headerlink" title="部署dashboard"></a>部署dashboard</h3><pre><code>[root@linux-node1 ~]# kubectl create -f dashboard/
# 查看集群详情
[root@linux-node1 ~]# kubectl cluster-info
Kubernetes master is running at https://192.168.56.11:6443
CoreDNS is running at https://192.168.56.11:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy
kubernetes-dashboard is running at https://192.168.56.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.


[root@linux-node1 src]# kubectl get services kubernetes-dashboard -n kube-system
NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.1.27.148   &lt;none&gt;        443:28966/TCP   3d
</code></pre>
<h3 id="登录"><a href="#登录" class="headerlink" title="登录:"></a>登录:</h3><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/06/long.jpeg">￼</p>
<p>获取登录令牌:</p>
<pre><code>[root@linux-node1 src]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
# 复制 token的内容部分 输入 登录
</code></pre>
<p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/06/15280837316773.jpg">￼</p>
]]></content>
      <categories>
        <category>虚拟化&amp;amp;云计算&amp;amp;大数据</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>09-K8s集群搭建---监控展示界面安装</title>
    <url>/2018/06/08/09k8s-ji-qun-da-jianjian-kong-zhan-shi-jie-mian-an/</url>
    <content><![CDATA[<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>Heapster提供了整个集群的资源监控，并支持持久化数据存储到InfluxDB、Google Cloud Monitoring或者其他的存储后端。<br>Heapster从kubelet提供的API采集节点和容器的资源占用。另外，Heapster的 /metrics API提供了Prometheus格式的数据。<br>InfluxDB是一个开源分布式时序、事件和指标数据库；而Grafana则是InfluxDB的 dashboard，提供了强大的图表展示功能。它们常被组合使用展示图表化的监控数据，也可以将Zabbix作为数据源，进行zabbix的监控数据展示。<br>Heapster、InfluxDB和Grafana均以Pod的形式启动和运行，其中Heapster需要与Kubernetes Master进行安全连接。</p>
<h2 id="2-安装配置Heapster、InfluxDB和Grafana"><a href="#2-安装配置Heapster、InfluxDB和Grafana" class="headerlink" title="2. 安装配置Heapster、InfluxDB和Grafana"></a>2. 安装配置Heapster、InfluxDB和Grafana</h2><p><a href="https://github.com/kubernetes/heapster/releases">到Heapster获取安装包:</a></p>
<pre><code>[root@linux-node1 tmp]# wget https://github.com/kubernetes/heapster/archive/v1.5.3.tar.gz
[root@linux-node1 tmp]# tar -xf v1.5.3.tar.gz &amp;&amp; cd heapster-1.5.3/deploy/kube-config/influxdb/
[root@linux-node1 kube-config]# ll influxdb/
total 12
-rw-rw-r-- 1 root root 2290 May  1 05:13 grafana.yaml
-rw-rw-r-- 1 root root 1114 May  1 05:13 heapster.yaml
-rw-rw-r-- 1 root root  974 May  1 05:13 influxdb.yaml
[root@linux-node1 kube-config]# ll rbac/
total 4
-rw-rw-r-- 1 root root 264 May  1 05:13 heapster-rbac.yaml
[root@linux-node1 kube-config]#

# 以上是所需要用到的文件
</code></pre>
<h2 id="3-配置Docker-Socks5代理-为下载所需镜像做准备"><a href="#3-配置Docker-Socks5代理-为下载所需镜像做准备" class="headerlink" title="3. 配置Docker Socks5代理(为下载所需镜像做准备)"></a>3. 配置Docker Socks5代理(为下载所需镜像做准备)</h2><p>由于某国的 &amp;@#￥%&amp;￥&amp;&amp;*R#￥…… 你懂的. 导致一些镜像无法pull 我这边利用自己搭建的梯子来进行代理</p>
<pre><code>[root@linux-node1 kube-config]# grep "gcr.io" influxdb/heapster.yaml
        image: gcr.io/google_containers/heapster-amd64:v1.5.3
[root@linux-node1 kube-config]#

# 修改docker服务文件配置, 在[Service]部分添加你自己的ss服务器地址
[root@linux-node1 kube-config]# vim /usr/lib/systemd/system/docker.service
......
......
[Service]
Environment="ALL_PROXY=socks5://192.168.56.1:1080"
......
......
</code></pre>
<p>将其复制到其他所有节点，并重载、重启Docker服务, 直接pull一下验证即可</p>
<pre><code>[root@linux-node1 kube-config]# docker pull gcr.io/google_containers/heapster-amd64:v1.5.3
</code></pre>
<h2 id="4-关键点说明"><a href="#4-关键点说明" class="headerlink" title="4. 关键点说明:"></a>4. 关键点说明:</h2><pre><code>[root@linux-node1 kube-config]# more influxdb/heapster.yaml
......
......
     - /heapster
        - --source=kubernetes:https://kubernetes.default
        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086
......
......
</code></pre>
<ul>
<li>source：配置采集源，为Master URL地址：–source=kubernetes:<a href="https://kubernetes.default/">https://kubernetes.default</a></li>
<li>sink：配置后端存储系统，使用InfluxDB系统：–sink=influxdb:<a href="http://monitoring-influxdb:8086/">http://monitoring-influxdb:8086</a></li>
</ul>
<p>这里保持默认即可</p>
<p>【注意】：URL中的主机名地址使用的是InfluxDB的Service名字，这需要DNS服务正常工作，如果没有配置DNS服务，则也可以使用Service的ClusterIP地址。<br>另外，InfluxDB服务的名称没有加上命名空间，是因为Heapster服务与InfluxDB服务属于相同的命名空间kube-system。也可以使用上命名空间的全服务名，例如：<code>http://monitoring-influxdb.kube-system:8086</code></p>
<h4 id="修改-grafana-yaml文件"><a href="#修改-grafana-yaml文件" class="headerlink" title="修改 grafana.yaml文件"></a>修改 grafana.yaml文件</h4><pre><code>[root@linux-node1 kube-config]# vim influxdb/grafana.yaml
......
......
apiVersion: v1
kind: Service
metadata:
  labels:
    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)
    # If you are NOT using this as an addon, you should comment out this line.
    kubernetes.io/cluster-service: 'true'
    kubernetes.io/name: monitoring-grafana
  name: monitoring-grafana
  namespace: kube-system
spec:
  # In a production setup, we recommend accessing Grafana through an external Loadbalancer
  # or through a public IP.
  # type: LoadBalancer
  # You could also use NodePort to expose the service at a randomly-generated port
  type: NodePort # 去掉注释即可
  ports:
  - port: 80
    targetPort: 3000
  selector:
    k8s-app: grafana
......
......
</code></pre>
<ul>
<li>定义端口类型为 NodePort，将Grafana暴露在宿主机Node的端口上，以便后续浏览器访问 grafana 的 admin UI 界面</li>
</ul>
<h2 id="5-执行"><a href="#5-执行" class="headerlink" title="5. 执行"></a>5. 执行</h2><pre><code>[root@linux-node1 kube-config]# kubectl create -f influxdb/ &amp;&amp; kubectl create -f rbac/
deployment.extensions "monitoring-grafana" created
service "monitoring-grafana" created
serviceaccount "heapster" created
deployment.extensions "heapster" created
service "heapster" created
deployment.extensions "monitoring-influxdb" created
service "monitoring-influxdb" created
clusterrolebinding.rbac.authorization.k8s.io "heapster" created
</code></pre>
<h4 id="检查执行结果"><a href="#检查执行结果" class="headerlink" title="检查执行结果"></a>检查执行结果</h4><h5 id="1-检查Deployment"><a href="#1-检查Deployment" class="headerlink" title="1. 检查Deployment"></a>1. 检查Deployment</h5><pre><code>[root@linux-node1 kube-config]# kubectl get deployments -n kube-system -o wide | grep -E 'heapster|monitoring'
heapster               1         1         1            1           11s       heapster               gcr.io/google_containers/heapster-amd64:v1.5.3             k8s-app=heapster,task=monitoring
monitoring-grafana     1         1         1            1           11s       grafana                gcr.io/google_containers/heapster-grafana-amd64:v4.4.3     k8s-app=grafana,task=monitoring
monitoring-influxdb    1         1         1            1           11s       influxdb               gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3    k8s-app=influxdb,task=monitoring
</code></pre>
<h5 id="2-检查POD"><a href="#2-检查POD" class="headerlink" title="2. 检查POD"></a>2. 检查POD</h5><pre><code>[root@linux-node1 kube-config]# kubectl get pods -n kube-system -o wide | grep -E 'heapster|monitoring'
heapster-589b7db6c9-pwrks               1/1       Running   0          32s       10.2.97.77   192.168.56.13
monitoring-grafana-d8c8d486c-l9dhx      1/1       Running   0          33s       10.2.97.76   192.168.56.13
monitoring-influxdb-54bd58b4c9-q489p    1/1       Running   0          33s       10.2.70.76   192.168.56.12
</code></pre>
<h5 id="3-检查Service"><a href="#3-检查Service" class="headerlink" title="3. 检查Service"></a>3. 检查Service</h5><pre><code>[root@linux-node1 kube-config]# kubectl get service -n kube-system -o wide | grep -E 'heapster|monitoring'
heapster               ClusterIP   10.1.102.233   &lt;none&gt;        80/TCP          50s       k8s-app=heapster
monitoring-grafana     NodePort    10.1.18.167    &lt;none&gt;        80:21240/TCP    50s       k8s-app=grafana
monitoring-influxdb    ClusterIP   10.1.244.92    &lt;none&gt;        8086/TCP        50s       k8s-app=influxdb
</code></pre>
<h5 id="4-检查-kubernets-dashboard-界面，看是显示各-Nodes、Pods-的-CPU、内存、负载等利用率曲线图"><a href="#4-检查-kubernets-dashboard-界面，看是显示各-Nodes、Pods-的-CPU、内存、负载等利用率曲线图" class="headerlink" title="4. 检查 kubernets dashboard 界面，看是显示各 Nodes、Pods 的 CPU、内存、负载等利用率曲线图"></a>4. 检查 kubernets dashboard 界面，看是显示各 Nodes、Pods 的 CPU、内存、负载等利用率曲线图</h5><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/06/15284524671834.jpg"></p>
<h5 id="5-Grafana的访问"><a href="#5-Grafana的访问" class="headerlink" title="5. Grafana的访问:"></a>5. Grafana的访问:</h5><p>上面我们不是修改了官方提供的那个grafana.yaml中的那个NodePort嘛，所以我们访问就是<br><a href="http://NodeIP:NodePort">http://NodeIP:NodePort</a><br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15319972498850.jpg"><br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15319972699157.jpg"></p>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>10-K8s集群搭建---新增Node节点</title>
    <url>/2018/06/29/10k8s-ji-qun-da-jianxin-zengnode-jie-dian/</url>
    <content><![CDATA[<p>什么？资源不够用了？ 怼服务器配置啊(向上扩展)，怼机器啊(横向扩展)！  </p>
<p>(注: 该节点添加是基于之前k8s集群搭建的环境进行)<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15304316863865.jpg"></p>
<h2 id="1-系统初始化"><a href="#1-系统初始化" class="headerlink" title="1.系统初始化:"></a>1.系统初始化:</h2><p>a. 主机名配置</p>
<pre><code>node4:
echo "linux-node4.example.com" &gt; /etc/hostname
</code></pre>
<p>b. 设置/etc/hosts保证主机名能够解析</p>
<pre><code>node4:
echo "192.168.56.14 linux-node4 linux-node4.example.com" &gt;&gt; /etc/hosts
</code></pre>
<p>c. 关闭SELinux及防火墙</p>
<pre><code>node4:
systemctl disable firewalld; systemctl stop firewalld 
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
</code></pre>
<p>d. 环境变量配置(后续k8s相关命令都会放到/opt/kubernetes/bin目录下)</p>
<pre><code>echo "PATH=$PATH:$HOME/bin:/opt/kubernetes/bin" &gt;&gt;  ~/.bash_profile
source ~/.bash_profile
</code></pre>
<h2 id="4-安装Docker"><a href="#4-安装Docker" class="headerlink" title="4.安装Docker"></a>4.安装Docker</h2><p>a：使用国内Docker源</p>
<pre><code>[root@linux-node4 ~]# cd /etc/yum.repos.d/
[root@linux-node4 yum.repos.d]# wget \
https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</code></pre>
<p>b：Docker安装：</p>
<pre><code>[root@linux-node4 ~]# yum install -y docker-ce
</code></pre>
<p>c：启动后台进程：</p>
<pre><code>[root@linux-node4 ~]# systemctl enable docker
[root@linux-node4 ~]# systemctl start docker
</code></pre>
<h2 id="5-准备部署目录"><a href="#5-准备部署目录" class="headerlink" title="5.准备部署目录"></a>5.准备部署目录</h2><pre><code>[root@linux-node4 ~]#  mkdir -p /opt/kubernetes/{cfg,bin,ssl,log}
# 目录结构, 所有文件均存放在/opt/kubernetes目录下：
[root@linux-node4 ~]# tree -L 1 /opt/kubernetes/
/opt/kubernetes/
├── bin   #二进制文件
├── cfg   #配置文件
├── log   #日志文件
└── ssl   #证书文件
</code></pre>
<h2 id="6-做好master节点跟其他node节点的ssh互信-便于搭建"><a href="#6-做好master节点跟其他node节点的ssh互信-便于搭建" class="headerlink" title="6.做好master节点跟其他node节点的ssh互信,便于搭建"></a>6.做好master节点跟其他node节点的ssh互信,便于搭建</h2><pre><code>[root@linux-node1 ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.56.14
</code></pre>
<h2 id="7-拷贝配置-from-master"><a href="#7-拷贝配置-from-master" class="headerlink" title="7.拷贝配置(from master)"></a>7.拷贝配置(from master)</h2><h3 id="1-拷贝CFSSL"><a href="#1-拷贝CFSSL" class="headerlink" title="1.拷贝CFSSL"></a>1.拷贝CFSSL</h3><pre><code>[root@linux-node1 ~]# scp /opt/kubernetes/bin/cfssl* 192.168.56.14:/opt/kubernetes/bin
</code></pre>
<h3 id="2-分发证书"><a href="#2-分发证书" class="headerlink" title="2.分发证书"></a>2.分发证书</h3><pre><code>[root@linux-node1 ssl]# scp ca.csr ca.pem ca-key.pem ca-config.json 192.168.56.14:/opt/kubernetes/ssl
</code></pre>
<h3 id="3-拷贝kubernetes-证书和私钥"><a href="#3-拷贝kubernetes-证书和私钥" class="headerlink" title="3.拷贝kubernetes 证书和私钥"></a>3.拷贝kubernetes 证书和私钥</h3><pre><code>[root@linux-node1 ssl]# scp /opt/kubernetes/ssl/kubernetes*.pem 192.168.56.14:/opt/kubernetes/ssl/
</code></pre>
<h3 id="4-拷贝kubelet，kube-proxy软件包"><a href="#4-拷贝kubelet，kube-proxy软件包" class="headerlink" title="4.拷贝kubelet，kube-proxy软件包"></a>4.拷贝kubelet，kube-proxy软件包</h3><pre><code>[root@linux-node1 ~]# cd /usr/local/src/kubernetes/server/bin/
[root@linux-node1 bin]# scp kubelet kube-proxy 192.168.56.14:/opt/kubernetes/bin/
</code></pre>
<h3 id="5-拷贝角色绑定文件"><a href="#5-拷贝角色绑定文件" class="headerlink" title="5.拷贝角色绑定文件"></a>5.拷贝角色绑定文件</h3><pre><code>[root@linux-node1 ~]# scp /opt/kubernetes/cfg/bootstrap.kubeconfig 192.168.56.14:/opt/kubernetes/cfg
</code></pre>
<h3 id="6-拷贝CNI支持配置"><a href="#6-拷贝CNI支持配置" class="headerlink" title="6.拷贝CNI支持配置"></a>6.拷贝CNI支持配置</h3><pre><code>[root@linux-node1 ~]# ssh linux-node4 "mkdir /etc/cni/net.d -p"
[root@linux-node1 ~]# scp /etc/cni/net.d/10-default.conf linux-node4:/etc/cni/net.d/
</code></pre>
<h3 id="7-创建kubelet所需目录"><a href="#7-创建kubelet所需目录" class="headerlink" title="7.创建kubelet所需目录"></a>7.创建kubelet所需目录</h3><pre><code>[root@linux-node1 ~]# ssh linux-node4 "mkdir /var/lib/kubelet"
</code></pre>
<h3 id="8-拷贝Flannel网络证书"><a href="#8-拷贝Flannel网络证书" class="headerlink" title="8.拷贝Flannel网络证书"></a>8.拷贝Flannel网络证书</h3><pre><code>[root@linux-node1 ~]# scp  /opt/kubernetes/ssl/flanneld*.pem 192.168.56.14:/opt/kubernetes/ssl/
</code></pre>
<h3 id="9-复制flanner相关软件包"><a href="#9-复制flanner相关软件包" class="headerlink" title="9.复制flanner相关软件包"></a>9.复制flanner相关软件包</h3><pre><code>[root@linux-node1 ~]# ssh linux-node4 "mkdir /opt/kubernetes/bin/cni"
[root@linux-node1 ~]# scp /opt/kubernetes/bin/cni/* 192.168.56.14:/opt/kubernetes/bin/cni/
[root@linux-node1 ~]# scp /usr/lib/systemd/system/flannel.service 192.168.56.14:/usr/lib/systemd/system/
[root@linux-node1 ~]# scp /opt/kubernetes/cfg/flannel 192.168.56.14:/opt/kubernetes/cfg/
[root@linux-node1 ~]# cd /usr/local/src
[root@linux-node1 src]# scp flanneld mk-docker-opts.sh 192.168.56.14:/opt/kubernetes/bin/
[root@linux-node1 src]# cd /usr/local/src/kubernetes/cluster/centos/node/bin/
[root@linux-node1 bin]# scp remove-docker0.sh 192.168.56.14:/opt/kubernetes/bin/

# docker 服务脚本
[root@linux-node1 bin]# scp /usr/lib/systemd/system/docker.service 192.168.56.14:/usr/lib/systemd/system/
</code></pre>
<h3 id="10-拷贝kubelet系统服务配置"><a href="#10-拷贝kubelet系统服务配置" class="headerlink" title="10.拷贝kubelet系统服务配置"></a>10.拷贝kubelet系统服务配置</h3><pre><code>[root@linux-node1 ~]# scp /usr/lib/systemd/system/kubelet.service 192.168.56.14:/usr/lib/systemd/system/
[root@linux-node1 ~]# ssh linux-node4 "systemctl daemon-reload"
[root@linux-node1 ~]# ssh linux-node4 "systemctl enable kubelet"
[root@linux-node1 ~]# ssh linux-node4 "systemctl start kubelet"
[root@linux-node1 ~]# ssh linux-node4 "systemctl status kubelet"
# 然后在node1节点查看TLS证书请求，通过以下就行了，如果结果为空，就需要查看kubelet的日志啦，反正我在这里就跳进了一回自己挖的坑o(╥﹏╥)o

[root@linux-node1 ~]# kubectl get csr
[root@linux-node1 ~]# kubectl get csr|grep 'Pending' | awk 'NR&gt;0{print $1}'| xargs kubectl certificate
[root@linux-node1 ~]# kubectl get nodes
NAME            STATUS    ROLES     AGE       VERSION
192.168.56.11   Ready     master    1d        v1.10.1
192.168.56.12   Ready     node      31d       v1.10.1
192.168.56.13   Ready     node      31d       v1.10.1
192.168.56.14   Ready     node      1d        v1.10.1

# 以上，新的计算节点192.168.56.14已经成功添加
</code></pre>
<h3 id="11-配置kube-proxy使用LVS"><a href="#11-配置kube-proxy使用LVS" class="headerlink" title="11.配置kube-proxy使用LVS"></a>11.配置kube-proxy使用LVS</h3><pre><code>[root@linux-node4 ~]# yum install -y ipvsadm ipset conntrack
</code></pre>
<h3 id="12-分发kube-proxy证书"><a href="#12-分发kube-proxy证书" class="headerlink" title="12.分发kube-proxy证书"></a>12.分发kube-proxy证书</h3><pre><code>[root@linux-node1 ssl]# scp kube-proxy*.pem 192.168.56.14:/opt/kubernetes/ssl/
</code></pre>
<h3 id="13-分发kubeconfig配置文件"><a href="#13-分发kubeconfig配置文件" class="headerlink" title="13.分发kubeconfig配置文件"></a>13.分发kubeconfig配置文件</h3><pre><code>[root@linux-node1 ssl]# scp ../cfg/kube-proxy.kubeconfig 192.168.56.14:/opt/kubernetes/cfg/
</code></pre>
<h3 id="14-安装nfs-因为node节点作为客户端也需要安装nfs客户端工具"><a href="#14-安装nfs-因为node节点作为客户端也需要安装nfs客户端工具" class="headerlink" title="14.安装nfs(因为node节点作为客户端也需要安装nfs客户端工具)"></a>14.安装nfs(因为node节点作为客户端也需要安装nfs客户端工具)</h3><pre><code>[root@linux-node1 ssl]# ssh linux-node4 "yum -y install nfs-utils rpcbind"
</code></pre>
<h3 id="15-创建工作目录"><a href="#15-创建工作目录" class="headerlink" title="15.创建工作目录"></a>15.创建工作目录</h3><pre><code>[root@linux-node1 ~]# ssh linux-node4 "mkdir /var/lib/kube-proxy"
</code></pre>
<h3 id="16-拷贝服务配置文件"><a href="#16-拷贝服务配置文件" class="headerlink" title="16.拷贝服务配置文件"></a>16.拷贝服务配置文件</h3><pre><code>[root@linux-node1 ~]# cp /usr/lib/systemd/system/kube-proxy.service 192.168.56.14:/usr/lib/systemd/system/kube-proxy.service

# 注意需要修改配置文件的IP
[root@linux-node1 ~]# ssh linux-node4 "systemctl daemon-reload"
[root@linux-node1 ~]# ssh linux-node4 "systemctl enable kube-proxy"
[root@linux-node1 ~]# ssh linux-node4 "systemctl start kube-proxy"
[root@linux-node1 ~]# ssh linux-node4 "systemctl status kube-proxy"
</code></pre>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>我们先来Scaling flask-app pod到20个, 然后看Kubenertes Scheduler是否能把请求调度到新的节点上面</p>
<pre><code>[root@linux-node1 ~]# kubectl scale --replicas=20 deployment.extensions/flask-app -n flask-app-extions-stage
deployment.extensions "flask-app" scaled
</code></pre>
<p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15304308410933.jpg"><br>上图可以看到 Scaling的pod已经由Kubenertes Scheduler调度到各个node运行，并且状态已经是Running</p>
<p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15304310244907.jpg"></p>
<p>以上，可以看到我新添加的节点已经成功加到了集群当中，而且运行结果也是我预期的；唯一不足的地方是这个步骤如果后期在工作中用到的话是不太方便维护的,但是手动这样会知道一个节点需要用到哪些东西，需要怎么配置，也算是对k8s的运行机制有更深点的认知。<br>后续有时间还是搞一下用SaltStack来部署吧~🍺🍺🍺</p>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker构建日志收集平台EFK(TLS)</title>
    <url>/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>如题， 主要记录一下之前搭建部署的一套日志收集系统。这里采用docker-compose的方式运行，还是那句话主要是思路。</p>
<p>方式: <code>一台服务器上面利用docker-compose运行三个ES节点跟Kibana,并启用SSL,再使用Filebeat来收集服务器上面的应用日志到ES， 最后Kibana来做展示</code></p>
<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><h3 id="目录创建"><a href="#目录创建" class="headerlink" title="目录创建"></a>目录创建</h3><figure class="highlight haskell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="title">mkdir</span> -p /<span class="class"><span class="keyword">data</span>/{<span class="title">es</span>/<span class="title">node</span>-1/{<span class="title">data</span>,<span class="title">certs</span>,<span class="title">logs</span>,<span class="title">config</span>},plugins}</span></span><br><span class="line"><span class="title">mkdir</span> -p /<span class="class"><span class="keyword">data</span>/{<span class="title">es</span>/<span class="title">node</span>-2/{<span class="title">data</span>,<span class="title">certs</span>,<span class="title">logs</span>,<span class="title">config</span>},plugins}</span></span><br><span class="line"><span class="title">mkdir</span> -p /<span class="class"><span class="keyword">data</span>/{<span class="title">es</span>/<span class="title">node</span>-3/{<span class="title">data</span>,<span class="title">certs</span>,<span class="title">logs</span>,<span class="title">config</span>},plugins}</span></span><br><span class="line"><span class="title">mkdir</span> -p /<span class="class"><span class="keyword">data</span>/kibana/{<span class="title">certs</span>,<span class="title">config</span>} -p</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="部署文件"><a href="#部署文件" class="headerlink" title="部署文件"></a>部署文件</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">version:</span> <span class="string">"3"</span></span><br><span class="line"><span class="params">services:</span></span><br><span class="line">  <span class="params">node-1:</span></span><br><span class="line">    <span class="params">image:</span> registry.cn-hangzhou.aliyuncs.com<span class="operator">/</span>bigdata_cloudnative<span class="operator">/</span>elasticsearch:<span class="number">7.17</span>.<span class="number">5</span></span><br><span class="line">    <span class="params">networks:</span> </span><br><span class="line">      <span class="params">bitdata:</span></span><br><span class="line">        <span class="params">ipv4_address:</span> <span class="number">172.20</span>.<span class="number">0.3</span></span><br><span class="line">    <span class="params">container_name:</span> node-<span class="number">1</span></span><br><span class="line">    <span class="params">hostname:</span> node-<span class="number">1</span></span><br><span class="line">    <span class="params">environment:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"ES_JAVA_OPTS=-Xms1024m -Xmx1024m"</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"TZ=Asia/Shanghai"</span></span><br><span class="line">    <span class="params">ulimits:</span></span><br><span class="line">      <span class="params">memlock:</span></span><br><span class="line">        <span class="params">soft:</span> <span class="operator">-</span><span class="number">1</span></span><br><span class="line">        <span class="params">hard:</span> <span class="operator">-</span><span class="number">1</span></span><br><span class="line">      <span class="params">nofile:</span></span><br><span class="line">        <span class="params">soft:</span> <span class="number">65536</span></span><br><span class="line">        <span class="params">hard:</span> <span class="number">65536</span></span><br><span class="line">    <span class="params">ports:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"9200:9200"</span></span><br><span class="line">    <span class="params">logging:</span></span><br><span class="line">      <span class="params">driver:</span> <span class="string">"json-file"</span></span><br><span class="line">      <span class="params">options:</span></span><br><span class="line">        <span class="params">max-size:</span> <span class="string">"50m"</span></span><br><span class="line">    <span class="params">volumes:</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">1</span><span class="operator">/</span>config<span class="operator">/</span>elasticsearch.yml:<span class="symbol">/usr/share/elasticsearch/config/elasticsearch.yml</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>plugins:<span class="symbol">/usr/share/elasticsearch/plugins</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">1</span><span class="operator">/</span>data:<span class="symbol">/usr/share/elasticsearch/data</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">1</span><span class="operator">/</span>certs:<span class="symbol">/usr/share/elasticsearch/config/certs</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">1</span><span class="operator">/</span>log:<span class="symbol">/usr/share/elasticsearch/log</span></span><br><span class="line">    <span class="params">healthcheck:</span></span><br><span class="line">      <span class="params">test:</span> [<span class="string">"CMD-SHELL"</span>, <span class="string">"curl -I http://localhost:9200 || exit 1"</span>]</span><br><span class="line">      <span class="params">interval:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">timeout:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">retries:</span> <span class="number">5</span></span><br><span class="line">  <span class="params">node-2:</span></span><br><span class="line">    <span class="params">image:</span> registry.cn-hangzhou.aliyuncs.com<span class="operator">/</span>bigdata_cloudnative<span class="operator">/</span>elasticsearch:<span class="number">7.17</span>.<span class="number">5</span></span><br><span class="line">    <span class="params">networks:</span> </span><br><span class="line">      <span class="params">bitdata:</span></span><br><span class="line">        <span class="params">ipv4_address:</span> <span class="number">172.20</span>.<span class="number">0.4</span></span><br><span class="line">    <span class="params">container_name:</span> node-<span class="number">2</span></span><br><span class="line">    <span class="params">hostname:</span> node-<span class="number">2</span></span><br><span class="line">    <span class="params">environment:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"ES_JAVA_OPTS=-Xms1024m -Xmx1024m"</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"TZ=Asia/Shanghai"</span></span><br><span class="line">    <span class="params">ulimits:</span></span><br><span class="line">      <span class="params">memlock:</span></span><br><span class="line">        <span class="params">soft:</span> <span class="operator">-</span><span class="number">1</span></span><br><span class="line">        <span class="params">hard:</span> <span class="operator">-</span><span class="number">1</span></span><br><span class="line">      <span class="params">nofile:</span></span><br><span class="line">        <span class="params">soft:</span> <span class="number">65536</span></span><br><span class="line">        <span class="params">hard:</span> <span class="number">65536</span></span><br><span class="line">    <span class="params">ports:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"9201:9200"</span></span><br><span class="line">    <span class="params">logging:</span></span><br><span class="line">      <span class="params">driver:</span> <span class="string">"json-file"</span></span><br><span class="line">      <span class="params">options:</span></span><br><span class="line">        <span class="params">max-size:</span> <span class="string">"50m"</span></span><br><span class="line">    <span class="params">volumes:</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">2</span><span class="operator">/</span>config<span class="operator">/</span>elasticsearch.yml:<span class="symbol">/usr/share/elasticsearch/config/elasticsearch.yml</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>plugins:<span class="symbol">/usr/share/elasticsearch/plugins</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">2</span><span class="operator">/</span>data:<span class="symbol">/usr/share/elasticsearch/data</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">2</span><span class="operator">/</span>certs:<span class="symbol">/usr/share/elasticsearch/config/certs</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">2</span><span class="operator">/</span>log:<span class="symbol">/usr/share/elasticsearch/log</span></span><br><span class="line">    <span class="params">healthcheck:</span></span><br><span class="line">      <span class="params">test:</span> [<span class="string">"CMD-SHELL"</span>, <span class="string">"curl -I http://localhost:9200 || exit 1"</span>]</span><br><span class="line">      <span class="params">interval:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">timeout:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">retries:</span> <span class="number">5</span></span><br><span class="line">  <span class="params">node-3:</span></span><br><span class="line">    <span class="params">image:</span> registry.cn-hangzhou.aliyuncs.com<span class="operator">/</span>bigdata_cloudnative<span class="operator">/</span>elasticsearch:<span class="number">7.17</span>.<span class="number">5</span></span><br><span class="line">    <span class="params">networks:</span> </span><br><span class="line">      <span class="params">bitdata:</span></span><br><span class="line">        <span class="params">ipv4_address:</span> <span class="number">172.20</span>.<span class="number">0.5</span></span><br><span class="line">    <span class="params">container_name:</span> node-<span class="number">3</span></span><br><span class="line">    <span class="params">hostname:</span> node-<span class="number">3</span></span><br><span class="line">    <span class="params">environment:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"ES_JAVA_OPTS=-Xms1024m -Xmx1024m"</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"TZ=Asia/Shanghai"</span></span><br><span class="line">    <span class="params">ulimits:</span></span><br><span class="line">      <span class="params">memlock:</span></span><br><span class="line">        <span class="params">soft:</span> <span class="operator">-</span><span class="number">1</span></span><br><span class="line">        <span class="params">hard:</span> <span class="operator">-</span><span class="number">1</span></span><br><span class="line">      <span class="params">nofile:</span></span><br><span class="line">        <span class="params">soft:</span> <span class="number">65536</span></span><br><span class="line">        <span class="params">hard:</span> <span class="number">65536</span></span><br><span class="line">    <span class="params">ports:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"9202:9200"</span></span><br><span class="line">    <span class="params">logging:</span></span><br><span class="line">      <span class="params">driver:</span> <span class="string">"json-file"</span></span><br><span class="line">      <span class="params">options:</span></span><br><span class="line">        <span class="params">max-size:</span> <span class="string">"50m"</span></span><br><span class="line">    <span class="params">volumes:</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">3</span><span class="operator">/</span>config<span class="operator">/</span>elasticsearch.yml:<span class="symbol">/usr/share/elasticsearch/config/elasticsearch.yml</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>plugins:<span class="symbol">/usr/share/elasticsearch/plugins</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">3</span><span class="operator">/</span>data:<span class="symbol">/usr/share/elasticsearch/data</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">3</span><span class="operator">/</span>certs:<span class="symbol">/usr/share/elasticsearch/config/certs</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>es<span class="operator">/</span>node-<span class="number">3</span><span class="operator">/</span>log:<span class="symbol">/usr/share/elasticsearch/log</span></span><br><span class="line">    <span class="params">healthcheck:</span></span><br><span class="line">      <span class="params">test:</span> [<span class="string">"CMD-SHELL"</span>, <span class="string">"curl -I http://localhost:9200 || exit 1"</span>]</span><br><span class="line">      <span class="params">interval:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">timeout:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">retries:</span> <span class="number">5</span></span><br><span class="line">  <span class="params">kibana:</span></span><br><span class="line">    <span class="params">networks:</span> </span><br><span class="line">      <span class="params">bitdata:</span></span><br><span class="line">        <span class="params">ipv4_address:</span> <span class="number">172.20</span>.<span class="number">0.6</span></span><br><span class="line">    <span class="params">container_name:</span> kibana</span><br><span class="line">    <span class="params">hostname:</span> kibana</span><br><span class="line">    <span class="params">image:</span> registry.cn-hangzhou.aliyuncs.com<span class="operator">/</span>bigdata_cloudnative<span class="operator">/</span>kibana:<span class="number">7.17</span>.<span class="number">5</span></span><br><span class="line">    <span class="params">environment:</span></span><br><span class="line">      <span class="params">TZ:</span> 'Asia<span class="operator">/</span>Shanghai'</span><br><span class="line">    <span class="params">volumes:</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>kibana<span class="operator">/</span>cert:<span class="symbol">/etc/kibana/cert</span></span><br><span class="line">      <span class="operator">-</span> .<span class="operator">/</span>kibana<span class="operator">/</span>config<span class="operator">/</span>kibana.yml:<span class="symbol">/usr/share/kibana/config/kibana.yml</span></span><br><span class="line">    <span class="params">ports:</span></span><br><span class="line">      <span class="operator">-</span> <span class="number">5601</span>:<span class="number">5601</span></span><br><span class="line">    <span class="params">healthcheck:</span></span><br><span class="line">      <span class="params">test:</span> [<span class="string">"CMD-SHELL"</span>, <span class="string">"curl -I http://localhost:5601 || exit 1"</span>]</span><br><span class="line">      <span class="params">interval:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">timeout:</span> <span class="number">10</span>s</span><br><span class="line">      <span class="params">retries:</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接外部网络</span></span><br><span class="line"><span class="params">networks:</span></span><br><span class="line">  <span class="params">bitdata:</span></span><br><span class="line">    <span class="params">driver:</span> bridge</span><br><span class="line">    <span class="params">ipam:</span> </span><br><span class="line">      <span class="params">config:</span></span><br><span class="line">        <span class="operator">-</span> <span class="params">subnet:</span> <span class="number">172.20</span>.<span class="number">0.0</span><span class="operator">/</span><span class="number">24</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="非SSL"><a href="#非SSL" class="headerlink" title="非SSL"></a>非SSL</h2><h3 id="elasticsearch-yaml配置文件"><a href="#elasticsearch-yaml配置文件" class="headerlink" title="elasticsearch.yaml配置文件"></a>elasticsearch.yaml配置文件</h3><p>这里以<code>node-1</code>为例，其他不同的地方就是 <code>node.name</code></p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">vim <span class="symbol">/data/es/node-1/config/elasticsearch.yml</span></span><br><span class="line"><span class="comment">#集群名称</span></span><br><span class="line">cluster.<span class="params">name:</span> elastic</span><br><span class="line"><span class="comment">#当前该节点的名称</span></span><br><span class="line">node.<span class="params">name:</span> node-<span class="number">1</span></span><br><span class="line"><span class="comment">#是不是有资格竞选主节点</span></span><br><span class="line">node.<span class="params">master:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment">#是否存储数据</span></span><br><span class="line">node.<span class="params">data:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment">#最大集群节点数</span></span><br><span class="line">node.<span class="params">max_local_storage_nodes:</span> <span class="number">3</span></span><br><span class="line"><span class="comment">#给当前节点自定义属性（可以省略）</span></span><br><span class="line"><span class="comment">#node.attr.rack: r1</span></span><br><span class="line"><span class="comment">#数据存档位置</span></span><br><span class="line">path.<span class="params">data:</span> <span class="symbol">/usr/share/elasticsearch/data</span></span><br><span class="line"><span class="comment">#日志存放位置</span></span><br><span class="line">path.<span class="params">logs:</span> <span class="symbol">/usr/share/elasticsearch/log</span></span><br><span class="line"><span class="comment">#是否开启时锁定内存（默认为是）</span></span><br><span class="line"><span class="comment">#bootstrap.memory_lock: true</span></span><br><span class="line"><span class="comment">#设置网关地址，我是被这个坑死了，这个地址我原先填写了自己的实际物理IP地址，</span></span><br><span class="line"><span class="comment">#然后启动一直报无效的IP地址，无法注入9300端口，这里只需要填写0.0.0.0</span></span><br><span class="line">network.<span class="params">host:</span> <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="comment">#设置映射端口</span></span><br><span class="line">http.<span class="params">port:</span> <span class="number">9200</span></span><br><span class="line"><span class="comment">#内部节点之间沟通端口</span></span><br><span class="line">transport.tcp.<span class="params">port:</span> <span class="number">9300</span></span><br><span class="line"><span class="comment">#集群发现默认值为127.0.0.1:9300,如果要在其他主机上形成包含节点的群集,如果搭建集群则需要填写</span></span><br><span class="line"><span class="comment">#es7.x 之后新增的配置，写入候选主节点的设备地址，在开启服务后可以被选为主节点，也就是说把所有的节点都写上</span></span><br><span class="line">discovery.<span class="params">seed_hosts:</span> [<span class="string">"172.20.0.3"</span>,<span class="string">"172.20.0.4"</span>,<span class="string">"172.17.0.5"</span>]</span><br><span class="line"><span class="comment">#当你在搭建集群的时候，选出合格的节点集群，有些人说的太官方了，</span></span><br><span class="line"><span class="comment">#其实就是，让你选择比较好的几个节点，在你节点启动时，在这些节点中选一个做领导者，</span></span><br><span class="line"><span class="comment">#如果你不设置呢，elasticsearch就会自己选举，这里我们把三个节点都写上</span></span><br><span class="line">cluster.<span class="params">initial_master_nodes:</span> [<span class="string">"node-1"</span>,<span class="string">"node-2"</span>,<span class="string">"node-3"</span>]</span><br><span class="line"><span class="comment">#在群集完全重新启动后阻止初始恢复，直到启动N个节点</span></span><br><span class="line"><span class="comment">#简单点说在集群启动后，至少复活多少个节点以上，那么这个服务才可以被使用，否则不可以被使用，</span></span><br><span class="line">gateway.<span class="params">recover_after_nodes:</span> <span class="number">2</span></span><br><span class="line"><span class="comment">#删除索引是是否需要显示其名称，默认为显示</span></span><br><span class="line"><span class="comment">#action.destructive_requires_name: true</span></span><br><span class="line"><span class="comment"># 禁用安全配置</span></span><br><span class="line">xpack.security.<span class="params">enabled:</span> <span class="literal">false</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="kibana-yaml"><a href="#kibana-yaml" class="headerlink" title="kibana.yaml"></a>kibana.yaml</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">vim <span class="symbol">/data/kibana/config/kibana.yml</span></span><br><span class="line">server.<span class="params">host:</span> <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="comment"># 监听端口</span></span><br><span class="line">server.<span class="params">port:</span> <span class="number">5601</span></span><br><span class="line">server.<span class="params">name:</span> <span class="string">"kibana"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kibana访问es服务器的URL,就可以有多个，以逗号","隔开</span></span><br><span class="line">elasticsearch.<span class="params">hosts:</span> [<span class="string">"http://172.20.0.3:9200"</span>,<span class="string">"http://172.20.0.4:9200"</span>,<span class="string">"http://172.20.0.5:9200"</span>]</span><br><span class="line">monitoring.ui.container.elasticsearch.<span class="params">enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># kibana访问Elasticsearch的账号与密码(如果ElasticSearch设置了的话)</span></span><br><span class="line">elasticsearch.<span class="params">username:</span> <span class="string">""</span></span><br><span class="line">elasticsearch.<span class="params">password:</span> <span class="string">""</span></span><br><span class="line"><span class="comment"># kibana web语言</span></span><br><span class="line">i18n.<span class="params">locale:</span> <span class="string">"zh-CN"</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight crmsh"><table><tbody><tr><td class="code"><pre><span class="line">root@ip-<span class="number">10</span>-<span class="number">10</span>-<span class="number">10</span>-<span class="number">29</span>:/data<span class="comment"># docker-compose up -d</span></span><br><span class="line">[+] Running <span class="number">5</span>/<span class="number">5</span></span><br><span class="line"> ⠿ Network data_bitdata  Created                                                 <span class="number">0.0s</span></span><br><span class="line"> ⠿ Container <span class="keyword">node</span><span class="title">-2</span>      <span class="literal">Started</span>                                                 <span class="number">0.6s</span></span><br><span class="line"> ⠿ Container <span class="keyword">node</span><span class="title">-3</span>      <span class="literal">Started</span>                                                 <span class="number">0.8s</span></span><br><span class="line"> ⠿ Container <span class="keyword">node</span><span class="title">-1</span>      <span class="literal">Started</span>                                                 <span class="number">0.8s</span></span><br><span class="line"> ⠿ Container kibana      <span class="literal">Started</span>                                                 <span class="number">0.5s</span></span><br><span class="line"></span><br><span class="line">CONTAINER ID   IMAGE                                                                        COMMAND                  CREATED              STATUS                         PORTS                                                  NAMES</span><br><span class="line"><span class="number">4</span>d5dae9021be   registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:<span class="number">7.17</span>.<span class="number">5</span>   <span class="string">"/bin/tini -- /usr/l…"</span>   About a minute ago   Up About a minute (healthy)    <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">9200</span>-&gt;<span class="number">9200</span>/tcp, :::<span class="number">9200</span>-&gt;<span class="number">9200</span>/tcp, <span class="number">9300</span>/tcp    <span class="keyword">node</span><span class="title">-1</span></span><br><span class="line"><span class="number">84</span>e191d6f875   registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:<span class="number">7.17</span>.<span class="number">5</span>   <span class="string">"/bin/tini -- /usr/l…"</span>   About a minute ago   Up About a minute (healthy)    <span class="number">9300</span>/tcp, <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">9202</span>-&gt;<span class="number">9200</span>/tcp, :::<span class="number">9202</span>-&gt;<span class="number">9200</span>/tcp    <span class="keyword">node</span><span class="title">-3</span></span><br><span class="line"><span class="number">758</span>a4bff2a73   registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/elasticsearch:<span class="number">7.17</span>.<span class="number">5</span>   <span class="string">"/bin/tini -- /usr/l…"</span>   About a minute ago   Up About a minute (healthy)    <span class="number">9300</span>/tcp, <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">9201</span>-&gt;<span class="number">9200</span>/tcp, :::<span class="number">9201</span>-&gt;<span class="number">9200</span>/tcp    <span class="keyword">node</span><span class="title">-2</span></span><br><span class="line">fe55f0446902   registry.cn-hangzhou.aliyuncs.com/bigdata_cloudnative/kibana:<span class="number">7.17</span>.<span class="number">5</span>          <span class="string">"/bin/tini -- /usr/l…"</span>   About a minute ago   Up About a minute (healthy)    <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">5601</span>-&gt;<span class="number">5601</span>/tcp, :::<span class="number">5601</span>-&gt;<span class="number">5601</span>/tcp              kibana</span><br></pre></td></tr></tbody></table></figure>
<p>上面已经启动，如果有报错查看容器日志就行了，之前部署的时候报错内容还是很清晰</p>
<p>进入容器查看：</p>
<figure class="highlight elixir"><table><tbody><tr><td class="code"><pre><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment"># curl localhost:9200</span></span><br><span class="line">{</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"node-1"</span>,</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elastic"</span>,</span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"tcKUxyWVQb-7LV4zmomsgg"</span>,</span><br><span class="line">  <span class="string">"version"</span> : {</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"7.17.5"</span>,</span><br><span class="line">    <span class="string">"build_flavor"</span> : <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"build_type"</span> : <span class="string">"docker"</span>,</span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84"</span>,</span><br><span class="line">    <span class="string">"build_date"</span> : <span class="string">"2022-06-23T21:57:28.736740635Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"8.11.1"</span>,</span><br><span class="line">    <span class="string">"minimum_wire_compatibility_version"</span> : <span class="string">"6.8.0"</span>,</span><br><span class="line">    <span class="string">"minimum_index_compatibility_version"</span> : <span class="string">"6.0.0-beta1"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">}</span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment"># curl 172.20.3:9200</span></span><br><span class="line">{</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"node-1"</span>,</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elastic"</span>,</span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"tcKUxyWVQb-7LV4zmomsgg"</span>,</span><br><span class="line">  <span class="string">"version"</span> : {</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"7.17.5"</span>,</span><br><span class="line">    <span class="string">"build_flavor"</span> : <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"build_type"</span> : <span class="string">"docker"</span>,</span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84"</span>,</span><br><span class="line">    <span class="string">"build_date"</span> : <span class="string">"2022-06-23T21:57:28.736740635Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"8.11.1"</span>,</span><br><span class="line">    <span class="string">"minimum_wire_compatibility_version"</span> : <span class="string">"6.8.0"</span>,</span><br><span class="line">    <span class="string">"minimum_index_compatibility_version"</span> : <span class="string">"6.0.0-beta1"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">}</span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment"># curl 172.20.4:9200</span></span><br><span class="line">{</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"node-2"</span>,</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elastic"</span>,</span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"tcKUxyWVQb-7LV4zmomsgg"</span>,</span><br><span class="line">  <span class="string">"version"</span> : {</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"7.17.5"</span>,</span><br><span class="line">    <span class="string">"build_flavor"</span> : <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"build_type"</span> : <span class="string">"docker"</span>,</span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84"</span>,</span><br><span class="line">    <span class="string">"build_date"</span> : <span class="string">"2022-06-23T21:57:28.736740635Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"8.11.1"</span>,</span><br><span class="line">    <span class="string">"minimum_wire_compatibility_version"</span> : <span class="string">"6.8.0"</span>,</span><br><span class="line">    <span class="string">"minimum_index_compatibility_version"</span> : <span class="string">"6.0.0-beta1"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">}</span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment"># curl 172.20.5:9200</span></span><br><span class="line">{</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"node-3"</span>,</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elastic"</span>,</span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"tcKUxyWVQb-7LV4zmomsgg"</span>,</span><br><span class="line">  <span class="string">"version"</span> : {</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"7.17.5"</span>,</span><br><span class="line">    <span class="string">"build_flavor"</span> : <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"build_type"</span> : <span class="string">"docker"</span>,</span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"8d61b4f7ddf931f219e3745f295ed2bbc50c8e84"</span>,</span><br><span class="line">    <span class="string">"build_date"</span> : <span class="string">"2022-06-23T21:57:28.736740635Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"8.11.1"</span>,</span><br><span class="line">    <span class="string">"minimum_wire_compatibility_version"</span> : <span class="string">"6.8.0"</span>,</span><br><span class="line">    <span class="string">"minimum_index_compatibility_version"</span> : <span class="string">"6.0.0-beta1"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">}</span><br><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data</span><span class="comment"># curl -s 172.20.0.3:9200/_cluster/health | python3 -m json.tool</span></span><br><span class="line">{</span><br><span class="line">    <span class="string">"cluster_name"</span>: <span class="string">"elastic"</span>,</span><br><span class="line">    <span class="string">"status"</span>: <span class="string">"green"</span>,</span><br><span class="line">    <span class="string">"timed_out"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"number_of_nodes"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"number_of_data_nodes"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"active_primary_shards"</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">"active_shards"</span>: <span class="number">6</span>,</span><br><span class="line">    <span class="string">"relocating_shards"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"initializing_shards"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"unassigned_shards"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"delayed_unassigned_shards"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"number_of_pending_tasks"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"number_of_in_flight_fetch"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"task_max_waiting_in_queue_millis"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"active_shards_percent_as_number"</span>: <span class="number">100.0</span></span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>以上说明ES集群已经正常工作，看一下通过nginx代理转发到kibana的结果</p>
<p><img src="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/1.png"></p>
<p>kibana也能正常工作，只是这里在访问的时候直接就进入了kibana，如果不加一个身份验证功能，这就等于裸奔，即便在放在内网也是极其不安全的，所以需要加一个身份验证。</p>
<h2 id="SSL方式"><a href="#SSL方式" class="headerlink" title="SSL方式"></a>SSL方式</h2><p>利用自带的工具生成证书，也可以自行生成证书，但注意要限制域名和IP，否则在进行https通讯时会校验失败</p>
<p>进入node-1容器 ，操作</p>
<figure class="highlight elixir"><table><tbody><tr><td class="code"><pre><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:~</span><span class="comment"># docker exec -it node-1 bash</span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据实际情况配置生成证书的yaml</span></span><br><span class="line">cat &lt;&lt;<span class="title class_">EOF</span> &gt; /usr/share/elasticsearch/node.yml</span><br><span class="line"><span class="symbol">instances:</span></span><br><span class="line">  - <span class="symbol">name:</span> <span class="string">"node"</span></span><br><span class="line">    <span class="symbol">ip:</span></span><br><span class="line">      - <span class="string">"172.20.0.3"</span></span><br><span class="line">      - <span class="string">"172.20.0.4"</span></span><br><span class="line">      - <span class="string">"172.20.0.5"</span></span><br><span class="line">      - <span class="string">"127.0.0.1"</span></span><br><span class="line">      - <span class="string">"172.20.0.6"</span></span><br><span class="line">      - <span class="string">"10.10.10.29"</span> <span class="comment"># 这个IP是服务器的IP地址，因为我这里使用的是docker运行es集群，这个必须加上去</span></span><br><span class="line">    <span class="symbol">dns:</span></span><br><span class="line">      - <span class="string">"node-1"</span></span><br><span class="line">      - <span class="string">"node-2"</span></span><br><span class="line">      - <span class="string">"node-3 "</span></span><br><span class="line">      - <span class="string">"localhost"</span></span><br><span class="line">      - <span class="string">"kibana"</span></span><br><span class="line">      - <span class="string">"others"</span></span><br><span class="line"><span class="title class_">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成ca证书，默认文件名为elastic-stack-ca.p12，可添加密码</span></span><br><span class="line">bin/elasticsearch-certutil ca <span class="comment"># 一路回车即可</span></span><br><span class="line"><span class="comment"># 生成证书，默认文件名为certificate-bundle.zip</span></span><br><span class="line">bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --silent --<span class="keyword">in</span> node.yml  <span class="comment"># 一路回车即可</span></span><br><span class="line"><span class="comment"># 将证书复制到node-1配置目录下</span></span><br><span class="line"></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment"># cp certificate-bundle.zip elastic-stack-ca.p12 config/certs/</span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment"># cd config/certs/</span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config/certs</span><span class="comment"># ls</span></span><br><span class="line">certificate-bundle.zip  elastic-stack-ca.p12</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解压证书，目录结构为 实例名/实例名.p12 ,此处为node/node.p12</span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config/certs</span><span class="comment"># unzip certificate-bundle.zip </span></span><br><span class="line"><span class="symbol">Archive:</span>  certificate-bundle.zip</span><br><span class="line">   <span class="symbol">creating:</span> node/</span><br><span class="line">  <span class="symbol">inflating:</span> node/node.p12           </span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config/certs</span><span class="comment"># ls</span></span><br><span class="line">certificate-bundle.zip  elastic-stack-ca.p12  node</span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config/certs</span><span class="comment"># ls node/</span></span><br><span class="line">node.p12</span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config/certs</span><span class="comment"># rm -rf node certificate-bundle.zip</span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config/certs</span><span class="comment"># ls</span></span><br><span class="line">certificate-bundle.zip  elastic-stack-ca.p12  node.p12</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改证书访问权限</span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config</span><span class="comment"># chown -R root:elasticsearch certs/       </span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch/config</span><span class="comment"># chmod -R a+rx certs/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时我们需要的证书已经生成了，同样的方式复制到node-2,node3后修改权限，修改属主等操作</span></span><br><span class="line"><span class="comment"># 退出容器,进入node-1的挂在目录：</span></span><br><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data/es/node-</span><span class="number">1</span>/certs<span class="comment"># pwd</span></span><br><span class="line">/data/es/node<span class="number">-1</span>/certs</span><br><span class="line"></span><br><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data/es/node-</span><span class="number">1</span>/certs<span class="comment"># cd ..</span></span><br><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data/es/node-</span><span class="number">1</span><span class="comment"># ls</span></span><br><span class="line">certs  config  data  log</span><br><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data/es/node-</span><span class="number">1</span><span class="comment"># cp -rf certs ../node-2/</span></span><br><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data/es/node-</span><span class="number">1</span><span class="comment"># cp -rf certs ../node-3/</span></span><br><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data/es/node-</span><span class="number">1</span><span class="comment"># </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 证书已经准备完毕</span></span><br><span class="line">es/</span><br><span class="line">├── node<span class="number">-1</span></span><br><span class="line">│&nbsp;&nbsp; ├── certs</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elastic-stack-ca.p12</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; └── node.p12</span><br><span class="line">│&nbsp;&nbsp; ├── config</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elasticsearch.yml</span><br><span class="line">│&nbsp;&nbsp; ├── data</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; └── nodes</span><br><span class="line">│&nbsp;&nbsp; └── log</span><br><span class="line">├── node<span class="number">-2</span></span><br><span class="line">│&nbsp;&nbsp; ├── certs</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elastic-stack-ca.p12</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; └── node.p12</span><br><span class="line">│&nbsp;&nbsp; ├── config</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elasticsearch.yml</span><br><span class="line">│&nbsp;&nbsp; ├── data</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; └── nodes</span><br><span class="line">│&nbsp;&nbsp; └── log</span><br><span class="line">├── node<span class="number">-3</span></span><br><span class="line">│&nbsp;&nbsp; ├── certs</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elastic-stack-ca.p12</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; └── node.p12</span><br><span class="line">│&nbsp;&nbsp; ├── config</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── elasticsearch.yml</span><br><span class="line">│&nbsp;&nbsp; ├── data</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; └── nodes</span><br><span class="line">│&nbsp;&nbsp; └── log</span><br><span class="line">└── plugins</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>修改配置文件/data/es/node-1/config/elasticsearch.yml. 以节点<code>node-1</code>为例</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 这里的内容同非ssl部署一样</span></span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line"><span class="comment"># 开启xpack安全特性</span></span><br><span class="line">xpack.security.<span class="params">enabled:</span> <span class="literal">true</span></span><br><span class="line">xpack.security.authc.api_key.<span class="params">enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># 开启https并配置证书</span></span><br><span class="line">xpack.security.http.ssl.<span class="params">enabled:</span> <span class="literal">true</span></span><br><span class="line">xpack.security.http.ssl.keystore.<span class="params">path:</span> certs<span class="symbol">/node.p12</span></span><br><span class="line">xpack.security.http.ssl.truststore.<span class="params">path:</span> certs<span class="symbol">/node.p12</span></span><br><span class="line"><span class="comment"># 开启节点间通讯ssl并配置证书</span></span><br><span class="line">xpack.security.transport.ssl.<span class="params">enabled:</span> <span class="literal">true</span></span><br><span class="line">xpack.security.transport.ssl.<span class="params">verification_mode:</span> certificate</span><br><span class="line">xpack.security.transport.ssl.<span class="params">client_authentication:</span> required</span><br><span class="line">xpack.security.transport.ssl.keystore.<span class="params">path:</span> certs<span class="symbol">/node.p12</span></span><br><span class="line">xpack.security.transport.ssl.truststore.<span class="params">path:</span> certs<span class="operator">/</span>node.p12</span><br></pre></td></tr></tbody></table></figure>
<p>每个节点都需修改后，重启一下es集群 </p>
<p>重启完毕后，查看状态和日志，检查是否有异常  </p>
<p>若无异常，可以开始设置密码， 进入容器 <code>node-1</code></p>
<figure class="highlight elixir"><table><tbody><tr><td class="code"><pre><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-29</span><span class="symbol">:/data</span><span class="comment"># docker exec -it node-1 bash</span></span><br><span class="line">root<span class="variable">@node</span><span class="number">-1</span><span class="symbol">:/usr/share/elasticsearch</span><span class="comment"># ./bin/elasticsearch-setup-passwords auto </span></span><br><span class="line"><span class="title class_">Initiating</span> the setup of passwords <span class="keyword">for</span> reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user.</span><br><span class="line"><span class="title class_">The</span> passwords will be randomly generated <span class="keyword">and</span> printed to the console.</span><br><span class="line"><span class="title class_">Please</span> confirm that you would like to continue [y/N]y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若集群没问题的话这里就会自动生成系统的一些密码：</span></span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以手动自定义密码</span></span><br><span class="line">./bin/elasticsearch-setup-passwords interactive</span><br></pre></td></tr></tbody></table></figure>
<p>使用 ES API 检查各个节点是否正常</p>
<figure class="highlight crmsh"><table><tbody><tr><td class="code"><pre><span class="line">curl -k --<span class="keyword">user</span> <span class="title">elastic</span>:密码 -X GET <span class="string">"https://172.20.0.3:9200"</span> --cert-<span class="keyword">type</span> P12 --cert /usr/share/elasticsearch/config/certs/node.p12</span><br><span class="line">curl -k --<span class="keyword">user</span> <span class="title">elastic</span>:密码 -X GET <span class="string">"https://172.20.0.4:9200"</span> --cert-<span class="keyword">type</span> P12 --cert /usr/share/elasticsearch/config/certs/node.p12</span><br><span class="line">curl -k --<span class="keyword">user</span> <span class="title">elastic</span>:密码 -X GET <span class="string">"https://172.20.0.5:9200"</span> --cert-<span class="keyword">type</span> P12 --cert /usr/share/elasticsearch/config/certs/node.p12</span><br></pre></td></tr></tbody></table></figure>

<p>此时ssl已配置完毕，配置kibana</p>
<figure class="highlight stata"><table><tbody><tr><td class="code"><pre><span class="line"># 把之前放在node-1下面的证书复制到kibana/cert下面：</span><br><span class="line"># 提取公钥</span><br><span class="line">openssl pkcs12 -<span class="keyword">in</span> elastic-<span class="keyword">stack</span>-<span class="keyword">ca</span>.p12 -<span class="keyword">out</span> elastic-<span class="keyword">stack</span>-<span class="keyword">ca</span>.pem -nokeys -clcerts</span><br><span class="line">openssl pkcs12 -<span class="keyword">in</span> node.p12 -<span class="keyword">out</span> node.pem -nokeys -clcerts</span><br><span class="line"># 提取私钥</span><br><span class="line">openssl pkcs12 -<span class="keyword">in</span> node.p12 -<span class="keyword">out</span> node.key -nocerts -nodes</span><br></pre></td></tr></tbody></table></figure>

<p>修改配置文件kibana/config/kibana.yml</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">server.<span class="params">host:</span> <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="comment"># 监听端口</span></span><br><span class="line">server.<span class="params">port:</span> <span class="number">5601</span></span><br><span class="line">server.<span class="params">name:</span> <span class="string">"kibana"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kibana访问es服务器的URL,就可以有多个，以逗号","隔开</span></span><br><span class="line"><span class="comment"># es 开启了 ssl，所以这里的url 必须使用https协议</span></span><br><span class="line">elasticsearch.<span class="params">hosts:</span> [<span class="string">"https://172.20.0.3:9200"</span>,<span class="string">"https://172.20.0.4:9200"</span>,<span class="string">"https://172.20.0.5:9200"</span>]</span><br><span class="line">monitoring.ui.container.elasticsearch.<span class="params">enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># kibana访问Elasticsearch的账号与密码(如果ElasticSearch设置了的话)</span></span><br><span class="line">elasticsearch.<span class="params">username:</span> <span class="string">"kibana_system"</span></span><br><span class="line">elasticsearch.<span class="params">password:</span> <span class="string">"xxxxxxxxxxxx"</span></span><br><span class="line"><span class="comment"># kibana web语言</span></span><br><span class="line">i18n.<span class="params">locale:</span> <span class="string">"zh-CN"</span></span><br><span class="line"><span class="comment"># es证书配置, 这里是在docker-compose中已经挂载进容器了</span></span><br><span class="line">elasticsearch.ssl.<span class="params">certificate:</span> <span class="symbol">/etc/kibana/cert/node.pem</span></span><br><span class="line">elasticsearch.ssl.<span class="params">key:</span> <span class="symbol">/etc/kibana/cert/node.key</span></span><br><span class="line"><span class="comment"># es的ca证书</span></span><br><span class="line">elasticsearch.ssl.<span class="params">certificateAuthorities:</span> [ <span class="string">"/etc/kibana/cert/elastic-stack-ca.pem"</span> ]</span><br></pre></td></tr></tbody></table></figure>
<p>修改完毕后 ，重新部署一下 kibana服务即可</p>
<p><img src="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/2.png"></p>
<p>以上只要登录就可以使用了。</p>
<h2 id="Filebeat-日志采集"><a href="#Filebeat-日志采集" class="headerlink" title="Filebeat 日志采集"></a>Filebeat 日志采集</h2><p>这里我才用的方式是 docker运行 filebeat服务，然后将网关服务器的Nginx日志挂载进filebeat</p>
<figure class="highlight dts"><table><tbody><tr><td class="code"><pre><span class="line">vim <span class="keyword">/opt/</span>filbeat/docker-compose.yaml</span><br><span class="line"><span class="symbol">version:</span> <span class="string">"3"</span></span><br><span class="line"><span class="symbol">services:</span></span><br><span class="line"><span class="symbol">  filebeat:</span></span><br><span class="line"><span class="symbol">    container_name:</span> filebeat_test</span><br><span class="line"><span class="symbol">    image:</span> elastic/filebeat:<span class="number">7.9</span><span class="number">.0</span></span><br><span class="line"><span class="symbol">    user:</span> root</span><br><span class="line"><span class="symbol">    volumes:</span></span><br><span class="line">      - <span class="keyword">/data/</span>wwwlogs/:<span class="keyword">/var/</span>log/nginx:ro</span><br><span class="line">      - ./filebeat.yml:<span class="keyword">/usr/</span>share<span class="keyword">/filebeat/</span>filebeat.yml:ro</span><br><span class="line">      - ./ca.crt:<span class="keyword">/usr/</span>share<span class="keyword">/filebeat/</span>certs/ca.crt</span><br><span class="line">      <span class="meta"># 这个证书在接入kibana的时候已经提取出来了，即文件：elastic-stack-ca.pem</span></span><br><span class="line">      <span class="meta"># 他们内容是一样的只是文件名、格式不一样</span></span><br><span class="line"><span class="symbol">    command:</span> filebeat -e</span><br></pre></td></tr></tbody></table></figure>

<p>filebeat的配置文件在 /opt/filebeat/filebeat.yml</p>
<figure class="highlight clean"><table><tbody><tr><td class="code"><pre><span class="line">vim /opt/filebeat/filebeat.yml</span><br><span class="line">filebeat.inputs:  </span><br><span class="line">##################################################################################</span><br><span class="line">- type: log</span><br><span class="line">  paths:  </span><br><span class="line">    - /var/log/nginx/testnet-xxxxxxxxxxxx.com.log</span><br><span class="line">  tags: [<span class="string">"proxy-testnet-xxxxxxxxxxxx.com.log"</span>]  </span><br><span class="line">  fields:  </span><br><span class="line">    index: <span class="string">"proxy-testnet-xxxxxxxxxxxx.com.log"</span>  </span><br><span class="line">  json.keys_under_root: true</span><br><span class="line">  json.overwrite_keys: true</span><br><span class="line">  processors:  </span><br><span class="line">    - add_kubernetes_metadata:  </span><br><span class="line">        matchers:  </span><br><span class="line">        - logs_path:  </span><br><span class="line">            logs_path: <span class="string">"/var/log/nginx/"</span>  </span><br><span class="line">    - decode_json_fields:  </span><br><span class="line">        when:  </span><br><span class="line">           regexp:  </span><br><span class="line">             message: <span class="string">"{*}"</span>  </span><br><span class="line">        fields: [<span class="string">"message"</span>]  </span><br><span class="line">        overwrite_keys: true  </span><br><span class="line">##################################################################################</span><br><span class="line">output.elasticsearch:  </span><br><span class="line">  hosts: [<span class="string">"https://10.10.10.29:9200"</span>,<span class="string">"https://10.10.10.29:9201"</span>,<span class="string">"https://10.10.10.29:9201"</span>]</span><br><span class="line">  username: elastic</span><br><span class="line">  password: vDNnbd8FXqR2i13NYGfj</span><br><span class="line">  ssl.certificate_authorities:</span><br><span class="line">    - /usr/share/filebeat/certs/ca.crt</span><br><span class="line">  indices:  </span><br><span class="line">    - index: <span class="string">"proxy-testnet-xxxxxxxxxxxx.com.log-%{+yyyy.MM.dd}"</span>  </span><br><span class="line">      when.contains:  </span><br><span class="line">        fields:  </span><br><span class="line">          index: <span class="string">"proxy-testnet-xxxxxxxxxxxx.com.log"</span> </span><br><span class="line">setup.kibana:</span><br><span class="line">  hosts: <span class="string">"http://10.10.10.29:5601"</span></span><br><span class="line">  username: <span class="string">"elastic"</span></span><br><span class="line">  password: <span class="string">"xxxxxxxxxxxx"</span></span><br></pre></td></tr></tbody></table></figure>
<p>配置完毕之后，查看日志是否异常，如果没有异常，到<code>node-1</code>上面通过ES API查询是否有index生成</p>
<figure class="highlight asciidoc"><table><tbody><tr><td class="code"><pre><span class="line">root@node-1:/usr/share/elasticsearch# curl -k --user elastic:xxxxxxxxxxxx -X GET "https://172.20.0.3:9200/<span class="emphasis">_cat/indices " --cert-type P12 --cert /usr/share/elasticsearch/config/certs/node.p12</span></span><br><span class="line"><span class="emphasis">......</span></span><br><span class="line"><span class="emphasis">......</span></span><br><span class="line"><span class="emphasis">......</span></span><br><span class="line"><span class="emphasis">green open proxy-testnet-xxxxxxxxxxxx.com.log-2024.07.23 Su-T62zGTheqbDsDZLzH_</span>A 1 1 2911    0   1.7mb 914.2kb</span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="Kibana配置"><a href="#Kibana配置" class="headerlink" title="Kibana配置"></a>Kibana配置</h2><p>上面可以看到filebeat已经正常往ES写入数据，在kibana中配置一下：</p>
<p><code>Stack Manager</code> - <code>Index Patterns</code> - <code>Create index pattern</code></p>
<p><img src="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/3.png"></p>
<p>直接创建后菜单栏： <code>Discover</code> 选择刚刚创建的这个Index Pattern</p>
<p><img src="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/4.png"><br>最后直接就可以看到收集到的日志kibana已经做好了字段分隔，只需要选择需要的字段，或者匹配某些字段就可以了。</p>
<p>比如 我想看状态码大于等于500的： <code>status &gt;= 500</code>，都可以在上面的搜索框中去定义筛选</p>
<p>当然也可以把这个日志做成可视化，但选择某个指标的时候，其他指标也可以联动起来，例如：<br><img src="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/5.png"></p>
<p>最后再结合前面写过的 <a href="https://blog.sctux.cc/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/">如何利用ElastAlert2对ES中的日志创建告警</a> 就可以轻松的对日志进行监控告警啦~</p>
<p>写在最后 </p>
<p>上面过程仅仅是记录，主要还是思路。</p>
<p><img src="/2024/07/23/Docker%E6%9E%84%E5%BB%BA%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B9%B3%E5%8F%B0EFK-TLS/6.png"></p>
<h2 id="k8s-version"><a href="#k8s-version" class="headerlink" title="k8s version"></a>k8s version</h2><figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">---  </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span>  </span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span>  </span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">filebeat-config</span>  </span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ops</span></span><br><span class="line">  <span class="attr">labels:</span>  </span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">filebeat</span>  </span><br><span class="line"><span class="attr">data:</span>  </span><br><span class="line">  <span class="attr">filebeat.yml:</span> <span class="string">|-  </span></span><br><span class="line"><span class="string">    #====================== input =================  </span></span><br><span class="line"><span class="string">    filebeat.inputs:  </span></span><br><span class="line"><span class="string">    # nginx-ingress  </span></span><br><span class="line"><span class="string">    - type: container  </span></span><br><span class="line"><span class="string">      paths:  </span></span><br><span class="line"><span class="string">        - /var/log/containers/*goerli-testnet-collector*.log  </span></span><br><span class="line"><span class="string">      tags: ["goerli-testnet-collector"]  </span></span><br><span class="line"><span class="string">      fields:  </span></span><br><span class="line"><span class="string">        index: "goerli-testnet-collector"  </span></span><br><span class="line"><span class="string">      processors:  </span></span><br><span class="line"><span class="string">        - add_kubernetes_metadata:  </span></span><br><span class="line"><span class="string">            host: ${NODE_NAME}  </span></span><br><span class="line"><span class="string">            matchers:  </span></span><br><span class="line"><span class="string">            - logs_path:  </span></span><br><span class="line"><span class="string">                logs_path: "/var/log/containers/"  </span></span><br><span class="line"><span class="string">        - decode_json_fields:  </span></span><br><span class="line"><span class="string">            when:  </span></span><br><span class="line"><span class="string">               regexp:  </span></span><br><span class="line"><span class="string">                 message: "{*}"  </span></span><br><span class="line"><span class="string">            fields: ["message"]  </span></span><br><span class="line"><span class="string">            overwrite_keys: true  </span></span><br><span class="line"><span class="string">            target: ""  </span></span><br><span class="line"><span class="string">    # testnet-bsc-collector  </span></span><br><span class="line"><span class="string">    - type: container  </span></span><br><span class="line"><span class="string">      paths:  </span></span><br><span class="line"><span class="string">        - /var/log/containers/*testnet-bsc-collector*.log  </span></span><br><span class="line"><span class="string">      tags: ["testnet-bsc-collector"]  </span></span><br><span class="line"><span class="string">      fields:  </span></span><br><span class="line"><span class="string">        index: "testnet-bsc-collector"  </span></span><br><span class="line"><span class="string">      processors:  </span></span><br><span class="line"><span class="string">        - add_kubernetes_metadata:  </span></span><br><span class="line"><span class="string">            host: ${NODE_NAME}  </span></span><br><span class="line"><span class="string">            matchers:  </span></span><br><span class="line"><span class="string">            - logs_path:  </span></span><br><span class="line"><span class="string">                logs_path: "/var/log/containers/"  </span></span><br><span class="line"><span class="string">        - decode_json_fields:  </span></span><br><span class="line"><span class="string">            when:  </span></span><br><span class="line"><span class="string">               regexp:  </span></span><br><span class="line"><span class="string">                 message: "{*}"  </span></span><br><span class="line"><span class="string">            fields: ["message"]  </span></span><br><span class="line"><span class="string">            #overwrite_keys: true  </span></span><br><span class="line"><span class="string">            target: ""  </span></span><br><span class="line"><span class="string">    #================ output =====================  </span></span><br><span class="line"><span class="string">    output.elasticsearch:  </span></span><br><span class="line"><span class="string">      hosts: ["https://10.10.20.23:9200", "https://10.10.20.120:9200", "https://10.10.20.160:9200"]</span></span><br><span class="line"><span class="string">      username: elastic</span></span><br><span class="line"><span class="string">      password: r40SgkMhmjwK8rIpClFM</span></span><br><span class="line"><span class="string">      ssl.certificate_authorities:</span></span><br><span class="line"><span class="string">        - /usr/share/filebeat/ca.crt</span></span><br><span class="line"><span class="string">      indices:  </span></span><br><span class="line"><span class="string">        - index: "INDEX-NAME-%{+yyyy.MM.dd}"  </span></span><br><span class="line"><span class="string">          when.contains:  </span></span><br><span class="line"><span class="string">            fields:  </span></span><br><span class="line"><span class="string">              index: "INDEX-NAME"  </span></span><br><span class="line"><span class="string">        - index: "testnet-bsc-collector-%{+yyyy.MM.dd}"  </span></span><br><span class="line"><span class="string">          when.contains:  </span></span><br><span class="line"><span class="string">            fields:  </span></span><br><span class="line"><span class="string">              index: "testnet-bsc-collector"  </span></span><br><span class="line"><span class="string"></span>  </span><br><span class="line">    <span class="comment">#============== Elasticsearch template setting ==========  </span></span><br><span class="line">    <span class="attr">setup.ilm.enabled:</span> <span class="literal">false</span>  </span><br><span class="line">    <span class="attr">setup.template.name:</span> <span class="string">'k8s-logs'</span>  </span><br><span class="line">    <span class="attr">setup.template.pattern:</span> <span class="string">'k8s-logs-*'</span>  </span><br><span class="line">    <span class="attr">processors:</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">drop_fields:</span>  </span><br><span class="line">          <span class="attr">fields:</span> [<span class="string">"agent"</span>,<span class="string">"kubernetes.labels"</span>,<span class="string">"input.type"</span>,<span class="string">"log"</span>,<span class="string">"ecs.version"</span>,<span class="string">"host.name"</span>,<span class="string">"kubernetes.replicaset.name"</span>,<span class="string">"kubernetes.pod.uid"</span>,<span class="string">"kubernetes.pod.uid"</span>,<span class="string">"tags"</span>,<span class="string">"stream"</span>,<span class="string">"kubernetes.container.name"</span>]  </span><br><span class="line"><span class="meta">---  </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span>  </span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span>  </span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">filebeat</span>  </span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ops</span></span><br><span class="line">  <span class="attr">labels:</span>  </span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">filebeat</span>  </span><br><span class="line"><span class="attr">spec:</span>  </span><br><span class="line">  <span class="attr">selector:</span>  </span><br><span class="line">    <span class="attr">matchLabels:</span>  </span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">filebeat</span>  </span><br><span class="line">  <span class="attr">template:</span>  </span><br><span class="line">    <span class="attr">metadata:</span>  </span><br><span class="line">      <span class="attr">labels:</span>  </span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">filebeat</span>  </span><br><span class="line">    <span class="attr">spec:</span>  </span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">filebeat</span>  </span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span>  </span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span>  </span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirstWithHostNet</span>  </span><br><span class="line">      <span class="attr">tolerations:</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoSchedule</span>  </span><br><span class="line">        <span class="attr">operator:</span> <span class="string">Exists</span>  </span><br><span class="line">      <span class="attr">containers:</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">filebeat</span>  </span><br><span class="line">        <span class="attr">image:</span> <span class="string">elastic/filebeat:7.9.0</span>  </span><br><span class="line">        <span class="attr">args:</span> [  </span><br><span class="line">          <span class="string">"-c"</span>, <span class="string">"/etc/filebeat.yml"</span>,  </span><br><span class="line">          <span class="string">"-e"</span>,  </span><br><span class="line">        ]  </span><br><span class="line">        <span class="attr">env:</span>  </span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NODE_NAME</span>  </span><br><span class="line">          <span class="attr">valueFrom:</span>  </span><br><span class="line">            <span class="attr">fieldRef:</span>  </span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span>  </span><br><span class="line">        <span class="attr">securityContext:</span>  </span><br><span class="line">          <span class="attr">runAsUser:</span> <span class="number">0</span>  </span><br><span class="line">          <span class="comment"># If using Red Hat OpenShift uncomment this:  </span></span><br><span class="line">          <span class="comment">#privileged: true  </span></span><br><span class="line">        <span class="attr">resources:</span>  </span><br><span class="line">          <span class="attr">limits:</span>  </span><br><span class="line">            <span class="attr">memory:</span> <span class="string">200Mi</span>  </span><br><span class="line">          <span class="attr">requests:</span>  </span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span>  </span><br><span class="line">            <span class="attr">memory:</span> <span class="string">100Mi</span>  </span><br><span class="line">        <span class="attr">volumeMounts:</span>  </span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config</span>  </span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/filebeat.yml</span>  </span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span>  </span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">filebeat.yml</span>  </span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span>  </span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/filebeat/data</span>  </span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span>  </span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/containerd/</span>  </span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span>  </span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span>  </span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/log</span>  </span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span>  </span><br><span class="line">      <span class="attr">volumes:</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config</span>  </span><br><span class="line">        <span class="attr">configMap:</span>  </span><br><span class="line">          <span class="attr">defaultMode:</span> <span class="number">0600</span>  </span><br><span class="line">          <span class="attr">name:</span> <span class="string">filebeat-config</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span>  </span><br><span class="line">        <span class="attr">hostPath:</span>  </span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/containerd/</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span>  </span><br><span class="line">        <span class="attr">hostPath:</span>  </span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/log</span>  </span><br><span class="line">      <span class="comment"># data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart  </span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span>  </span><br><span class="line">        <span class="attr">hostPath:</span>  </span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/filebeat-data</span>  </span><br><span class="line">          <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span>  </span><br><span class="line"><span class="meta">---  </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span>  </span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span>  </span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">filebeat</span>  </span><br><span class="line"><span class="attr">subjects:</span>  </span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">filebeat</span>  </span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ops</span></span><br><span class="line"><span class="attr">roleRef:</span>  </span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">filebeat</span>  </span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span>  </span><br><span class="line"><span class="meta">---  </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span>  </span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span>  </span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">filebeat</span>  </span><br><span class="line">  <span class="attr">labels:</span>  </span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">filebeat</span>  </span><br><span class="line"><span class="attr">rules:</span>  </span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">""</span>] <span class="comment"># "" indicates the core API group  </span></span><br><span class="line">  <span class="attr">resources:</span>  </span><br><span class="line">  <span class="bullet">-</span> <span class="string">namespaces</span>  </span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span>  </span><br><span class="line">  <span class="attr">verbs:</span>  </span><br><span class="line">  <span class="bullet">-</span> <span class="string">get</span>  </span><br><span class="line">  <span class="bullet">-</span> <span class="string">watch</span>  </span><br><span class="line">  <span class="bullet">-</span> <span class="string">list</span>  </span><br><span class="line"><span class="meta">---  </span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span>  </span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span>  </span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line">  <span class="attr">name:</span> <span class="string">filebeat</span>  </span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ops</span></span><br><span class="line">  <span class="attr">labels:</span>  </span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">filebeat</span>  </span><br><span class="line"><span class="meta">---  </span></span><br></pre></td></tr></tbody></table></figure>
]]></content>
  </entry>
  <entry>
    <title>如何让添加定时作业任务变得更加优雅</title>
    <url>/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/</url>
    <content><![CDATA[<p> <img src="/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/info.png"></p>
<h1 id="序-APSCheduler-简介"><a href="#序-APSCheduler-简介" class="headerlink" title="序 APSCheduler 简介"></a>序 APSCheduler 简介</h1><p>在平常的工作中有些工作都需要定时任务来推动，例如项目中有一个定时刷新排行榜的程序脚本、定时爬出网站的URL程序、定时检测钓鱼网站的程序等等，都涉及到了关于定时任务的问题；虽然这些定时任务在服务器上面都能通过crontab来做；其次想到的是利用time模块的time.sleep()方法使程序休眠来达到定时任务的目的，虽然前两者也可以，但是总觉得不是那么的专业，😁所以就找到了python的定时任务模块APScheduler;</p>
<p><strong>APScheduler基于Quartz的一个Python定时任务框架，实现了Quartz的所有功能，使用起来十分方便。提供了基于日期、固定时间间隔以及crontab类型的任务，并且可以持久化任务。基于这些功能，我们可以很方便的实现一个python定时任务系统。</strong></p>
<p>同时APScheduler还提供了Flask的扩展<code>Flask-Apscheduler</code>, 那正好就可以拿来做一个集成定时任务平台;</p>
<p><img src="/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/liuchengtu.png" alt="APScheduler工作原理"></p>
<h1 id="APScheduler有四个组成部分"><a href="#APScheduler有四个组成部分" class="headerlink" title="APScheduler有四个组成部分"></a>APScheduler有四个组成部分</h1><ol>
<li><p><code>触发器(trigger)</code>包含调度逻辑，每一个作业有它自己的触发器，用于决定接下来哪一个作业会运行。除了他们自己初始配置意外，触发器完全是无状态的。</p>
 <figure class="highlight livecodeserver"><table><tbody><tr><td class="code"><pre><span class="line">cron: 类linux下的crontab格式,属于定时调度</span><br><span class="line">interval:每隔多久调度一次</span><br><span class="line"><span class="built_in">date</span>:一次性调度</span><br><span class="line"></span><br><span class="line"><span class="comment">#1. cron风格</span></span><br><span class="line">(int|str) 表示参数既可以是int类型，也可以是str类型</span><br><span class="line">(datetime | str) 表示参数既可以是datetime类型，也可以是str类型</span><br><span class="line">year (int|str) – <span class="number">4</span>-digit year -（表示四位数的年份，如<span class="number">2008</span>年）</span><br><span class="line">month (int|str) – month (<span class="number">1</span><span class="number">-12</span>) -（表示取值范围为<span class="number">1</span><span class="number">-12</span>月）</span><br><span class="line">day (int|str) – day <span class="keyword">of</span> <span class="keyword">the</span> (<span class="number">1</span><span class="number">-31</span>) -（表示取值范围为<span class="number">1</span><span class="number">-31</span>日）</span><br><span class="line">week (int|str) – ISO week (<span class="number">1</span><span class="number">-53</span>) -（格里历<span class="number">2006</span>年<span class="number">12</span>月<span class="number">31</span>日可以写成<span class="number">2006</span>年-W52<span class="number">-7</span>（扩展形式）或<span class="number">2006</span>W527（紧凑形式））</span><br><span class="line">day_of_week (int|str) – <span class="built_in">number</span> <span class="keyword">or</span> name <span class="keyword">of</span> weekday (<span class="number">0</span><span class="number">-6</span> <span class="keyword">or</span> mon,tue,wed,thu,fri,sat,sun) - （表示一周中的第几天，既可以用<span class="number">0</span><span class="number">-6</span>表示也可以用其英语缩写表示）</span><br><span class="line">hour (int|str) – hour (<span class="number">0</span><span class="number">-23</span>) - （表示取值范围为<span class="number">0</span><span class="number">-23</span>时）</span><br><span class="line">minute (int|str) – minute (<span class="number">0</span><span class="number">-59</span>) - （表示取值范围为<span class="number">0</span><span class="number">-59</span>分）</span><br><span class="line"><span class="keyword">second</span> (int|str) – <span class="keyword">second</span> (<span class="number">0</span><span class="number">-59</span>) - （表示取值范围为<span class="number">0</span><span class="number">-59</span>秒）</span><br><span class="line">start_date (datetime|str) – earliest possible <span class="built_in">date</span>/<span class="built_in">time</span> <span class="built_in">to</span> trigger <span class="keyword">on</span> (<span class="title">inclusive</span>) - （表示开始时间）</span><br><span class="line">end_date (datetime|str) – latest possible <span class="built_in">date</span>/<span class="built_in">time</span> <span class="built_in">to</span> trigger <span class="keyword">on</span> (<span class="title">inclusive</span>) - （表示结束时间）</span><br><span class="line">timezone (datetime.tzinfo|str) – <span class="built_in">time</span> zone <span class="built_in">to</span> use <span class="keyword">for</span> <span class="keyword">the</span> <span class="built_in">date</span>/<span class="built_in">time</span> calculations (defaults <span class="built_in">to</span> scheduler timezone) -（表示时区取值）</span><br><span class="line"><span class="comment">#如:在6,7,8,11,12月份的第三个星期五的00:00,01:00,02:00,03:00 执行该程序</span></span><br><span class="line">sched.add_job(my_job, <span class="string">'cron'</span>, month=<span class="string">'6-8,11-12'</span>, day=<span class="string">'3rd fri'</span>, hour=<span class="string">'0-3'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.interval风格</span></span><br><span class="line">weeks (int) – <span class="built_in">number</span> <span class="keyword">of</span> weeks <span class="built_in">to</span> <span class="built_in">wait</span></span><br><span class="line">days (int) – <span class="built_in">number</span> <span class="keyword">of</span> days <span class="built_in">to</span> <span class="built_in">wait</span></span><br><span class="line">hours (int) – <span class="built_in">number</span> <span class="keyword">of</span> hours <span class="built_in">to</span> <span class="built_in">wait</span></span><br><span class="line">minutes (int) – <span class="built_in">number</span> <span class="keyword">of</span> minutes <span class="built_in">to</span> <span class="built_in">wait</span></span><br><span class="line"><span class="built_in">seconds</span> (int) – <span class="built_in">number</span> <span class="keyword">of</span> <span class="built_in">seconds</span> <span class="built_in">to</span> <span class="built_in">wait</span></span><br><span class="line">start_date (datetime|str) – starting point <span class="keyword">for</span> <span class="keyword">the</span> interval calculation</span><br><span class="line">end_date (datetime|str) – latest possible <span class="built_in">date</span>/<span class="built_in">time</span> <span class="built_in">to</span> trigger <span class="keyword">on</span></span><br><span class="line">timezone (datetime.tzinfo|str) – <span class="built_in">time</span> zone <span class="built_in">to</span> use <span class="keyword">for</span> <span class="keyword">the</span> <span class="built_in">date</span>/<span class="built_in">time</span> calculations</span><br><span class="line"><span class="comment">#如:每隔2分钟执行一次</span></span><br><span class="line">scheduler.add_job(myfunc, <span class="string">'interval'</span>, minutes=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.date风格</span></span><br><span class="line">run_date (datetime|str) – <span class="keyword">the</span> <span class="built_in">date</span>/<span class="built_in">time</span> <span class="built_in">to</span> run <span class="keyword">the</span> job <span class="keyword">at</span>  -（任务开始的时间）</span><br><span class="line">timezone (datetime.tzinfo|str) – <span class="built_in">time</span> zone <span class="keyword">for</span> run_date <span class="keyword">if</span> <span class="keyword">it</span> doesn’t have <span class="literal">one</span> already</span><br><span class="line"><span class="comment">#如:在2009年11月6号16时30分5秒时执行</span></span><br><span class="line">sched.add_job(my_job, <span class="string">'date'</span>, run_date=datetime(<span class="number">2009</span>, <span class="number">11</span>, <span class="number">6</span>, <span class="number">16</span>, <span class="number">30</span>, <span class="number">5</span>), args=[<span class="string">'text'</span>])</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><code>作业存储(job store)</code>存储被调度的作业，默认的作业存储是简单地把作业保存在内存中，其他的作业存储是将作业保存在数据库中。一个作业的数据讲在保存在持久化作业存储时被序列化，并在加载时被反序列化。调度器不能分享同一个作业存储。</p>
<p> jobstore则是指的是job持久化,默认job运行在内存中,可持久化在数据库,指定为mongo的MongoDBJobStore或者是使用sqlite的SQLAlchemyJobStore,同时可指定多种jobstore</p>
</li>
<li><p><code>执行器(executor)</code>处理作业的运行，他们通常通过在作业中提交制定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。</p>
<p> 说白了就是指定任务是以线程池/进程池里运行,这在初始化时可以指定,同时可以指定最大的工作池,默认的为default: ThreadPoolExecutor,max-worker为20,当然也可以指定为processpool,默认max-worker为5</p>
</li>
<li><p><code>调度器(scheduler)</code>是其他的组成部分。你通常在应用只有一个调度器，应用的开发者通常不会直接处理作业存储、调度器和触发器，相反，调度器提供了处理这些的合适的接口。配置作业存储和执行器可以在调度器中完成，例如添加、修改和移除作业。<br>调度器分为以下几种,可根据不同的使用场景选用不同的调度器:</p>
 <figure class="highlight avrasm"><table><tbody><tr><td class="code"><pre><span class="line"><span class="symbol">BlockingScheduler:</span> 很明显这是种阻塞型,一般用在没有其它进程运行的场景下</span><br><span class="line"><span class="symbol">BackGroundScheduler:</span> 后台式,也就是单起一个进程/线程运行该任务,不影响主程序</span><br><span class="line"><span class="symbol">ASyncIOScheduler:</span></span><br><span class="line"><span class="symbol">GeventScheduler:</span></span><br><span class="line"><span class="symbol">TornadoScheduler:</span></span><br><span class="line"><span class="symbol">TwistedScheduler:</span></span><br><span class="line"><span class="symbol">QtScheduler:</span></span><br></pre></td></tr></tbody></table></figure>
<p> 本人只使用过BlockingScheduler跟BackGroundScheduler,flask-scheduler使用的即为BackGroundScheduler,其它的后续再研究研究<br> 选择类型也很简单,初始化时直接实例化:</p>
 <figure class="highlight mipsasm"><table><tbody><tr><td class="code"><pre><span class="line">from apscheduler.<span class="keyword">schedulers.background </span>import <span class="keyword">BackgroundScheduler</span></span><br><span class="line"><span class="keyword"></span><span class="keyword">scheduler </span>= <span class="keyword">BackgroundScheduler()</span></span><br><span class="line"><span class="keyword"></span><span class="comment">#启动</span></span><br><span class="line"><span class="keyword">scheduler.start()</span></span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<p>以上就是apscheduler主要组成的四个方面。那么既然有提供flask-apscheduler这个扩展，那一定是把apscheduler封装过后为我们提供更加方便的集成开发，这里我的存储选择了MySQL作为作业任务存储，因为我这里的平台的用户是用它来存储的；为了方便统一使用了MySQL;</p>
<hr>
<p>以下是这个项目的结构:</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"> <span class="symbol">/data/study/github_repo/JobCenter</span> $ tree <span class="operator">-</span>L <span class="number">2</span></span><br><span class="line">.</span><br><span class="line">├── LICENSE</span><br><span class="line">├── Pipfile</span><br><span class="line">├── Pipfile.lock</span><br><span class="line">├── README.md</span><br><span class="line">├── app</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;&nbsp; ├── auth<span class="symbol">/</span>  <span class="comment"># 平台登录相关</span></span><br><span class="line">│&nbsp;&nbsp; ├── commands.py</span><br><span class="line">│&nbsp;&nbsp; ├── config.py  <span class="comment"># 平台配置相关</span></span><br><span class="line">│&nbsp;&nbsp; ├── decorators.py</span><br><span class="line">│&nbsp;&nbsp; ├── email.py  <span class="comment"># 邮件发送</span></span><br><span class="line">│&nbsp;&nbsp; ├── extensions.py</span><br><span class="line">│&nbsp;&nbsp; ├── job<span class="symbol">/</span>  <span class="comment"># 定时任务主要模块</span></span><br><span class="line">│&nbsp;&nbsp; ├── main<span class="symbol">/</span>  <span class="comment"># 主要用于路由</span></span><br><span class="line">│&nbsp;&nbsp; ├── models.py  <span class="comment"># 数据模型</span></span><br><span class="line">│&nbsp;&nbsp; ├── static				</span><br><span class="line">│&nbsp;&nbsp; ├── templates</span><br><span class="line">└── test.py       </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>目前主要实现的功能有:</p>
<ul>
<li>可视化界面操作</li>
<li>定时任务统一管理</li>
<li>完全兼容Crontab</li>
<li>支持秒级定时任务</li>
<li>作业任务可搜索、暂停、编辑、删除</li>
<li>作业任务持久化存储、三种不同触发器类型作业动态添加</li>
</ul>
<hr>
<p><img src="/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/login.png" alt="清爽的登录界面"></p>
<p><img src="/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/pausejob.png" alt="任务列表中暂停、恢复已添加任务"></p>
<p><img src="/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/addjob.png" alt="针对不同触发器动态增加任务"></p>
<p><img src="/2019/03/19/Flask%E7%BB%93%E5%90%88APScheduler%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E6%A1%86%E6%9E%B6%E5%B9%B3%E5%8F%B0/stdout.png" alt="任务执行输出日志持久化存放并展示"></p>
<p>欢迎star 欢迎提建议~<br>项目地址: <a href="https://github.com/guomaoqiu/JobCenter">https://github.com/guomaoqiu/JobCenter</a><br>Demo地址: <a href="https://jobcenter.sctux.cc/">https://jobcenter.sctux.cc</a></p>
]]></content>
      <categories>
        <category>定时任务</category>
      </categories>
      <tags>
        <tag>Flask APScheduler</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S中MySQL使用NFS挂载的异常问题处理</title>
    <url>/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h1><p>在k8s中使用nfs作为持久存储方式，根据不同环境对应不同名称空间的pod使用的存储类/存储卷 相对应，发现mysql8.0.23 在启动时挂载NFS会卡住的情况：</p>
<h1 id="问题复现"><a href="#问题复现" class="headerlink" title="问题复现"></a>问题复现</h1><p>这里在某台宿主机上面创建了一个本地目录，然后按照默认方式：</p>
<figure class="highlight elixir"><table><tbody><tr><td class="code"><pre><span class="line">mount -t nfs   <span class="number">10.10</span>.<span class="number">10.142</span><span class="symbol">:/data/bbb</span> /tmp/cc</span><br></pre></td></tr></tbody></table></figure>


<p>将nfs挂载到本地，然后映射到容器中去，所以这里跟k8s环境没多大联系，主要是nfs挂载问题：</p>
<p> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/1.png"></p>
<p>初始化一直卡着，而且在挂载目录发现只有这个文件：</p>
<p> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/0.png"></p>
<h1 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h1><p>检查一下nfs挂载的参数情况：</p>
<figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line">mount -v  | grep nfs</span><br><span class="line"></span><br><span class="line">10.10.10.137:<span class="string">/data/nfs_share_137</span> on <span class="string">/tmp/fans</span> type nfs <span class="params">(rw,relatime,<span class="attr">vers</span>=3,<span class="attr">rsize</span>=1048576,<span class="attr">wsize</span>=1048576,<span class="attr">namlen</span>=255,hard,<span class="attr">proto</span>=tcp,<span class="attr">timeo</span>=600,<span class="attr">retrans</span>=2,<span class="attr">sec</span>=sys,<span class="attr">mountaddr</span>=10.10.10.137,<span class="attr">mountvers</span>=3,<span class="attr">mo</span>=udp,<span class="attr">local_lock</span>=none,<span class="attr">addr</span>=10.10.10.137)</span></span><br><span class="line"></span><br><span class="line">10.10.10.142:<span class="string">/data/bbb</span> on <span class="string">/tmp/cc</span> type nfs <span class="params">(rw,relatime,<span class="attr">vers</span>=3,<span class="attr">rsize</span>=1048576,<span class="attr">wsize</span>=1048576,<span class="attr">namlen</span>=255,hard,<span class="attr">proto</span>=tcp,<span class="attr">timeo</span>=600,<span class="attr">retrans</span>=2,<span class="attr">sec</span>=sys,<span class="attr">mountaddr</span>=10.10.10.142,<span class="attr">mountvers</span>=3,<span class="attr">mountport</span>=<span class="attr">569lock</span>=none,<span class="attr">addr</span>=10.10.10.142)</span></span><br></pre></td></tr></tbody></table></figure>

<p>如上可以看出我们如果使用默认的方式直接挂载，它会有一些默认参数</p>
<p>后面也是通过该容器运行的宿主机上的内核日志才发现，这个问题：<br> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/3.png"></p>
<h1 id="问题处理"><a href="#问题处理" class="headerlink" title="问题处理"></a>问题处理</h1><p>这个错误提示实际上是NFS客户端无法获得文件锁定，而不是无法与NFS服务器建立联系。当NFS客户端无法获得文件锁定时，它会尝试向NFS服务器发送锁定请求，并等待服务器响应。如果NFS服务器没有响应，则可能会出现类似的错误提示。可能造成这个错误的原因如下：</p>
<ol>
<li>检查NFS服务器的负载情况，如果负载过高，则可能需要优化服务器配置或增加服务器数量。</li>
<li>检查NFS客户端与服务器之间的网络连接，如果存在网络故障，则可能需要修复网络问题或更改网络配置。</li>
<li>考虑使用NFS挂载选项来优化NFS客户端的行为，例如使用soft选项而不是hard选项，或者使用intr选项允许中断NFS操作。</li>
<li>如果NFS客户端和服务器在不同的时区中运行，则可能需要使用noac选项来禁用NFS客户端的属性缓存，以确保文件锁定请求和响应的时间戳是正确的。</li>
<li>如果您的NFS客户端和服务器运行的是不同的操作系统，则可能需要使用适当的挂载选项来确保文件锁定机制得到正确支持，例如使用nolock选项来禁用文件锁定机制。</li>
</ol>
<p>因为我们使用了kuboard, 无法全局对这个存储类做暂时不能影响业务，不能所有变更：<br> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/4.png"><br>这里只是单独这个mysql服务没有办法挂在使用，是由需要添加挂载参数：<br> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/5.png"></p>
<p> 找到我们在页面上新建的pv:</p>
 <figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"> kubectl edit pv <span class="operator">-</span>n testnet pvc-b0de2c7f-<span class="number">49</span>de-<span class="number">4</span>e16-<span class="number">83</span>cc-<span class="number">492</span>de8292e5f</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Please edit the object below. Lines beginning with a '#' will be ignored,</span></span><br><span class="line"><span class="comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span></span><br><span class="line"><span class="comment"># reopened with the relevant failures.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> PersistentVolume</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">annotations:</span></span><br><span class="line">    pv.kubernetes.io<span class="operator">/</span><span class="params">provisioned-by:</span> nfs-testnet</span><br><span class="line">  <span class="params">creationTimestamp:</span> <span class="string">"2023-07-20T16:29:30Z"</span></span><br><span class="line">  <span class="params">finalizers:</span></span><br><span class="line">  <span class="operator">-</span> kubernetes.io<span class="symbol">/pv-protection</span></span><br><span class="line">  <span class="params">name:</span> pvc-b0de2c7f-<span class="number">49</span>de-<span class="number">4</span>e16-<span class="number">83</span>cc-<span class="number">492</span>de8292e5f</span><br><span class="line">  <span class="params">resourceVersion:</span> <span class="string">"147600146"</span></span><br><span class="line">  <span class="params">uid:</span> e9f111ac-<span class="number">293</span>2-<span class="number">46</span>a0-b6bc-<span class="number">876275</span>dfa02f</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">accessModes:</span></span><br><span class="line">  <span class="operator">-</span> ReadWriteOnce</span><br><span class="line">  <span class="params">capacity:</span></span><br><span class="line">    <span class="params">storage:</span> <span class="number">2</span>G</span><br><span class="line">  <span class="params">claimRef:</span></span><br><span class="line">    <span class="params">apiVersion:</span> v1</span><br><span class="line">    <span class="params">kind:</span> PersistentVolumeClaim</span><br><span class="line">    <span class="params">name:</span> dassssssssssssssss</span><br><span class="line">    <span class="params">namespace:</span> testnet</span><br><span class="line">    <span class="params">resourceVersion:</span> <span class="string">"147600141"</span></span><br><span class="line">    <span class="params">uid:</span> b0de2c7f-<span class="number">49</span>de-<span class="number">4</span>e16-<span class="number">83</span>cc-<span class="number">492</span>de8292e5f</span><br><span class="line">  <span class="params">nfs:</span></span><br><span class="line">    <span class="params">path:</span> <span class="symbol">/data/testnet-dassssssssssssssss-pvc-b0de2c7f-49de-4e16-83cc-492de8292e5f</span></span><br><span class="line">    <span class="params">server:</span> <span class="number">10.10</span>.<span class="number">10.142</span></span><br><span class="line">  <span class="params">persistentVolumeReclaimPolicy:</span> Retain</span><br><span class="line">  <span class="params">storageClassName:</span> testnet</span><br><span class="line">  <span class="params">volumeMode:</span> Filesystem</span><br><span class="line"><span class="params">status:</span></span><br><span class="line">  <span class="params">phase:</span> Bound</span><br></pre></td></tr></tbody></table></figure>


<p>然后我们在<code>volumeMode同级</code> 就可以定义挂载参数(具体参数：<a href="https://poe.com/s/FyTCOkthr3VREHU3mByA)%EF%BC%9A">https://poe.com/s/FyTCOkthr3VREHU3mByA)：</a></p>
<figure class="highlight ldif"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">mountOptions</span>:</span><br><span class="line"><span class="literal">-</span> nolock</span><br><span class="line"><span class="literal">-</span> timeo=120</span><br><span class="line"><span class="literal">-</span> intr</span><br><span class="line"><span class="literal">-</span> retrans=10</span><br></pre></td></tr></tbody></table></figure>

<p>这样我们单独为MySQL服务创建的这个pvc就拥有了这些挂载参数，初始化，运行正常：<br> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/6.png"></p>
<p>在这个pod运行的宿主机上 查看结果，可以看到我们修改pv后的参数已经生效，并且在nfs服务端可以看到我们的MySQL初始化文件已经生成：<br> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/7.png"><br> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/8.png"></p>
<p>最终该挂载问题完美解决。</p>
<p>对于特殊，不同的deployment 可能需要特殊处理，这里按照mysql的部署，记录一下顺序</p>
<ol>
<li>创建pvc</li>
<li>修改默认pvc配置，增加nfs挂载参数</li>
<li>启动pod</li>
<li>检查挂载目录数据是否正常生成</li>
<li>检查挂载选项是否已经增加</li>
</ol>
<p> <img src="/2024/05/10/K8S%E4%B8%ADMySQL%E4%BD%BF%E7%94%A8NFS%E6%8C%82%E8%BD%BD%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/00.jpeg"></p>
<p>By the way:<br>谁说的K8S不能跑MySQL， 我嫩死TA 🔪🔪🔪</p>
]]></content>
      <tags>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus+Consul实现企业级宿主机+容器监控告警</title>
    <url>/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/</url>
    <content><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>因公司历史遗留原因有个别环境暂时没有使用kubernets, 现在需要将这批服务器的监控系统从zabbix替换到Prometheus, 于是乎这边有个问题就是需要将所有服务器上面的所有的exporter mertics（即 target）地址写到Prometheus配置文件中，这样一来，维护一个文件，似乎还算可以，但是这里我采用Prometheus+Consul的方式来管理服务器上的所有exporter， 那么这样做的好处是我们能更清晰的通过Consul来管理上面的每个exporter url , 以及通过consul自带的自定义元数据，再结合Promethues无疑是个很灵活的方式。</p>
<h1 id="二、架构"><a href="#二、架构" class="headerlink" title="二、架构"></a>二、架构</h1><p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/arch.png"></p>
<ol>
<li>promtheus配置数据源为consu_sd_configs 地址指向一个consul客户端地址；</li>
<li>通过脚本调用consulapi的方式将宿主机上面的cadvisor-exporter,node-exporter metrics的地址注册到consul中；</li>
<li>promtheus 检测到了新增服务后，会通过这个<a href="http://xxxxxx:xxx/metrics">http://xxxxxx:xxx/metrics</a> url 抓取采集数据；</li>
<li>后续的数据采集就跟平常我们使用的方式一样了，采集到的数据时序保存在promtheus；</li>
<li>Grafana作为采集数据的一个展示,通过各种label，更加方便的对面板进行配置；</li>
<li>添加node-exporter， cadvisor-exporter的指标报警规则，通过Alertmanager 发出主机或容器的告警；</li>
</ol>
<h1 id="三、实现"><a href="#三、实现" class="headerlink" title="三、实现"></a>三、实现</h1><h2 id="Prometheus以及周边组件部署"><a href="#Prometheus以及周边组件部署" class="headerlink" title="Prometheus以及周边组件部署"></a>Prometheus以及周边组件部署</h2><p>该编排文件中部署了</p>
<ol>
<li>Prometheus: 监控系统主程序</li>
<li>Alertmanager: 告警发送</li>
<li>Grafana: 数据展示</li>
<li>Prometheus-dingding-webhook: 钉钉告警推送</li>
<li>alertmanager-dashboard: 主要用于告警条目展示，该软件是开源项目，随意下载部署</li>
</ol>
<figure class="highlight nestedtext"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">cat &gt; docker-compose.yaml &lt;&lt; EOF</span></span><br><span class="line"><span class="attribute">version</span><span class="punctuation">:</span> <span class="string">'3.7'</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">services</span><span class="punctuation">:</span></span><br><span class="line"><span class="punctuation"></span></span><br><span class="line">  <span class="attribute">prometheus</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">prom/prometheus:latest</span></span><br><span class="line">    <span class="attribute">volumes</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./prometheus/:/etc/prometheus/</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/data/prometheus-data:/prometheus</span></span><br><span class="line">    <span class="attribute">command</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--config.file=/etc/prometheus/prometheus.yml'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--storage.tsdb.path=/prometheus'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--web.console.libraries=/usr/share/prometheus/console_libraries'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--web.console.templates=/usr/share/prometheus/consoles'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--web.enable-lifecycle'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--web.external-url=http://192.168.18.178:9090'</span></span><br><span class="line">    <span class="attribute">ports</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">9090:9090</span></span><br><span class="line">    <span class="attribute">links</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">alertmanager:alertmanager</span></span><br><span class="line">    <span class="attribute">restart</span><span class="punctuation">:</span> <span class="string">always</span></span><br><span class="line">    <span class="attribute">deploy</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">resources</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">limits</span><span class="punctuation">:</span></span><br><span class="line">          <span class="attribute">cpus</span><span class="punctuation">:</span> <span class="string">"1.0"</span></span><br><span class="line">          <span class="attribute">memory</span><span class="punctuation">:</span> <span class="string">500M</span></span><br><span class="line"></span><br><span class="line">  <span class="attribute">alertmanager</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">prom/alertmanager</span></span><br><span class="line">    <span class="attribute">ports</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">9093:9093</span></span><br><span class="line">    <span class="attribute">volumes</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./alertmanager/:/etc/alertmanager/</span></span><br><span class="line">    <span class="attribute">restart</span><span class="punctuation">:</span> <span class="string">always</span></span><br><span class="line">    <span class="attribute">command</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--config.file=/etc/alertmanager/config.yml'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--storage.path=/alertmanager'</span></span><br><span class="line">    <span class="attribute">deploy</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">resources</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">limits</span><span class="punctuation">:</span></span><br><span class="line">          <span class="attribute">cpus</span><span class="punctuation">:</span> <span class="string">"1.0"</span></span><br><span class="line">          <span class="attribute">memory</span><span class="punctuation">:</span> <span class="string">500M</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="attribute">grafana</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">grafana/grafana:8.4.0</span></span><br><span class="line">    <span class="attribute">depends_on</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">prometheus</span></span><br><span class="line">    <span class="attribute">ports</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">3000:3000</span></span><br><span class="line">    <span class="attribute">volumes</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/data/grafana-data:/var/lib/grafana</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./grafana/provisioning/:/etc/grafana/provisioning/</span></span><br><span class="line">    <span class="attribute">env_file</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./grafana/config.monitoring</span></span><br><span class="line">    <span class="attribute">restart</span><span class="punctuation">:</span> <span class="string">always</span></span><br><span class="line">    <span class="attribute">deploy</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">resources</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">limits</span><span class="punctuation">:</span></span><br><span class="line">          <span class="attribute">cpus</span><span class="punctuation">:</span> <span class="string">"1.0"</span></span><br><span class="line">          <span class="attribute">memory</span><span class="punctuation">:</span> <span class="string">500M</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="attribute">webhook</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">timonwong/prometheus-webhook-dingtalk</span></span><br><span class="line">    <span class="attribute">depends_on</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">prometheus</span></span><br><span class="line">    <span class="attribute">volumes</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./prometheus-webhook-dingtalk/config.yml:/config/config.yml</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./prometheus-webhook-dingtalk/dingding.tmpl:/config/dingding.tmpl</span></span><br><span class="line">    <span class="attribute">ports</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">9060:8060</span></span><br><span class="line">    <span class="attribute">command</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--web.listen-address=:8060'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--config.file=/config/config.yml'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--web.enable-ui'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--web.enable-lifecycle'</span></span><br><span class="line">    <span class="attribute">restart</span><span class="punctuation">:</span> <span class="string">always</span></span><br><span class="line">    <span class="attribute">deploy</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">resources</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">limits</span><span class="punctuation">:</span></span><br><span class="line">          <span class="attribute">cpus</span><span class="punctuation">:</span> <span class="string">"1.0"</span></span><br><span class="line">          <span class="attribute">memory</span><span class="punctuation">:</span> <span class="string">500M</span></span><br><span class="line"></span><br><span class="line">  <span class="attribute">alertmanager-dashboard</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">ghcr.io/prymitive/karma:latest</span></span><br><span class="line">    <span class="attribute">ports</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">9094:8080</span></span><br><span class="line">    <span class="attribute">restart</span><span class="punctuation">:</span> <span class="string">always</span></span><br><span class="line">    <span class="attribute">environment</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ALERTMANAGER_URI=http://192.168.18.178:9093</span></span><br><span class="line">    <span class="attribute">deploy</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">resources</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">limits</span><span class="punctuation">:</span></span><br><span class="line">          <span class="attribute">cpus</span><span class="punctuation">:</span> <span class="string">"1.0"</span></span><br><span class="line">          <span class="attribute">memory</span><span class="punctuation">:</span> <span class="string">500M</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># run起来</span></span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>以上运行起来后，基本的功能已经完成，此时只需要将target添加到prometheus中就可以了</p>
<h2 id="3-2-exporter-数据采集器-部署"><a href="#3-2-exporter-数据采集器-部署" class="headerlink" title="3.2 exporter(数据采集器)部署"></a>3.2 exporter(数据采集器)部署</h2><p>上面提到我们这部分环境没有迁移到k8s，所有微服务都是docker方式运行在宿主机上面的，所以，这里需要单独需要去部署对宿主机跟容器的eporter(数据采集器), 目前的需求就是监控宿主机以及容器，并将采集到的数据进行展示、告警等。</p>
<ul>
<li>宿主机的监控：利用Prometheus搭建部署监控系统，那么对于服务器层面的数据采集我们首选的是node-exporter；</li>
<li>容器的监控：在调研容器监控这块儿的时候发现container-exporter监控指标虽然能满足我们的需求，但是发现采集的数据过于简洁且在运行过程中会造成内存积压最终导致服务不可用，除非是人为干涉重启一下，释放内存，但是这种事儿不应该发生。因此还是采用cadvisor exporter来进行容器的数据采集</li>
</ul>
<p>需要在每台宿主机上面部署两个exporter</p>
<h3 id="创建目录-一般放到普通用户app家目录即可"><a href="#创建目录-一般放到普通用户app家目录即可" class="headerlink" title="创建目录(一般放到普通用户app家目录即可)"></a>创建目录(一般放到普通用户app家目录即可)</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> exporter-deploy &amp;&amp; <span class="built_in">cd</span> exporter-deploy</span><br></pre></td></tr></tbody></table></figure>
<h3 id="编排文件准备"><a href="#编排文件准备" class="headerlink" title="编排文件准备"></a>编排文件准备</h3><figure class="highlight nestedtext"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">cat &gt; docker-compose.yaml &lt;&lt; EOF</span></span><br><span class="line"><span class="attribute">version</span><span class="punctuation">:</span> <span class="string">'3.7'</span></span><br><span class="line"><span class="attribute">services</span><span class="punctuation">:</span></span><br><span class="line">  <span class="comment"># 宿主机数据采集</span></span><br><span class="line">  <span class="attribute">node-exporter</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">bitnami/node-exporter:latest</span></span><br><span class="line">    <span class="attribute">volumes</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/proc:/host/proc:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/sys:/host/sys:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/:/rootfs:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/:/host:ro,rslave</span></span><br><span class="line">    <span class="attribute">command</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--path.rootfs=/host'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--path.procfs=/host/proc'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">'--path.sysfs=/host/sys'</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--web.disable-exporter-metrics</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--collector.filesystem.ignored-mount-points</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"</span></span><br><span class="line">    <span class="attribute">ports</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">9100:9100</span></span><br><span class="line">    <span class="attribute">restart</span><span class="punctuation">:</span> <span class="string">always</span></span><br><span class="line">    <span class="attribute">deploy</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">resources</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">limits</span><span class="punctuation">:</span></span><br><span class="line">          <span class="attribute">cpus</span><span class="punctuation">:</span> <span class="string">"1.0"</span></span><br><span class="line">          <span class="attribute">memory</span><span class="punctuation">:</span> <span class="string">500M</span></span><br><span class="line">          </span><br><span class="line">  <span class="comment"># 容器数据采集</span></span><br><span class="line">  <span class="attribute">cadvisor-exporter</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">gcr.io/cadvisor/cadvisor/cadvisor:v0.45.0</span></span><br><span class="line">    <span class="attribute">volumes</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/:/rootfs:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/var/run:/var/run:rw</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/sys:/sys:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/var/lib/docker/:/var/lib/docker:ro</span></span><br><span class="line">    <span class="attribute">command</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"-docker_only=true"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"-housekeeping_interval=10s"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"-docker_only=true"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"--allow_dynamic_housekeeping=false"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"--storage_duration=20s"</span></span><br><span class="line">    <span class="attribute">ports</span><span class="punctuation">:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">9105:8080</span></span><br><span class="line">    <span class="attribute">restart</span><span class="punctuation">:</span> <span class="string">always</span></span><br><span class="line">    <span class="attribute">deploy</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">mode</span><span class="punctuation">:</span> <span class="string">global</span></span><br><span class="line">      <span class="attribute">resources</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">limits</span><span class="punctuation">:</span></span><br><span class="line">          <span class="attribute">cpus</span><span class="punctuation">:</span> <span class="string">"2.0"</span></span><br><span class="line">          <span class="attribute">memory</span><span class="punctuation">:</span> <span class="string">500M</span></span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line"><span class="comment"># run起来</span></span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>以上，只需要此配置文件就部署好了 “exporter客户端” </p>
<p>下面就是 将这台服务器以及连个exporter的url 注册到consul中，如果有批量安装需求，直接在jump上面通过ansible 推送即可</p>
<h2 id="3-3-consul-集群部署"><a href="#3-3-consul-集群部署" class="headerlink" title="3.3 consul 集群部署"></a>3.3 consul 集群部署</h2><h3 id="手动创建一个docker-网络"><a href="#手动创建一个docker-网络" class="headerlink" title="手动创建一个docker 网络"></a>手动创建一个docker 网络</h3><figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">docker</span> network create --driver bridge --subnet <span class="number">10.10.0.0</span>/<span class="number">24</span>  consul-network</span><br></pre></td></tr></tbody></table></figure>

<h3 id="创建consul的数据、配置目录"><a href="#创建consul的数据、配置目录" class="headerlink" title="创建consul的数据、配置目录"></a>创建consul的数据、配置目录</h3><figure class="highlight haskell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="title">sudo</span> mkdir  -p /<span class="class"><span class="keyword">data</span>/consul/consul-server{1..3}/{<span class="title">data</span>,<span class="title">config</span>}</span></span><br><span class="line"><span class="title">sudo</span> mkdir  -p /<span class="class"><span class="keyword">data</span>/consul/consul-client{1..2}/{<span class="title">data</span>,<span class="title">config</span>}</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="编辑docker-compose配置"><a href="#编辑docker-compose配置" class="headerlink" title="编辑docker-compose配置"></a>编辑docker-compose配置</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; docker-compose-consul-cluster.yaml &lt;&lt; \EOF</span><br><span class="line">version: <span class="string">'3.7'</span></span><br><span class="line"> </span><br><span class="line">services:</span><br><span class="line">  consul-server1:</span><br><span class="line">    image: consul:latest</span><br><span class="line">    network_mode: consul-network</span><br><span class="line">    container_name: consul-server1</span><br><span class="line">    restart: always</span><br><span class="line">    <span class="built_in">command</span>: agent -server -client=0.0.0.0 -bootstrap-expect=3 -node=consul-server1  -<span class="built_in">bind</span>=0.0.0.0  -config-dir=/consul/config</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/consul/consul-server1/data:/consul/data</span><br><span class="line">      - /data/consul/consul-server1/config:/consul/config</span><br><span class="line"> </span><br><span class="line">  consul-server2:</span><br><span class="line">    image: consul:latest</span><br><span class="line">    network_mode: consul-network</span><br><span class="line">    container_name: consul-server2</span><br><span class="line">    restart: always</span><br><span class="line">    <span class="built_in">command</span>: agent -server -client=0.0.0.0 -retry-join=consul-server1 -node=consul-server2  -<span class="built_in">bind</span>=0.0.0.0   -config-dir=/consul/config</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/consul/consul-server2/data:/consul/data</span><br><span class="line">      - /data/consul/consul-server2/config:/consul/config</span><br><span class="line">    depends_on:</span><br><span class="line">      - consul-server1</span><br><span class="line"> </span><br><span class="line">  consul-server3:</span><br><span class="line">    image: consul:latest</span><br><span class="line">    network_mode: consul-network</span><br><span class="line">    container_name: consul-server3</span><br><span class="line">    restart: always</span><br><span class="line">    <span class="built_in">command</span>: agent -server -client=0.0.0.0 -retry-join=consul-server1 -node=consul-server3  -<span class="built_in">bind</span>=0.0.0.0   -config-dir=/consul/config</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/consul/consul-server3/data:/consul/data</span><br><span class="line">      - /data/consul/consul-server3/config:/consul/config</span><br><span class="line">    depends_on:</span><br><span class="line">      - consul-server1</span><br><span class="line"> </span><br><span class="line">  consul-client1:</span><br><span class="line">    image: consul:latest</span><br><span class="line">    network_mode: consul-network</span><br><span class="line">    container_name: consul-client1</span><br><span class="line">    restart: always</span><br><span class="line">    ports:</span><br><span class="line">      - 8500:8500</span><br><span class="line">    <span class="built_in">command</span>: agent -client=0.0.0.0 -retry-join=consul-server1 -ui -node=consul-client1  -<span class="built_in">bind</span>=0.0.0.0   -config-dir=/consul/config</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/consul/consul-client1/data:/consul/data</span><br><span class="line">      - /data/consul/consul-client1/config:/consul/config</span><br><span class="line">    depends_on:</span><br><span class="line">      - consul-server2</span><br><span class="line">      - consul-server3</span><br><span class="line"> </span><br><span class="line">  consul-client2:</span><br><span class="line">    image: consul:latest</span><br><span class="line">    network_mode: consul-network</span><br><span class="line">    container_name: consul-client2</span><br><span class="line">    restart: always</span><br><span class="line">    ports:</span><br><span class="line">      - 8501:8500</span><br><span class="line">    <span class="built_in">command</span>: agent -client=0.0.0.0 -retry-join=consul-server1 -ui -node=consul-client2  -<span class="built_in">bind</span>=0.0.0.0  -config-dir=/consul/config</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/consul/consul-client2/data:/consul/data</span><br><span class="line">      - /data/consul/consul-client2/config:/consul/config</span><br><span class="line">    depends_on:</span><br><span class="line">      - consul-server2</span><br><span class="line">      - consul-server3</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 运行起来：</span></span><br><span class="line">docker-compose -f docker-compose-consul-cluster.yaml up -d</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">docker ps -a</span><br><span class="line">CONTAINER ID   IMAGE                                                                COMMAND                  CREATED         STATUS                     PORTS                                                                                         NAMES</span><br><span class="line">223826237a7b   consul:latest                                                        <span class="string">"docker-entrypoint.s…"</span>   3 seconds ago   Up 2 seconds               8300-8302/tcp, 8301-8302/udp, 8600/tcp, 8600/udp, 0.0.0.0:8501-&gt;8500/tcp, :::8501-&gt;8500/tcp   consul-client2</span><br><span class="line">337a3188fb75   consul:latest                                                        <span class="string">"docker-entrypoint.s…"</span>   3 seconds ago   Up 2 seconds               8300-8302/tcp, 8301-8302/udp, 8600/tcp, 8600/udp, 0.0.0.0:8500-&gt;8500/tcp, :::8500-&gt;8500/tcp   consul-client1</span><br><span class="line">685440aefca3   consul:latest                                                        <span class="string">"docker-entrypoint.s…"</span>   4 seconds ago   Up 3 seconds               8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp                                    consul-server2</span><br><span class="line">5f9927f2ecac   consul:latest                                                        <span class="string">"docker-entrypoint.s…"</span>   4 seconds ago   Up 3 seconds               8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp                                    consul-server3</span><br><span class="line">612b93b95ad9   consul:latest                                                        <span class="string">"docker-entrypoint.s…"</span>   5 seconds ago   Up 4 seconds               8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp                                    consul-server1</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 查看集群状态：</span></span><br><span class="line">docker <span class="built_in">exec</span> -it consul-server1 consul  members</span><br><span class="line">Node            Address         Status  Type    Build   Protocol  DC   Partition  Segment</span><br><span class="line">consul-server1  10.10.0.2:8301  alive   server  1.13.1  2         dc1  default    &lt;all&gt;</span><br><span class="line">consul-server2  10.10.0.3:8301  alive   server  1.13.1  2         dc1  default    &lt;all&gt;</span><br><span class="line">consul-server3  10.10.0.4:8301  alive   server  1.13.1  2         dc1  default    &lt;all&gt;</span><br><span class="line">consul-client1  10.10.0.6:8301  alive   client  1.13.1  2         dc1  default    &lt;default&gt;</span><br><span class="line">consul-client2  10.10.0.5:8301  alive   client  1.13.1  2         dc1  default    &lt;default&gt;</span><br></pre></td></tr></tbody></table></figure>

<h3 id="访问consul的web页面"><a href="#访问consul的web页面" class="headerlink" title="访问consul的web页面"></a>访问consul的web页面</h3><p><a href="http://192.168.18.179:8501/ui/dc1/overview/server-status">http://192.168.18.179:8501/ui/dc1/overview/server-status</a><br><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/consul.png"></p>
<p>此时集群已经搭建完毕</p>
<h3 id="将服务器的exporter注册到consul"><a href="#将服务器的exporter注册到consul" class="headerlink" title="将服务器的exporter注册到consul"></a>将服务器的exporter注册到consul</h3><p>在consul ui中可以看到以下结果</p>
<p>以上就是将191.168.18.21主机上的cadvisor服务注册到了consu上面，将这类服务的名称统称为 cadvisor-exporter ,因为服务实例id必须唯一，所以这里需要特殊定义一下 exporter type - 主机名 - ip地址</p>
<p>最后就是将两个epxorter都将21这台主机的两个exporter都注册到了consul.</p>
<h3 id="配置promeheus-使其支持consul"><a href="#配置promeheus-使其支持consul" class="headerlink" title="配置promeheus 使其支持consul"></a>配置promeheus 使其支持consul</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">......</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line"><span class="params">scrape_configs:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">job_name:</span> 'prometheus'</span><br><span class="line">    <span class="params">scrape_interval:</span> <span class="number">15</span>s</span><br><span class="line">    <span class="params">static_configs:</span></span><br><span class="line">         <span class="operator">-</span> <span class="params">targets:</span> ['localhost:<span class="number">9090</span>']</span><br><span class="line"></span><br><span class="line">  <span class="operator">-</span> <span class="params">job_name:</span> <span class="string">"cnosul-prometheus"</span></span><br><span class="line">    <span class="params">consul_sd_configs:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">server:</span> <span class="string">"192.168.18.178:8501"</span></span><br><span class="line">        <span class="params">services:</span> []</span><br><span class="line">    <span class="params">relabel_configs:</span> <span class="comment"># relabe 重写</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">source_labels:</span> [__meta_consul_service] <span class="comment"># service源标签</span></span><br><span class="line">        <span class="params">regex:</span> <span class="string">"consul"</span> <span class="comment"># service匹配</span></span><br><span class="line">        <span class="params">action:</span> drop  <span class="comment"># 执行的动作, 这里不需要consul的metrics，所以drop掉</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">source_labels:</span> [__meta_consul_service_metadata_hostname] <span class="comment"># 将此标签的值重写为instance</span></span><br><span class="line">        <span class="params">target_label:</span> instance</span><br><span class="line">        <span class="params">action:</span> replace</span><br><span class="line">      <span class="operator">-</span> <span class="params">source_labels:</span> [__meta_consul_service_metadata_group]</span><br><span class="line">        <span class="params">target_label:</span> group</span><br><span class="line">        <span class="params">action:</span> replace</span><br><span class="line">      <span class="operator">-</span> <span class="params">source_labels:</span> [__scheme__, __address__, __metrics_path__]</span><br><span class="line">        <span class="params">regex:</span> <span class="string">"(http|https)(.*)"</span>    <span class="comment"># 两个分组</span></span><br><span class="line">        <span class="params">separator:</span> <span class="string">""</span></span><br><span class="line">        <span class="params">target_label:</span> <span class="string">"endpoint"</span></span><br><span class="line">        <span class="params">replacement:</span> <span class="string">"<span class="subst">${<span class="number">1</span>}</span>://<span class="subst">${<span class="number">2</span>}</span>"</span>   <span class="comment"># 引用两个分组</span></span><br><span class="line">        <span class="params">action:</span> replace</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">......</span><br></pre></td></tr></tbody></table></figure>
<p>新增加以上配置以后重新加载配置promtheus服务</p>
<p>curl -I -X POST <a href="http://127.0.0.1:9090/-/reload">http://127.0.0.1:9090/-/reload</a></p>
<p>注意：prometheus开启api热加载，需要加上启动参数： –web.enable-lifecycle</p>
<p>consul注册服务/实例的时候就需要添加这两个元标签，再通过标签重新 为我们需要的label，便于后面prometheus/grafana使用</p>
<p>以上，通过配置 prometheus 为 consul_sd_configs，之后能够正确通过服务发现获取到两个服务得metircs url。</p>
<p>服务器众多的情况下，可以那么这里也可以通过脚本的方式去添加，因为consul有以下元标签，那么更容易使得我们的数据可控，例如上面我们在注册服务的时候加了一个 group ，hostname, consul传递到promtheus之后我们可以通过relable的方式去重写, 以上代表的就是这个主机属于哪个分组，后续可以作为grafan面板中，或者报警中的一个关键指标。</p>
<h3 id="如何批量注册cadvisor-node-exporter-到-consul"><a href="#如何批量注册cadvisor-node-exporter-到-consul" class="headerlink" title="如何批量注册cadvisor/node exporter 到 consul"></a>如何批量注册cadvisor/node exporter 到 consul</h3><p>consul 自身提供了API， 就像上面一样做成脚本以后，批量注册服务/实例即可：</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">cat &gt; register_exporter_to_consul.py &lt;&lt; \EOF</span><br><span class="line"> </span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># date: 2022-09-06</span></span><br><span class="line"><span class="comment"># author: guomaoqiu</span></span><br><span class="line"><span class="comment"># desc: 批量注册cadvisor-exporter, node-exporter服务到consul</span></span><br><span class="line"> </span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">import  json</span><br><span class="line"> </span><br><span class="line">consul_url = <span class="string">"http://192.168.18.179:8501/v1/agent/service/register"</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 注册node-exporter</span></span><br><span class="line">def register_node_exporter(ip,hostname,group):</span><br><span class="line">    nodedata = {</span><br><span class="line">            <span class="string">"id"</span>: <span class="string">"node-exporter-{0}-{1}"</span>.format(hostname,ip),</span><br><span class="line">            # <span class="string">"id"</span>: hostname,</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"node-exporter"</span>,</span><br><span class="line">            <span class="string">"address"</span>: ip,</span><br><span class="line">            <span class="string">"port"</span>: 9100,</span><br><span class="line">            <span class="string">"tags"</span>: [],</span><br><span class="line">            <span class="string">"meta"</span>: {</span><br><span class="line">                <span class="string">"hostname"</span>: hostname,</span><br><span class="line">                <span class="string">"group"</span>:<span class="built_in"> group</span></span><br><span class="line"><span class="built_in"></span>            },</span><br><span class="line">            <span class="string">"checks"</span>: [</span><br><span class="line">                {</span><br><span class="line">                <span class="string">"http"</span>: <span class="string">"http://{0}:9100/metrics"</span>.format(ip),</span><br><span class="line">                <span class="string">"interval"</span>: <span class="string">"5s"</span></span><br><span class="line">                }</span><br><span class="line">        ]</span><br><span class="line">    }</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">print</span>(nodedata)</span><br><span class="line">    try:</span><br><span class="line">        r = requests.put(<span class="attribute">url</span>=consul_url, <span class="attribute">data</span>=json.dumps(nodedata))</span><br><span class="line">        <span class="keyword">if</span> r.status_code == 200:</span><br><span class="line">            <span class="built_in">print</span>(ip, <span class="string">"Node Exporter 注册成功"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(ip, <span class="string">"Node Exporter 注册失败"</span>)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        <span class="built_in">print</span>(e)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 注册cadvisor-exporter</span></span><br><span class="line">def register_container_exporter(ip,hostname,group):</span><br><span class="line">    container_data = {</span><br><span class="line">            <span class="string">"id"</span>: <span class="string">"cadvisor-exporter-{0}-{1}"</span>.format(hostname,ip),</span><br><span class="line">            <span class="string">"name"</span>: <span class="string">"cadvisor-exporter"</span>,</span><br><span class="line">            <span class="string">"address"</span>: ip,</span><br><span class="line">            <span class="string">"port"</span>: 9105,</span><br><span class="line">            <span class="string">"tags"</span>: [],</span><br><span class="line">            <span class="string">"meta"</span>: {</span><br><span class="line">                <span class="string">"hostname"</span>: hostname,</span><br><span class="line">            <span class="string">"group"</span>:<span class="built_in"> group</span></span><br><span class="line"><span class="built_in"></span>            },</span><br><span class="line">            <span class="string">"checks"</span>: [</span><br><span class="line">                {</span><br><span class="line">                <span class="string">"http"</span>: <span class="string">"http://{0}:9105/metrics"</span>.format(ip),</span><br><span class="line">                <span class="string">"interval"</span>: <span class="string">"5s"</span></span><br><span class="line">                }</span><br><span class="line">        ]</span><br><span class="line">    }</span><br><span class="line">    try:</span><br><span class="line">        r = requests.put(<span class="attribute">url</span>=consul_url, <span class="attribute">data</span>=json.dumps(container_data))</span><br><span class="line">        <span class="built_in">print</span>(r.status_code,r.content)</span><br><span class="line">        <span class="keyword">if</span> r.status_code == 200:</span><br><span class="line">            <span class="built_in">print</span>(ip, <span class="string">"Container Exporter 注册成功"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(ip, <span class="string">"Container Exporter 注册失败"</span>)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        <span class="built_in">print</span>(e)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">def register():</span><br><span class="line">    with open(<span class="string">"server_list.txt"</span>,<span class="string">"r"</span>) as f:</span><br><span class="line">        lines = (f.readlines())</span><br><span class="line">         </span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> lines:</span><br><span class="line">           <span class="built_in"> ip </span>=  (str(data).split(<span class="string">"\t"</span>)[1].strip(<span class="string">"\n"</span>))</span><br><span class="line">           <span class="built_in"> group </span>=  (str(data).split(<span class="string">"\t"</span>)[2].strip(<span class="string">"\n"</span>))</span><br><span class="line">            hostname =  (str(data).split(<span class="string">"\t"</span>)[0])</span><br><span class="line">            register_node_exporter(<span class="attribute">ip</span>=ip,hostname=hostname,group=group)</span><br><span class="line">            register_container_exporter(<span class="attribute">ip</span>=ip,hostname=hostname,group=group)</span><br><span class="line"> </span><br><span class="line">register()</span><br><span class="line"> </span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line"><span class="comment">### 以上需要提前准备好一个文本文件，每行一条数据里面格式如下：</span></span><br><span class="line"> </span><br><span class="line">cat server_list.txt</span><br><span class="line">hostname1 192.168.18.21   IDE</span><br><span class="line">hostname2 192.168.18.22   IDE</span><br><span class="line">hostname3 192.168.18.31   IDE</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>.</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>.</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>.</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 这里是线下环境的一个配置，所以单纯只是将环境作为分组，这里可以自定义，但是不能少了或多了字段数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行脚本：</span></span><br><span class="line">python3 register_exporter_to_consul.py </span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p8.png"></p>
<p>执行以上脚本以后就可以在consul以及promtheus web页面进行查看检验了：</p>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p9.png"></p>
<p>从上面可以看到已经注册了54个，但是这其实只添加了27台服务器而已，因为每台服务器上面运行了两个exporter,一个是node-exporter,一个是cadvisor-exporter， 他们有各自的metrics URL ,最后再到prometheus检查是否添加即可。</p>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p10.png"></p>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p11.png"><br>以上可以看到 ，服务数量、exporter数据获取正常。</p>
<p>至此，consul的部署，服务/实例的注册，promtheus的consul_sd_configs服务自动发现 ，数据采集已经完成，后续就是将prometheus 接入到 grafana：</p>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p12.png"></p>
<p>上图中可以看到通过自定义的label进行分组</p>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p13.png"></p>
<h2 id="告警系统接入"><a href="#告警系统接入" class="headerlink" title="告警系统接入"></a>告警系统接入</h2><p>已经可以看到容器&amp;宿主机的数据已经正常采集并且已经能够在grafan面板上进行展示</p>
<p>报警系统使用的是Alertmanager 搭配 Webhook来实现：</p>
<h3 id="alertmanager配置"><a href="#alertmanager配置" class="headerlink" title="alertmanager配置"></a>alertmanager配置</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span> alertmanager<span class="symbol">/config.yml</span> <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">global:</span> <span class="comment"># 全局配置</span></span><br><span class="line">  <span class="params">resolve_timeout:</span> <span class="number">10</span>m <span class="comment"># 超时时间 默认10m</span></span><br><span class="line"><span class="params">inhibit_rules:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">source_match:</span>      <span class="comment">## 源报警规则</span></span><br><span class="line">      <span class="params">alertname:</span> 'critical'</span><br><span class="line">    <span class="params">target_match:</span></span><br><span class="line">      <span class="params">alertname:</span> 'warning'</span><br><span class="line">    <span class="params">equal:</span> ['alertname']  <span class="comment"># 通过alertname去抑制</span></span><br><span class="line"><span class="params">route:</span></span><br><span class="line">  <span class="params">receiver:</span> default-receiver</span><br><span class="line">  <span class="params">group_wait:</span> <span class="number">30</span>s</span><br><span class="line">  <span class="params">group_interval:</span> <span class="number">5</span>m</span><br><span class="line">  <span class="params">repeat_interval:</span> <span class="number">3</span>h</span><br><span class="line">  <span class="params">group_by:</span> ['alertname']</span><br><span class="line">  <span class="params">routes:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">receiver:</span> webhook-ide</span><br><span class="line">    <span class="params">matchers:</span></span><br><span class="line">    <span class="operator">-</span> group <span class="operator">=</span> IDE  <span class="comment"># 通过prometehus alert rule 查询出来的结果必须包含 group字段，否则该条报警不能发送出来</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">receiver:</span> webhook-stage</span><br><span class="line">    <span class="params">matchers:</span></span><br><span class="line">    <span class="operator">-</span> group <span class="operator">=</span> Stage</span><br><span class="line">  <span class="operator">-</span> <span class="params">receiver:</span> webhook-Pet</span><br><span class="line">    <span class="params">matchers:</span></span><br><span class="line">    <span class="operator">-</span> group <span class="operator">=</span> Pet</span><br><span class="line"><span class="params">receivers:</span></span><br><span class="line"><span class="operator">-</span> <span class="params">name:</span> default-receiver <span class="comment"># 这里不需要设置，需要精确匹配到每一条告规则 ，但是这里必须要存在，上面强调了必须要有一个默认的receiver</span></span><br><span class="line"><span class="operator">-</span> <span class="params">name:</span> webhook-ide</span><br><span class="line">  <span class="params">webhook_configs:</span> <span class="comment"># webhook告警配置</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">url:</span> 'http:<span class="operator">//</span><span class="number">192.168</span>.<span class="number">18.178</span>:<span class="number">9060</span><span class="operator">/</span>dingtalk<span class="operator">/</span>webhook-ide<span class="operator">/</span>send'</span><br><span class="line">    <span class="params">send_resolved:</span> <span class="literal">true</span></span><br><span class="line"><span class="operator">-</span> <span class="params">name:</span> webhook-stage</span><br><span class="line">  <span class="params">webhook_configs:</span> <span class="comment"># webhook告警配置</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">url:</span> 'http:<span class="operator">//</span><span class="number">192.168</span>.<span class="number">18.178</span>:<span class="number">9060</span><span class="operator">/</span>dingtalk<span class="operator">/</span>webhook-stage<span class="operator">/</span>send'</span><br><span class="line">    <span class="params">send_resolved:</span> <span class="literal">true</span></span><br><span class="line"><span class="operator">-</span> <span class="params">name:</span> webhook-Pet</span><br><span class="line">  <span class="params">webhook_configs:</span> <span class="comment"># webhook告警配置</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">url:</span> 'http:<span class="operator">//</span><span class="number">192.168</span>.<span class="number">18.178</span>:<span class="number">9060</span><span class="operator">/</span>dingtalk<span class="operator">/</span>webhook-Pet<span class="operator">/</span>send'</span><br><span class="line">    <span class="params">send_resolved:</span> <span class="literal">true</span> </span><br><span class="line"></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启服务加载</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h3 id="dingding-webhook配置"><a href="#dingding-webhook配置" class="headerlink" title="dingding-webhook配置"></a>dingding-webhook配置</h3><p>这里按照环境进行了分组，将不同的告警信息通过环境发送到不同的钉钉群</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span> prometheus-dingding-webhook<span class="symbol">/config.yml</span> <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="comment">## Request timeout</span></span><br><span class="line"><span class="comment"># timeout: 5s</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## Uncomment following line in order to write template from scratch (be careful!)</span></span><br><span class="line"><span class="comment">#no_builtin_template: false</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## Customizable templates path</span></span><br><span class="line"><span class="comment">#templates:</span></span><br><span class="line"><span class="comment">#     - /config/dingding.tmpl</span></span><br><span class="line"><span class="params">templates:</span></span><br><span class="line">  <span class="operator">-</span> <span class="symbol">/config/template.tmpl</span></span><br><span class="line"><span class="comment">## You can also override default template using `default_message`</span></span><br><span class="line"><span class="comment">## The following example to use the 'legacy' template from v0.3.0</span></span><br><span class="line"><span class="comment">#default_message:      </span></span><br><span class="line"><span class="comment">#  title: '{{ template "legacy.title" . }}'</span></span><br><span class="line"><span class="comment">#  text: '{{ template "legacy.content" . }}'</span></span><br><span class="line"><span class="comment">## Targets, previously was known as "profiles"</span></span><br><span class="line"><span class="comment">#    text: '{{ template "_dingtalk.link.content" . }}'</span></span><br><span class="line"><span class="params">targets:</span></span><br><span class="line"></span><br><span class="line">  <span class="params">webhook1:</span></span><br><span class="line">    <span class="params">url:</span> https:<span class="operator">//</span>oapi.dingtalk.com<span class="operator">/</span>robot<span class="operator">/</span>send<span class="operator">?</span>access_token<span class="operator">=</span><span class="number">92268</span>cd2c48db0ec2a10a753213dda6a11e4d54ea8fdbce356f217fa44925a7f</span><br><span class="line">    <span class="params">secret:</span> 填写你的钉钉secret</span><br><span class="line">    </span><br><span class="line">  <span class="params">webhook-ide:</span></span><br><span class="line">    <span class="params">url:</span> https:<span class="operator">//</span>oapi.dingtalk.com<span class="operator">/</span>robot<span class="operator">/</span>send<span class="operator">?</span>access_token<span class="operator">=</span><span class="number">92268</span>cd2c48db0ec2a10a753213dda6a11e4d54ea8fdbce356f217fa44925a7f</span><br><span class="line">    <span class="params">secret:</span> 填写你的钉钉secret</span><br><span class="line"> </span><br><span class="line">  <span class="params">webhook-stage:</span></span><br><span class="line">    <span class="params">url:</span> https:<span class="operator">//</span>oapi.dingtalk.com<span class="operator">/</span>robot<span class="operator">/</span>send<span class="operator">?</span>access_token<span class="operator">=</span><span class="number">9</span>d7172785d72f50ab335367bd7db8cb013073f62bd0fd7bfc9605020a6a4b95c</span><br><span class="line">    <span class="params">secret:</span> 填写你的钉钉secret</span><br><span class="line"> </span><br><span class="line">  <span class="params">webhook-Pet:</span></span><br><span class="line">    <span class="params">url:</span> https:<span class="operator">//</span>oapi.dingtalk.com<span class="operator">/</span>robot<span class="operator">/</span>send<span class="operator">?</span>access_token<span class="operator">=</span><span class="number">89</span>b881f5798ec7cdff1538ecf3a7001dd30d19c7e5281e5e8329cb1e243b813e</span><br><span class="line">    <span class="params">secret:</span> 你的钉钉secret</span><br><span class="line">    </span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p>这里还自定义了报警模板，上述文件中需要引入：</p>
<figure class="highlight handlebars"><table><tbody><tr><td class="code"><pre><span class="line"><span class="language-xml">cat &gt; dingding.tmpl &lt;&lt; EOF</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">define</span> <span class="string">"__subject"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">[</span><span class="template-variable">{{ <span class="name">.Status</span> | toUpper }}</span><span class="template-variable">{{ <span class="name"><span class="built_in">if</span></span> eq .Status <span class="string">"firing"</span> }}</span><span class="language-xml">:</span><span class="template-variable">{{ <span class="name">.Alerts.Firing</span> | len }}</span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml">]</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">define</span> <span class="string">"__alert_list"</span> }}</span><span class="template-variable">{{ <span class="name">range</span> . }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">---</span></span><br><span class="line"><span class="language-xml">**告警名称**: </span><span class="template-variable">{{ <span class="name">index</span> .Annotations <span class="string">"title"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警级别**: </span><span class="template-variable">{{ <span class="name">.Labels.severity</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警主机组**: </span><span class="template-variable">{{ <span class="name">.Labels.group</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警主机**: </span><span class="template-variable">{{ <span class="name">.Labels.instance</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**图表数据**: [Click](</span><span class="template-variable">{{ <span class="name">.GeneratorURL</span> }}</span><span class="language-xml">)</span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警信息**: </span><span class="template-variable">{{ <span class="name">index</span> .Annotations <span class="string">"description"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警时间**: </span><span class="template-variable">{{ <span class="name">dateInZone</span> <span class="string">"2006.01.02 15:04:05"</span> (<span class="name">.StartsAt</span>) <span class="string">"Asia/Shanghai"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**预案链接**: [CONF文档](</span><span class="template-variable">{{ <span class="name">index</span> .Annotations <span class="string">"plan_url"</span> }}</span><span class="language-xml">)</span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">define</span> <span class="string">"__resolved_list"</span> }}</span><span class="template-variable">{{ <span class="name">range</span> . }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">---</span></span><br><span class="line"><span class="language-xml">**告警名称**: </span><span class="template-variable">{{ <span class="name">index</span> .Annotations <span class="string">"title"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警级别**: </span><span class="template-variable">{{ <span class="name">.Labels.severity</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警主机**: </span><span class="template-variable">{{ <span class="name">.Labels.instance</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**图表数据**: [Click](</span><span class="template-variable">{{ <span class="name">.GeneratorURL</span> }}</span><span class="language-xml">)</span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警信息**: </span><span class="template-variable">{{ <span class="name">index</span> .Annotations <span class="string">"description"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**告警时间**: </span><span class="template-variable">{{ <span class="name">dateInZone</span> <span class="string">"2006.01.02 15:04:05"</span> (<span class="name">.StartsAt</span>) <span class="string">"Asia/Shanghai"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**恢复时间**: </span><span class="template-variable">{{ <span class="name">dateInZone</span> <span class="string">"2006.01.02 15:04:05"</span> (<span class="name">.EndsAt</span>) <span class="string">"Asia/Shanghai"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">define</span> <span class="string">"default.title"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name"><span class="built_in">template</span></span> <span class="string">"__subject"</span> . }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">define</span> <span class="string">"default.content"</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name"><span class="built_in">if</span></span> gt (<span class="name">len</span> .Alerts.Firing) <span class="number">0</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**💔💔💔侦测到</span><span class="template-variable">{{ <span class="name">.Alerts.Firing</span> | len  }}</span><span class="language-xml">个告警💔💔💔**</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name"><span class="built_in">template</span></span> <span class="string">"__alert_list"</span> .Alerts.Firing }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">---</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name"><span class="built_in">if</span></span> gt (<span class="name">len</span> .Alerts.Resolved) <span class="number">0</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">**💚💚💚恢复</span><span class="template-variable">{{ <span class="name">.Alerts.Resolved</span> | len  }}</span><span class="language-xml">个告警💚💚💚**</span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name"><span class="built_in">template</span></span> <span class="string">"__resolved_list"</span> .Alerts.Resolved }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">define</span> <span class="string">"ding.link.title"</span> }}</span><span class="template-variable">{{ <span class="name"><span class="built_in">template</span></span> <span class="string">"default.title"</span> . }}</span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name">define</span> <span class="string">"ding.link.content"</span> }}</span><span class="template-variable">{{ <span class="name"><span class="built_in">template</span></span> <span class="string">"default.content"</span> . }}</span><span class="template-variable">{{ <span class="name">end</span> }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name"><span class="built_in">template</span></span> <span class="string">"default.title"</span> . }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">{{ <span class="name"><span class="built_in">template</span></span> <span class="string">"default.content"</span> . }}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">EOF</span></span><br></pre></td></tr></tbody></table></figure>
<p>这里需要注意的是，这个模板中我们加了自定义的一些自己想要的东西：</p>
<ul>
<li><p>需要在告警的时候加上针对每个告警的日常处理方式的一个预案，那么这个就给出一个文档的链接就行了，日常运维人员看到告警之后，一般都会进行一个故障处理过程或记录吧，这样能更加友好。那如何定义？<br>首先在告警规则中去定义字段,比如这里我添加了一个 <code>plan_url</code>:<br><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p15.png"><br>然后再模板中引用即可.</p>
</li>
<li><p>Prometheus中需要开启 外部url 访问，即在启动的时候加入参数<code>- '--web.external-url=http://192.168.18.178:9090'</code>, 否则告警模板中的 <code>.GeneratorURL </code>这个值应用的是prometheus的主机名，即运行prometheus的那个容器的主机名，那样访问是访问不到的，所以需要修改</p>
</li>
</ul>
<p>另外在部署prometheus 我们还部署了一个容器上是github上有人开源了一个接入alertmanager api 的方式获取报警内容并进行展示。比起alertmanager这个组件自身的那个ui更加好用。出处：(<a href="https://github.com/prymitive/karma%EF%BC%89">https://github.com/prymitive/karma）</a></p>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p17.png"></p>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/p18.png"></p>
<p>至此整个监控系统就搭建完毕，还是有比较细节的地方需要处理比如告警规则、告警频率这些需要去优化。</p>
<hr>
<p><img src="/2022/09/12/Prometheus-Consul%E5%AE%9E%E7%8E%B0%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/111.jpeg"></p>
]]></content>
      <categories>
        <category>monitor</category>
      </categories>
      <tags>
        <tag>Prometheus</tag>
        <tag>Consul</tag>
        <tag>Alertmanager</tag>
      </tags>
  </entry>
  <entry>
    <title>&quot;Devops&quot; Demo(如果文中图片显示不完整，请多次刷新)</title>
    <url>/2016/12/16/devops-demo/</url>
    <content><![CDATA[<h4 id="今年断断续续在工作之余学习了一下-Python-Web框架-下面简要说下学的东西主要有以下几个："><a href="#今年断断续续在工作之余学习了一下-Python-Web框架-下面简要说下学的东西主要有以下几个：" class="headerlink" title="今年断断续续在工作之余学习了一下 Python Web框架,下面简要说下学的东西主要有以下几个："></a>今年断断续续在工作之余学习了一下 Python Web框架,下面简要说下学的东西主要有以下几个：</h4><ul>
<li><h4 id="Flask-包括各个子系统组件-Bootstrap"><a href="#Flask-包括各个子系统组件-Bootstrap" class="headerlink" title="Flask(包括各个子系统组件) + Bootstrap"></a>Flask(包括各个子系统组件) + Bootstrap</h4><h5 id="主要是第一个接触的就是Flask这个框架-期初给我的感觉就是开发起来非常的快捷、方便-各个子系统也有单独的学习资料-所以从一开始我也就一直断断续续的在折腾；"><a href="#主要是第一个接触的就是Flask这个框架-期初给我的感觉就是开发起来非常的快捷、方便-各个子系统也有单独的学习资料-所以从一开始我也就一直断断续续的在折腾；" class="headerlink" title="主要是第一个接触的就是Flask这个框架,期初给我的感觉就是开发起来非常的快捷、方便,各个子系统也有单独的学习资料,所以从一开始我也就一直断断续续的在折腾；"></a>主要是第一个接触的就是Flask这个框架,期初给我的感觉就是开发起来非常的快捷、方便,各个子系统也有单独的学习资料,所以从一开始我也就一直断断续续的在折腾；</h5><h5 id="Bootstrap-DiaoBao了"><a href="#Bootstrap-DiaoBao了" class="headerlink" title="Bootstrap DiaoBao了."></a>Bootstrap DiaoBao了.</h5></li>
<li><h4 id="SaltStack-API-netapi-rest-cherrypy"><a href="#SaltStack-API-netapi-rest-cherrypy" class="headerlink" title="SaltStack API(netapi/rest_cherrypy)"></a>SaltStack API(netapi/rest_cherrypy)</h4><h5 id="这个就不用说了吧，运维人都知道；"><a href="#这个就不用说了吧，运维人都知道；" class="headerlink" title="这个就不用说了吧，运维人都知道；"></a>这个就不用说了吧，运维人都知道；</h5><h5 id="在掌握了SaltStack在命令行的用法之后就需要去学习、探索更多更便利的方法来实现我们想要的效果；"><a href="#在掌握了SaltStack在命令行的用法之后就需要去学习、探索更多更便利的方法来实现我们想要的效果；" class="headerlink" title="在掌握了SaltStack在命令行的用法之后就需要去学习、探索更多更便利的方法来实现我们想要的效果；"></a>在掌握了SaltStack在命令行的用法之后就需要去学习、探索更多更便利的方法来实现我们想要的效果；</h5></li>
<li><h4 id="Zabbix-API"><a href="#Zabbix-API" class="headerlink" title="Zabbix API"></a>Zabbix API</h4><h5 id="监控利器-zabbix-更不用说了。"><a href="#监控利器-zabbix-更不用说了。" class="headerlink" title="监控利器 zabbix, 更不用说了。"></a>监控利器 zabbix, 更不用说了。</h5><h5 id="由于工作上的所需-增减服务器是常有的事-那怎么快速的去添加-删除主机呢-当然就要让接口帮忙去处理啦；"><a href="#由于工作上的所需-增减服务器是常有的事-那怎么快速的去添加-删除主机呢-当然就要让接口帮忙去处理啦；" class="headerlink" title="由于工作上的所需,增减服务器是常有的事,那怎么快速的去添加/删除主机呢,当然就要让接口帮忙去处理啦；"></a>由于工作上的所需,增减服务器是常有的事,那怎么快速的去添加/删除主机呢,当然就要让接口帮忙去处理啦；</h5></li>
</ul>
<h4 id="下面是demo的雏形："><a href="#下面是demo的雏形：" class="headerlink" title="下面是demo的雏形："></a>下面是demo的雏形：</h4><h5 id="登录系统使用的是Flask自带的Flask-Login组件，验证码用的是Google的ReCAPTCHA-当然这个在Flask表单中已"><a href="#登录系统使用的是Flask自带的Flask-Login组件，验证码用的是Google的ReCAPTCHA-当然这个在Flask表单中已" class="headerlink" title="登录系统使用的是Flask自带的Flask-Login组件，验证码用的是Google的ReCAPTCHA,当然这个在Flask表单中已"></a>登录系统使用的是Flask自带的Flask-Login组件，验证码用的是Google的ReCAPTCHA,当然这个在Flask表单中已</h5><p>经集成了； <img src="/2016/12/16/devops-demo/14818093599191.jpg">￼</p>
<h5 id="SaltStack-命令执行-调用的是SaltApi-这里需要手动填写主机名-是个需要优化的地方-￼"><a href="#SaltStack-命令执行-调用的是SaltApi-这里需要手动填写主机名-是个需要优化的地方-￼" class="headerlink" title="SaltStack 命令执行,调用的是SaltApi,这里需要手动填写主机名,是个需要优化的地方.￼"></a>SaltStack 命令执行,调用的是SaltApi,这里需要手动填写主机名,是个需要优化的地方.<img src="/2016/12/16/devops-demo/14818078774983.jpg">￼</h5><p><img src="/2016/12/16/devops-demo/14818079617121.jpg">￼</p>
<h5 id="当然有了这个执行命令-哥子就要做坏事了-这怎么行-那就把一些危险的命令添加到block-list当中-避免误操作"><a href="#当然有了这个执行命令-哥子就要做坏事了-这怎么行-那就把一些危险的命令添加到block-list当中-避免误操作" class="headerlink" title="当然有了这个执行命令,哥子就要做坏事了,这怎么行!那就把一些危险的命令添加到block_list当中,避免误操作."></a>当然有了这个执行命令,哥子就要做坏事了,这怎么行!那就把一些危险的命令添加到block_list当中,避免误操作.</h5><p><img src="/2016/12/16/devops-demo/14818560611617.jpg">￼</p>
<h5 id="⊙o⊙-…-这个做的不是太美观；数据是从数据库查询出来的-而这些服务器的”静态”数据都是通过SaltStack的grains获取到的-看到旁边那个Collet按钮了么-那个的功能就是一键获取各个minion的grains然后导入到数据库中，每当有新的minion加入时只要在此点击Collet-Salt就会马不停蹄的到新minion端拉取相关信息-然后写入数据库-删除按钮的作用是从数据库当中删除此条记录-￼"><a href="#⊙o⊙-…-这个做的不是太美观；数据是从数据库查询出来的-而这些服务器的”静态”数据都是通过SaltStack的grains获取到的-看到旁边那个Collet按钮了么-那个的功能就是一键获取各个minion的grains然后导入到数据库中，每当有新的minion加入时只要在此点击Collet-Salt就会马不停蹄的到新minion端拉取相关信息-然后写入数据库-删除按钮的作用是从数据库当中删除此条记录-￼" class="headerlink" title="(⊙o⊙)… 这个做的不是太美观；数据是从数据库查询出来的;而这些服务器的”静态”数据都是通过SaltStack的grains获取到的,看到旁边那个Collet按钮了么;那个的功能就是一键获取各个minion的grains然后导入到数据库中，每当有新的minion加入时只要在此点击Collet Salt就会马不停蹄的到新minion端拉取相关信息,然后写入数据库, 删除按钮的作用是从数据库当中删除此条记录.￼"></a>(⊙o⊙)… 这个做的不是太美观；数据是从数据库查询出来的;而这些服务器的”静态”数据都是通过SaltStack的grains获取到的,看到旁边那个<code>Collet</code>按钮了么;那个的功能就是一键获取各个minion的grains然后导入到数据库中，每当有新的minion加入时只要在此点击<code>Collet</code> Salt就会马不停蹄的到新minion端拉取相关信息,然后写入数据库, <code>删除</code>按钮的作用是从数据库当中删除此条记录.<img src="/2016/12/16/devops-demo/14818081121387.jpg">￼</h5><h5 id="点击上图的详细就可以看到关于这台主机的详细状态了-Status这个是实时抓取的当前这台主机的当前状态-而下面的两张监控图形则是调用了Grafana"><a href="#点击上图的详细就可以看到关于这台主机的详细状态了-Status这个是实时抓取的当前这台主机的当前状态-而下面的两张监控图形则是调用了Grafana" class="headerlink" title="点击上图的详细就可以看到关于这台主机的详细状态了,Status这个是实时抓取的当前这台主机的当前状态,而下面的两张监控图形则是调用了Grafana."></a>点击上图的<code>详细</code>就可以看到关于这台主机的详细状态了,Status这个是实时抓取的当前这台主机的当前状态,而下面的两张监控图形则是调用了Grafana.</h5><h5 id="本想自己画图的-但是没太多的时间去研究前端方面的东西-况且图形化的东西也蛮多了-就比如zabbix里面的监控图不美观的话加上grafana渲染；再次就是ELK里面的图形-本想去调用ELK-API的-发现结合到这里也没啥卵用-要看图直接访问Kibana就行了；"><a href="#本想自己画图的-但是没太多的时间去研究前端方面的东西-况且图形化的东西也蛮多了-就比如zabbix里面的监控图不美观的话加上grafana渲染；再次就是ELK里面的图形-本想去调用ELK-API的-发现结合到这里也没啥卵用-要看图直接访问Kibana就行了；" class="headerlink" title="本想自己画图的,但是没太多的时间去研究前端方面的东西,况且图形化的东西也蛮多了,就比如zabbix里面的监控图不美观的话加上grafana渲染；再次就是ELK里面的图形,本想去调用ELK API的,发现结合到这里也没啥卵用,要看图直接访问Kibana就行了；"></a>本想自己画图的,但是没太多的时间去研究前端方面的东西,况且图形化的东西也蛮多了,就比如zabbix里面的监控图不美观的话加上grafana渲染；再次就是ELK里面的图形,本想去调用ELK API的,发现结合到这里也没啥卵用,要看图直接访问Kibana就行了；</h5><p><img src="/2016/12/16/devops-demo/14818082338278.jpg">￼</p>
<h5 id="下面两张是关于ZabbixAPI的操作-添加服务器是通过上传一个固定格式的-xls格式文件-然后台去解析文件内容-调用API进行主机的添加-删除主机也同样调用API操作实现-￼"><a href="#下面两张是关于ZabbixAPI的操作-添加服务器是通过上传一个固定格式的-xls格式文件-然后台去解析文件内容-调用API进行主机的添加-删除主机也同样调用API操作实现-￼" class="headerlink" title="下面两张是关于ZabbixAPI的操作,添加服务器是通过上传一个固定格式的.xls格式文件,然后台去解析文件内容,调用API进行主机的添加,删除主机也同样调用API操作实现.￼"></a>下面两张是关于ZabbixAPI的操作,添加服务器是通过上传一个固定格式的.xls格式文件,然后台去解析文件内容,调用API进行主机的添加,删除主机也同样调用API操作实现.<img src="/2016/12/16/devops-demo/14818090897130.jpg">￼</h5><p><img src="/2016/12/16/devops-demo/14818092530495.jpg">￼</p>
<h5 id="差不多实现的就是上面这些-其他功能还在继续学习、研究中…"><a href="#差不多实现的就是上面这些-其他功能还在继续学习、研究中…" class="headerlink" title="差不多实现的就是上面这些,其他功能还在继续学习、研究中…"></a>差不多实现的就是上面这些,其他功能还在继续学习、研究中…</h5><h5 id="望各路大神勿喷-希望能得到各位大神的建议-谢谢。"><a href="#望各路大神勿喷-希望能得到各位大神的建议-谢谢。" class="headerlink" title="望各路大神勿喷,希望能得到各位大神的建议,谢谢。"></a>望各路大神勿喷,希望能得到各位大神的建议,谢谢。</h5><p><img src="/2016/12/16/devops-demo/14818127314229.jpg">￼</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>自动化运维</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Harbor搭建企业级私有镜像仓库</title>
    <url>/2019/01/07/docker-harbor/</url>
    <content><![CDATA[<h1 id="基于harbor搭建企业docker镜像仓库"><a href="#基于harbor搭建企业docker镜像仓库" class="headerlink" title="基于harbor搭建企业docker镜像仓库"></a>基于harbor搭建企业docker镜像仓库</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>docker中要使用镜像，一般会从本地、docker Hup公共仓库和其它第三方公共仓库中下载镜像，一般出于安全和外网(墙)资源下载速率的原因考虑企业级上不会轻易使用。那么有没有一种办法可以存储自己的镜像又有安全认证的仓库呢? </p>
<pre><code>     —-&gt; 企业级环境中基于Harbor搭建自己的安全认证仓库。
</code></pre>
<p>Harbor是VMware公司最近开源的企业级Docker Registry项目, 其目标是帮助用户迅速搭建一个企业级的Docker registry服务。</p>
<h2 id="安装Harbor"><a href="#安装Harbor" class="headerlink" title="安装Harbor"></a>安装Harbor</h2><p>harbor需要安装docker和docker-compose才能使用，安装docker步骤省略，</p>
<h3 id="安装docker-dompose"><a href="#安装docker-dompose" class="headerlink" title="安装docker-dompose"></a>安装docker-dompose</h3><p>docker-dompose安装步骤如下： </p>
<p>下载最新版的docker-compose文件 </p>
<figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line">$ curl -L https:<span class="regexp">//gi</span>thub.com<span class="regexp">/docker/</span>compose<span class="regexp">/releases/</span>download<span class="regexp">/1.23.2/</span>docker-compose-$(uname -s)-$(uname -m) -o <span class="regexp">/usr/</span>local<span class="regexp">/bin/</span>docker-compose</span><br></pre></td></tr></tbody></table></figure>

<p>添加可执行权限</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> +x /usr/local/bin/docker-compose</span></span><br></pre></td></tr></tbody></table></figure>

<p>验证版本</p>
<figure class="highlight applescript"><table><tbody><tr><td class="code"><pre><span class="line">$ docker-compose -v</span><br><span class="line">docker-compose <span class="built_in">version</span> <span class="number">1.23</span><span class="number">.2</span>, build <span class="number">1110</span>ad01</span><br></pre></td></tr></tbody></table></figure>
<h3 id="获取Harbor软件包"><a href="#获取Harbor软件包" class="headerlink" title="获取Harbor软件包"></a>获取Harbor软件包</h3><figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">https</span>://storage.googleapis.com/harbor-releases/release-<span class="number">1</span>.<span class="number">7</span>.<span class="number">0</span>/harbor-offline-installer-v1.<span class="number">7</span>.<span class="number">1</span>.tgz</span><br></pre></td></tr></tbody></table></figure>

<p>解压</p>
<figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">tar</span> -xf harbor-offline-installer-v1.<span class="number">7</span>.<span class="number">1</span>.tgz -C /usr/local/</span><br></pre></td></tr></tbody></table></figure>

<p>编辑配置文件</p>
<figure class="highlight makefile"><table><tbody><tr><td class="code"><pre><span class="line">$ cd /usr/local/harbor</span><br><span class="line">$ vim harbor.cfg</span><br><span class="line">hostname = reg.for-k8s.com 		  <span class="comment"># 本机外网IP或域名，该地址供用户通过UI进行访问，不要使用127.0.0.1</span></span><br><span class="line">ui_url_protocol = https             <span class="comment"># 用户访问私仓时使用的协议，默认时http，配置成https</span></span><br><span class="line">db_password = root123           　　 <span class="comment"># 指定mysql数据库管理员密码</span></span><br><span class="line">harbor_admin_password：Harbor12345   <span class="comment"># harbor的管理员账户密码</span></span><br><span class="line">ssl_cert = /data/cert/reg.for-k8s.com.crt 　　<span class="comment"># 设置证书文件路径</span></span><br><span class="line">ssl_cert_key = /data/cert/reg.for-k8s.com.key  <span class="comment"># 设置证书密钥文件路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他配置选项按需填写即可</span></span><br></pre></td></tr></tbody></table></figure>



<h3 id="生成ssl证书"><a href="#生成ssl证书" class="headerlink" title="生成ssl证书"></a>生成ssl证书</h3><p>生成根证书</p>
<figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line">$ <span class="keyword">cd</span> /dada/cert/</span><br><span class="line"></span><br><span class="line">$ openssl req  -newkey rs<span class="variable">a:4096</span> -nodes -<span class="built_in">sha256</span> -keyout <span class="keyword">ca</span>.key -x509 -days <span class="number">365</span> -out <span class="keyword">ca</span>.crt -subj <span class="string">"/C=CN/L=Shanghai/O=harbor/CN=harbor-registry"</span></span><br></pre></td></tr></tbody></table></figure>
<p>生成一个证书签名, 设置访问域名为 reg.for-k8s.com</p>
<figure class="highlight stylus"><table><tbody><tr><td class="code"><pre><span class="line">$ openssl req -newkey rsa:<span class="number">4096</span> -nodes -sha256 -keyout reg<span class="selector-class">.for-k8s</span><span class="selector-class">.com</span><span class="selector-class">.key</span> -out server<span class="selector-class">.csr</span> -subj <span class="string">"/C=CN/L=Shanghai/O=harbor/CN=reg.for-k8s.com"</span></span><br></pre></td></tr></tbody></table></figure>
<p>生成主机证书</p>
<figure class="highlight stata"><table><tbody><tr><td class="code"><pre><span class="line">$ openssl x509 -req -days 365 -<span class="keyword">in</span> server.csr -<span class="keyword">CA</span> <span class="keyword">ca</span>.crt -CAkey <span class="keyword">ca</span>.key -CAcreateserial -<span class="keyword">out</span> <span class="keyword">reg</span>.<span class="keyword">for</span>-k8s.com.crt</span><br></pre></td></tr></tbody></table></figure>


<h3 id="通过自带脚本一键安装"><a href="#通过自带脚本一键安装" class="headerlink" title="通过自带脚本一键安装"></a>通过自带脚本一键安装</h3><figure class="highlight lasso"><table><tbody><tr><td class="code"><pre><span class="line">$ cd /usr/<span class="built_in">local</span>/harbor/</span><br><span class="line">./install.sh</span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line">✔ ---<span class="params">-Harbor</span> has been installed <span class="literal">and</span> started successfully.----</span><br><span class="line"></span><br><span class="line">Now you should be able <span class="keyword">to</span> visit the admin <span class="keyword">portal</span> at https:<span class="comment">//reg.for-k8s.com.</span></span><br><span class="line">For more details, please visit https:<span class="comment">//github.com/goharbor/harbor .</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>然后绑定hosts访问即可:<br><img src="/2019/01/07/docker-harbor/15468488877244.jpg"></p>
<p>默认账号密码<code>admin / Harbor12345</code><br><img src="/2019/01/07/docker-harbor/15468489958269.jpg"></p>
<p>ok 那上面的私有仓库服务已经搭建完毕了，该怎么使用呢？</p>
<ol start="0">
<li><p>首先在harbor上创建一个项目<code>myproject</code>(我这里不使用默认的libary)<br><img src="/2019/01/07/docker-harbor/15468494639235.jpg"><br>这里我选择私有仓库, pull/push都需要在主机上面执行<code>docker login</code>才行;</p>
</li>
<li><p>当我通过Dockerfile构建一个新镜像的时候, 直接指明registry和标签, 比如:</p>
</li>
</ol>
<figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line">$ docker build -t <span class="keyword">reg</span>.<span class="keyword">for</span>-k8s.<span class="keyword">com</span>/myproject/mydocker-image:v1.<span class="number">0.1</span> .</span><br><span class="line">Sending build context <span class="keyword">to</span> Docker daemon  <span class="number">97.21</span>MB</span><br><span class="line">Step <span class="number">1</span>/<span class="number">12</span> : FROM  <span class="number">1</span>and1internet/ubuntu-<span class="number">16</span></span><br><span class="line"> ---&gt; dbf985f1f449</span><br><span class="line">Step <span class="number">2</span>/<span class="number">12</span> : MAINTAINER guomaoqiu &lt;guomaoqiu@gmail.<span class="keyword">com</span>&gt;</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; <span class="number">598894333</span>db9</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">Successfully built b190966f3773</span><br><span class="line">Successfully tagged <span class="keyword">reg</span>.<span class="keyword">for</span>-k8s.<span class="keyword">com</span>/myproject/mydocker-image:v1.<span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ docker images | <span class="keyword">grep</span> myproject</span><br><span class="line"><span class="keyword">reg</span>.<span class="keyword">for</span>-k8s.<span class="keyword">com</span>/myproject/mydocker-image  v1.<span class="number">0.1</span>   b190966f3773   <span class="number">44</span> seconds ago    <span class="number">482</span>MB</span><br></pre></td></tr></tbody></table></figure>

<ol start="2">
<li>加入当你从别处获取的镜像想上传到私有仓库呢？就是打个tag就行啦, 比如我想把从官网的这个nginx镜像放到我的仓库:</li>
</ol>
<figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line">$ docker <span class="keyword">tag</span> nginx <span class="keyword">reg</span>.<span class="keyword">for</span>-k8s.<span class="keyword">com</span>/myproject/mynginx:latest</span><br><span class="line">$ docker images | <span class="keyword">grep</span> myproject</span><br><span class="line"><span class="keyword">reg</span>.<span class="keyword">for</span>-k8s.<span class="keyword">com</span>/myproject/mydocker-image    v1.<span class="number">0.1</span> b190966f3773  <span class="number">2</span> minutes ago       <span class="number">482</span>MB</span><br><span class="line"><span class="keyword">reg</span>.<span class="keyword">for</span>-k8s.<span class="keyword">com</span>/myproject/mynginx           latest <span class="number">568</span>c4670fa80  <span class="number">5</span> weeks ago         <span class="number">109</span>MB</span><br></pre></td></tr></tbody></table></figure>

<ol start="3">
<li>登录仓库</li>
</ol>
<figure class="highlight stylus"><table><tbody><tr><td class="code"><pre><span class="line">$ docker login -u admin -<span class="selector-tag">p</span> Harbor12345 reg<span class="selector-class">.for-k8s</span><span class="selector-class">.com</span></span><br><span class="line">Username: admin</span><br><span class="line">Password:</span><br><span class="line">WARNING! Your password will be stored unencrypted <span class="keyword">in</span> /root/.docker/config<span class="selector-class">.json</span>.</span><br><span class="line">Configure <span class="selector-tag">a</span> credential helper to remove this warning. See</span><br><span class="line">https:<span class="comment">//docs.docker.com/engine/reference/commandline/login/#credentials-store</span></span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<ol start="4">
<li>最后把本地的镜像push到仓库<br>当我执行这个的时候报错了：</li>
</ol>
<figure class="highlight subunit"><table><tbody><tr><td class="code"><pre><span class="line">docker push reg.for-k8s.com/myproject/mynginx:latest</span><br><span class="line"><span class="keyword">Error </span>response from daemon: Get https://reg.for-k8s.com/v2/: x509: certificate signed by unknown authority</span><br></pre></td></tr></tbody></table></figure>

<p>解决办法就是如果不在客户端部署证书，那么在Docker启动时设置参数 “–insecure-registry IP/仓库域名”,然后重载服务重启docker进程；注意的是我这里使用的这个域名是自定义的，那么需要在需要上传下载镜像的机器上，同样需要修改docker进程参数，并且绑定hosts,否则即使配置了参数，这个域名没法解析也是push/pull不到镜像的。</p>
<ol start="5">
<li>再次执行push操作:</li>
</ol>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ docker push reg.for-k8s.com<span class="operator">/</span>myproject<span class="operator">/</span>mynginx:latest</span><br><span class="line">The push refers to repository [reg.for-k8s.com<span class="operator">/</span>myproject<span class="operator">/</span>mynginx]</span><br><span class="line"><span class="params">b7efe781401d:</span> Pushed</span><br><span class="line"><span class="params">c9c2a3696080:</span> Pushed</span><br><span class="line"><span class="number">7</span><span class="params">b4e562e58dc:</span> Pushed</span><br><span class="line"><span class="params">latest:</span> <span class="params">digest:</span> sha256:e2847e35d4e0e2d459a7696538cbfea42ea2d3b8a1ee8329ba7e68694950afd3 <span class="params">size:</span> <span class="number">948</span></span><br><span class="line"></span><br><span class="line">$ [root@k8s-m1 kubectl-terminal-ubuntu]<span class="comment"># docker push reg.for-k8s.com/myproject/mydocker-image:v1.0.1</span></span><br><span class="line">The push refers to repository [reg.for-k8s.com<span class="operator">/</span>myproject<span class="operator">/</span>mydocker-image]</span><br><span class="line"><span class="number">96</span><span class="params">dca48ee72c:</span> Pushed</span><br><span class="line"><span class="params">fa879b69764c:</span> Pushed</span><br><span class="line"><span class="number">4</span><span class="params">d823b00e6b7:</span> Pushed</span><br><span class="line"><span class="number">6</span><span class="params">bf6e96da4a0:</span> Pushed</span><br><span class="line"><span class="params">eedda540c6a8:</span> Pushed</span><br><span class="line"><span class="params">f2a971e53afa:</span> Pushed</span><br><span class="line"><span class="number">3</span><span class="params">ee1a3b3fd18:</span> Pushed</span><br><span class="line"><span class="number">8</span><span class="params">a225cfa6dea:</span> Pushed</span><br><span class="line"><span class="number">428</span><span class="params">c1ba11354:</span> Pushed</span><br><span class="line"><span class="params">b097f5edab7b:</span> Pushed</span><br><span class="line"><span class="number">27712</span><span class="params">caf4371:</span> Pushed</span><br><span class="line"><span class="number">8241</span><span class="params">afc74c6f:</span> Pushed</span><br><span class="line">v1.<span class="number">0.1</span>: <span class="params">digest:</span> sha256:a20629f62d73cff93bf73b31958878a1d76c2dd42e36ebb2cb6d0ac294a46da7 <span class="params">size:</span> <span class="number">2826</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2019/01/07/docker-harbor/15468535525128.jpg"></p>
<p>以上push成功；</p>
<p>测试pull<br>那为了测试pull并且能成功运行，我这里通过kuernetes运行一个DaemonSet，镜像采用: <code>mynginx</code> ,并且设置镜像pull策略为<code>Always</code>, 然后创建一个服务在集群内部通过ClusterIP能够访问, yaml如下:</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">&gt;&gt;</span> <span class="string">test.yaml</span> <span class="string">&lt;&lt;</span> <span class="string">EOF</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">mynginx-service</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mynginx-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="number">80</span><span class="number">-80</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mynginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">reg.for-k8s.com/myproject/mynginx:latest</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">mynginx</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">$</span>  <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">daemonset.yaml</span></span><br><span class="line"><span class="string">service/mynginx-service</span> <span class="string">created</span></span><br><span class="line"><span class="string">daemonset.extensions/mynginx</span> <span class="string">create</span></span><br></pre></td></tr></tbody></table></figure>
<p>由于我刚才创建仓库的时候设置的仓库隐私性为私有的<br>需要docker login 登录成功之后，k8s kubectl create 就拉取不了镜像；<br>如果设置为公开，那么久不需要配置这一步骤。只需要docker login 登录成功之后，k8s kubectl create 就可以拉取镜像; 但是我不想让其为公开的；所以还需要配置如下步骤：</p>
<p>配置一个私有仓库harbor的secret：</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">kubectl create<span class="built_in"> secret </span>docker-registry registry-secret <span class="attribute">--namespace</span>=default \</span><br><span class="line"><span class="attribute">--docker-server</span>=https://reg.for-k8s.com <span class="attribute">--docker-username</span>=admin \</span><br><span class="line"><span class="attribute">--docker-password</span>=Harbor12345</span><br></pre></td></tr></tbody></table></figure>


<p>部署时指定imagePullSecrets, 修改在上面的yaml中添加这个选项:</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">cat</span> <span class="string">&gt;&gt;</span> <span class="string">test.yaml</span> <span class="string">&lt;&lt;</span> <span class="string">EOF</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">mynginx-service</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mynginx-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="number">80</span><span class="number">-80</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mynginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">mynginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">reg.for-k8s.com/myproject/mynginx:latest</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">mynginx</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">registry-secret</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="string">$</span>  <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">daemonset.yaml</span></span><br><span class="line"><span class="string">service/mynginx-service</span> <span class="string">created</span></span><br><span class="line"><span class="string">daemonset.extensions/mynginx</span> <span class="string">create</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2019/01/07/docker-harbor/15468562743522.jpg"></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker-registry</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker之gitlab-Ce</title>
    <url>/2016/08/05/docker-zhigitlabce/</url>
    <content><![CDATA[<h3 id="1、"><a href="#1、" class="headerlink" title="1、"></a>1、</h3><pre><code>docker pull gitlab-ce
</code></pre>
<h3 id="2、"><a href="#2、" class="headerlink" title="2、"></a>2、</h3><pre><code>mkdir -p /data/gitlab/{config,data,logs}
</code></pre>
<h3 id="3、"><a href="#3、" class="headerlink" title="3、"></a>3、</h3><pre><code>docker run --detach \
-p 443:443 -p 80:80 -p 2222:22 \
--name gitlab-ce \
--restart=always \    
--volume /data/gitlab/config:/etc/gitlab \     
--volume /data/gitlab/logs:/var/log/gitlab \
--volume /data/gitlab/data:/var/opt/gitlab \
docker.io/gitlab/gitlab-ce
</code></pre>
<h3 id="或者省略第一个步骤直接创建目录，然后run起来；"><a href="#或者省略第一个步骤直接创建目录，然后run起来；" class="headerlink" title="或者省略第一个步骤直接创建目录，然后run起来；"></a>或者省略第一个步骤直接创建目录，然后run起来；</h3><pre><code>docker run --detach -p 4433:443 -p 880:80 -p 2222:22 --name gitlab-ce --restart=always --volume /data/gitlab/config:/etc/gitlab --volume /data/gitlab/logs:/var/log/gitlab --volume /data/gitlab/data:/var/opt/gitlab docker.io/gitlab/gitlab-ce
</code></pre>
<h3 id="GitLab-修改主机名与更换-IP-配置"><a href="#GitLab-修改主机名与更换-IP-配置" class="headerlink" title="GitLab 修改主机名与更换 IP 配置"></a>GitLab 修改主机名与更换 IP 配置</h3><p>vim /data/gitlab/config/gitlab.rb<br>13 liens:<br>添加：[将<code>external_url = 'http://git.example.com'</code>修改为’<a href="http://docker宿主机ip//]">http://docker宿主机IP/\]</a><br>267 lines:<br>gitlab_rails[‘gitlab_shell_ssh_port’] = 2222 #修改使用ssh协议时的端口2222</p>
<p>重读配置:<br>进入容器，然后执行gitlab-ctl reconfigure</p>
<h3 id="访问："><a href="#访问：" class="headerlink" title="访问："></a>访问：</h3><p>由于ssh使用了非22标准段端口，所以在这里使用这样连接gitlab即可.<br>git clone ssh://<a href="mailto:git@192.168.1.89">git@192.168.1.89</a>:2222/guomaoqiu/test.git</p>
]]></content>
      <categories>
        <category>Monitor</category>
      </categories>
  </entry>
  <entry>
    <title>劝君莫惜金缕衣，劝君惜取少年时。</title>
    <url>/2018/07/02/e5-8a-9d-e5-90-9b-e8-8e-ab-e6-83-9c-e9-87-91-e7-bc-95-e8-a1-a3-ef-bc-8c-e5-8a-9d-e5-90-9b-e6-83-9c-e5-8f-96-e5-b0-91-e5-b9-b4-e6-97-b6-e3-80-82-trashed/</url>
    <content><![CDATA[<p>人生短短几十年、工作、学习(专业技能知识)也许就占了我们生命中的大部分的时间、也是我们生存下来的基础。但是能不能在这基础之上找点缝隙学习、尝试一些不一样的东西呢，这个还是看个人吧~~~我个人倒是现在除了工作、学习还是需要培养一点其他的兴趣爱好，从各个方面不断丰富、完善自己。 回想起年幼时父亲用最严厉的方式逼着我练字，当时根本不懂为什么非要逼着我练习，现在回想起来我只能打心底的感激父亲让我有了这个习惯或者说是培养了我这个兴趣爱好。以至于能写出还算拿得出手的字体。 劝君莫惜金缕衣，劝君惜取少年时。 花开堪折直须折，莫待花开空折枝。 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/IMG_4953.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/IMG_4953.jpg"></a></p>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
  </entry>
  <entry>
    <title>如何让计划任务实现秒级执行</title>
    <url>/2014/06/16/e5-a6-82-e4-bd-95-e8-ae-a9-e8-ae-a1-e5-88-92-e4-bb-bb-e5-8a-a1-e5-ae-9e-e7-8e-b0-e7-a7-92-e7-ba-a7-e6-89-a7-e8-a1-8c/</url>
    <content><![CDATA[<p>最近有个应用需求，根据实际要求最好是每3秒执行一次，但是crond只能支持到分。这该如何是好？ 第一种方法： 首先想到的是通过一个触发的脚本，然后在脚本中使用死循环来解决此问题，如：</p>
<p>cat test.sh<br>----------------<br>#!/bin/bash<br>while : ;do<br>        /home/script/test.sh 2&gt;/dev/null &amp;<br>        sleep 3<br>done<br>----------------</p>
<p>注意第一次运行时请不要使用sh test.sh&nbsp;&amp; 这种后台运行的方式，它会僵死的。 可以把它放到计划任务使其运行，然后将计划任务中的此条目删除即可。最后把这个脚本放到/etc/rc.local让它每次开机都可以被运行。 第二种方法： 和第二种方法类似，但是比起来更便捷一些。</p>
<p>cat cron-seconds.sh<br>----------------<br>#!/bin/bash<br>#For excuting the scripts every 3 seconds in crond.</p>
<p>for((i=1;i&lt;=20;i++));do</p>
<pre><code>    /home/script/test.sh 2&gt;/dev/null &amp;
    sleep 3
</code></pre>
<p>done<br>----------------</p>
<p>然后写入的crontab里每分钟执行一次，如下:</p>
<p>crontab -e<br>----------------<br>* * * * * /bin/bash /home/somedir/cron-seconds.sh<br>----------------</p>
<p>第三种方法： 那么如何使用计划任务来直接实现呢？ 最后解决方案如下，虽然条目看起来很多，但是经验证，脚本运行非常稳定。</p>
<p>crontab -e<br>#—————————————————————–<br>## For excuting test.sh.sh every 3 seconds<br>* * * * *  /home/script/test.sh.sh<br>* * * * * sleep 3 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 6 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 9 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 12 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 15 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 18 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 21 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 24 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 27 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 30 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 33 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 36 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 39 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 42 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 45 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 48 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 51 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 54 &amp;&amp;  /home/script/test.sh.sh<br>* * * * * sleep 57 &amp;&amp;  /home/script/test.sh.sh<br>#—————————————————————–</p>
<p>&nbsp; 个人还是比较倾向于第三种方法，毕竟你执行test.sh也是需要时间的。但是如果对于时间精度不是很高的，推荐使用第二种方法。</p>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>crond</tag>
      </tags>
  </entry>
  <entry>
    <title>学习SaltStack小记---第一章《安装及简单测试》</title>
    <url>/2015/08/12/e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-b8-80-e7-ab-a0-e3-80-8a-e5-ae-89-e8-a3-85-e5-8f-8a-e7-ae-80-e5-8d-95-e6-b5-8b-e8-af-95-e3-80-8b/</url>
    <content><![CDATA[<p><strong>一、什么是saltstack</strong> Salt，,一种全新的基础设施管理方式，部署轻松，在几分钟内可运行起来，扩展性好，很容易管理上万台服务器，速度够快，服务器之间秒级通讯。salt底层采用动态的连接总线, 使其可以用于编配, 远程执行, 配置管理等等. SaltStack 是继 Puppet、Chef 之后新出现的配置管理及远程执行工具， 目前，SaltStack 正得到越来越多的瞩目。与 Puppet 相比，SaltStack 没有那么笨重，感觉较为轻量；不像 Puppet 有 一套自己的 DSL 用来写配置，SaltStack 使用 YAML 作为配置文件格式，写 起来既简单又容易，同时也便于动态生成；此外，SaltStack 在远程执行命令 时的速度非常快，也包含丰富的模块。 官方站点：<a href="http://www.saltstack.com/">http://www.saltstack.com/</a> 官方文档：<a href="http://docs.saltstack.com/">http://docs.saltstack.com/</a> 中文站点：<a href="http://www.saltstack.cn/">http://www.saltstack.cn/</a> 中文手册：<a href="http://docs.saltstack.cn/">http://docs.saltstack.cn/</a> 中文wiki：<a href="http://wiki.saltstack.cn/doc">http://wiki.saltstack.cn/doc</a> &nbsp; <strong>二、安装</strong> 这里采用的是从EPEL源直接yum安装，当然我们要更新epel源：</p>
<p># rpm -Uvh <a href="http://dl.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm">http://dl.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm</a> SaltStack-Master（主服务器） # yum -y install salt-master SaltStack-Minion（从服务器） # yum -y install salt-minion</p>
<p>&nbsp; <strong>三、简单配置及使用</strong> 1、基本信息：</p>
<p>Master端：192.168.1.21  saltstack-node1.example.com<br>Minion端：192.168.1.22  saltstack-node2.example.com</p>
<p>2、启动命令：</p>
<p>Master端：/etc/init.d/salt-master {start|stop|status|restart|condrestart|reload}<br>Minion端：/etc/init.d/salt-minion {start|stop|status|restart|condrestart|reload}</p>
<p>3.主配置文件：</p>
<p>Master端：/etc/salt/master<br>Minion端：/etc/salt/master</p>
<p>4.配置Master端 修改监听地址默认为监听所有</p>
<p>sed -ie ‘s/^#.*interface:.*/\  interface: 192.168.1.111/g’ /etc/salt/master<br>###注：如果使用主机名，请绑定hosts</p>
<p>5.配置minion端 客户端minion的配置，vim /etc/salt/minion,添加master IP地址和minion ID号，ID建议用主机名来配置,然后开启日志功能，为了不重复操作我下面写了一个脚本，当我们配置minion时能快速搞定：</p>
<p>#/bin/bash<br>#desc : salt client setttings<br>read -p “Input Mster IP: “ MIP    ###指定masterIP<br>COMM1=”sed -i \“s/#master: salt/master: $MIP/\“ /etc/salt/minion”<br>eval $COMM1</p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>echo ‘ ‘<br>read -p “Input Minion ID: “ MID  ###修改minion ID<br>COMM2=”sed -i \“s/#id:.*/id: $MID/\“ /etc/salt/minion”<br>eval $COMM2<br>###  开启日志<br>sed -i “s@#log_file: /var/log/salt/minion@log_file: /var/log/salt/minion@1” /etc/salt/minion<br>sed -i ‘488d’ /etc/salt/minion</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><p>sed -i “s@#key_logfile: /var/log/salt/key@key_logfile: /var/log/salt/key@” /etc/salt/minion</p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><p>echo ‘ ‘<br>echo -e “\033[42;37m Salt-minion Information: \033[0m”<br>echo ‘##############################’<br>sed -e ‘/^#/d;/^$/d’ /etc/salt/minion<br>echo ‘##############################’<br>echo ‘ ‘<br>###启动服务<br>/etc/init.d/salt-minion start<br>###只要有新的minion客户端添加进来我们运行这个脚本就可快速完成minion端的配置啦.</p>
<p>6.修改完毕我们就可以重启服务啦. &nbsp; <strong>四、开始和Master端通信</strong> 1.Minion第一次与Master端通信会向其申请签发证书。在Master端执行# salt-key 即可看到已经签发、待签发以及拒绝的Minion列表。上面那台Minion刚刚申请，所以现在在Master上执行命令后SaltStack- Minion01将会出现在待签发的列表中。 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/Screenshot-from-2015-08-12-153102.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/Screenshot-from-2015-08-12-153102.png" alt="Screenshot from 2015-08-12 15:31:02"></a> 对于批量管理，首先建议的就是打开Master端的自动签发证书，要不然就得在服务器上执行命令# salt-key -a Minion-ID或者# salt-key -A，但相比都比较麻烦,于是我们可执行以下命令将其自动签发</p>
<p>sed -ie ‘s/^#auto_accept:.*/\auto_accept: True/g’ /etc/salt/master<br>/etc/init.d/salt-master restart</p>
<p>此时再次执行# salt-key 将会看到saltstack-node2.example.com出现在已签发的列表中。 &nbsp; <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/PIC2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/PIC2.png" alt="PIC2"></a> 2.简单测试其功能： 目前为止，Master端已经可以和Minion正常通信。 首先测试下，向saltstack-node2.example.com执行一条查看内存的命令 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/PIC3.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/PIC3.png" alt="PIC3"></a> 这里就先不解释命令的含义啦,后面在细说。 至此，简单测试完毕 当然SaltStack的功能不止于此，下篇正式看下他是如何工作以及各种配置的编写.</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>saltstack</tag>
      </tags>
  </entry>
  <entry>
    <title>学习SaltStack小记—第二章《编写配置及应用》</title>
    <url>/2015/08/13/e5-ad-a6-e4-b9-a0saltstack-e5-b0-8f-e8-ae-b0-e7-ac-ac-e4-ba-8c-e7-ab-a0-e3-80-8a-e7-bc-96-e5-86-99-e9-85-8d-e7-bd-ae-e5-8f-8a-e5-ba-94-e7-94-a8-e3-80-8b/</url>
    <content><![CDATA[<p>上次记录了一下salt的安装和配置，下面记录一下如何去编写一个配置并且应用到minion. SaltStack默认的配置文件路径在/srv/slat下，如果没有这个目录就新建个。如果不确定可以打开Master的主配置文件看下。 在master主配置文件中，打开一下三行的注释，这就是salt的默认配置路径。 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/1.png" alt="1"></a> 这里默认配置文件是注释了的，如果需要直接取消注释就行了。 切记，每次修改了配置文件都要重启服务 下面以两个需求为例子进行学习: a.在minion上面安装httpd服务，并且启动之 b.自建index.html文件，在master端应用配置完成后直接访问minion的http服务，在浏览器中显示我自定的index.html文件内容 额，刚开始学习这个的时候配置文件编写很是一件头疼的事情； <strong>一、配置文件说明</strong> 1.引导配置文件 top.sls saltstack的第一个配置文件默认为top.sls，是位于/srv/salt/目录下的，当然这不是绝对的，可以修改配置文件的哦。这个文件是必须要有的。 例如：</p>
<p>[root@saltstack-node1 salt]# more top.sls<br>base:                       ###可以理解为仓库<br>  ‘node0.example.com’:      ###对象名，就是针对那些minion，这里可以支持组什么的，就是各种匹配吧<br>    - apache                ###资源名(自定义)</p>
<p>说明：资源名需要在/srv/salt目录新建文件为apache.sls，注意：所有生效的配置文件都是以sls结尾的。 2.资源配置文件 /srv/salt/apache.sls 这个就是上面top中指定的资源名</p>
<p>[root@saltstack-node1 salt]# cat apache.sls<br>apache-service:            ###ID,自定义<br>  pkg.installed:           ###使用包管理的insalled方法<br>   - names:                ###名称<br>     - httpd               ###软件包名<br>     - httpd-devel<br>  service.running:         ###模块名称，安装好之后要启动它<br>   - name: httpd           ###启动什么<br>   - enable: True          ###开机自启动<br>   - reload: True          ###监视这个文件，如果有变动，我们要重载服务<br>   - watch:<br>      - file: /var/www/html/index.html  ###文件路径<br>   - require:              ###应用前提，httpd这个软件包安装好之后，才执行service这个模块<br>      - pkg: httpd<br>  file.managed:            ###模块名称，使用file模块的managed这个方法，目的，文件管理<br>    - name: /var/www/html/index.html  ###minion端的文件具体路径<br>    - source: salt://index.html       ###源文件<br>    - user: apache                    ###这个文件的一些属性<br>    - group: root<br>    - mode: 644<br>    - backup: minion                  ###改变之前备份<br>    - require:                        ###同上，执行完上面的，在执行这个<br>       - pkg: httpd</p>
<p>说明：源文件的位置就是在/srv/salt目录下新建一个文件index.html。 我这里内容自定义： echo “</p><h1>SaltStack</h1>“ &gt; index.html 配置编写完毕最后的目录结构如下： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/2.png" alt="2"></a> <strong>二、应用于客户端</strong> 经过上面Master端编写配置文件后，此时已经生效。Minions只要同步资源即可实现上面的两个需求。那么就是如何来同步？ 这里有两种方式： 第一种手动在Master上执行推送命令，第二种是在Minion设置时间间隔自动想Master端同步最新的资源。 1、Master手动同步 在Master上执行# salt ‘*’ state.highstate ###意思是向所有Minions推送最新资源 2、Minion自动同步 在/etc/salt/minion配置文件中增加<p></p>
<p>schedule:<br>highstate:<br>function: state.highstate<br>seconds: 30    ###意思是每30秒向Master同步一次最新资源，也可设置分-mintus、小时-hours</p>
<p>下面是在master端执行的结果： 当然在推送资源时我们可以先测试一下，命令： salt ‘node0.example.com’ state.highstate test=True <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/3.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/3.png" alt="3"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/4.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/4.png" alt="4"></a> 上面的推送已经完成，下面验证结果： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/6.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/08/6.png" alt="6"></a> 至此上面两个需求已经实现了。这也是salt最基本的用法及功能啦。后面继续学习。 看来搞个lamp或者lnmp平台也可以轻而易举的实现了。</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
  </entry>
  <entry>
    <title>基于CentOS6.5 X86_64 源码搭建GitLab</title>
    <url>/2015/09/30/e5-9f-ba-e4-ba-8ecentos6-5-x86-64-e6-ba-90-e7-a0-81-e6-90-ad-e5-bb-bagitlab/</url>
    <content><![CDATA[<p>系统：CentOS6.5 X86_64 已完成初始化：防火墙、SELinux 关闭、不必要服务停止，不必要用户删除……… 1.添加epel源</p>
<p>wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 <a href="https://www.fedoraproject.org/static/0608B895.txt">https://www.fedoraproject.org/static/0608B895.txt</a> &amp;&amp; rpm –import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6</p>
<p>#查看Key是否安装成功<br>rpm -qa gpg*</p>
<p>&nbsp; 2.安装epel源</p>
<p>rpm -Uvh <a href="http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm">http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</a></p>
<p>&nbsp; 3.添加PUIAS源</p>
<p>wget -O /etc/yum.repos.d/PUIAS_6_computational.repo <a href="https://gitlab.com/gitlab-org/gitlab-recipes/raw/master/install/centos/PUIAS/_6/_computational.repo">https://gitlab.com/gitlab-org/gitlab-recipes/raw/master/install/centos/PUIAS\_6\_computational.repo</a></p>
<p>&nbsp; 4.下载并安装gpg-key</p>
<p>wget -O /etc/pki/rpm-gpg/RPM-GPG-KEY-puias <a href="http://springdale.math.ias.edu/data/puias/6/x86_64/os/RPM-GPG-KEY-puias">http://springdale.math.ias.edu/data/puias/6/x86_64/os/RPM-GPG-KEY-puias</a> &amp;&amp; rpm –import /etc/pki/rpm-gpg/RPM-GPG-KEY-puias</p>
<p>#查看源是否添加成功[如果不成功，执行：yum-config-manager –enable epel –enable PUIAS_6_computational]<br>yum repolist</p>
<p>&nbsp; 5.安装整个搭建gitlab的相关的依赖包</p>
<p>yum -y groupinstall ‘Development Tools’<br>yum -y install readline readline-devel ncurses-devel gdbm-devel glibc-devel tcl-devel openssl-devel curl-devel expat-devel db4-devel byacc sqlite-devel libyaml libyaml-devel libffi libffi-devel libxml2 libxml2-devel libxslt libxslt-devel libicu libicu-devel system-config-firewall-tui redis sudo wget crontabs logwatch logrotate perl-Time-HiRes git cmake libcom_err-devel.i686 libcom_err-devel.x86_64 nodejs</p>
<p>&nbsp; 6.配置默认编辑器</p>
<p>yum -y install vim-enhanced<br>update-alternatives –set editor /usr/bin/vim.basic<br>ln -s /usr/bin/vim /usr/bin/edito</p>
<p>#reStructuredText markup语法支持，需要安装依赖包：<br>yum install -y python-docutils</p>
<p>&nbsp; 7.安装git(git&gt;=1.7.10 如果低于我们可以从新编译安装)</p>
<p>yum install zlib-devel perl-CPAN gettext curl-devel expat-devel gettext-devel openssl-devel<br>mkdir /tmp/git &amp;&amp; cd /tmp/git<br>curl –progress <a href="https://www.kernel.org/pub/software/scm/git/git-2.1.3.tar.gz">https://www.kernel.org/pub/software/scm/git/git-2.1.3.tar.gz</a> | tar xz<br>cd git-2.1.3/ &amp;&amp; ./configure –prefix=/usr/local/git &amp;&amp; make &amp;&amp; make install</p>
<p>&nbsp; 8.安装ruby(如果系统中Ruby的版本是2.0以前的那么请移除，GitLab只支持Ruby 2.0+版本)</p>
<p>yum remove ruby</p>
<p>mkdir /tmp/ruby &amp;&amp; cd /tmp/ruby<br>curl –progress <a href="ftp://ftp.ruby-lang.org/pub/ruby/2.1/ruby-2.1.2.tar.gz">ftp://ftp.ruby-lang.org/pub/ruby/2.1/ruby-2.1.2.tar.gz</a> | tar xz<br>cd ruby-2.1.2<br>./configure –prefix=/usr/local/ –disable-install-rdoc<br>make<br>make install<br>#记得检查ruby的版本是否是最新<br>ruby –version</p>
<p>&nbsp; 9.安装Bundler Gem(由于`<a href="http://rubygems.org/%60%E5%B7%B2%E8%A2%AB%E5%A2%99%EF%BC%8C%E8%BF%99%E9%87%8C%E6%9B%BF%E6%8D%A2%E6%BA%90%E4%B8%BA/%60https://ruby.taobao.org/%60">http://rubygems.org\`已被墙，这里替换源为\`https://ruby.taobao.org\`</a>)</p>
<p># 添加淘宝源并且移除官方源<br>gem sources -a <a href="https://ruby.taobao.org/">https://ruby.taobao.org</a><br>gem sources -r <a href="https://rubygems.org/">https://rubygems.org/</a></p>
<p>&nbsp; 10.安装Bundler</p>
<p>gem install bundler -v’1.5.2’ –no-doc</p>
<p>&nbsp; 11.System Users为GitLab创建用户</p>
<p>adduser –system –shell /bin/bash –comment ‘GitLab’ –create-home –home-dir /home/git/ git</p>
<p>&nbsp; 12.编辑sudoers文件，将ruby和git的程序路径添加到PATH中，使git用户作为root来使用gem命令</p>
<p>sed -ie ‘s@Defaults\    secure_path\ \=\ \/sbin\:\/bin\:\/usr\/sbin\:\/usr\/bin@Defaults\    secure_path\ \=\ \/sbin\:\/bin\:\/usr\/sbin\:\/usr\/bin\:/usr\/local\/bin@g’ /etc/sudoers</p>
<p>&nbsp; 13.安装mysql</p>
<p>yum install -y mysql mysql-devel mysql-server<br>service mysqld start</p>
<p>#为gitlab创建数据库<br>#创建用户<br>mysql -e “CREATE USER ‘git‘@’localhost’ IDENTIFIED BY ‘xxxxx’;”</p>
<p>#创建数据库<br>mysql -e “CREATE DATABASE IF NOT EXISTS \`gitlabhq_production\` DEFAULT CHARACTER SET \`utf8\` COLLATE \`utf8_unicode_ci\`;”</p>
<p>#授权<br>mysql -e “GRANT SELECT, LOCK TABLES, INSERT, UPDATE, DELETE, CREATE, DROP, INDEX, ALTER ON \`gitlabhq_production\`.* TO ‘git‘@’localhost’;”</p>
<p>#用git用户尝试登陆验证是否成功<br>sudo -u git -H mysql -u git -pxxxxxx  -e ‘show databases;’</p>
<p>#记得要给root用户密码哦。</p>
<p>&nbsp; 14.安装redis(已经在前面的依赖包安装时安装了)</p>
<p>chkconfig redis on</p>
<p>#配置Redis使用sockets<br>cp /etc/redis.conf /etc/redis.conf.orig</p>
<p># 关闭Redis 监听于TCP<br>sed ‘s/^port .*/port 0/‘ /etc/redis.conf.orig | sudo tee /etc/redis.conf</p>
<p>#启用Redis socket<br>echo ‘unixsocket /var/run/redis/redis.sock’ | sudo tee -a /etc/redis.conf<br>echo -e ‘unixsocketperm 0770’ | sudo tee -a /etc/redis.conf</p>
<p>#设置socket目录属主（组）为redis<br>chown redis:redis /var/run/redis<br>chmod 755 /var/run/redis</p>
<p>#将git用户添加至redis组<br>usermod -aG redis git</p>
<p>#启动redis服务<br>service redis start</p>
<p>&nbsp; 15.安装gitlab，以及相关配置</p>
<p>cd /home/git/<br>sudo -u git -H git clone <a href="http://git.oschina.net/Yxnt/gitlab">http://git.oschina.net/Yxnt/gitlab</a></p>
<p># 配置gitlab<br>cd /home/git/gitlab &amp;&amp; sudo -u git -H cp config/gitlab.yml.example config/gitlab.yml</p>
<p>#访问地址为本机IP<br>sudo -u git -H sed -ie “s/host: .*/host: `hostname –all-ip-addresses`/g” config/gitlab.yml</p>
<p>#创建satellites目录<br>sudo -u git -H mkdir /home/git/gitlab-satellites &amp;&amp; chmod u+rwx,g=rx,o-rwx /home/git/gitlab-satellites</p>
<p>#确认gitlab可以写入tmp/pids、tmp/sockets以及public/uploads/目录<br>chmod -R u+rwX tmp/pids/ &amp;&amp; chmod -R u+rwX tmp/sockets/ &amp;&amp; chmod -R u+rwX  public/uploads</p>
<p>#复制Unicorn配置文件<br>sudo -u git -H cp config/unicorn.rb.example config/unicorn.rb</p>
<p>#复制Rack attack配置文件<br>sudo -u git -H cp config/initializers/rack_attack.rb.example config/initializers/rack_attack.rb</p>
<p>#配置Git 全局设置<br>sudo -u git -H git config –global user.name “GitLab”<br>sudo -u git -H git config –global user.email “<a href="mailto:example@example.com">example@example.com</a>“<br>sudo -u git -H git config –global core.autocrlf input</p>
<p>#配置Redis连接设置<br>sudo -u git -H cp config/resque.yml.example config/resque.yml<br>sudo -u git -H sed -ie “s/develo.*/development:\ unix:\/var\/run\/redis\/redis.sock/g” config/resque.yml</p>
<p>#配置Gitlab 数据库设置（注意这里的socket文件位置，不同版本的mysql位置不一样)<br>sudo -u git cp config/database.yml.mysql config/database.yml<br>sudo -u git sed -ie “12s/password: \“secure password\“/password: $MYSQL_PASS/g” config/database.yml<br>sudo -u git sed -ie “13,14s/#\ //g” config/database.yml<br>sudo -u git sed -ie “14s@socket: \/tmp\/mysql.sock@socket: \/var\/lib/mysql\/mysql.sock@g” config/database.yml</p>
<p>#确定database.yml文件只为git用户可读<br>sudo -u git -H chmod o-rwx config/database.yml</p>
<p>&nbsp; 16.安装Gems</p>
<p>cd /home/git/gitlab</p>
<p>#更换配置文件中的ruby源<br>sudo -u git -H sed -ie “s@source\ \“http:\/\/rubygems.org\“@source\ \“http:\/\/ruby.taobao.org\“@g” ../gitlab-shell/Gemfile</p>
<p>#安装<br>sudo -u git -H bundle install –deployment –without development test postgres aws</p>
<p>&nbsp; 17.安装,配置gitlab-shell</p>
<p>cd /home/git/gitlab/<br>sudo -u git -H bundle exec rake gitlab<span class="github-emoji"><span>🐚</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f41a.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>install[v2.6.3] REDIS_URL=unix:/var/run/redis/redis.sock RAILS_ENV=production</p>
<p>#配置(在安装过程中其实gitlab-shell配置已经自动生成了，但是有必要检查核实一下里面的配置，尤其是gitlab_url)<br>cp /home/git/gitlab-shell/config.yml /home/git/gitlab-shell/config.yml.bak</p>
<p>&nbsp; 18.初始化数据并且激活高级特性</p>
<p>#这里会提示输入yes/no，输入yes即可（我这里直接传递一个yes过去）<br>echo yes| sudo -u git -H bundle exec rake gitlab:setup RAILS_ENV=production<br>#这里会生成一个账户和密码：<br>#login………root<br>#password……5iveL!fe 这个密码可以更改： sudo -u git -H bundle exec rake gitlab:setup RAILS_ENV=production GITLAB_ROOT_PASSWORD=YOUR_NETPASSWORD</p>
<p>#生成js.css等文件（如果没有这一步的话，你在访问时网页css等是乱的）<br>cd /home/git/gitlab<br>bundle exec rake assets:precompile RAILS_ENV=production</p>
<p>&nbsp; 19.安装gitlab启动管理脚本</p>
<p>wget -O /etc/init.d/gitlab <a href="https://gitlab.com/gitlab-org/gitlab-recipes/raw/master/init/sysvinit/centos/gitlab-unicorn">https://gitlab.com/gitlab-org/gitlab-recipes/raw/master/init/sysvinit/centos/gitlab-unicorn</a><br>chmod +x /etc/init.d/gitlab<br>chkconfig –add gitlab<br>chkconfig gitlab on</p>
<p>#设置gitlab服务的日志滚动<br>cp lib/support/logrotate/gitlab /etc/logrotate.d/gitlab</p>
<p>&nbsp; 20.检查应用程序状态</p>
<p>sudo -u git -H bundle exec rake gitlab:env:info RAILS_ENV=production<br>#执行该命令 如果顺利的话，会将关于gitlab的 相关详细信息展示出来</p>
<p>和下图差不多 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/gitlab-information.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/gitlab-information.png" alt="gitlab-information"></a> &nbsp; 21.启动gitlab</p>
<p>#启动gitlab<br>service gitlab start</p>
<p>&nbsp; 22.安装nginx ，将访问请求反代到后端的8080端口</p>
<p>yum install -y nginx &amp;&amp; chkconfig nginx on</p>
<p>#删除nginx中的这个默认配置文件<br>rm -rf /etc/nginx/conf.d/default.conf</p>
<p>#获取gitlab的nginx模板配置文件<br>wget -O /etc/nginx/conf.d/gitlab.conf <a href="https://gitlab.com/gitlab-org/gitlab-ce/raw/master/lib/support/nginx/gitlab">https://gitlab.com/gitlab-org/gitlab-ce/raw/master/lib/support/nginx/gitlab</a></p>
<p>#对模板文件的一些修改<br>sed -ie “38s/.*/server\ localhost:8080;/g” /etc/nginx/conf.d/gitlab.conf<br>sed -ie “s/YOUR_SERVER_FQDN/`hostname`/g” /etc/nginx/conf.d/gitlab.conf<br>sed -ie “52d” /etc/nginx/conf.d/gitlab.conf</p>
<p>#将用户nginx加入到git组(这步非常的关键)<br>usermod -a -G git nginx<br>chmod g+rx /home/git/</p>
<p>#启动nginx<br>service nginx start</p>
<p>&nbsp; ok ，至此gitlab的安装已经结束。我们通过<a href="http://your-server-ip/">http://YOUR-SERVER-IP/</a> 就可以访问到你的gitlab平台了 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/gitlab-login.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/gitlab-login.png" alt="gitlab-login"></a></p>
]]></content>
      <categories>
        <category>Web相关</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>我的51CTO博客文章链接汇总</title>
    <url>/2016/01/07/e6-88-91-e7-9a-8451cto-e5-8d-9a-e5-ae-a2-e6-96-87-e7-ab-a0-e9-93-be-e6-8e-a5-e6-b1-87-e6-80-bb/</url>
    <content><![CDATA[<p>将之前在51CTO的所有博客整理了一番, 点击文章名称就可以跳到51cto读阅啦^o^</p>
<h1 id="集群-高可用-负载均衡"><a href="#集群-高可用-负载均衡" class="headerlink" title="集群/高可用/负载均衡"></a>集群/高可用/负载均衡</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1405684">LVS_NAT实现过程…</a> <a href="http://maoqiu.blog.51cto.com/8570467/1414425">LVS_DR实现过程…</a> <a href="http://maoqiu.blog.51cto.com/8570467/1405675">Keepalived基础知识</a> <a href="http://maoqiu.blog.51cto.com/8570467/1399876">Linux HA集群之DRBD详解</a> <a href="http://maoqiu.blog.51cto.com/8570467/1405875">基于keepalived的Haproxy高可用配置</a></p>
<h1 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1409676">分布式缓存varnish简介</a> <a href="http://maoqiu.blog.51cto.com/8570467/1409382">分布式文件系统MogileFS简介</a></p>
<h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1392816">Mysql知识总结（一）</a> <a href="http://maoqiu.blog.51cto.com/8570467/1392818">Mysql知识总结（二）</a> <a href="http://maoqiu.blog.51cto.com/8570467/1394389">Mysql知识总结（三）</a> <a href="http://maoqiu.blog.51cto.com/8570467/1398202">MariaDB/Mysql之主从架构的复制原理及主从/双主配置详解(一)</a> <a href="http://maoqiu.blog.51cto.com/8570467/1398241">MariaDB/Mysql之主从架构的复制原理及主从/双主配置详解(二)</a> <a href="http://maoqiu.blog.51cto.com/8570467/1396244">MariaDB之备份恢复准则（三）</a> <a href="http://maoqiu.blog.51cto.com/8570467/1396211">MariaDB之基于mysqldump与lvm-snapshot备份恢复Databases or Tables（一）</a> <a href="http://maoqiu.blog.51cto.com/8570467/1396217">MariaDB之基于Percona Xtrabackup备份大数据库【完整备份与增量备份】（二）</a></p>
<h1 id="Linux-剖析"><a href="#Linux-剖析" class="headerlink" title="Linux 剖析"></a>Linux 剖析</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1370675">Linux系统-小倒腾之Linux DIY定制裁剪(附带简单网络功能)o_o(一)</a> <a href="http://maoqiu.blog.51cto.com/8570467/1390908">Linux系统-小倒腾之Linux DIY定制裁剪(New kernel+Busybox)o_o（二）</a> <a href="http://maoqiu.blog.51cto.com/8570467/1390910">Linux系统-小倒腾之Linux DIY定制裁剪(定制Linux+SSH/Nginx)o_o（三）</a></p>
<h1 id="文件同步"><a href="#文件同步" class="headerlink" title="文件同步"></a>文件同步</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1387058">LinuxTools—Rsync—原理及其应用(一)</a> <a href="http://maoqiu.blog.51cto.com/8570467/1387062">LinuxTools—Rsync—原理及其应用(二)</a></p>
<h1 id="安全管理"><a href="#安全管理" class="headerlink" title="安全管理"></a>安全管理</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1386888">Linux安全管理-Iptables-NAT技术应用</a> <a href="http://maoqiu.blog.51cto.com/8570467/1386704">Linux安全管理-Iptables原理及其应用</a></p>
<h1 id="Linux-Service"><a href="#Linux-Service" class="headerlink" title="Linux Service"></a>Linux Service</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1381723">Linux网络服务-Web Service之【HTTP协议简介】(一)</a> <a href="http://maoqiu.blog.51cto.com/8570467/1381724">Linux网络服务-Web Service之【Apache-Prefork、Worker和Event三种工作模式分析】(二)</a> <a href="http://maoqiu.blog.51cto.com/8570467/1386861">Linux网络服务-Web Service之【apache的功能、安装、配置文件介绍以及实验实例】(三)</a> <a href="http://maoqiu.blog.51cto.com/8570467/1386643">Linux网络服务-LAMP之基于NFS+Fastcgi的LAMP搭建</a> <a href="http://maoqiu.blog.51cto.com/8570467/1384026">Linux网络服务-LAMP之Php基于Apache的模块实现</a> <a href="http://maoqiu.blog.51cto.com/8570467/1386616">Linux网络服务之DNS服务器介绍及配置实例详解</a></p>
<h1 id="系统管理"><a href="#系统管理" class="headerlink" title="系统管理"></a>系统管理</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1377421">Linux系统基础-管理之加密、解密、Openssl基本应用及CA实现过程</a> <a href="http://maoqiu.blog.51cto.com/8570467/1368586">Linux系统基础-管理之系统启动过程及系统初始化学习总结</a> <a href="http://maoqiu.blog.51cto.com/8570467/1364131">Linux系统基础-管理之软件包管理【附http源码安装实例】</a> <a href="http://maoqiu.blog.51cto.com/8570467/1364072">Linux系统基础-管理之find命令学习总结</a> <a href="http://maoqiu.blog.51cto.com/8570467/1360448">Shell编程入门进阶之Grep命令及正则表达式知识梳理</a> <a href="http://maoqiu.blog.51cto.com/8570467/1359596">Linux系统基础-管理之用户、权限管理</a> <a href="http://maoqiu.blog.51cto.com/8570467/1359589">Shell编程入门进阶之bash配置文件介绍</a> <a href="http://maoqiu.blog.51cto.com/8570467/1359585">Shell编程入门进阶之Bash Shell特性</a> <a href="http://maoqiu.blog.51cto.com/8570467/1359519">Linux命令的格式、常用命令汇总以及一些系统基本概念</a> <a href="http://maoqiu.blog.51cto.com/8570467/1358968">Linux系统基础-管理之如何在终端上获取Linux命令帮助.</a> <a href="http://maoqiu.blog.51cto.com/8570467/1358373">Linux系统基础-管理之终端,伪终端概念详解之tty,pty等</a> <a href="http://maoqiu.blog.51cto.com/8570467/1358265">Linux系统基础-管理之Linux 目录配置标准：FHS：FileSystem Hierarchy Standard</a></p>
<h1 id="杂谈"><a href="#杂谈" class="headerlink" title="杂谈"></a>杂谈</h1><p><a href="http://maoqiu.blog.51cto.com/8570467/1358231">假如Linux版本都如女人</a></p>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
  </entry>
  <entry>
    <title>数据库链接超时，可能原因由于空间不足造成</title>
    <url>/2015/04/15/e6-95-b0-e6-8d-ae-e5-ba-93-e9-93-be-e6-8e-a5-e8-b6-85-e6-97-b6-ef-bc-8c-e5-8f-af-e8-83-bd-e5-8e-9f-e5-9b-a0-e7-94-b1-e4-ba-8e-e7-a9-ba-e9-97-b4-e4-b8-8d-e8-b6-b3-e9-80-a0-e6-88-90/</url>
    <content><![CDATA[<p>今天开发人员告知，测试服务器数据库链接超时 我登录测试服务器<br>1、检查mysql运行状态，结果：正常 通过执行ss -tunl | grep “3306” 以及 ps aux | grep “mysqld” 来判定<br>2、检查mysql日志，结果：正常 通过查看mysql的错误日志<br>3、由于系统用的是CentOS7 于是乎通过另外一种方式查看mysql的状态<br>systemctl status mysqld<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/nospace.png" alt="nospace"><br>可以看出来，虽然连接数据库时出现了点问题，但是mysql 并未停止运行。 </p>
<p>解决办法：清理磁盘空间，为mysql数据 所在分区腾出空间。</p>
]]></content>
      <categories>
        <category>故障处理</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>nospace</tag>
      </tags>
  </entry>
  <entry>
    <title>清理Elasticsearch的索引</title>
    <url>/2015/10/23/e6-b8-85-e7-90-86elasticsearch-e7-9a-84-e7-b4-a2-e5-bc-95/</url>
    <content><![CDATA[<p>最近在使用 logstash来做日志收集 并用 elasticsearch来搜索，因为日志没有进行过滤，没几天就发现elasticsearch的索引文件大的吓人，之前还真没清理过。其实要说清理 也简单，直接到 elasticsearch data文件夹里删掉就行了，但怎么也得做的有点技术含量不是？ 上网站看了看文档，其实也挺简单一条命令就行了</p>
<p> 1,  # curl -XDELETE ‘<a href="http://localhost:9200/logstash-">http://localhost:9200/logstash-</a>*’<br> 2,  清理掉了所有的索引文件，我发现curl删除比rm删除要快出很多</p>
<p>下面是主页上的详细介绍，其他部分可以自己看， <a href="http://www.elasticsearch.org/guide/reference/api/delet">http://www.elasticsearch.org/guide/reference/api/delet</a></p>
]]></content>
      <categories>
        <category>故障处理</category>
      </categories>
      <tags>
        <tag>logstash</tag>
      </tags>
  </entry>
  <entry>
    <title>检查文件时间戳，对比时间的Shell脚本</title>
    <url>/2015/10/16/e6-a3-80-e6-9f-a5-e6-96-87-e4-bb-b6-e6-97-b6-e9-97-b4-e6-88-b3-ef-bc-8c-e5-af-b9-e6-af-94-e6-97-b6-e9-97-b4-e7-9a-84shell-e8-84-9a-e6-9c-ac/</url>
    <content><![CDATA[<p>应开发需求，有些锁文件生成之后不会在固定时间内删除，造成程序的计划任务会卡死，于是需要一个检查对比locks文件的脚本来实时检测这个目录下的*.locks文件</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="comment">#获取当前时间</span></span><br><span class="line">curren_time=\`<span class="built_in">date</span> +%H:%M:%S\`  </span><br><span class="line"></span><br><span class="line"><span class="comment">#--time-stype=FORMAT</span></span><br><span class="line"><span class="built_in">ls</span> -l --time-style=+%H:%M:%S /xxxxxx/*.locks &gt;/tmp/locks 2&gt;/dev/null</span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> -r line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="comment">#文件内容</span></span><br><span class="line">file_time=\`<span class="built_in">echo</span> <span class="variable">$line</span> | awk -F <span class="string">' '</span> <span class="string">'{print $6}'</span>\`  </span><br><span class="line"><span class="comment">#文件绝对路径及名称</span></span><br><span class="line">File=\`<span class="built_in">echo</span> <span class="variable">$line</span> | awk -F <span class="string">' '</span> <span class="string">'{print $7}'</span>\`</span><br><span class="line"></span><br><span class="line"><span class="comment">#转换成Unix时间戳</span></span><br><span class="line">date1=\`<span class="built_in">date</span> -d <span class="string">"<span class="variable">$curren_time</span>"</span> +%s\`</span><br><span class="line">date2=\`<span class="built_in">date</span> -d <span class="string">"<span class="variable">$file_time</span>"</span> +%s\`</span><br><span class="line"></span><br><span class="line"><span class="comment">#当前时间减去文件生产时间的差值进行比较(shell 的运算用expr关键字)</span></span><br><span class="line">DD=$(<span class="built_in">expr</span> <span class="variable">$date1</span> - <span class="variable">$date2</span>)</span><br><span class="line"><span class="keyword">if</span> \[ <span class="variable">$DD</span> -gt 120 \];<span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$curren_time</span> --- Greater than 2 minutes, <span class="variable">$File</span> will be delete."</span></span><br><span class="line">    <span class="built_in">sleep</span> 2</span><br><span class="line">    <span class="built_in">rm</span> -rf <span class="variable">$File</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span> &lt; /tmp/locks</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">chmod</span> +x /home/shell/check\_locks\_file.sh</span><br><span class="line"><span class="built_in">nohup</span> /bin/bash /home/shell/check\_locks\_file.sh &gt;/var/log/check_locks.log &amp;</span><br></pre></td></tr></tbody></table></figure>
<p>两个关键点<br>1、ls命令的用法 ： –time-type=FORMAT 参数<br>2、将常规的时间转换成Unix时间戳</p>
]]></content>
      <categories>
        <category>Shell</category>
        <category>脚本编程</category>
      </categories>
      <tags>
        <tag>expr</tag>
      </tags>
  </entry>
  <entry>
    <title>获取Linux服务器硬件信息</title>
    <url>/2014/06/29/e8-8e-b7-e5-8f-96linux-e6-9c-8d-e5-8a-a1-e5-99-a8-e7-a1-ac-e4-bb-b6-e4-bf-a1-e6-81-af/</url>
    <content><![CDATA[<p>1.查看服务器型号、序列号：</p>
<p>[root@localhots ~]#dmidecode|grep “System Information” -A9|egrep  “Manufacturer|Product|Serial”<br>    Manufacturer: HP<br>    Product Name: ProLiant DL360 G6<br>    Serial Number: JPT0012J2W    </p>
<p>2.Linux 查看内存的插槽数,已经使用多少插槽.每条内存多大:</p>
<p>[root@localhost ~]#dmidecode|grep -A5 “Memory Device”|grep Size|grep -v Range<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: 4096 MB<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: 4096 MB<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: 4096 MB<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: 4096 MB<br>    Size: No Module Installed<br>    Size: No Module Installed<br>    Size: No Module Installed</p>
<p>3.Linux 查看内存的频率:</p>
<p>[root@localhost ~]#dmidecode|grep -A16 “Memory Device”|grep ‘Speed’<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: 1067 MHz<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: 1067 MHz<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: 1067 MHz<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: 1067 MHz<br>    Speed: Unknown<br>    Speed: Unknown<br>    Speed: Unknown</p>
<p>4.查看cpu的统计信息:</p>
<p>[root@localhost ~]# lscpu              #-&gt;如该命令找不到,请安装软件包util-linux-ng<br>Architecture:          x86_64          #-&gt;cpu架构<br>CPU op-mode(s):        32-bit, 64-bit<br>Byte Order:            Little Endian   #-&gt;小尾序<br>CPU(s):                8               #-&gt;总共有8核<br>On-line CPU(s) list:   0-7<br>Thread(s) per core:    1               #-&gt;每个cpu核，只能支持一个线程，即不支持超线程<br>Core(s) per socket:    4               #-&gt;每个cpu，有4个核<br>Socket(s):             2               #-&gt;总共有2一个cpu<br>NUMA node(s):          2<br>Vendor ID:             GenuineIntel    #-&gt;cpu产商 intel<br>CPU family:            6<br>Model:                 26<br>Stepping:              5<br>CPU MHz:               2266.877<br>BogoMIPS:              4532.68<br>Virtualization:        VT-x            #-&gt;支持cpu虚拟化技术<br>L1d cache:             32K<br>L1i cache:             32K<br>L2 cache:              256K<br>L3 cache:              8192K<br>NUMA node0 CPU(s):     0,2,4,6<br>NUMA node1 CPU(s):     1,3,5,7</p>
<p>5.查看/proc/cpuinfo,可以知道每个cpu信息，如每个CPU的型号，主频等:</p>
<p>[root@localhost ~]#cat /proc/cpuinfo<br>processor   : 0<br>vendor_id   : GenuineIntel<br>cpu family  : 6<br>model       : 26<br>model name  : Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz<br>…..<br>…..<br>…..<br>#-&gt;上面输出的是第一个cpu部分信息，还有7个cpu信息省略了</p>
<p>6.查看物理CPU个数:</p>
<p>[root@localhost ~]# grep “physical id” /proc/cpuinfo  | sort -u<br>physical id    : 0<br>physical id    : 1</p>
<p>7.查看CPU核心数:</p>
<p>[root@localhost ~]# grep ‘core id’ /proc/cpuinfo | sort -u | wc -l<br>4</p>
<p>8.查看CPU线程数:</p>
<p>[root@localhost ~]# grep ‘processor’ /proc/cpuinfo | sort -u | wc -l<br>8</p>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>cpu</tag>
      </tags>
  </entry>
  <entry>
    <title>运维人,你应该了解的三张武功心法图(转载)</title>
    <url>/2015/10/23/e8-bf-90-e7-bb-b4-e4-ba-ba-e4-bd-a0-e5-ba-94-e8-af-a5-e4-ba-86-e8-a7-a3-e7-9a-84-e4-b8-89-e5-bc-a0-e6-ad-a6-e5-8a-9f-e5-bf-83-e6-b3-95-e5-9b-be-e8-bd-ac-e8-bd-bd/</url>
    <content><![CDATA[<blockquote>
<p><strong>一、运维技能图</strong></p>
</blockquote>
<p>做为一个运维工程师，你知道你应该学习什么？怎么学习吗？朝哪个方向发展吗？下面一张运维工程师技能图，让你了解！ 图片链接，<a href="http://blog.sctux.com/images/1111.png">点我^_^</a></p>
<blockquote>
<p><strong>二、自动化运维路线图</strong></p>
</blockquote>
<p>运维自动化在国内已经声名远躁了，随着互联网快速的发展，运维不单单是几个脚本，几个文档可以胜任的！DevOps在国内很受热捧，但是真正的自动化之路，你走到了哪？你知道该怎么走吗？下面的武功心法图告诉你该怎么走！ 图片链接，<a href="http://blog.sctux.com/images/2222.png">点我^_^</a></p>
<blockquote>
<p><strong>三、云计算知识大宝典</strong></p>
</blockquote>
<p>从2013年开始，我国云计算持续快速发展，产业规模不断扩大，产业链日趋完善，产业环境不断优化。在这种情况下，不少创业者看到了市场，不少云计算公司崛起。但是人才在哪里，哪些是真正的云计算人才？云计算人才他应该会什么，下面Cloud computing image告诉你 图片链接，<a href="http://blog.sctux.com/images/3333.png">点我^_^</a> <strong>由于原作者制作的图片较大，这里不能正常的显示，点击链接然后扩大可清晰看到！</strong></p>
<p>天下武功唯快不攻，这句话运用到互联网，就是你最快、最好、最早的掌握了互联网的最新技术，你就是比较吃香的人才！就像最新比较人们的容器Docker技术一样，如果你是先行者，你现在至少是一家Docker生态技术创业服务的合伙人！革命还未胜利，同志还需努力，各位苦逼的互联网工作者，加油！</p>
]]></content>
      <categories>
        <category>心情随笔</category>
      </categories>
  </entry>
  <entry>
    <title>通过 Ulimit 改善系统性能</title>
    <url>/2015/04/23/e9-80-9a-e8-bf-87-ulimit-e6-94-b9-e5-96-84-e7-b3-bb-e7-bb-9f-e6-80-a7-e8-83-bd/</url>
    <content><![CDATA[<p><strong>概述</strong> 系统性能一直是一个受关注的话题，如何通过最简单的设置来实现最有效的性能调优，如何在有限资源的条件下保证程序的运作，ulimit 是我们在处理这些问题时，经常使用的一种简单手段。ulimit 是一种 linux 系统的内键功能，它具有一套参数集，用于为由它生成的 shell 进程及其子进程的资源使用设置限制。本文将在后面的章节中详细说明 ulimit 的功能，使用以及它的影响，并以具体的例子来详细地阐述它在限制资源使用方面的影响。 <strong>ulimit 的功能和用法</strong> <strong>ulimit 功能简述</strong> 假设有这样一种情况，当一台 Linux 主机上同时登陆了 10 个人，在系统资源无限制的情况下，这 10 个用户同时打开了 500 个文档，而假设每个文档的大小有 10M，这时系统的内存资源就会受到巨大的挑战。 而实际应用的环境要比这种假设复杂的多，例如在一个嵌入式开发环境中，各方面的资源都是非常紧缺的，对于开启文件描述符的数量，分配堆栈的大小，CPU 时间，虚拟内存大小，等等，都有非常严格的要求。资源的合理限制和分配，不仅仅是保证系统可用性的必要条件，也与系统上软件运行的性能有着密不可分的联系。这时，ulimit 可以起到很大的作用，它是一种简单并且有效的实现资源限制的方式。 ulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制：所创建的内核文件的大小、进程数据块的大小、Shell 进程创建文件的大小、内存锁住的大小、常驻内存集的大小、打开文件描述符的数量、分配堆栈的最大大小、CPU 时间、单个用户的最大线程数、Shell 进程所能使用的最大虚拟内存。同时，它支持硬资源和软资源的限制。 作为临时限制，ulimit 可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。而对于长期的固定限制，ulimit 命令语句又可以被添加到由登录 shell 读取的文件中，作用于特定的 shell 用户。 <img src="https://www.ibm.com/developerworks/cn/linux/l-cn-ulimit/images/image001.jpg" alt="ulimit 的使用"> 在下面的章节中，将详细介绍如何使用 ulimit 做相应的资源限制。 如何使用 ulimit ulimit 通过一些参数选项来管理不同种类的系统资源。在本节，我们将讲解这些参数的使用。 ulimit 命令的格式为：ulimit [options] [limit] 具体的 options 含义以及简单示例可以参考以下表格。</p>
<p><strong>表 1. ulimit 参数说明</strong></p>
<p>选项 [options]</p>
<p>含义</p>
<p>例子</p>
<p>-H</p>
<p>设置硬资源限制，一旦设置不能增加。</p>
<p>ulimit – Hs 64；限制硬资源，线程栈大小为 64K。</p>
<p>-S</p>
<p>设置软资源限制，设置后可以增加，但是不能超过硬资源设置。</p>
<p>ulimit – Sn 32；限制软资源，32 个文件描述符。</p>
<p>-a</p>
<p>显示当前所有的 limit 信息。</p>
<p>ulimit – a；显示当前所有的 limit 信息。</p>
<p>-c</p>
<p>最大的 core 文件的大小， 以 blocks 为单位。</p>
<p>ulimit – c unlimited； 对生成的 core 文件的大小不进行限制。</p>
<p>-d</p>
<p>进程最大的数据段的大小，以 Kbytes 为单位。</p>
<p>ulimit -d unlimited；对进程的数据段大小不进行限制。</p>
<p>-f</p>
<p>进程可以创建文件的最大值，以 blocks 为单位。</p>
<p>ulimit – f 2048；限制进程可以创建的最大文件大小为 2048 blocks。</p>
<p>-l</p>
<p>最大可加锁内存大小，以 Kbytes 为单位。</p>
<p>ulimit – l 32；限制最大可加锁内存大小为 32 Kbytes。</p>
<p>-m</p>
<p>最大内存大小，以 Kbytes 为单位。</p>
<p>ulimit – m unlimited；对最大内存不进行限制。</p>
<p>-n</p>
<p>可以打开最大文件描述符的数量。</p>
<p>ulimit – n 128；限制最大可以使用 128 个文件描述符。</p>
<p>-p</p>
<p>管道缓冲区的大小，以 Kbytes 为单位。</p>
<p>ulimit – p 512；限制管道缓冲区的大小为 512 Kbytes。</p>
<p>-s</p>
<p>线程栈大小，以 Kbytes 为单位。</p>
<p>ulimit – s 512；限制线程栈的大小为 512 Kbytes。</p>
<p>-t</p>
<p>最大的 CPU 占用时间，以秒为单位。</p>
<p>ulimit – t unlimited；对最大的 CPU 占用时间不进行限制。</p>
<p>-u</p>
<p>用户最大可用的进程数。</p>
<p>ulimit – u 64；限制用户最多可以使用 64 个进程。</p>
<p>-v</p>
<p>进程最大可用的虚拟内存，以 Kbytes 为单位。</p>
<p>ulimit – v 200000；限制最大可用的虚拟内存为 200000 Kbytes。</p>
<p>新装的linux默认只有1024，当作负载较大的服务器时，很容易遇到error: too many open files。因此，需要将其改大。 使用 ulimit -n 65535 可即时修改，但重启后就无效了。（注ulimit -SHn 65535 等效 ulimit -n 65535，-S指soft，-H指hard) 有如下三种修改方式： 1.在/etc/rc.local 中增加一行 ulimit -SHn 65535 2.在/etc/profile 中增加一行 ulimit -SHn 65535 3.在/etc/security/limits.conf最后增加如下两行记录 * soft nofile 65535 * hard nofile 65535 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/04/QQ%E6%88%AA%E5%9B%BE20150423202805.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/04/QQ%E6%88%AA%E5%9B%BE20150423202805.png" alt="QQ截图20150423202805"></a></p>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>ulimit</tag>
      </tags>
  </entry>
  <entry>
    <title>通过Inode删除linux下的文件</title>
    <url>/2015/09/19/e9-80-9a-e8-bf-87inode-e5-88-a0-e9-99-a4linux-e4-b8-8b-e7-9a-84-e6-96-87-e4-bb-b6/</url>
    <content><![CDATA[<p>由于某些原因在我们linux系统上面总会出现一些乱码文件，或者不能正常输入的文件名，当遇到这些无法正常输入的文件名要删除的时候就需要使用文件对应的inode号对文件进行删除。 inode的原理这里就不再说了，具体说明参见：<a href="http://www.ruanyifeng.com/blog/2011/12/inode.html">http://www.ruanyifeng.com/blog/2011/12/inode.html</a> 下面这个目录下的文件是我在网上下载的一个网页模板，里面包含了一个不能rm 的文件： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-19-132604.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-19-132604.png" alt="Screenshot from 2015-09-19 13:26:04"></a> 那从何得知 -?+?.txt 这个文件的inode号呢，ls&nbsp; 命令有个参数 -i</p>
<p>  -i, –inode                print the index number of each file</p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-19-133027.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-19-133027.png" alt="Screenshot from 2015-09-19 13:30:27"></a> 上图中：<strong>291606</strong>&nbsp; 这个号码就是 这个文件的inode 号啦。然后我们结合find命令就可以将它删除啦  <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-19-133514.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-19-133514.png" alt="Screenshot from 2015-09-19 13:35:14"></a></p>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>inode</tag>
      </tags>
  </entry>
  <entry>
    <title>ELK+Kafka 企业日志收集平台(一)</title>
    <url>/2015/11/14/elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-b8-80/</url>
    <content><![CDATA[<h3 id="背景："><a href="#背景：" class="headerlink" title="背景："></a><strong>背景：</strong></h3><p>最近线上上了ELK，但是只用了一台Redis在中间作为消息队列，以减轻前端es集群的压力，Redis的集群解决方案暂时没有接触过，并且Redis作为消息队列并不是它的强项；所以最近将Redis换成了专业的消息信息发布订阅系统Kafka, Kafka的更多介绍大家可以看这里：<a href="http://blog.csdn.net/lizhitao/article/details/39499283">传送门</a>&nbsp; ,关于ELK的知识网上有很多的哦，&nbsp;此篇博客主要是总结一下目前线上这个平台的实施步骤，ELK是怎么跟Kafka结合起来的。好吧，动手！</p>
<h3 id="ELK架构拓扑："><a href="#ELK架构拓扑：" class="headerlink" title="ELK架构拓扑："></a><strong>ELK架构拓扑：</strong></h3><p>然而我这里的整个日志收集平台就是这样的拓扑： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/1.png" alt="1"></a> 1，使用一台Nginx代理访问kibana的请求; 2，两台es组成es集群，并且在两台es上面都安装kibana;（以下对elasticsearch简称es） 3，中间三台服务器就是我的kafka(zookeeper)集群啦; 上面写的消费者/生产者这是kafka(zookeeper)中的概念; 4，最后面的就是一大堆的生产服务器啦，上面使用的是logstash，当然除了logstash也可以使用其他的工具来收集你的应用程序的日志，例如：Flume，Scribe，Rsyslog，Scripts…… <strong>角色：</strong> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/11111.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/11111.png" alt="11111"></a> <strong>软件选用：</strong></p>
<p>elasticsearch-1.7.3.tar.gz #这里需要说明一下，前几天使用了最新的elasticsearch2.0，java-1.8.0报错，目前未找到原因，故这里使用1.7.3版本<br>Logstash-2.0.0.tar.gz<br>kibana-4.1.2-linux-x64.tar.gz<br>以上软件都可以从官网下载:<a href="https://www.elastic.co/downloads">https://www.elastic.co/downloads</a></p>
<p>java-1.8.0，nginx采用yum安装</p>
<p><strong>部署步骤：</strong> 1.ES集群安装配置; 2.Logstash客户端配置(直接写入数据到ES集群，写入系统messages日志); 3.Kafka(zookeeper)集群配置;(Logstash写入数据到Kafka消息系统); 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用;</p>
<h3 id="ES集群安装配置"><a href="#ES集群安装配置" class="headerlink" title="ES集群安装配置;"></a>ES集群安装配置;</h3><p>es1.example.com: 1.安装java-1.8.0以及依赖包</p>
<p>yum install -y epel-release<br>yum install -y java-1.8.0 git wget lrzsz</p>
<p>2.获取es软件包</p>
<p>wget <a href="https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.3.tar.gz">https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.3.tar.gz</a><br>tar -xf elasticsearch-1.7.3.tar.gz -C /usr/local<br>ln -sv /usr/local/elasticsearch-1.7.3 /usr/local/elasticsearch</p>
<p>3.修改配置文件</p>
<p>[root@es1 ~]# vim /usr/local/elasticsearch/config/elasticsearch.yml<br>32 cluster.name: es-cluster                         #组播的名称地址<br>40 node.name: “es-node1 “                           #节点名称，不能和其他节点重复<br>47 node.master: true                                #节点能否被选举为master<br>51 node.data: true                                  #节点是否存储数据<br>107 index.number_of_shards: 5                       #索引分片的个数<br>111 index.number_of_replicas: 1                     #分片的副本个数<br>145 path.conf: /usr/local/elasticsearch/config/     #配置文件的路径<br>149 path.data: /data/es/data                        #数据目录路径<br>159 path.work: /data/es/worker                      #工作目录路径<br>163 path.logs:  /usr/local/elasticsearch/logs/      #日志文件路径<br>167 path.plugins:  /data/es/plugins                 #插件路径<br>184 bootstrap.mlockall: true                        #内存不向swap交换<br>232 http.enabled: true                              #启用http</p>
<p>4.创建相关目录</p>
<p>mkdir /data/es/{data,worker,plugins} -p</p>
<p>5.获取es服务管理脚本</p>
<p>​[root@es1 ~]# git clone <a href="https://github.com/elastic/elasticsearch-servicewrapper.git">https://github.com/elastic/elasticsearch-servicewrapper.git</a><br>[root@es1 ~]# mv elasticsearch-servicewrapper/service /usr/local/elasticsearch/bin/<br>[root@es1 ~]# /usr/local/elasticsearch/bin/service/elasticsearch install<br>Detected RHEL or Fedora:<br>Installing the Elasticsearch daemon..<br>[root@es1 ~]#<br>#这时就会在/etc/init.d/目录下安装上es的管理脚本啦</p>
<p>#修改其配置:<br>[root@es1 ~]#<br>set.default.ES_HOME=/usr/local/elasticsearch   #安装路径<br>set.default.ES_HEAP_SIZE=1024                  #jvm内存大小，根据实际环境调整即可</p>
<p>6.启动es ，并检查其服务是否正常</p>
<p>[root@es1 ~]# netstat -nlpt | grep -E “9200|”9300<br>tcp        0      0 0.0.0.0:9200                0.0.0.0:*                   LISTEN      1684/java<br>tcp        0      0 0.0.0.0:9300                0.0.0.0:*                   LISTEN      1684/java</p>
<p>访问<a href="http://192.168.2.18:9200/">http://192.168.2.18:9200/</a> 如果出现以下提示信息说明安装配置完成啦， <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/2.png" alt="2"></a> 7.es1节点好啦，我们直接把目录复制到es2</p>
<p>[root@es1 local]# scp -r elasticsearch-1.7.3  192.168.12.19:/usr/local/</p>
<p>[root@es2 local]# ln -sv elasticsearch-1.7.3 elasticsearch<br>[root@es2 local]# elasticsearch/bin/service/elasticsearch install</p>
<p>#es2只需要修改node.name即可，其他都与es1相同配置</p>
<p>8.安装es的管理插件 es官方提供一个用于管理es的插件，可清晰直观看到es集群的状态，以及对集群的操作管理，安装方法如下：</p>
<p>[root@es1 local]# /usr/local/elasticsearch/bin/plugin -i mobz/elasticsearch-head</p>
<p>安装好之后，访问方式为： <a href="http://192.168.2.18:9200/_plugin/head%EF%BC%8C%E7%94%B1%E4%BA%8E%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%8E%B0%E5%9C%A8%E6%9A%82%E6%97%B6%E6%B2%A1%E6%9C%89%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%89%80%E4%BB%A5%E6%98%BE%E7%A4%BA%E4%B8%BA%E7%A9%BA">http://192.168.2.18:9200/_plugin/head，由于集群中现在暂时没有数据，所以显示为空</a>, <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/3.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/3.png" alt="3"></a> &nbsp; &nbsp; &nbsp; <strong>此时，es集群的部署完成。</strong></p>
<h3 id="Logstash客户端安装配置"><a href="#Logstash客户端安装配置" class="headerlink" title="Logstash客户端安装配置;"></a>Logstash客户端安装配置;</h3><p>在webserve1上面安装Logstassh 1.downloads &nbsp;软件包 ，这里注意，Logstash是需要依赖java环境的，所以这里还是需要yum install -y java-1.8.0.</p>
<p>[root@webserver1 ~]# wget <a href="https://download.elastic.co/logstash/logstash/logstash-2.0.0.tar.gz">https://download.elastic.co/logstash/logstash/logstash-2.0.0.tar.gz</a><br>[root@webserver1 ~]# tar -xf logstash-2.0.0.tar.gz -C /usr/local<br>[root@webserver1 ~]# cd /usr/local/<br>[root@webserver1 local]# ln -sv logstash-2.0.0 logstash<br>[root@webserver1 local]# mkdir logs etc</p>
<p>2.提供logstash管理脚本，其中里面的配置路径可根据实际情况修改</p>
<p>#!/bin/bash<br>#chkconfig: 2345 55 24<br>#description: logstash service manager<br>#auto: Maoqiu Guo<br>FILE=’/usr/local/logstash/etc/*.conf’    #logstash配置文件<br>LOGBIN=’/usr/local/logstash/bin/logstash agent –verbose –config’  #指定logstash配置文件的命令<br>LOCK=’/usr/local/logstash/locks’         #用锁文件配合服务启动与关闭<br>LOGLOG=’–log /usr/local/logstash/logs/stdou.log’  #日志</p>
<p>START() {<br>	if [ -f $LOCK ];then<br>		echo -e “Logstash is already \033[32mrunning\033[0m, do nothing.”<br>	else<br>		echo -e “Start logstash service.\033[32mdone\033[m”<br>		nohup ${LOGBIN} ${FILE} ${LOGLOG} &amp;<br>		touch $LOCK<br>	fi<br>}</p>
<p>STOP() {<br>	if [ ! -f $LOCK ];then<br>		echo -e “Logstash is already stop, do nothing.”<br>	else<br>		echo -e “Stop logstash serivce \033[32mdone\033[m”<br>		rm -rf $LOCK<br>		ps -ef | grep logstash | grep -v “grep” | awk ‘{print $2}’ | xargs kill -s 9 &gt;/dev/null<br>	fi<br>}</p>
<p>STATUS() {<br>	ps aux | grep logstash | grep -v “grep” &gt;/dev/null<br>	if [ -f $LOCK ] &amp;&amp; [ $? -eq 0 ]; then<br>		echo -e “Logstash is: \033[32mrunning\033[0m…”<br>	else<br>		echo -e “Logstash is: \033[31mstopped\033[0m…”<br>	fi<br>}</p>
<p>TEST(){<br>	${LOGBIN} ${FILE} –configtest<br>}</p>
<p>case “$1” in<br>  start)<br>	START<br>	;;<br>  stop)<br>	STOP<br>	;;<br>  status)<br>	STATUS<br>	;;<br>  restart)<br>	STOP<br>        sleep 2<br>        START<br>	;;<br>  test)<br>	TEST<br>	;;<br>  *)<br>	echo “Usage: /etc/init.d/logstash (test|start|stop|status|restart)”<br>	;;<br>esac</p>
<p>3.Logstash 向es集群写数据 (1)编写一个logstash配置文件</p>
<p>[root@webserver1 etc]# cat logstash.conf<br>input {              #数据的输入从标准输入<br>  stdin {}<br>}</p>
<p>output {             #数据的输出我们指向了es集群<br>  elasticsearch {<br>    hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”]　　　＃es主机的ip及端口<br>  }<br>}<br>[root@webserver1 etc]#</p>
<p>(2)检查配置文件是否有语法错</p>
<p>[root@webserver1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf –configtest –verbose<br>Configuration OK<br>[root@webserver1 etc]# </p>
<p>(3)既然配置ok我们手动启动它，然后写点东西看能否写到es <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/4.png.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/4.png.png" alt="4.png"></a> ok.上图已经看到logstash已经可以正常的工作啦. ４.下面演示一下如何收集系统日志 将之前的配置文件修改如下所示内容，然后启动logstash服务就可以在web页面中看到messages的日志写入es，并且创建了一条索引</p>
<p>[root@webserver1 etc]# cat logstash.conf<br>input {　　　　　　　#这里的输入使用的文件，即日志文件messsages<br>  file {　　　<br>    path =&gt; “/var/log/messages”　　　＃这是日志文件的绝对路径<br>    start_position =&gt; “beginning”　＃这个表示从messages的第一行读取，即文件开始处<br>  }<br>}</p>
<p>output {　　　　＃输出到es<br>  elasticsearch {<br>    hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”]<br>    index =&gt; “system-messages-%{+YYYY-MM}”　　＃这里将按照这个索引格式来创建索引<br>  }<br>}<br>[root@webserver1 etc]#</p>
<p>启动logstash后，我们来看head这个插件的web页面 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/5.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/5.png" alt="5"></a> ok，系统日志我们已经成功的收集，并且已经写入到es集群中，那上面的演示是logstash直接将日志写入到es集群中的，这种场合我觉得如果量不是很大的话直接像上面已将将输出output定义到es集群即可，如果量大的话需要加上消息队列来缓解es集群的压力。前面已经提到了我这边之前使用的是单台redis作为消息队列，但是redis不能作为list类型的集群，也就是redis单点的问题没法解决，所以这里我选用了kafka ;下面就在三台server上面安装kafka集群</p>
<h3 id="Kafka集群安装配置"><a href="#Kafka集群安装配置" class="headerlink" title="Kafka集群安装配置;"></a>Kafka集群安装配置;</h3><p>在搭建kafka集群时，需要提前安装zookeeper集群，当然kafka已经自带zookeeper程序只需要解压并且安装配置就行了 kafka1上面的配置： 1.获取软件包.官网：<a href="http://kafka.apache.org/">http://kafka.apache.org</a></p>
<p>[root@kafka1 ~]# wget <a href="http://mirror.rise.ph/apache/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz">http://mirror.rise.ph/apache/kafka/0.8.2.1/kafka_2.11-0.8.2.1.tgz</a><br>[root@kafka1 ~]# tar -xf kafka_2.11-0.8.2.1.tgz -C /usr/local/<br>[root@kafka1 ~]# cd /usr/local/<br>[root@kafka1 local]# ln -sv kafka_2.11-0.8.2.1 kafka</p>
<p>2.配置zookeeper集群，修改配置文件</p>
<p>[root@kafka1 ~]# vim /usr/local/kafka/config/zookeeper.propertie<br>dataDir=/data/zookeeper<br>clientPort=2181<br>tickTime=2000<br>initLimit=20<br>syncLimit=10<br>server.2=192.168.2.22:2888:3888<br>server.3=192.168.2.23:2888:3888<br>server.4=192.168.2.24:2888:3888</p>
<p>＃说明：<br>tickTime: 这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。<br>2888端口：表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；<br>3888端口：表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p>
<p>3.创建zookeeper所需要的目录</p>
<p>[root@kafka1 ~]# mkdir /data/zookeeper</p>
<p>4.在/data/zookeeper目录下创建myid文件，里面的内容为数字，用于标识主机，如果这个文件没有的话，zookeeper是没法启动的哦</p>
<p>[root@kafka1 ~]# echo 2 &gt; /data/zookeeper/myid</p>
<p>以上就是zookeeper集群的配置，下面等我配置好kafka之后直接复制到其他两个节点即可 5.kafka配置</p>
<p>[root@kafka1 ~]# vim /usr/local/kafka/config/server.properties<br>broker.id=2    　　　　    ＃　唯一，填数字，本文中分别为2/3/4<br>prot=9092　　　　　　　     ＃　这个broker监听的端口　<br>host.name=192.168.2.22　  ＃　唯一，填服务器IP<br>log.dir=/data/kafka-logs  #  该目录可以不用提前创建，在启动时自己会创建<br>zookeeper.connect=192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181　　＃这个就是zookeeper的ip及端口<br>num.partitions=16         # 需要配置较大 分片影响读写速度<br>log.dirs=/data/kafka-logs # 数据目录也要单独配置磁盘较大的地方<br>log.retention.hours=168   # 时间按需求保留过期时间 避免磁盘满</p>
<p>6.将kafka(zookeeper)的程序目录全部拷贝至其他两个节点</p>
<p>[root@kafka1 ~]# scp -r /usr/local/kafka 192.168.2.23:/usr/local/<br>[root@kafka1 ~]# scp -r /usr/local/kafka 192.168.2.24:/usr/local/</p>
<p>7.修改两个借点的配置，注意这里除了以下两点不同外，都是相同的配置</p>
<p>（1）zookeeper的配置<br>mkdir /data/zookeeper<br>echo “x” &gt; /data/zookeeper/myid<br>（2）kafka的配置<br>broker.id=2<br>host.name=192.168.2.22</p>
<p>8.修改完毕配置之后我们就可以启动了，这里先要启动zookeeper集群，才能启动kafka 我们按照顺序来，kafka1 –&gt; kafka2 –&gt;kafka3</p>
<p>[root@kafka1 ~]# /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties &amp;   #zookeeper启动命令<br>[root@kafka1 ~]# /usr/local/kafka/bin/zookeeper-server-stop.sh                                                   #zookeeper停止的命令</p>
<p>注意，如果zookeeper有问题 nohup的日志文件会非常大，把磁盘占满，这个zookeeper服务可以通过自己些服务脚本来管理服务的启动与关闭。 后面两台执行相同操作，在启动过程当中会出现以下报错信息</p>
<p>[2015-11-13 19:18:04,225] WARN Cannot open channel to 3 at election address /192.168.2.23:3888 (org.apache.zookeeper.server.quorum.QuorumCnxManager)<br>java.net.ConnectException: Connection refused<br>	at java.net.PlainSocketImpl.socketConnect(Native Method)<br>	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)<br>	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)<br>	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)<br>	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)<br>	at java.net.Socket.connect(Socket.java:589)<br>	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368)<br>	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402)<br>	at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840)<br>	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)<br>[2015-11-13 19:18:04,232] WARN Cannot open channel to 4 at election address /192.168.2.24:3888 (org.apache.zookeeper.server.quorum.QuorumCnxManager)<br>java.net.ConnectException: Connection refused<br>	at java.net.PlainSocketImpl.socketConnect(Native Method)<br>	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)<br>	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)<br>	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)<br>	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)<br>	at java.net.Socket.connect(Socket.java:589)<br>	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368)<br>	at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402)<br>	at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840)<br>	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)<br>[2015-11-13 19:18:04,233] INFO Notification time out: 6400 (org.apache.zookeeper.server.quorum.FastLeaderElection)</p>
<p>由于zookeeper集群在启动的时候，每个结点都试图去连接集群中的其它结点，先启动的肯定连不上后面还没启动的，所以上面日志前面部分的异常是可以忽略的。通过后面部分可以看到，集群在选出一个Leader后，最后稳定了。 其他节点也可能会出现类似的情况，属于正常。 9.zookeeper服务检查</p>
<p>[root@kafka1~]#  netstat -nlpt | grep -E “2181|2888|3888”<br>tcp        0      0 192.168.2.24:3888           0.0.0.0:*                   LISTEN      1959/java<br>tcp        0      0 0.0.0.0:2181                0.0.0.0:*                   LISTEN      1959/java                       </p>
<p>[root@kafka2 ~]#  netstat -nlpt | grep -E “2181|2888|3888”<br>tcp        0      0 192.168.2.23:3888           0.0.0.0:*                   LISTEN      1723/java<br>tcp        0      0 0.0.0.0:2181                0.0.0.0:*                   LISTEN      1723/java           </p>
<p>[root@kafka3 ~]#  netstat -nlpt | grep -E “2181|2888|3888”<br>tcp        0      0 192.168.2.24:3888           0.0.0.0:*                   LISTEN      950/java<br>tcp        0      0 0.0.0.0:2181                0.0.0.0:*                   LISTEN      950/java<br>tcp        0      0 192.168.2.24:2888           0.0.0.0:*                   LISTEN      950/java            </p>
<p>#可以看出，如果哪台是Leader,那么它就拥有2888这个端口</p>
<p>ok. &nbsp;这时候zookeeper集群已经启动起来了，下面启动kafka，也是依次按照顺序启动</p>
<p>[root@kafka1 ~]# nohup /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &amp;   #kafka启动的命令<br>[root@kafka1 ~]#  /usr/local/kafka/bin/kafka-server-stop.sh                                                         #kafka停止的命令</p>
<p>注意，跟zookeeper服务一样，如果kafka有问题 nohup的日志文件会非常大,把磁盘占满，这个kafka服务同样可以通过自己些服务脚本来管理服务的启动与关闭。 此时三台上面的zookeeper及kafka都已经启动完毕，来检测以下吧 (1)建立一个主题</p>
<p>[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 3 –partitions 1 –topic summer<br>#注意：factor大小不能超过broker数</p>
<p>(2)查看有哪些主题已经创建</p>
<p>[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –list –zookeeper 192.168.2.22:2181   #列出集群中所有的topic<br>summer  #已经创建成功</p>
<p>(3)查看summer这个主题的详情</p>
<p>[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –describe –zookeeper 192.168.2.22:2181 –topic summer<br>Topic:summer	PartitionCount:1	ReplicationFactor:3	Configs:<br>	Topic: summer	Partition: 0	Leader: 2	Replicas: 2,4,3	Isr: 2,4,3</p>
<p>#主题名称：summer<br>#Partition:只有一个，从0开始<br>#leader ：id为2的broker<br>#Replicas 副本存在于broker id为2,3,4的上面<br>#Isr:活跃状态的broker</p>
<p>(4)发送消息，这里使用的是生产者角色</p>
<p>[root@kafka1 ~]# /bin/bash /usr/local/kafka/bin/kafka-console-producer.sh –broker-list 192.168.2.22:9092 –topic summer<br>This is a messages<br>welcome to kafka    </p>
<p>(5)接收消息，这里使用的是消费者角色</p>
<p>[root@kafka2 ~]# /usr/local/kafka/bin/kafka-console-consumer.sh –zookeeper  192.168.2.24:2181 –topic summer –from-beginning<br>This is a messages<br>welcome to kafka</p>
<p>如果能够像上面一样能够接收到生产者发过来的消息，那说明基于kafka的zookeeper集群就成功啦。 10，下面我们将webserver1上面的logstash的输出改到kafka上面，将数据写入到kafka中 (1)修改webserver1上面的logstash配置，如下所示：各个参数可以到<a href="https://www.elastic.co/">官网</a>查询.</p>
<p>root@webserver1 etc]# cat logstash.conf<br>input {             #这里的输入还是定义的是从日志文件输入<br>  file {<br>    type =&gt; “system-message”<br>    path =&gt; “/var/log/messages”<br>    start_position =&gt; “beginning”<br>  }<br>}</p>
<p>output {<br>    #stdout { codec =&gt; rubydebug }   #这是标准输出到终端，可以用于调试看有没有输出，注意输出的方向可以有多个<br>    kafka {   #输出到kafka<br>      bootstrap_servers =&gt; “192.168.2.22:9092,192.168.2.23:9092,192.168.2.24:9092”   #他们就是生产者<br>      topic_id =&gt; “system-messages”  #这个将作为主题的名称，将会自动创建<br>      compression_type =&gt; “snappy”   #压缩类型<br>    }<br>}<br>[root@webserver1 etc]#</p>
<p>(2)配置检测</p>
<p>[root@webserver1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf –configtest –verbose<br>Configuration OK<br>[root@webserver1 etc]# </p>
<p>(2)启动Logstash，这里我直接在命令行执行即可</p>
<p>[root@webserver1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf</p>
<p>(3)验证数据是否写入到kafka，这里我们检查是否生成了一个叫system-messages的主题</p>
<p>[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –list –zookeeper 192.168.2.22:2181<br>summer<br>system-messages   #可以看到这个主题已经生成了</p>
<p>#再看看这个主题的详情:<br>[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –describe –zookeeper 192.168.2.22:2181 –topic system-messages<br>Topic:system-messages	PartitionCount:16	ReplicationFactor:1	Configs:<br>	Topic: system-messages	Partition: 0	Leader: 2	Replicas: 2	Isr: 2<br>	Topic: system-messages	Partition: 1	Leader: 3	Replicas: 3	Isr: 3<br>	Topic: system-messages	Partition: 2	Leader: 4	Replicas: 4	Isr: 4<br>	Topic: system-messages	Partition: 3	Leader: 2	Replicas: 2	Isr: 2<br>	Topic: system-messages	Partition: 4	Leader: 3	Replicas: 3	Isr: 3<br>	Topic: system-messages	Partition: 5	Leader: 4	Replicas: 4	Isr: 4<br>	Topic: system-messages	Partition: 6	Leader: 2	Replicas: 2	Isr: 2<br>	Topic: system-messages	Partition: 7	Leader: 3	Replicas: 3	Isr: 3<br>	Topic: system-messages	Partition: 8	Leader: 4	Replicas: 4	Isr: 4<br>	Topic: system-messages	Partition: 9	Leader: 2	Replicas: 2	Isr: 2<br>	Topic: system-messages	Partition: 10	Leader: 3	Replicas: 3	Isr: 3<br>	Topic: system-messages	Partition: 11	Leader: 4	Replicas: 4	Isr: 4<br>	Topic: system-messages	Partition: 12	Leader: 2	Replicas: 2	Isr: 2<br>	Topic: system-messages	Partition: 13	Leader: 3	Replicas: 3	Isr: 3<br>	Topic: system-messages	Partition: 14	Leader: 4	Replicas: 4	Isr: 4<br>	Topic: system-messages	Partition: 15	Leader: 2	Replicas: 2	Isr: 2<br>[root@kafka1 ~]# </p>
<p>可以看出，这个主题生成了16个分区，每个分区都有对应自己的Leader，但是我想要有10个分区，3个副本如何办？还是跟我们上面一样命令行来创建主题就行，当然对于logstash输出的我们也可以提前先定义主题，然后启动logstash 直接往定义好的主题写数据就行啦，命令如下：</p>
<p>[root@kafka1 ~]# /usr/local/kafka/bin/kafka-topics.sh –create –zookeeper 192.168.2.22:2181 –replication-factor 3 –partitions 10 –topic TOPIC_NAME</p>
<p>好了，我们将logstash收集到的数据写入到了kafka中了，在实验过程中我使用while脚本测试了如果不断的往kafka写数据的同时停掉两个节点，数据写入没有任何问题。 那如何将数据从kafka中读取然后给我们的es集群呢？那下面我们在kafka集群上安装Logstash，安装步骤不再赘述；三台上面的logstash 的配置如下，作用是将kafka集群的数据读取然后转交给es集群，这里为了测试我让他新建一个索引文件，注意这里的输入日志还是messages，主题名称还是“system-messages”</p>
<p>[root@kafka1 etc]# more logstash.conf<br>input {<br>    kafka {<br>        zk_connect =&gt; “192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181”   #消费者们<br>        topic_id =&gt; “system-messages”<br>        codec =&gt; plain<br>        reset_beginning =&gt; false<br>        consumer_threads =&gt; 5<br>        decorate_events =&gt; true<br>    }<br>}</p>
<p>output {<br>    elasticsearch {<br>      hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”]<br>      index =&gt; “test-system-messages-%{+YYYY-MM}”           #为了区分之前实验，我这里新生成的所以名字为“test-system-messages-%{+YYYY-MM}”<br>  }<br>  }</p>
<p>在三台kafka上面启动Logstash，注意我这里是在命令行启动的；</p>
<p>[root@kafka1 etc]# pwd<br>/usr/local/logstash/etc<br>[root@kafka1 etc]# /usr/local/logstash/bin/logstash -f logstash.conf<br>[root@kafka2 etc]# pwd<br>/usr/local/logstash/etc<br>[root@kafka2 etc]# /usr/local/logstash/bin/logstash -f logstash.conf<br>[root@kafka3 etc]# pwd<br>/usr/local/logstash/etc<br>[root@kafka3 etc]# /usr/local/logstash/bin/logstash -f logstash.conf </p>
<p>在webserver1上写入测试内容，即webserver1上面利用message这个文件来测试，我先将其清空，然后启动</p>
<p>[root@webserver1 etc]# &gt;/var/log/messages<br>[root@webserver1 etc]# echo “我将通过kafka集群达到es集群哦^0^” &gt;&gt; /var/log/messages<br>#启动logstash,让其读取messages中的内容</p>
<p>下图为我在客户端写入到kafka集群的同时也将其输入到终端，这里写入了三条内容 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/6.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/6.png" alt="6"></a> 而下面三张图侧可以看出，三台Logstash 很平均的从kafka集群当中读取出来了日志内容 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/7.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/7.png" alt="7"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/9.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/9.png" alt="9"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/8.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/8.png" alt="8"></a> 再来看看我们的es管理界面 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/10.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/10.png" alt="10"></a> ok ,看到了吧， 流程差不多就是下面 酱紫咯 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/111.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/111.png" alt="111"></a> 由于篇幅较长，我将 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用; 放到下一篇博客。</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>ELK+Kafka 企业日志收集平台(二)</title>
    <url>/2015/11/14/elkkafka-e4-bc-81-e4-b8-9a-e6-97-a5-e5-bf-97-e6-94-b6-e9-9b-86-e5-b9-b3-e5-8f-b0-e4-ba-8c/</url>
    <content><![CDATA[<p>上篇博文主要总结了一下elk、基于kafka的zookeeper集群搭建，以及系统日志通过zookeeper集群达到我们集群的整个过程。下面我们接着下面这个未完成的几个主题 4.Kibana部署; 5.Nginx负载均衡Kibana请求; 6.案例：nginx日志收集以及MySQL慢日志收集; 7.Kibana报表基本使用; &nbsp;</p>
<h3 id="Kibana的部署"><a href="#Kibana的部署" class="headerlink" title="Kibana的部署;"></a>Kibana的部署;</h3><p>Kibana的作用，想必大家都知道了就是一个展示工具，报表内容非常的丰富； 下面我们在两台es上面搭建两套kibana 1.获取kibana软件包</p>
<p>[root@es1 ~]# wget <a href="https://download.elastic.co/kibana/kibana/kibana-4.1.2-linux-x64.tar.gz">https://download.elastic.co/kibana/kibana/kibana-4.1.2-linux-x64.tar.gz</a><br>[root@es1 ~]# tar -xf kibana-4.2.0-linux-x64.tar.gz -C /usr/local/</p>
<p>2.修改配置文件</p>
<p>[root@es1 ~]# cd /usr/local/<br>[root@es1 local]# ln -sv kibana-4.1.2-linux-x64 kibana<br>`kibana’ -&gt; `kibana-4.2.0-linux-x64’<br>[root@es1 local]# cd kibana</p>
<p>[root@es1 kibana]# vim config/kibana.yml<br>server.port: 5601      #默认端口可以修改的<br>server.host: “0.0.0.0” #kibana监听的ip<br>elasticsearch.url: “<a href="http://localhost:9200/">http://localhost:9200</a>“ #由于es在本地主机上面，所以这个选项打开注释即可</p>
<p>3.提供kibana服务管理脚本，我这里写了个相对简单的脚本</p>
<p>[root@es1 config]# cat /etc/init.d/kibana<br>#!/bin/bash<br>#chkconfig: 2345 55 24<br>#description: kibana service manager</p>
<p>KIBBIN=’/usr/local/kibana/bin/kibana’<br>LOCK=’/usr/local/kibana/locks’</p>
<p>START() {<br>	if [ -f $LOCK ];then<br>		echo -e “kibana is already \033[32mrunning\033[0m, do nothing.”<br>	else<br>		echo -e “Start kibana service.\033[32mdone\033[m”<br>		cd  /usr/local/kibana/bin<br>    	nohup ./kibana &amp; &gt;/dev/null<br> 		touch $LOCK<br>	fi<br>}</p>
<p>STOP() {<br>	if [ ! -f $LOCK ];then<br>		echo -e “kibana is already stop, do nothing.”<br>	else<br>		echo -e “Stop kibana serivce \033[32mdone\033[m”<br>		rm -rf $LOCK<br>		ps -ef | grep kibana | grep -v “grep” | awk ‘{print $2}’ | xargs kill -s 9 &gt;/dev/null<br>	fi<br>}</p>
<p>STATUS() {<br>        Port=$(netstat -tunl | grep “:5602”)<br>	if [ “$Port” != “” ] &amp;&amp; [ -f $LOCK ];then<br>		echo -e “kibana is: \033[32mrunning\033[0m…”<br>	else<br>		echo -e “kibana is: \033[31mstopped\033[0m…”<br>	fi<br>}</p>
<p>case “$1” in<br>  start)<br>	START<br>	;;<br>  stop)<br>	STOP<br>	;;<br>  status)<br>	STATUS<br>	;;<br>  restart)<br>	STOP<br>    sleep 2<br>    START<br>	;;<br>  *)<br>	echo “Usage: /etc/init.d/kibana (|start|stop|status|restart)”<br>	;;<br>esac</p>
<p>4.启动kibana服务</p>
<p>[root@es1 config]# chkconfig –add kibana<br>[root@es1 config]# service kibana start<br>Start kibana service.done<br>[root@es1 config]#</p>
<p>5.服务检查</p>
<p>[root@es1 config]# ss -tunl | grep “5601”<br>tcp    LISTEN     0      511                    *:5601                  <em>:</em><br>[root@es1 config]#</p>
<p>ok，此时我直接访问es1这台主机的5601端口 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/11.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/11.png" alt="11"></a> ok，能成功的访问5601端口，那我把es1这台的配置放到es2上面去然后启动，效果跟访问es1一样</p>
<h3 id="Nginx负载均衡kibana的请求"><a href="#Nginx负载均衡kibana的请求" class="headerlink" title="Nginx负载均衡kibana的请求"></a>Nginx负载均衡kibana的请求</h3><p>1.在nginx-proxy上面yum安装nginx</p>
<p>yum install -y nignx</p>
<p>2.编写配置文件es.conf</p>
<p>[root@saltstack-node1 conf.d]# pwd<br>/etc/nginx/conf.d<br>[root@saltstack-node1 conf.d]# cat es.conf<br>upstream es {<br>    server 192.168.2.18:5601 max_fails=3 fail_timeout=30s;<br>    server 192.168.2.19:5601 max_fails=3 fail_timeout=30s;<br>}</p>
<p>server {<br>    listen       80;<br>    server_name  localhost;</p>
<pre><code>location / {
    proxy_pass http://es/;
    index index.html index.htm;
    #auth
    auth_basic "ELK Private";
    auth\_basic\_user_file /etc/nginx/.htpasswd;
}
</code></pre>
<p> }</p>
<p>3.创建认证</p>
<p>[root@saltstack-node1 conf.d]# htpasswd -cm /etc/nginx/.htpasswd elk<br>New password:<br>Re-type new password:<br>Adding password for user elk-user<br>[root@saltstack-node1 conf.d]# /etc/init.d/nginx restart<br>Stopping nginx:                                            [  OK  ]<br>Starting nginx:                                            [  OK  ]<br>[root@saltstack-node1 conf.d]# </p>
<p>4.直接输入认证用户及密码就可访问啦<a href="http://192.168.2.21/">http://192.168.2.21/</a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/22.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/22.png" alt="22"></a></p>
<h3 id="Nginx及MySQL慢日志收集"><a href="#Nginx及MySQL慢日志收集" class="headerlink" title="Nginx及MySQL慢日志收集"></a>Nginx及MySQL慢日志收集</h3><p>首先我们在webserver1上面都分别安装了nginx 及mysql. 1.为了方便nginx日志的统计搜索，这里设置nginx访问日志格式为json (1)修改nginx主配置文件 说明：如果想实现日志的报表展示，最好将业务日志直接以json格式输出，这样可以极大减轻cpu负载，也省得运维需要写负载的filter过滤正则。</p>
<p>[root@webserver1 nginx]# vim nginx.conf<br>log_format json ‘{“@timestamp”:”$time_iso8601”,’<br>                ‘“@version”:”1”,’<br>                ‘“client”:”$remote_addr”,’<br>                ‘“url”:”$uri”,’<br>                ‘“status”:”$status”,’<br>                ‘“domain”:”$host”,’<br>                ‘“host”:”$server_addr”,’<br>                ‘“size”:$body_bytes_sent,’<br>                ‘“responsetime”:$request_time,’<br>                ‘“referer”: “$http_referer”,’<br>                ‘“ua”: “$http_user_agent”‘<br>                ‘}’;<br>  access_log  /var/log/access_json.log  json;</p>
<p>(2)收集nginx日志和MySQL日志到消息队列中；这个文件我们是定义在客户端，即生产服务器上面的Logstash文件哦. 注意：这里刚搭建完毕，没有什么数据，为了展示效果，我这里导入了线上的nginx和MySQL慢日志</p>
<p>input {<br>  file {             #从nginx日志读入<br>    type =&gt; “nginx-access”<br>    path =&gt; “/var/log/nginx/access.log”<br>    start_position =&gt; “beginning”<br>    codec =&gt; “json”  #这里指定 codec格式为json<br>  }<br>  file {  #从MySQL慢日志读入<br>   type =&gt; “slow-mysql”<br>   path =&gt; “/var/log/mysql/slow-mysql.log”<br>   start_position =&gt; “beginning”<br>   codec =&gt; multiline {         #这里用到了logstash的插件功能，将本来属于一行的多行日志条目整合在一起，让他属于一条<br>     pattern =&gt; “^# User@Host”  #用到了正则去匹配<br>     negate =&gt; true<br>     what =&gt; “previous”<br>   }<br>  }<br>}</p>
<p>output {<br>#  stdout { codec=&gt; rubydebug }<br>  if [type] == “nginx-access” {    #通过判断input中定义的type，来让它在kafka集群中生成的主题名称<br>    kafka {                        #输出到kafka集群<br>      bootstrap_servers =&gt; “192.168.2.22:9092,192.168.2.23:9092,192.168.2.24:9092”  #生产者们<br>      topic_id =&gt; “nginx-access”   #主题名称<br>      compression_type =&gt; “snappy” #压缩类型<br>    }<br> }<br>  if [type] == “slow-mysql” {<br>    kafka {<br>      bootstrap_servers =&gt; “192.168.2.22:9092,192.168.2.23:9092,192.168.2.24:9092”<br>      topic_id =&gt; “slow-mysql”<br>      compression_type =&gt; “snappy”<br>    }<br> }<br>}</p>
<p>(3)Logstash 从kafka集群中读取日志存储到es中，这里的定义logstash文件是在三台kafka服务器上面的哦，并且要保持一致，你可以在一台上面修改测试好之后，拷贝至另外两台即可。</p>
<p>input {<br>    kafka {<br>        zk_connect =&gt; “192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181”<br>        type =&gt; “nginx-access”<br>        topic_id =&gt; “nginx-access”<br>        codec =&gt; plain<br>        reset_beginning =&gt; false<br>        consumer_threads =&gt; 5<br>        decorate_events =&gt; true<br>    }<br>    kafka {<br>        zk_connect =&gt; “192.168.2.22:2181,192.168.2.23:2181,192.168.2.24:2181”<br>        type =&gt; “slow-mysql”<br>        topic_id =&gt; “slow-mysql”<br>        codec =&gt; plain<br>        reset_beginning =&gt; false<br>        consumer_threads =&gt; 5<br>        decorate_events =&gt; true<br>    }<br>}</p>
<p>output {<br>#  stdout { codec=&gt; rubydebug }<br>  if [type] == “nginx-access” {<br>    elasticsearch {<br>      hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”]<br>      index =&gt; “nginx-access-%{+YYYY-MM}”<br>    }<br>  }<br>  if [type] == “slow-mysql” {<br>    elasticsearch {<br>      hosts =&gt; [“192.168.2.18:9200”,”192.168.2.19:9200”]<br>      index =&gt; “slow-mysql-%{+YYYY-MM}”<br>    }<br>  }<br>}</p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/13.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/13.png" alt="13"></a> 通过上图可以看到，nginx日志以及MySQL慢日志已经成功抵达es集群 然后我们在kibana上面创建索引就可以啦 (4)创建nginx-access 日志索引 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/11.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/11.png" alt="11"></a> 此时就可以看到索引啦 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/16.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/16.png" alt="16"></a> (5)创建MySQL慢日志索引 p<a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/15.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/15.png" alt="15"></a> MySQL的索引也出来啦 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/17.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/17.png" alt="17"></a>Kibana报表展示 kibana报表功能非常的强大，也就是可视化；可以制作出下面不同类型的图形 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/18.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/18.png" alt="18"></a> 下面就是我简单的一些图形展示 <img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/19.png" alt="19"> 由于篇幅问题，可以看官方介绍。 参考： <a href="https://github.com/liquanzhou/ops_doc/tree/master/Service/kafka">https://github.com/liquanzhou/ops_doc/tree/master/Service/kafka</a> <a href="http://www.lujinhong.com/kafka%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97.html">http://www.lujinhong.com/kafka%E9%9B%86%E7%BE%A4%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97.html</a> <a href="http://www.it165.net/admin/html/201405/3192.html">http://www.it165.net/admin/html/201405/3192.html</a> <a href="http://blog.csdn.net/lizhitao/article/details/39499283">http://blog.csdn.net/lizhitao/article/details/39499283</a> <a href="https://taoistwar.gitbooks.io/spark-operationand-maintenance-management/content/spark_relate_software/zookeeper_install.html">https://taoistwar.gitbooks.io/spark-operationand-maintenance-management/content/spark_relate_software/zookeeper_install.html</a></p>
]]></content>
      <categories>
        <category>Monitor</category>
        <category>Other</category>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>ELK</tag>
        <tag>kafka</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Expect详解(ssh自动登录)</title>
    <url>/2015/09/16/expect-e8-af-a6-e8-a7-a3ssh-e8-87-aa-e5-8a-a8-e7-99-bb-e5-bd-95/</url>
    <content><![CDATA[<p>Expect是一个用来处理<strong>交互</strong>的命令。借助Expect，我们可以将交互过程写在一个脚本上，使之自动化完成。形象的说，ssh登录，ftp登录等都符合<strong>交互</strong>的定义。下文我们首先提出一个问题，然后介绍基础知四个命令，最后提出解决方法。 Expect中最关键的四个命令是send,expect,spawn,interact。</p>
<p>send：用于向进程发送字符串<br>expect：从进程接收字符串<br>spawn：启动新的进程<br>interact：允许用户交互</p>
<h3 id="1-send命令"><a href="#1-send命令" class="headerlink" title="1. send命令"></a>1. send命令</h3><p>send命令接收一个字符串参数，并将该参数发送到进程。</p>
<p>expect1.1&gt; send “hello world\n”<br>hello world</p>
<h3 id="2-expect命令"><a href="#2-expect命令" class="headerlink" title="2. expect命令"></a>2. expect命令</h3><h3 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="(1)基础知识"></a>(1)基础知识</h3><p>expect命令和send命令正好相反，expect通常是用来等待一个进程的反馈。expect可以接收一个字符串参数，也可以接收正则表达式参数。和上文的send命令结合，现在我们可以看一个最简单的交互式的例子：</p>
<p>expect “hi\n”<br>send “hello there!\n”</p>
<p>这两行代码的意思是：从标准输入中等到hi和换行键后，向标准输出输出hello there。</p>
<p>tips： $expect_out(buffer)存储了所有对expect的输入，&lt;$expect_out(0,string)&gt;存储了匹配到expect参数的输入。</p>
<p>比如如下程序：</p>
<p>expect “hi\n”<br>send “you typed &lt;$expect_out(buffer)&gt;”<br>send “but I only expected &lt;$expect_out(0,string)&gt;”</p>
<p>当在标准输入中输入</p>
<p>test<br>hi</p>
<p>是，运行结果如下</p>
<p>you typed: test<br>hi<br>I only expect: hi</p>
<h4 id="2-模式-动作"><a href="#2-模式-动作" class="headerlink" title="(2)模式-动作"></a>(2)模式-动作</h4><p>expect最常用的语法是来自tcl语言的模式-动作。这种语法极其灵活，下面我们就各种语法分别说明。 单一分支模式语法：</p>
<p>expect “hi” {send “You said hi”}</p>
<p>匹配到hi后，会输出”you said hi” 多分支模式语法：</p>
<p>expect “hi” { send “You said hi\n” } <br>“hello” { send “Hello yourself\n” } <br>“bye” { send “That was unexpected\n” }</p>
<p>匹配到hi,hello,bye任意一个字符串时，执行相应的输出。等同于如下写法：</p>
<p>expect {<br>“hi” { send “You said hi\n”}<br>“hello” { send “Hello yourself\n”}<br>“bye” { send “That was unexpected\n”}<br>}</p>
<h3 id="3-spawn命令"><a href="#3-spawn命令" class="headerlink" title="3. spawn命令"></a>3. spawn命令</h3><p>上文的所有demo都是和标准输入输出进行交互，但是我们跟希望他可以和某一个进程进行交互。spawm命令就是用来启动新的进程的。spawn后 的send和expect命令都是和spawn打开的进程进行交互的。结合上文的send和expect命令我们可以看一下更复杂的程序段了。</p>
<p>set timeout -1<br>spawn ftp ftp.test.com      //打开新的进程，该进程用户连接远程ftp服务器<br>expect “Name”             //进程返回Name时<br>send “user\r”        //向进程输入anonymous\r<br>expect “Password:”        //进程返回Password:时<br>send “123456\r”    //向进程输入<a href="mailto:don@libes.com">don@libes.com</a>\r<br>expect “ftp&gt; “            //进程返回ftp&gt;时<br>send “binary\r”           //向进程输入binary\r<br>expect “ftp&gt; “            //进程返回ftp&gt;时<br>send “get test.tar.gz\r”  //向进程输入get test.tar.gz\r</p>
<p>这段代码的作用是登录到ftp服务器ftp ftp.uu.net上，并以二进制的方式下载服务器上的文件test.tar.gz。程序中有详细的注释。</p>
<h3 id="4-interact"><a href="#4-interact" class="headerlink" title="4.interact"></a>4.interact</h3><p>到现在为止，我们已经可以结合spawn、expect、send自动化的完成很多任务了。但是，如何让人在适当的时候干预这个过程了。比如下载完 ftp文件时，仍然可以停留在ftp命令行状态，以便手动的执行后续命令。interact可以达到这些目的。下面的demo在自动登录ftp后，允许用 户交互。</p>
<p>spawn ftp ftp.test.com<br>expect “Name”<br>send “user\r”<br>expect “Password:”<br>send “123456\r”<br>interact</p>
<p>如何实现expect ssh自动登录？</p>
<p>yum install -y expect </p>
<p>vim loging_myvps.sh<br>#!/usr/bin/expect -f<br>set ip xxx.xxx.xxx.xxx<br>set password **********<br>set timeout 10<br>spawn ssh USERNAME@$ip<br>expect {<br>“*yes/no” { send “yes\r”; exp_continue}<br>“*password:” { send “$password\r” }<br>}<br>interact</p>
<p>./login_myvps.sh   即可登录</p>
]]></content>
      <categories>
        <category>Shell</category>
        <category>脚本编程</category>
      </categories>
      <tags>
        <tag>expect</tag>
      </tags>
  </entry>
  <entry>
    <title>Flask RestApi 后端开发项目说明</title>
    <url>/2018/10/17/flask-restapi-hou-duan-kai-fa-xiang-mu-shuo-ming/</url>
    <content><![CDATA[<p>最近一直在做的一个项目就是打算将之前的MVC风格的后台,重构为前后端分离式,由于个人对于Flask框架熟悉程度比起Django来更熟悉一些，所以最终还是选择他作为开发框架来进行后端的开发，目前呢打算的是把基础的平台功能做出来作为一个模板，然后通过这个模板再去结合业务方面的开发。前端方面暂时未开始，目前后端开发进度:</p>
<hr>
<p>功能</p>
<p>完成度</p>
<p>methods</p>
<p>api</p>
<p>备注</p>
<p>用户注册</p>
<p>🚀%100</p>
<p>POST</p>
<p>/auth/register</p>
<p>null</p>
<p>用户登录</p>
<p>🚀%100</p>
<p>POST</p>
<p>/auth/login</p>
<p>null</p>
<p>用户登出</p>
<p>🚀%100</p>
<p>POST</p>
<p>/auth/logout</p>
<p>null</p>
<p>邮件确认</p>
<p>🚀%100</p>
<p>POST</p>
<p>/auth/confirm/{confirm_token}</p>
<p>null</p>
<p>Token刷新</p>
<p>🚀%100</p>
<p>GET</p>
<p>/auth/refresh_token</p>
<p>null</p>
<p>用户获取</p>
<p>🚀%100</p>
<p>GET</p>
<p>/user/</p>
<p>null</p>
<p>用户删除</p>
<p>🚀%100</p>
<p>POST</p>
<p>/user/{email}</p>
<p>null</p>
<p>用户禁用/启用</p>
<p>🚀%0</p>
<p>POST</p>
<p>null</p>
<p>任务添加</p>
<p>🚀%0</p>
<p>POST</p>
<p>null</p>
<p>任务获取</p>
<p>🚀%0</p>
<p>GET</p>
<p>null</p>
<p>任务删除</p>
<p>🚀%0</p>
<p>POST</p>
<p>null</p>
<p>任务修改</p>
<p>🚀%0</p>
<p>PUT</p>
<p>null</p>
<p>API添加</p>
<p>🚀%0</p>
<p>POST</p>
<p>第三方(ex:saltapi,zabbixapi)</p>
<p>API获取</p>
<p>🚀%0</p>
<p>GET</p>
<p>第三方(ex:saltapi,zabbixapi)</p>
<p>API删除</p>
<p>🚀%0</p>
<p>POST</p>
<p>第三方(ex:saltapi,zabbixapi)</p>
<p>API修改</p>
<p>🚀%0</p>
<p>PUT</p>
<p>第三方(ex:saltapi,zabbixapi)</p>
<p>设计思路:<br>1.用户权限管理通过角色管理，分为user,admin,sa三种角色<br>2.采用了jwt token认证机制，访问资源必须携带access_token以验证其访问资源的权限<br>……</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>All</tag>
      </tags>
  </entry>
  <entry>
    <title>Python App on Kubernets Cluster</title>
    <url>/2018/06/27/flask_kubenertes/</url>
    <content><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>此次部署是在前面博文中搭建的K8S集群基础之上进行的，涉及使用到的容器已经推到Dockerhub<br>代码、文件：<a href="https://github.com/guomaoqiu/flask_kubernetes">https://github.com/guomaoqiu/flask_kubernetes</a></p>
<h2 id="配置清单"><a href="#配置清单" class="headerlink" title="配置清单"></a>配置清单</h2><p><img src="https://github.com/guomaoqiu/flask_kubernetes/blob/master/screenshots/1536823892.jpg?raw=true"></p>
<h2 id="逻辑流程图"><a href="#逻辑流程图" class="headerlink" title="逻辑流程图"></a>逻辑流程图</h2><p><img src="https://github.com/guomaoqiu/flask_kubernetes/blob/master/screenshots/15302459587264.jpg?raw=true"></p>
<h2 id="初始配置"><a href="#初始配置" class="headerlink" title="初始配置"></a>初始配置</h2><h5 id="1-创建一个namespace-供此次部署使用"><a href="#1-创建一个namespace-供此次部署使用" class="headerlink" title="1.创建一个namespace 供此次部署使用"></a>1.创建一个namespace 供此次部署使用</h5><figure class="highlight coffeescript"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl create namespace flask-app-extions-stage</span></span><br><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl get ns</span></span><br><span class="line">NAME                      STATUS    AGE</span><br><span class="line"><span class="keyword">default</span>                   Active    <span class="number">29</span>d</span><br><span class="line">flask-app-extions-stage   Active    <span class="number">1</span>m</span><br><span class="line">kube-public               Active    <span class="number">29</span>d</span><br><span class="line">kube-system               Active    <span class="number">29</span>d</span><br></pre></td></tr></tbody></table></figure>
<h5 id="1-创建用NFS存储目录"><a href="#1-创建用NFS存储目录" class="headerlink" title="1. 创建用NFS存储目录"></a>1. 创建用NFS存储目录</h5><figure class="highlight haskell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta"># 用于flask-app代码存放目录，当pod启动时通过NFS的方式挂载进去</span></span><br><span class="line">[root@linux-node1 ~]# mkdir /<span class="class"><span class="keyword">data</span>/flask-app-<span class="keyword">data</span></span></span><br><span class="line"><span class="meta"># 用于flask的mysql数据存放目录，当pod启动时通过NFS挂载进去</span></span><br><span class="line">[root@linux-node1 ~]# mkdir /<span class="class"><span class="keyword">data</span>/flask-app-db</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="2-安装NFS-Server-略"><a href="#2-安装NFS-Server-略" class="headerlink" title="2. 安装NFS Server(略)"></a>2. 安装NFS Server(略)</h5><h5 id="3-写入配置"><a href="#3-写入配置" class="headerlink" title="3. 写入配置"></a>3. 写入配置</h5><figure class="highlight autoit"><table><tbody><tr><td class="code"><pre><span class="line">[root<span class="symbol">@linux</span>-node1 ~]<span class="meta"># echo <span class="string">"/data/flask-app-db *(rw,sync,no_subtree_check,no_root_squash)"</span> &gt; /etc/exports</span></span><br><span class="line">[root<span class="symbol">@linux</span>-node1 ~]<span class="meta"># echo <span class="string">"/data/flask-app-data *(rw,sync,no_subtree_check,no_root_squash)"</span> &gt;&gt; /etc/exports</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="4-重启验证"><a href="#4-重启验证" class="headerlink" title="4.重启验证"></a>4.重启验证</h5><figure class="highlight crystal"><table><tbody><tr><td class="code"><pre><span class="line">[root<span class="variable">@linux</span>-node1 ~]<span class="comment"># systemctl restart nfs-server</span></span><br><span class="line">[root<span class="variable">@linux</span>-node1 ~]<span class="comment"># exportfs</span></span><br><span class="line"><span class="comment"># mysql data</span></span><br><span class="line"><span class="regexp">/data/flask</span>-app-db</span><br><span class="line"><span class="comment"># flask app code</span></span><br><span class="line"><span class="regexp">/data/flask</span>-app-data</span><br><span class="line">		</span><br></pre></td></tr></tbody></table></figure>
<h5 id="5-创建flask-app使用的pv及pvc"><a href="#5-创建flask-app使用的pv及pvc" class="headerlink" title="5.创建flask-app使用的pv及pvc"></a>5.创建flask-app使用的pv及pvc</h5><figure class="highlight autoit"><table><tbody><tr><td class="code"><pre><span class="line">[root<span class="symbol">@linux</span>-node1 ~]<span class="meta"># kubectl create -f flask_kubernetes/flask-app/flask_app_data_pv.yaml</span></span><br><span class="line">persistentvolume <span class="string">"flask-app-data-pv"</span> created</span><br><span class="line">[root<span class="symbol">@linux</span>-node1 ~]<span class="meta"># kubectl create -f flask_kubernetes/flask-app/flask_app_data_pvc.yaml</span></span><br><span class="line">persistentvolumeclaim <span class="string">"flask-app-data-pv-claim"</span> created</span><br></pre></td></tr></tbody></table></figure>
<h5 id="6-创建flask-app-db使用的pv及pvc"><a href="#6-创建flask-app-db使用的pv及pvc" class="headerlink" title="6.创建flask-app-db使用的pv及pvc"></a>6.创建flask-app-db使用的pv及pvc</h5><figure class="highlight excel"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app_db/flask_app_db_pv.yaml</span><br><span class="line">persistentvolume <span class="string">"flask-app-db-pv"</span> created</span><br><span class="line">[root@linux-node1 ~]# kubectl create -f flask_kubernetes/flask_app_db/flask_app_db_pvc.yaml</span><br><span class="line">persistentvolumeclaim <span class="string">"flask-app-db-pv-claim"</span> created</span><br><span class="line">[root@linux-node1 ~]# kubectl get <span class="built_in">pv</span> -<span class="built_in">n</span> flask-app-extions-stage</span><br><span class="line">NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE</span><br><span class="line">flask-app-data-<span class="built_in">pv</span>   <span class="number">5</span>Gi        RWO            Recycle          Bound     flask-app-extions-stage/flask-app-data-<span class="built_in">pv</span>-claim                            <span class="number">5</span>m</span><br><span class="line">flask-app-<span class="built_in">db</span>-<span class="built_in">pv</span>     <span class="number">5</span>Gi        RWO            Recycle          Bound     flask-app-extions-stage/flask-app-<span class="built_in">db</span>-<span class="built_in">pv</span>-claim                              <span class="number">26</span>s</span><br><span class="line">[root@linux-node1 ~]# kubectl get pvc -<span class="built_in">n</span> flask-app-extions-stage</span><br><span class="line">NAME                      STATUS    VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">flask-app-data-<span class="built_in">pv</span>-claim   Bound     flask-app-data-<span class="built_in">pv</span>   <span class="number">5</span>Gi        RWO                           <span class="number">4</span>m</span><br><span class="line">flask-app-<span class="built_in">db</span>-<span class="built_in">pv</span>-claim     Bound     flask-app-<span class="built_in">db</span>-<span class="built_in">pv</span>     <span class="number">5</span>Gi        RWO                           <span class="number">28</span>s</span><br><span class="line">[root@linux-node1 ~]#</span><br></pre></td></tr></tbody></table></figure>
<h2 id="部署-flask-app-db"><a href="#部署-flask-app-db" class="headerlink" title="部署 flask-app-db"></a>部署 flask-app-db</h2><p>运行flask交互数据的数据库使用的是mysql，上面我已经创建了用于基于NFS存储mysql数据的持久存储目录 /data/flask-app-db</p>
<h4 id="1-创建配置-MySQL-密码的-Secret"><a href="#1-创建配置-MySQL-密码的-Secret" class="headerlink" title="1. 创建配置 MySQL 密码的 Secret"></a>1. 创建配置 MySQL 密码的 Secret</h4><figure class="highlight fortran"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl create secret <span class="keyword">generic</span> mysql-<span class="keyword">pass</span> --from-literal=password=YOUR_PASSWORD</span><br><span class="line">[root@linux-node1 ~]# kubectl  get secret -n flask-app-extions-stage</span><br><span class="line"><span class="keyword">NAME</span>                  <span class="keyword">TYPE</span>                                  <span class="keyword">DATA</span>      AGE</span><br><span class="line"><span class="keyword">default</span>-token-fr2sg   kubernetes.io/service-account-token   <span class="number">3</span>         <span class="number">47</span>m</span><br><span class="line">mysql-<span class="keyword">pass</span>            Opaque                                <span class="number">1</span>         <span class="number">14</span>s</span><br></pre></td></tr></tbody></table></figure>
<h4 id="2-部署-MySQL："><a href="#2-部署-MySQL：" class="headerlink" title="2. 部署 MySQL："></a>2. 部署 MySQL：</h4><figure class="highlight pgsql"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl <span class="keyword">create</span> -f flask_kubernetes/flask_app_db/flask_app_db_deploy.yaml</span><br><span class="line">deployment.apps "flask-app-db" created</span><br><span class="line">[root@linux-node1 ~]# kubectl <span class="keyword">create</span> -f flask_kubernetes/flask_app_db/flask_app_db_service.yaml</span><br><span class="line">service "flask-app-db" created</span><br><span class="line"></span><br><span class="line"># 查看状态</span><br><span class="line"><span class="type">NAME</span>                                   READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">pod/flask-app-db<span class="number">-6</span>f55458666-h2dk7      <span class="number">1</span>/<span class="number">1</span>       Running   <span class="number">1</span>          <span class="number">22</span>h       <span class="number">10.2</span><span class="number">.15</span><span class="number">.108</span>   <span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span></span><br><span class="line"><span class="type">NAME</span>                      <span class="keyword">TYPE</span>        <span class="keyword">CLUSTER</span>-IP     <span class="keyword">EXTERNAL</span>-IP   PORT(S)          AGE       SELECTOR</span><br><span class="line">service/flask-app-db      NodePort    <span class="number">10.1</span><span class="number">.68</span><span class="number">.29</span>     &lt;<span class="keyword">none</span>&gt;        <span class="number">3306</span>:<span class="number">30006</span>/TCP   <span class="number">22</span>h       app=flask-app-db</span><br><span class="line"><span class="type">NAME</span>                                    DESIRED   <span class="keyword">CURRENT</span>   UP-<span class="keyword">TO</span>-<span class="type">DATE</span>   AVAILABLE   AGE       CONTAINERS   IMAGES                         SELECTOR</span><br><span class="line">deployment.extensions/flask-app-db      <span class="number">1</span>         <span class="number">1</span>         <span class="number">1</span>            <span class="number">1</span>           <span class="number">22</span>h       mysql        mysql:<span class="number">5.6</span>                      app=flask-app-db,tier=mysql</span><br><span class="line"></span><br><span class="line"># 以上可以知道该pod运行在节点<span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span>上面，我这里使用的是NodePort方式，然后映射了一个<span class="number">30006</span>端口出来到节点上面</span><br><span class="line"></span><br><span class="line"># 可以尝试登陆测试</span><br><span class="line">[root@linux-node1 flask_app_db]# mysql -uroot -pdevopsdemo -h192<span class="number">.168</span><span class="number">.56</span><span class="number">.12</span> -P <span class="number">30006</span></span><br><span class="line">Welcome <span class="keyword">to</span> the MariaDB monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL <span class="keyword">connection</span> id <span class="keyword">is</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">Server</span> <span class="keyword">version</span>: <span class="number">5.6</span><span class="number">.40</span> MySQL Community <span class="keyword">Server</span> (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2017</span>, Oracle, MariaDB Corporation Ab <span class="keyword">and</span> others.</span><br><span class="line"></span><br><span class="line"><span class="keyword">Type</span> <span class="string">'help;'</span> <span class="keyword">or</span> <span class="string">'\h'</span> <span class="keyword">for</span> help. <span class="keyword">Type</span> <span class="string">'\c'</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> <span class="keyword">input</span> <span class="keyword">statement</span>.</span><br><span class="line"></span><br><span class="line">MySQL [(<span class="keyword">none</span>)]&gt; <span class="keyword">show</span> databases;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| <span class="keyword">Database</span>           |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.02</span> sec)</span><br><span class="line"></span><br><span class="line">MySQL [(<span class="keyword">none</span>)]&gt;</span><br><span class="line"></span><br><span class="line"># 顺便我们在这里手动创建一下flask-app需要用到的数据库 devopsdemo, 因为在flask-app在启动过程中需要去初始化数据库并创建数据表。</span><br><span class="line">MySQL [(<span class="keyword">none</span>)]&gt; <span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> devopsdemo;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="keyword">row</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">#数据库目录,可以看到mysql数据已经通过nfs的方式存储到了我们nfs <span class="keyword">server</span>所在主机映射出去的目录</span><br><span class="line">[root@linux-node1 ~]# tree /data/flask-app-db/ -L <span class="number">1</span></span><br><span class="line">/data/flask-app-db/</span><br><span class="line">├── auto.cnf</span><br><span class="line">├── devopsdemo</span><br><span class="line">├── ibdata1</span><br><span class="line">├── ib_logfile0</span><br><span class="line">├── ib_logfile1</span><br><span class="line">├── mysql</span><br><span class="line">└── performance_schema</span><br><span class="line"></span><br><span class="line"><span class="number">3</span> directories, <span class="number">4</span> files</span><br></pre></td></tr></tbody></table></figure>

<h2 id="部署flask-app"><a href="#部署flask-app" class="headerlink" title="部署flask-app"></a>部署flask-app</h2><h4 id="1-将flask程序代码放到-data-flask-app-data"><a href="#1-将flask程序代码放到-data-flask-app-data" class="headerlink" title="1.将flask程序代码放到/data/flask-app-data/"></a>1.将flask程序代码放到/data/flask-app-data/</h4><figure class="highlight haskell"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# cp -rf flask_app_code/* /<span class="class"><span class="keyword">data</span>/flask-app-<span class="keyword">data</span>/</span></span><br><span class="line">[root@linux-node1 ~]# tree /<span class="class"><span class="keyword">data</span>/flask-app-<span class="keyword">data</span>/ -<span class="type">L</span> 1</span></span><br><span class="line">/<span class="class"><span class="keyword">data</span>/flask-app-<span class="keyword">data</span>/</span></span><br><span class="line">├── app</span><br><span class="line">├── config.py</span><br><span class="line">├── flask_uwsgi.ini</span><br><span class="line">├── <span class="type">LICENSE</span></span><br><span class="line">├── manage.py</span><br><span class="line">├── <span class="type">README</span>.md</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── screenshots</span><br><span class="line">├── supervisord.conf</span><br><span class="line">└── tests</span><br><span class="line"></span><br><span class="line"><span class="number">3</span> directories, <span class="number">9</span> files</span><br></pre></td></tr></tbody></table></figure>
<h4 id="2-修改flask程序连接数据库的配置信息"><a href="#2-修改flask程序连接数据库的配置信息" class="headerlink" title="2. 修改flask程序连接数据库的配置信息"></a>2. 修改flask程序连接数据库的配置信息</h4><figure class="highlight asciidoc"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim /data/flask-app-data/config.py</span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">    db_host = 'flask-app-db' # 在pod启动过程中会去加载k8s的环境变量；这个flask-app-db 就是mysql的svc</span></span><br><span class="line"><span class="code">    db_user = 'root'         # 默认为root</span></span><br><span class="line"><span class="code">    db_pass = "devopsdemo"   # 数据库密码</span></span><br><span class="line"><span class="code">    db_name = 'devopsdemo'   # 数据库名称</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line">......</span><br></pre></td></tr></tbody></table></figure>

<h4 id="3-部署flask-app"><a href="#3-部署flask-app" class="headerlink" title="3. 部署flask-app"></a>3. 部署flask-app</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl create -f flask_kubernetes/flask_app/flask_app_deployment.yaml</span></span><br><span class="line">deployment.apps <span class="string">"flask-app"</span> created</span><br><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl create -f flask_kubernetes/flask_app/flask_app_service.yaml</span></span><br><span class="line">service <span class="string">"flask-app"</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态:</span></span><br><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl get pod,svc,deployment,rc -o wide   -n flask-app-extions-stage</span></span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">pod<span class="symbol">/flask-app-65646687ff-4gg7g</span>         <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">17</span>h       <span class="number">10.2</span>.<span class="number">42.125</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-65646687ff-7d5g6</span>         <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">17</span>h       <span class="number">10.2</span>.<span class="number">42.124</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-65646687ff-xkq6k</span>         <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">17</span>h       <span class="number">10.2</span>.<span class="number">42.123</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-db-6f55458666-h2dk7</span>      <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">1</span>          <span class="number">22</span>h       <span class="number">10.2</span>.<span class="number">15.108</span>   <span class="number">192.168</span>.<span class="number">56.12</span></span><br><span class="line"></span><br><span class="line">NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE       SELECTOR</span><br><span class="line">service<span class="symbol">/flask-app</span>         ClusterIP   <span class="number">10.1</span>.<span class="number">173.169</span>   <span class="symbol">&lt;none&gt;</span>        <span class="number">3032</span><span class="symbol">/TCP</span>         <span class="number">22</span>h       app<span class="operator">=</span>flask-app</span><br><span class="line">service<span class="symbol">/flask-app-db</span>      NodePort    <span class="number">10.1</span>.<span class="number">68.29</span>     <span class="symbol">&lt;none&gt;</span>        <span class="number">3306</span>:<span class="number">30006</span><span class="symbol">/TCP</span>   <span class="number">22</span>h       app<span class="operator">=</span>flask-app-db</span><br><span class="line"></span><br><span class="line">NAME                                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES                         SELECTOR</span><br><span class="line">deployment.extensions<span class="symbol">/flask-app</span>         <span class="number">3</span>         <span class="number">3</span>         <span class="number">3</span>            <span class="number">3</span>           <span class="number">22</span>h       flask-app    guomaoqiu<span class="operator">/</span>python27baseenv:v2   app<span class="operator">=</span>flask-app,tier<span class="operator">=</span>frontend</span><br><span class="line">deployment.extensions<span class="symbol">/flask-app-db</span>      <span class="number">1</span>         <span class="number">1</span>         <span class="number">1</span>            <span class="number">1</span>           <span class="number">22</span>h       mysql        mysql:<span class="number">5.6</span>                      app<span class="operator">=</span>flask-app-db,tier<span class="operator">=</span>mysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以上可知 我的flask-app运行了三个副本；</span></span><br><span class="line"><span class="comment"># 并且采用的是ClusterIP Type,3032是flask+supervisor+uwsgi 之后 uwsgsi 暴露出来的二端口</span></span><br><span class="line"><span class="comment"># 此时我们访问是没有用的；因为uwgsi需要结合用到Nginx来访问；于是我下面将会部署nginx ---&gt; uwgsi(flask-app)</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="部署flask-app-nginx"><a href="#部署flask-app-nginx" class="headerlink" title="部署flask-app-nginx"></a>部署flask-app-nginx</h2><p>因为这个nginx的配置我这里需要修改做一些定制方面的配置，所以有一个将单个配置文件挂载到pod里面的需求；于是这里使用到了k8s的ConfigMap功能</p>
<h4 id="1-找到需要挂载进pod的nginx配置文件"><a href="#1-找到需要挂载进pod的nginx配置文件" class="headerlink" title="1. 找到需要挂载进pod的nginx配置文件:"></a>1. 找到需要挂载进pod的nginx配置文件:</h4><figure class="highlight stata"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl create  configmap nginx-<span class="keyword">conf</span> --from-<span class="keyword">file</span>=/root/flask_kubernetes/flask_app_nginx/nginx.<span class="keyword">conf</span> -<span class="keyword">n</span>  flask-<span class="keyword">app</span>-extions-stage</span><br><span class="line">[root@linux-node1 ~]# kubectl get configmap -<span class="keyword">n</span> flask-<span class="keyword">app</span>-extions-stage</span><br><span class="line">NAME         DATA      AGE</span><br><span class="line">nginx-<span class="keyword">conf</span>   1         11s</span><br><span class="line"># 通过<span class="keyword">describe</span>就可以看到这个<span class="keyword">conf</span>的详细内容，其实就在我们nginx.<span class="keyword">conf</span>配置文件的基础上加上了k8s一些特有的属性值</span><br><span class="line">[root@linux-node1 ~]# kubectl <span class="keyword">describe</span> configmap/nginx-<span class="keyword">conf</span> -<span class="keyword">n</span> flask-<span class="keyword">app</span>-extions-stage</span><br></pre></td></tr></tbody></table></figure>
<h4 id="2-部署flask-app-nginx"><a href="#2-部署flask-app-nginx" class="headerlink" title="2.部署flask-app-nginx"></a>2.部署flask-app-nginx</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 flask_app_nginx]<span class="comment"># kubectl create -f flask_app_nginx_deploy.yaml</span></span><br><span class="line">deployment.apps <span class="string">"flask-app-nginx"</span> created</span><br><span class="line">[root@linux-node1 flask_app_nginx]<span class="comment"># kubectl create -f flask_app_nginx_service.yaml</span></span><br><span class="line">service <span class="string">"flask-app-nginx"</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看状态：</span></span><br><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl get pod,svc,deployment,rc -o wide   -n flask-app-extions-stage</span></span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">pod<span class="symbol">/flask-app-65646687ff-4gg7g</span>         <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">17</span>h       <span class="number">10.2</span>.<span class="number">42.125</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-65646687ff-7d5g6</span>         <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">17</span>h       <span class="number">10.2</span>.<span class="number">42.124</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-65646687ff-xkq6k</span>         <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">17</span>h       <span class="number">10.2</span>.<span class="number">42.123</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-db-6f55458666-h2dk7</span>      <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">1</span>          <span class="number">22</span>h       <span class="number">10.2</span>.<span class="number">15.108</span>   <span class="number">192.168</span>.<span class="number">56.12</span></span><br><span class="line">pod<span class="symbol">/flask-app-nginx-657fd4c57c-p6qdx</span>   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">12</span>         <span class="number">18</span>h       <span class="number">10.2</span>.<span class="number">42.119</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-nginx-657fd4c57c-v4qsp</span>   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">12</span>         <span class="number">18</span>h       <span class="number">10.2</span>.<span class="number">42.117</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line">pod<span class="symbol">/flask-app-nginx-657fd4c57c-xtpmm</span>   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">12</span>         <span class="number">18</span>h       <span class="number">10.2</span>.<span class="number">42.118</span>   <span class="number">192.168</span>.<span class="number">56.13</span></span><br><span class="line"></span><br><span class="line">NAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE       SELECTOR</span><br><span class="line">service<span class="symbol">/flask-app</span>         ClusterIP   <span class="number">10.1</span>.<span class="number">173.169</span>   <span class="symbol">&lt;none&gt;</span>        <span class="number">3032</span><span class="symbol">/TCP</span>         <span class="number">22</span>h       app<span class="operator">=</span>flask-app</span><br><span class="line">service<span class="symbol">/flask-app-db</span>      NodePort    <span class="number">10.1</span>.<span class="number">68.29</span>     <span class="symbol">&lt;none&gt;</span>        <span class="number">3306</span>:<span class="number">30006</span><span class="symbol">/TCP</span>   <span class="number">22</span>h       app<span class="operator">=</span>flask-app-db</span><br><span class="line">service<span class="symbol">/flask-app-nginx</span>   ClusterIP   <span class="number">10.1</span>.<span class="number">193.82</span>    <span class="symbol">&lt;none&gt;</span>        <span class="number">80</span><span class="symbol">/TCP</span>           <span class="number">21</span>h       app<span class="operator">=</span>flask-app-nginx</span><br><span class="line"></span><br><span class="line">NAME                                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE       CONTAINERS   IMAGES                         SELECTOR</span><br><span class="line">deployment.extensions<span class="symbol">/flask-app</span>         <span class="number">3</span>         <span class="number">3</span>         <span class="number">3</span>            <span class="number">3</span>           <span class="number">22</span>h       flask-app    guomaoqiu<span class="operator">/</span>python27baseenv:v2   app<span class="operator">=</span>flask-app,tier<span class="operator">=</span>frontend</span><br><span class="line">deployment.extensions<span class="symbol">/flask-app-db</span>      <span class="number">1</span>         <span class="number">1</span>         <span class="number">1</span>            <span class="number">1</span>           <span class="number">22</span>h       mysql        mysql:<span class="number">5.6</span>                      app<span class="operator">=</span>flask-app-db,tier<span class="operator">=</span>mysql</span><br><span class="line">deployment.extensions<span class="symbol">/flask-app-nginx</span>   <span class="number">3</span>         <span class="number">3</span>         <span class="number">3</span>            <span class="number">3</span>           <span class="number">21</span>h       nginx        nginx:latest                   app<span class="operator">=</span>flask-app-nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以上 运行了三个flak-app-nginx ,采用的是ClusterIP Type</span></span><br><span class="line"><span class="comment"># 访问验证,直接访问的是flask-app-nginx 的VIP(ClusterIP)</span></span><br><span class="line">[root@linux-node1 ~]<span class="comment"># curl -I 10.1.193.82/auth/login</span></span><br><span class="line">HTTP<span class="symbol">/1.1</span> <span class="number">200</span> OK</span><br><span class="line"><span class="params">Server:</span> nginx<span class="symbol">/1.15.0</span></span><br><span class="line"><span class="params">Date:</span> Wed, <span class="number">27</span> Jun <span class="number">2018</span> <span class="number">04</span>:<span class="number">35</span>:<span class="number">12</span> GMT</span><br><span class="line"><span class="params">Content-Type:</span> text<span class="symbol">/html</span>; <span class="attr">charset</span><span class="operator">=</span>utf-<span class="number">8</span></span><br><span class="line"><span class="params">Content-Length:</span> <span class="number">4026</span></span><br><span class="line"><span class="params">Connection:</span> keep-alive</span><br><span class="line"><span class="params">Set-Cookie:</span> session<span class="operator">=</span>eyJjc3JmX3Rva2VuIjp7IiBiIjoiT0RrNVpXUmpZV014T1daalkyUmlOamRsWXpBNVpqUTVZMk0wTkRnMU9ERm1NVFUzTURrME5BPT0ifX0.DhSlgA.Biu7EYC7qfj4x8--HlR8VUZFUgk; HttpOnly; <span class="attr">Path</span><span class="operator">=</span><span class="operator">/</span></span><br></pre></td></tr></tbody></table></figure>
<p>以上说明我们能够成功的访问到我们的flask-app服务啦,在用linux 终端下的w3m <a href="http://10.1.193.82/auth/login">http://10.1.193.82/auth/login</a> 访问一下呢，说明也是没问题的；那后端uwgsi or flask-app or flask-nginx的访问日志此时毋庸置疑是已经有了的。</p>
<p><img src="https://github.com/guomaoqiu/flask_kubernetes/blob/master/screenshots/15300742642956.jpg?raw=true"></p>
<p>那问题来了；此时我只是在集群内部能够访问，那如何在外面访问呢？那就主要将flask_nginx_service.yaml中的Type该为nodePort,然后从新部署一下flask_app_nginx_services</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]<span class="comment"># more /root/flask_kubernetes/flask_app_nginx/flask_app_nginx_service.yaml</span></span><br><span class="line"><span class="params">kind:</span> Service</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> flask-app-nginx</span><br><span class="line">  <span class="params">namespace:</span> flask-app-extions-stage</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">type:</span> NodePort</span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">app:</span> flask-app-nginx</span><br><span class="line">  <span class="params">ports:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">protocol:</span> TCP</span><br><span class="line">    <span class="params">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="params">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="params">nodePort:</span> <span class="number">30001</span></span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">app:</span> flask-app-nginx</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl describe svc/flask-app-nginx -n flask-app-extions-stage</span></span><br><span class="line"><span class="params">Name:</span>              flask-app-nginx</span><br><span class="line"><span class="params">Namespace:</span>         flask-app-extions-stage</span><br><span class="line"><span class="params">Labels:</span>            <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Selector:</span>          app<span class="operator">=</span>flask-app-nginx</span><br><span class="line"><span class="params">Type:</span>              ClusterIP</span><br><span class="line"><span class="params">IP:</span>                <span class="number">10.1</span>.<span class="number">193.82</span></span><br><span class="line"><span class="params">Port:</span>              <span class="symbol">&lt;unset&gt;</span>  <span class="number">80</span><span class="symbol">/TCP</span></span><br><span class="line"><span class="params">TargetPort:</span>        <span class="number">80</span><span class="symbol">/TCP</span></span><br><span class="line"><span class="params">Endpoints:</span>         <span class="number">10.2</span>.<span class="number">42.117</span>:<span class="number">80</span>,<span class="number">10.2</span>.<span class="number">42.118</span>:<span class="number">80</span>,<span class="number">10.2</span>.<span class="number">42.119</span>:<span class="number">80</span></span><br><span class="line">Session <span class="params">Affinity:</span>  None</span><br><span class="line"><span class="params">Events:</span>            <span class="symbol">&lt;none&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<p>此时如果是外网访问就应该是NodeIP+nodePort啦：</p>
<p><img src="https://github.com/guomaoqiu/flask_kubernetes/blob/master/screenshots/15302385275244.jpg?raw=true"></p>
<p>但是这种方式并不是很理想；本来就是要让他直接访问80 port的，于是采用另外一种暴露端口的方式——Ingress</p>
<p>（这里我还是把flask-app-nginx的service改回了ClusterIP)</p>
<hr>
<h2 id="部署-Ingress-Nginx"><a href="#部署-Ingress-Nginx" class="headerlink" title="部署 Ingress-Nginx"></a>部署 Ingress-Nginx</h2><p>Ingress 使用开源的反向代理负载均衡器来实现对外暴漏服务，比如 Nginx、Apache、Haproxy等。Nginx Ingress 一般有三个组件组成：</p>
<ul>
<li>Nginx 反向代理负载均衡器</li>
<li>Ingress Controller 可以理解为控制器，它通过不断的跟 Kubernetes API 交互，实时获取后端 Service、Pod 等的变化，比如新增、删除等，然后结合 Ingress 定义的规则生成配置，然后动态更新上边的 Nginx 负载均衡器，并刷新使配置生效，来达到服务自动发现的作用。</li>
<li>Ingress 则是定义规则，通过它定义某个域名的请求过来之后转发到集群中指定的 Service。它可以通过 Yaml 文件定义，可以给一个或多个 Service 定义一个或多个 Ingress 规则。</li>
</ul>
<h4 id="1-获取官方提供的yaml"><a href="#1-获取官方提供的yaml" class="headerlink" title="1.获取官方提供的yaml"></a>1.获取官方提供的yaml</h4><figure class="highlight profile"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd flask_8s &amp;&amp;  git clone https://github.com/kubernetes/ingress-nginx.git</span><br><span class="line">[root@linux-node1 ~]# tree flask_8s/ingress-nginx/</span><br><span class="line">flask_8s/ingress-nginx/</span><br><span class="line">├── configmap.yaml               :提供configmap可以在线更行nginx的配置</span><br><span class="line">├── default-backend.yaml         :提供一个缺省的后台错误页面 <span class="number">404</span></span><br><span class="line">├── mandatory.yaml               :这个文件包含了这个目录下面所有yaml的内容，可以不用</span><br><span class="line">├── namespace.yaml               :创建一个独立的命名空间 ingress-nginx</span><br><span class="line">├── rbac.yaml			 :创建对应的role rolebinding 用于rbac</span><br><span class="line">├── tcp-services-configmap.yaml  :修改L4负载均衡配置的configmap</span><br><span class="line">├── udp-services-configmap.yaml  :修改L4负载均衡配置的configmap</span><br><span class="line">└── with-rbac.yaml               :有应用rbac的nginx-ingress-controller组件   </span><br><span class="line"></span><br><span class="line"><span class="number">0</span> directories, <span class="number">8</span> files</span><br></pre></td></tr></tbody></table></figure>

<h4 id="2-修改官方的配置"><a href="#2-修改官方的配置" class="headerlink" title="2.修改官方的配置"></a>2.修改官方的配置</h4><ol>
<li>kind: DaemonSet：官方原始文件使用的是deployment，replicate 为 1，这样将会在某一台节点上启动对应的nginx-ingress-controller pod。外部流量访问至该节点，由该节点负载分担至内部的service。测试环境考虑防止单点故障，改为DaemonSet然后删掉replicate ，配合亲和性部署在制定节点上启动nginx-ingress-controller pod，确保有多个节点启动nginx-ingress-controller pod，后续将这些节点加入到外部硬件负载均衡组实现高可用性。</li>
<li>hostNetwork: true：添加该字段，暴露nginx-ingress-controller pod的服务端口（80）</li>
<li>nodeSelector: 增加亲和性部署，有custom/ingress-controller-ready 标签的节点才会部署该DaemonSet</li>
</ol>
<h4 id="3-为需要部署nginx-ingress-controller的节点设置lable"><a href="#3-为需要部署nginx-ingress-controller的节点设置lable" class="headerlink" title="3.为需要部署nginx-ingress-controller的节点设置lable"></a>3.为需要部署nginx-ingress-controller的节点设置lable</h4><figure class="highlight moonscript"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl label nodes <span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span> custom/ingress-controller-ready=<span class="literal">true</span></span><br><span class="line">[root@linux-node1 ~]# kubectl label nodes <span class="number">192.168</span><span class="number">.56</span><span class="number">.13</span> custom/ingress-controller-ready=<span class="literal">true</span></span><br><span class="line">[root@linux-node1 ~]# kubectl get nodes <span class="comment">--show-labels</span></span><br><span class="line">NAME            STATUS    ROLES     AGE       VERSION   LABELS</span><br><span class="line"><span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span>   Ready     &lt;none&gt;    <span class="number">28</span>d       v1<span class="number">.10</span><span class="number">.1</span>   beta.kubernetes.<span class="built_in">io</span>/arch=amd64,beta.kubernetes.<span class="built_in">io</span>/<span class="built_in">os</span>=linux,custom/ingress-controller-ready=<span class="literal">true</span>,kubernetes.<span class="built_in">io</span>/hostname=<span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.56</span><span class="number">.13</span>   Ready     &lt;none&gt;    <span class="number">27</span>d       v1<span class="number">.10</span><span class="number">.1</span>   beta.kubernetes.<span class="built_in">io</span>/arch=amd64,beta.kubernetes.<span class="built_in">io</span>/<span class="built_in">os</span>=linux,custom/ingress-controller-ready=<span class="literal">true</span>,kubernetes.<span class="built_in">io</span>/hostname=<span class="number">192.168</span><span class="number">.56</span><span class="number">.13</span></span><br><span class="line"></span><br><span class="line"># 以上，因为我这里就两个计算节点；所以都打上custom/ingress-controller-ready=<span class="literal">true</span>这个标签</span><br></pre></td></tr></tbody></table></figure>

<h4 id="4-执行创建yaml文件"><a href="#4-执行创建yaml文件" class="headerlink" title="4.执行创建yaml文件:"></a>4.执行创建yaml文件:</h4><figure class="highlight sql"><table><tbody><tr><td class="code"><pre><span class="line">[root<span class="variable">@linux</span><span class="operator">-</span>node1 <span class="operator">~</span>]# kubectl <span class="keyword">create</span> <span class="operator">-</span>f namespace.yaml</span><br><span class="line">[root<span class="variable">@linux</span><span class="operator">-</span>node1 <span class="operator">~</span>]# kubectl <span class="keyword">create</span> <span class="operator">-</span>f <span class="keyword">default</span><span class="operator">-</span>backend.yaml</span><br><span class="line">[root<span class="variable">@linux</span><span class="operator">-</span>node1 <span class="operator">~</span>]# kubectl <span class="keyword">create</span> <span class="operator">-</span>f configmap.yaml</span><br><span class="line">[root<span class="variable">@linux</span><span class="operator">-</span>node1 <span class="operator">~</span>]# kubectl <span class="keyword">create</span> <span class="operator">-</span>f tcp<span class="operator">-</span>services<span class="operator">-</span>configmap.yaml</span><br><span class="line">[root<span class="variable">@linux</span><span class="operator">-</span>node1 <span class="operator">~</span>]# kubectl <span class="keyword">create</span> <span class="operator">-</span>f udp<span class="operator">-</span>services<span class="operator">-</span>configmap.yaml</span><br><span class="line">[root<span class="variable">@linux</span><span class="operator">-</span>node1 <span class="operator">~</span>]# kubectl <span class="keyword">create</span> <span class="operator">-</span>f rbac.yaml</span><br><span class="line">[root<span class="variable">@linux</span><span class="operator">-</span>node1 <span class="operator">~</span>]# kubectl <span class="keyword">create</span> <span class="operator">-</span>f <span class="keyword">with</span><span class="operator">-</span>rbac.yaml</span><br></pre></td></tr></tbody></table></figure>

<h4 id="5-查看创建状态："><a href="#5-查看创建状态：" class="headerlink" title="5.查看创建状态："></a>5.查看创建状态：</h4><p>创建过程会去拉镜像比较慢，可能会不成功，前面说过那都是因为`%#@%￥#@·的原因，<br>所以还是记得给docker一把梯子。</p>
<figure class="highlight pgsql"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl <span class="keyword">get</span> pod,svc,deployment,rc -o wide   -n ingress-nginx</span><br><span class="line"><span class="type">NAME</span>                                       READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">pod/<span class="keyword">default</span>-http-backend<span class="number">-5</span>c6d95c48-dwl56   <span class="number">1</span>/<span class="number">1</span>       Running   <span class="number">0</span>          <span class="number">16</span>h       <span class="number">10.2</span><span class="number">.42</span><span class="number">.130</span>     <span class="number">192.168</span><span class="number">.56</span><span class="number">.13</span></span><br><span class="line">pod/nginx-ingress-controller<span class="number">-55</span>trv         <span class="number">1</span>/<span class="number">1</span>       Running   <span class="number">0</span>          <span class="number">15</span>h       <span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span>   <span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span></span><br><span class="line">pod/nginx-ingress-controller<span class="number">-58</span>nf4         <span class="number">1</span>/<span class="number">1</span>       Running   <span class="number">0</span>          <span class="number">15</span>h       <span class="number">192.168</span><span class="number">.56</span><span class="number">.13</span>   <span class="number">192.168</span><span class="number">.56</span><span class="number">.13</span></span><br><span class="line"></span><br><span class="line"><span class="type">NAME</span>                           <span class="keyword">TYPE</span>        <span class="keyword">CLUSTER</span>-IP    <span class="keyword">EXTERNAL</span>-IP   PORT(S)   AGE       SELECTOR</span><br><span class="line">service/<span class="keyword">default</span>-http-backend   ClusterIP   <span class="number">10.1</span><span class="number">.89</span><span class="number">.141</span>   &lt;<span class="keyword">none</span>&gt;        <span class="number">80</span>/TCP    <span class="number">16</span>h       app=<span class="keyword">default</span>-http-backend</span><br><span class="line"></span><br><span class="line"><span class="type">NAME</span>                                         DESIRED   <span class="keyword">CURRENT</span>   UP-<span class="keyword">TO</span>-<span class="type">DATE</span>   AVAILABLE   AGE       CONTAINERS             IMAGES                                        SELECTOR</span><br><span class="line">deployment.extensions/<span class="keyword">default</span>-http-backend   <span class="number">1</span>         <span class="number">1</span>         <span class="number">1</span>            <span class="number">1</span>           <span class="number">16</span>h       <span class="keyword">default</span>-http-backend   gcr.io/google_containers/defaultbackend:<span class="number">1.4</span>   app=<span class="keyword">default</span>-http-backend</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 以上可以看到nginx-ingress-controller已经成功运行在了这两个打了标签的节点上面</span><br></pre></td></tr></tbody></table></figure>
<p>此时只是把ingress给搭建好，如果要用得根据实际情况写转发规则了，当前我们的目的是通过它定义某个域名的请求过来之后转发到集群中指定的 Service，即此次部署的 <code>flask-app-nginx</code></p>
<h4 id="6-创建规则："><a href="#6-创建规则：" class="headerlink" title="6.创建规则："></a>6.创建规则：</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ingress-nginx]<span class="comment"># cat &gt; test.ingress.yaml &lt; EOF</span></span><br><span class="line"><span class="params">apiVersion:</span> extensions<span class="symbol">/v1beta1</span></span><br><span class="line"><span class="params">kind:</span> Ingress</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test-ingress</span><br><span class="line">    <span class="params">namespace:</span> flask-app-extions-stage</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">rules:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">host:</span> test.flaskapp.ingress</span><br><span class="line">    <span class="params">http:</span></span><br><span class="line">      <span class="params">paths:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">path:</span> <span class="symbol">/</span></span><br><span class="line">        <span class="params">backend:</span></span><br><span class="line">          <span class="params">serviceName:</span> flask-app-nginx</span><br><span class="line">          <span class="params">servicePort:</span> <span class="number">80</span></span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># host: 对应的域名 </span></span><br><span class="line"><span class="comment"># path: url上下文 </span></span><br><span class="line"><span class="comment"># backend:后向转发 到对应的 serviceName: servicePort:</span></span><br><span class="line"></span><br><span class="line">[root@linux-node1 ingress-nginx]<span class="comment"># kubectl apply -f test-ingress.yaml</span></span><br><span class="line">[root@linux-node1 ingress-nginx]<span class="comment"># kubectl get ingress -n flask-app-extions-stage</span></span><br><span class="line">NAME           HOSTS                   ADDRESS   PORTS     AGE</span><br><span class="line">test-ingress   test.flaskapp.ingress             <span class="number">80</span>        <span class="number">15</span>h</span><br><span class="line">[root@linux-node1 ingress-nginx]<span class="comment"># kubectl describe ingress/test-ingress -n flask-app-extions-stage</span></span><br><span class="line"><span class="params">Name:</span>             test-ingress</span><br><span class="line"><span class="params">Namespace:</span>        flask-app-extions-stage</span><br><span class="line"><span class="params">Address:</span></span><br><span class="line">Default <span class="params">backend:</span>  default-http-backend:<span class="number">80</span> (<span class="symbol">&lt;none&gt;</span>)</span><br><span class="line"><span class="params">Rules:</span></span><br><span class="line">  Host                   Path  Backends</span><br><span class="line">  <span class="operator">-</span>---                   <span class="operator">-</span>---  <span class="operator">-</span>-------</span><br><span class="line">  test.flaskapp.ingress</span><br><span class="line">                         <span class="symbol">/</span>   flask-app-nginx:<span class="number">80</span> (<span class="symbol">&lt;none&gt;</span>)</span><br><span class="line"><span class="params">Annotations:</span></span><br><span class="line"><span class="params">Events:</span></span><br><span class="line">  Type    Reason  Age   From                      Message</span><br><span class="line">  <span class="operator">-</span>---    <span class="operator">-</span>-----  <span class="operator">-</span>---  <span class="operator">-</span>---                      <span class="operator">-</span>------</span><br><span class="line">  Normal  CREATE  <span class="number">15</span>h   nginx-ingress-controller  Ingress flask-app-extions-stage<span class="symbol">/test-ingress</span></span><br><span class="line">  Normal  CREATE  <span class="number">15</span>h   nginx-ingress-controller  Ingress flask-app-extions-stage<span class="symbol">/test-ingress</span></span><br><span class="line">  Normal  CREATE  <span class="number">15</span>h   nginx-ingress-controller  Ingress flask-app-extions-stage<span class="symbol">/test-ingress</span></span><br><span class="line">  Normal  CREATE  <span class="number">15</span>h   nginx-ingress-controller  Ingress flask-app-extions-stage<span class="operator">/</span>test-ingress</span><br></pre></td></tr></tbody></table></figure>
<h4 id="7-测试："><a href="#7-测试：" class="headerlink" title="7.测试："></a>7.测试：</h4><p>既然是通过域名访问，那这里我就在node1上面直接修改hosts的方式然后访问</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line">[<span class="string">root@linux-node1</span> <span class="string">~</span>]<span class="comment"># echo "192.168.56.12 test.flaskapp.ingress" &gt;&gt; /etc/hosts</span></span><br><span class="line">[<span class="string">root@linux-node1</span> <span class="string">~</span>]<span class="comment"># echo "192.168.56.13 test.flaskapp.ingress" &gt;&gt; /etc/hosts</span></span><br><span class="line"></span><br><span class="line">[<span class="string">root@linux-node1</span> <span class="string">ingress-nginx</span>]<span class="comment"># curl -I test.flaskapp.ingress/auth/login</span></span><br><span class="line"><span class="string">HTTP/1.1</span> <span class="number">200</span> <span class="string">OK</span></span><br><span class="line"><span class="attr">Server:</span> <span class="string">nginx/1.13.12</span></span><br><span class="line"><span class="attr">Date:</span> <span class="string">Thu,</span> <span class="number">28</span> <span class="string">Jun</span> <span class="number">2018 02:59:16 </span><span class="string">GMT</span></span><br><span class="line"><span class="attr">Content-Type:</span> <span class="string">text/html;</span> <span class="string">charset=utf-8</span></span><br><span class="line"><span class="attr">Content-Length:</span> <span class="number">4030</span></span><br><span class="line"><span class="attr">Connection:</span> <span class="string">keep-alive</span></span><br><span class="line"><span class="attr">Vary:</span> <span class="string">Accept-Encoding</span></span><br><span class="line"><span class="attr">Set-Cookie:</span> <span class="string">session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiTXpGbFlUWmlOelV4WXpabVlqTXlNVFJpWTJVMk1HWTVObVV5TURRd09HSTNPVFV4WXprd01RPT0ifX0.DhXghA.3HHbX3fvShem9KlkINr8jDGwcSc;</span> <span class="string">HttpOnly;</span> <span class="string">Path=/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者指定模拟的域名:</span></span><br><span class="line"></span><br><span class="line">[<span class="string">root@linux-node1</span> <span class="string">ingress-nginx</span>]<span class="comment"># curl -vI http://192.168.56.12/auth/login -H 'host: test.flaskapp.ingress'</span></span><br><span class="line"><span class="string">*</span> <span class="string">About</span> <span class="string">to</span> <span class="string">connect()</span> <span class="string">to</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span> <span class="string">port</span> <span class="number">80</span> <span class="string">(#0)</span></span><br><span class="line"><span class="string">*</span>   <span class="string">Trying</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span><span class="string">...</span></span><br><span class="line"><span class="string">*</span> <span class="string">Connected</span> <span class="string">to</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.12</span> <span class="string">(192.168.56.12)</span> <span class="string">port</span> <span class="number">80</span> <span class="string">(#0)</span></span><br><span class="line"><span class="string">&gt;</span> <span class="string">HEAD</span> <span class="string">/auth/login</span> <span class="string">HTTP/1.1</span></span><br><span class="line"><span class="string">&gt;</span> <span class="attr">User-Agent:</span> <span class="string">curl/7.29.0</span></span><br><span class="line"><span class="string">&gt;</span> <span class="attr">Accept:</span> <span class="string">*/*</span></span><br><span class="line"><span class="string">&gt;</span> <span class="attr">host:</span> <span class="string">test.flaskapp.ingress</span></span><br><span class="line"><span class="string">&gt;</span></span><br><span class="line"><span class="string">&lt;</span> <span class="string">HTTP/1.1</span> <span class="number">200</span> <span class="string">OK</span></span><br><span class="line"><span class="string">HTTP/1.1</span> <span class="number">200</span> <span class="string">OK</span></span><br><span class="line"><span class="string">&lt;</span> <span class="attr">Server:</span> <span class="string">nginx/1.13.12</span></span><br><span class="line"><span class="attr">Server:</span> <span class="string">nginx/1.13.12</span></span><br><span class="line"><span class="string">&lt;</span> <span class="attr">Date:</span> <span class="string">Thu,</span> <span class="number">28</span> <span class="string">Jun</span> <span class="number">2018 03:00:21 </span><span class="string">GMT</span></span><br><span class="line"><span class="attr">Date:</span> <span class="string">Thu,</span> <span class="number">28</span> <span class="string">Jun</span> <span class="number">2018 03:00:21 </span><span class="string">GMT</span></span><br><span class="line"><span class="string">&lt;</span> <span class="attr">Content-Type:</span> <span class="string">text/html;</span> <span class="string">charset=utf-8</span></span><br><span class="line"><span class="attr">Content-Type:</span> <span class="string">text/html;</span> <span class="string">charset=utf-8</span></span><br><span class="line"><span class="string">&lt;</span> <span class="attr">Content-Length:</span> <span class="number">4030</span></span><br><span class="line"><span class="attr">Content-Length:</span> <span class="number">4030</span></span><br><span class="line"><span class="string">&lt;</span> <span class="attr">Connection:</span> <span class="string">keep-alive</span></span><br><span class="line"><span class="attr">Connection:</span> <span class="string">keep-alive</span></span><br><span class="line"><span class="string">&lt;</span> <span class="attr">Vary:</span> <span class="string">Accept-Encoding</span></span><br><span class="line"><span class="attr">Vary:</span> <span class="string">Accept-Encoding</span></span><br><span class="line"><span class="string">&lt;</span> <span class="attr">Set-Cookie:</span> <span class="string">session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiT1RNMVpUWTBZV1V6TkdWaU5EazBZMkl5TmpZMFl6VTNPVFF3TVdaa09UVXpNall3T1RRNE1RPT0ifX0.DhXgxQ.BnjwR-swXvmD-kUGhtlvhpHuUIY;</span> <span class="string">HttpOnly;</span> <span class="string">Path=/</span></span><br><span class="line"><span class="attr">Set-Cookie:</span> <span class="string">session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiT1RNMVpUWTBZV1V6TkdWaU5EazBZMkl5TmpZMFl6VTNPVFF3TVdaa09UVXpNall3T1RRNE1RPT0ifX0.DhXgxQ.BnjwR-swXvmD-kUGhtlvhpHuUIY;</span> <span class="string">HttpOnly;</span> <span class="string">Path=/</span></span><br><span class="line"></span><br><span class="line"><span class="string">&lt;</span></span><br><span class="line"><span class="string">*</span> <span class="string">Connection</span> <span class="comment">#0 to host 192.168.56.12 left intact</span></span><br></pre></td></tr></tbody></table></figure>
<p>在外部如果要访问也需要绑定域名到node节点，然后访问<br><img src="https://github.com/guomaoqiu/flask_kubernetes/blob/master/screenshots/15302383360947.jpg?raw=true"></p>
<p>ok至此该项目就部署得差不多了，上面flask-app登录，注册正常！<br><img src="https://github.com/guomaoqiu/flask_kubernetes/blob/master/screenshots/15302375026023.jpg?raw=true"><br>那pod如何做到伸缩呢；执行以下命令就行了</p>
<figure class="highlight pgsql"><table><tbody><tr><td class="code"><pre><span class="line"># 目前我的flask-app是运行了<span class="number">3</span>个，怼<span class="number">20</span>个flask-app应用</span><br><span class="line">[root@linux-node1 ~]# kubectl <span class="keyword">get</span> deploy/flask-app -n flask-app-extions-stage</span><br><span class="line"><span class="type">NAME</span>        DESIRED   <span class="keyword">CURRENT</span>   UP-<span class="keyword">TO</span>-<span class="type">DATE</span>   AVAILABLE   AGE</span><br><span class="line">flask-app   <span class="number">3</span>         <span class="number">3</span>         <span class="number">3</span>            <span class="number">3</span>           <span class="number">23</span>h</span><br><span class="line">[root@linux-node1 ~]# kubectl scale <span class="comment">--replicas=20 deployment.extensions/flask-app -n flask-app-extions-stage</span></span><br><span class="line">deployment.extensions "flask-app" scaled</span><br><span class="line">[root@linux-node1 ~]# kubectl <span class="keyword">get</span> deploy/flask-app -n flask-app-extions-stage</span><br><span class="line"><span class="type">NAME</span>        DESIRED   <span class="keyword">CURRENT</span>   UP-<span class="keyword">TO</span>-<span class="type">DATE</span>   AVAILABLE   AGE</span><br><span class="line">flask-app   <span class="number">20</span>        <span class="number">20</span>        <span class="number">20</span>           <span class="number">20</span>          <span class="number">23</span>h</span><br><span class="line"># 所以副本数就是靠deploment中的 replicas 或者 命令行的scale <span class="comment">--replicas来控制</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="部署总结："><a href="#部署总结：" class="headerlink" title="部署总结："></a>部署总结：</h3><p>坑:<br>在上面我部署好ingress-nginx后，通过访问哪一步报错了；于是去查了 pod/nginx-ingress-controller-58nf4 的日志，错误日志刷屏啊<br><img src="https://github.com/guomaoqiu/flask_kubernetes/blob/master/screenshots/15301582604316.jpg?raw=true"></p>
<p>官方文档不完整啊，少了创建ingress-services的内容；<br>解决办法：</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">[root@linux-node1 ingress-nginx]<span class="comment"># cat &gt; ingress-nginx-services.yaml &lt; EOF</span></span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Service</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> ingress-nginx</span><br><span class="line">  <span class="params">namespace:</span> ingress-nginx</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">type:</span> NodePort</span><br><span class="line">  <span class="params">ports:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> http</span><br><span class="line">    <span class="params">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="params">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="params">protocol:</span> TCP</span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> https</span><br><span class="line">    <span class="params">port:</span> <span class="number">443</span></span><br><span class="line">    <span class="params">targetPort:</span> <span class="number">443</span></span><br><span class="line">    <span class="params">protocol:</span> TCP</span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">app:</span> ingress-nginx</span><br><span class="line">EOF</span><br><span class="line">[root@linux-node1 ingress-nginx]<span class="comment"># kubectl create -f ingress-nginx-services.yaml</span></span><br></pre></td></tr></tbody></table></figure>

<p>部署遇到的主要知识点：</p>
<ul>
<li>K8S deployment的yaml文件编写；</li>
<li>K8S 持久化存储NFS方式；配置管理ConfigMap;</li>
<li>K8S 集群中各种端口/IP类型的工作模式以及服务暴露方式；</li>
</ul>
<p>by the way: 可能看到我创建的pod等资源的AGE已经是过去十几个小时，这个没关系的；前面创建了之后笔记就停顿了一下。后续才继续写的🍺🍺🍺</p>
]]></content>
      <categories>
        <category>kubernets</category>
      </categories>
      <tags>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title>Flask+bootstrap写登录页面</title>
    <url>/2016/07/14/flaskbootstrap-e5-86-99-e7-99-bb-e5-bd-95-e9-a1-b5-e9-9d-a2/</url>
    <content><![CDATA[<p>最近有些需求、想法，毕竟devops的风还是很强烈的啊，所以就跟风学学dev方面的东西； flask是一个很小巧很方便的webframe，听朋友说起django非常的重，于是我就在还有点python基础的能力下选择flask学学；准备用这个框架开发新的平台，首先就要有用户登录页面，用flask可以这样实现： 代码结构：</p>
<p>flasky<br>├── run.py<br>├── static<br>│&nbsp;&nbsp; ├── av1.jpg<br>│&nbsp;&nbsp; ├── av2.jpg<br>│&nbsp;&nbsp; ├── av3.jpg<br>│&nbsp;&nbsp; ├── bootstrap.min.css<br>│&nbsp;&nbsp; ├── bootstrap.min.js<br>│&nbsp;&nbsp; ├── bootstrap-responsive.min.css<br>│&nbsp;&nbsp; ├── excanvas.min.js<br>│&nbsp;&nbsp; ├── fullcalendar.css<br>│&nbsp;&nbsp; ├── fullcalendar.min.js<br>│&nbsp;&nbsp; ├── glyphicons-halflings.png<br>│&nbsp;&nbsp; ├── GNU-Linux-Logo-Penguin-SVG.png<br>│&nbsp;&nbsp; ├── jquery.flot.min.js<br>│&nbsp;&nbsp; ├── jquery.flot.resize.min.js<br>│&nbsp;&nbsp; ├── jquery.min.js<br>│&nbsp;&nbsp; ├── jquery.peity.min.js<br>│&nbsp;&nbsp; ├── jquery.ui.custom.js<br>│&nbsp;&nbsp; ├── logo.png<br>│&nbsp;&nbsp; ├── Tux.jpg<br>│&nbsp;&nbsp; ├── unicorn.dashboard.js<br>│&nbsp;&nbsp; ├── unicorn.grey.css<br>│&nbsp;&nbsp; ├── unicorn.js<br>│&nbsp;&nbsp; ├── unicorn.login.css<br>│&nbsp;&nbsp; ├── unicorn.login.js<br>│&nbsp;&nbsp; └── unicorn.main.css<br>└── templates<br>    ├── base.html<br>    ├── check.html<br>    ├── index.html<br>    ├── login.html<br>    └── upload.html</p>
<p>&nbsp; 前端就用bootstrap展示，login.html</p>
<figure class="highlight django"><table><tbody><tr><td class="code"><pre><span class="line"><span class="template-tag">{% <span class="name"><span class="name">extends</span></span> \"base.html\" %}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">{% <span class="name"><span class="name">block</span></span> title %}</span><span class="language-xml"> Dachen FTP </span><span class="template-tag">{% <span class="name"><span class="name">endblock</span></span> %}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">style</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">{% <span class="name"><span class="name">block</span></span> body %}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">       <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"logo"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">            <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"{ {url_for('static',filename='GNU-Linux-Logo-Penguin-SVG.png')}}"</span> <span class="attr">alt</span>=<span class="string">""</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"loginbox"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">            <span class="tag">&lt;<span class="name">form</span> <span class="attr">id</span>=<span class="string">"loginform"</span> <span class="attr">class</span>=<span class="string">"form-vertical"</span> <span class="attr">action</span>=<span class="string">""</span> <span class="attr">method</span>=<span class="string">"post"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                                <span class="tag">&lt;<span class="name">p</span>&gt;</span>Sing in to blog.sctux.com<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                <span class="comment">&lt;!--&lt;div class="control-group"&gt; --&gt;</span></span></span><br><span class="line"><span class="language-xml">                <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"controls"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"input-prepend"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                            <span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">"username"</span> <span class="attr">value</span>=<span class="string">"{ {request.form.username}}"</span> <span class="attr">placeholder</span>=<span class="string">"Username"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">               &lt;!\-\- <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"control-group"</span>&gt;</span> --&gt;</span></span><br><span class="line"><span class="language-xml">                <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"controls"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"input-prepend"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                          <span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">"pass"</span> <span class="attr">placeholder</span>=<span class="string">"Password "</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">                <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-group"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"col-md-90 col-md-offset-12"</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">class</span>=<span class="string">"btn btn-primary"</span>&gt;</span>Sign in<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      </span><span class="template-tag">{% <span class="name"><span class="name">if</span></span> error %}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">	<span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">"red"</span>&gt;</span>{ { error }}<span class="tag">&lt;/<span class="name">font</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      </span><span class="template-tag">{% <span class="name"><span class="name">endif</span></span> %}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">            <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">        </span><span class="template-tag">{% <span class="name"><span class="name">endblock</span></span> %}</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure>
<p>&nbsp; run.py内容：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">\<span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> flask.ext.wtf <span class="keyword">import</span> Form</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render\_template, redirect, url\_<span class="keyword">for</span></span><br><span class="line"></span><br><span class="line">app = Flask(\_\_name\_\_)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">'/login'</span>,methods=\[<span class="string">'POST'</span>,<span class="string">'GET'</span>\]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">login</span>():</span><br><span class="line">  error=<span class="literal">False</span></span><br><span class="line">  <span class="keyword">if</span> request.method == <span class="string">'POST'</span>:</span><br><span class="line">    <span class="keyword">if</span> request.form\[<span class="string">'username'</span>\] != <span class="string">'user1'</span> <span class="keyword">or</span> request.form\[<span class="string">'pass'</span>\] != <span class="string">'user1@1qaz'</span>:</span><br><span class="line">      error=<span class="string">"username or password error !"</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> redirect(url_for(<span class="string">'index'</span>))</span><br><span class="line">  <span class="keyword">return</span> render_template(<span class="string">'login.html'</span>,error=error)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">'/index'</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">  <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>,result=result)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">'/upload'</span>,methods=\[<span class="string">'GET'</span>,<span class="string">'POST'</span>\]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upload</span>():</span><br><span class="line">  <span class="keyword">return</span> render_template(<span class="string">'upload.html'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">'/check'</span>,methods=\[<span class="string">'POST'</span>,<span class="string">'GET'</span>\]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check</span>():</span><br><span class="line">  <span class="keyword">return</span> render_template(<span class="string">'check.html'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">'/logout'</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logout</span>():</span><br><span class="line">  <span class="keyword">return</span> redirect(url_for(<span class="string">'login'</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> \_\_name\_\_==<span class="string">"\_\_main\_\_"</span>:</span><br><span class="line">   app.run(debug=<span class="literal">True</span>,host=<span class="string">'xxxxxxxxx'</span>,port=<span class="number">4000</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>启动 python run.py 访问 <a href="http://xxxxxxxxx:4000/">http://xxxxxxxxx:4000/</a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/07/login-page-1.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/07/login-page-1.jpg" alt="login page"></a> 登录主页的效果： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/07/2222.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/07/2222.jpg" alt="index"></a> 好啦，一个简单的登录页面就写好啦；flask 值得你拥有!</p>
]]></content>
      <categories>
        <category>Python</category>
        <category>脚本编程</category>
      </categories>
      <tags>
        <tag>bootstrap</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title>Flask-Login AttributeError: &#39;Bool&#39; Object Has No Attribute &#39;__Call__&#39;</title>
    <url>/2016/06/06/flasklogin-yong-hu-pan-duan-shi-hou-bao-cuo/</url>
    <content><![CDATA[<h4 id="今天在使用flask-login添加用户认证的时候服务器出现了报错"><a href="#今天在使用flask-login添加用户认证的时候服务器出现了报错" class="headerlink" title="今天在使用flask-login添加用户认证的时候服务器出现了报错"></a>今天在使用flask-login添加用户认证的时候服务器出现了报错</h4><p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/0.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/0-300x140.jpg" alt="0"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/1.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/1.jpg" alt="1"></a></p>
<h4 id="而我的-base-html是这样写的："><a href="#而我的-base-html是这样写的：" class="headerlink" title="而我的 base.html是这样写的："></a>而我的 base.html是这样写的：</h4><p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/2.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/2-300x56.jpg" alt="2"></a> ￼去看官方文档吧，选择对应版本的文档(我这里是0.3.1): <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/3.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/3-300x78.jpg" alt="3"></a> ￼再看看其前面的版本是不是这样定义的： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/4.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/06/4-300x68.jpg" alt="4"></a>￼</p>
<h4 id="看吧-版本问题造成。"><a href="#看吧-版本问题造成。" class="headerlink" title="看吧 版本问题造成。"></a>看吧 版本问题造成。</h4>]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
  </entry>
  <entry>
    <title>Git 代码拉取、同步、通知</title>
    <url>/2015/09/14/git-e4-bb-a3-e7-a0-81-e6-8b-89-e5-8f-96-e3-80-81-e5-90-8c-e6-ad-a5-e3-80-81-e9-80-9a-e7-9f-a5/</url>
    <content><![CDATA[<p>代码托管到gtihub，开发人员进行本地修改push到仓库后，这里使用pull在web服务器上定期拉去代码至开发开发服务器</p>
<p>more code_sync_template.sh<br>#!/bin/sh<br>update_dir=”/home/demo/“ #every time upload code main dir<br>webroot=”/var/www/template”<br>#bak_dir=”/codedata/bak/deploy” #backup file main dir<br>gibin=”/usr/bin/git” #git bin path<br>rsync_bin=”/usr/bin/rsync”  #rsync path<br>#这个变量定义的是代码拉取到本地之后同步到web目录时过排除一些特定的文件<br>#rsync_exclude=”.svn|<em>.gz|</em>.cache|<em>.txt|</em>.git|_tmp|temp|pma|dsn.php|gem*|global_config.php<br>“<br>Name=”template”<br>LOG=”/var/log/code_sync_template.log”<br>TIME=`date +%F\ %T`<br>OLD_IFS=”$IFS”  #backup IFS<br>COMMAND=”sed -n ‘/$TIME start/,\$p’ $LOG &gt; /tmp/template.log”</p>
<p>#get gitos code to local<br>pull_code()<br>{<br>if [ -d $update_dir/$Name ];then</p>
<pre><code>  if cd $update_dir/$Name &amp;&amp; $gibin pull git@这里是你的代码仓库地址 &amp;&gt;&gt;
</code></pre>
<p> $LOG ;then<br>           echo -e “\033[32m `date` git pull code success \033[m”<br>      else<br>           echo -e “\033[31m `date` git pull code error \033[m” &amp;&amp; exit 1<br>      fi<br>else<br>     echo -e “\033[32m Clone code to local. \033[m”<br>     cd $update_dir &amp;&amp; $gibin clone git@这里是你的代码仓库地址<br>fi<br>}</p>
<p>#sync local code to webroot<br>#定义的是将最新拉取到的代码同步到web目录<br>sync_code()<br>{	<br>#$rsync_bin -uavz $update_dir/$Name $webroot/$Name<br>if [ “$rsync_exclude” ];then<br>IFS=”|”<br>   exclude_arr=($rsync_exclude)<br>     for e_arr in ${exclude_arr[@]};do<br>          echo $e_arr &gt;&gt;/tmp/code_exclude_list<br>     done<br>	$rsync_bin -avz –exclude-from=”/tmp/code_exclude_list”  $update_dir/$Name/* $webr<br>oot/<br>else<br>	$rsync_bin -avz  $update_dir/$Name/* $webroot/<br>fi<br>IFS=”$OLD_IFS”<br>}</p>
<p>Notice(){<br>        #这个是通过一个eval命令来执行一段命令，这个命令描述的是从上次脚本执行记录日志后开始到最后的日志信息<br>eval $COMMAND<br>        #这里是在过滤此次同步是记录的日志中pull有没有文件更新<br>grep -E “files changed|Fast-forward” /tmp/template.log<br>if [ $? -eq 0 ];then<br>        #这里使用一个python脚本来获取pull时更新了的具体文件有哪些，然后将内容通过邮件发送到指定邮箱<br>	python /home/demo/mail-template.py<br>else<br>        exit 5<br>fi<br>}</p>
<p>echo ‘ ‘ &gt;&gt; $LOG<br>echo -e “\033[42;37m ############ Template $TIME start ############ \033[m” &gt;&gt; $LOG<br>echo -e “\033[47;30m Template Pull remote code to local. \033[0m” &gt;&gt; $LOG<br>echo -e “\033[47;30m Template Pull remote code to local. \033[0m”<br>pull_code &gt;&gt; $LOG<br>echo -e “\033[47;30m Template Sync local code to wwwroot.\033[0m” &gt;&gt; $LOG<br>echo -e “\033[47;30m Template Sync local code to wwwroot.\033[0m”<br>sync_code &gt;&gt; $LOG<br>echo -e “\033[46;37m ############ Template $TIME  end  ############ \033[m” &gt;&gt; $LOG<br>echo ‘ ‘ &gt;&gt; $LOG<br>Notice</p>
<p>&nbsp; 邮件通知脚本，能够在自定义的时间周期内获取到最新代码时 能发送邮件到指定的邮箱，能够自动提醒管理人员知悉这一动作。</p>
<p>#!/usr/bin/env python<br>#-*- coding: UTF-8 -*-<br>import smtplib<br>import os,sys<br>from email.mime.text import MIMEText</p>
<p>mailto_list=[‘<a href="mailto:guomaoqiu@gmail.com">guomaoqiu@gmail.com</a>‘]<br>mail_host=’smtp.qq.com’<br>mail_port=’25’<br>mail_user=’2399447849’<br>mail_pass=’xxxxxx’<br>mail_postfix=’qq.com’</p>
<p>filename = “/tmp/template.log”</p>
<p>fo = open(filename,”rb”)<br>filecon = fo.read();<br>str1 = “</p><pre>{0}</pre>“.format(filecon)<p></p>
<p>def send_mail(to_list,sub,content):<br>    me=”Code Sync Notice【Template】”+”&lt;”+mail_user+”@”+mail_postfix+”&gt;”<br>    msg = MIMEText(content,_subtype=’html’,_charset=’utt-8’)<br>    msg[‘Subject’] = sub<br>    msg[‘From’]=me<br>    msg[‘to’]=”;”.join(to_list)<br>    try:<br>        s = smtplib.SMTP()<br>   	s.connect(mail_host)<br>	s.login(mail_user,mail_pass)<br>	s.sendmail(me,to_list,msg.as_string())<br>	s.close()<br>	return True<br>    except Exception, e:<br>	print str(e)<br>	return False<br>if __name__==’__main__‘:<br>   if send_mail(mailto_list,”New Files Are Added”,str1):<br>	print “发送成功”<br>   else:<br>	print “发送失败”</p>
]]></content>
      <categories>
        <category>Python</category>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title>How to Change Ingress-Nginx Nginx&#39;s Configuration？</title>
    <url>/2018/07/03/how-to-change-ingressnginx-nginxs-configuration/</url>
    <content><![CDATA[<h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>今天访问了一下之前搭建的那套k8s应用，当我客户端通过域名访问(ingress-nginx通过域名转发请求到后端)时，出现了504的情况；第一想到的就是当后端一直没返回时；proxy超时了；</p>
<pre><code>192.168.56.1 - [192.168.56.1] - - [03/Jul/2018:09:16:54 +0000] "GET / HTTP/1.1" 504 586 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36" 412 10.002 [flask-app-extions-stage-flask-app-nginx-80] 10.2.17.5:80 0 10.003 504 497fe6db73abcd3ed749fdc646870432

# 响应10.003秒就报504😢
</code></pre>
<p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15306117528798.jpg">￼<br>问题就出在这里,ingress-nginx代理用户请求到flask-app-nginx SVC<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15306121028954.jpg">￼</p>
<p>而且后端在集群内部直接访问应用的ClusterIP是没有问题；<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15306118758212.jpg">￼</p>
<p>那解决办法就是修改ingress-nginx 的nginx配置；前面实验中我使用了configmap来挂nginx配置，ingerss-nginx的配置也是通过这种方式，只是说实现的方式不一样；他是在创建了ingress之后根据规则生成的nginx配置；<br>进入pod容器中发现默认的nginx配置参数为:</p>
<pre><code>            proxy_connect_timeout                   5s;
            proxy_send_timeout                      60s;
            proxy_read_timeout                      60s;
</code></pre>
<p>这个时间还是蛮短的，所以我需要修改它，那咋个修改嘛？不可能vi nginx.conf 然后保存，再从新reload pod？ 这样能解决，但是不合理，于是官网提供给了这种方式:<br><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/</a><br>也就是在官方提供的configmap.yaml中 添加覆盖原有默认值</p>
<pre><code>[root@linux-node1 ingress-nginx]# more configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
data:
  proxy-connect-timeout: "30"
  proxy-read-timeout: "120"
  proxy-send-timeout: "120"
</code></pre>
<p>然后apply一下</p>
<pre><code>kubectl apply -f configmap.yaml 
</code></pre>
<p>在执行这个命令的时候pod会去重新加载配置configmap的信息<br>通过pod的日志可以看到:</p>
<pre><code>[root@linux-node1 ~]# kubectl logs -f pod/nginx-ingress-controller-t9jh9 -n ingress-nginx
I0703 10:21:24.014062       5 event.go:218] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"nginx-configuration", UID:"df01bd6b-79f4-11e8-95a2-000c29c6d12b", APIVersion:"v1", ResourceVersion:"1013650", FieldPath:""}): type: 'Normal' reason: 'UPDATE' ConfigMap ingress-nginx/nginx-configuration
I0703 10:21:24.015721       5 controller.go:169] Configuration changes detected, backend reload required.
I0703 10:21:24.539971       5 controller.go:179] Backend successfully reloaded.
</code></pre>
<p>再一次进入容器查看nginx的配置参数是否改变:</p>
<pre><code>            proxy_connect_timeout                   30s;
            proxy_send_timeout                      120s;
            proxy_read_timeout                      120s;
</code></pre>
<p>可以看到原来默认的配置参数值已经修改成功；<br>客户端再次访问就不会出现这种问题了：<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15306159339443.jpg">￼</p>
]]></content>
      <categories>
        <category>Docker</category>
        <category>虚拟化&amp;amp;云计算&amp;amp;大数据</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Install Gateone(WebSSH)</title>
    <url>/2017/02/21/how-to-install-gateonewebssh/</url>
    <content><![CDATA[<h5 id="1-Install-dependent-package-CentOS7"><a href="#1-Install-dependent-package-CentOS7" class="headerlink" title="1.Install dependent package(CentOS7)"></a>1.Install dependent package(CentOS7)</h5><pre><code>yum install python-pip
</code></pre>
<h5 id="2-Get-install-source-code"><a href="#2-Get-install-source-code" class="headerlink" title="2.Get install source code"></a>2.Get install source code</h5><pre><code>git clone https://github.com/liftoff/GateOne.git
cd  GateOne/
python setup.py install
</code></pre>
<h5 id="3-Start-this-service-will-Generate-the-configuration-file-etc-gateone"><a href="#3-Start-this-service-will-Generate-the-configuration-file-etc-gateone" class="headerlink" title="3. Start this service will Generate the configuration file(/etc/gateone/*)"></a>3. Start this service will Generate the configuration file(/etc/gateone/*)</h5><pre><code>/etc/gateone # GateOne's default install dir
service gateone start
</code></pre>
<h5 id="4-You-will-see-some-messages-flash-by-wait-untill-you-see-something-like-GateOne-will-by-default-run-on-443-https-and-will-allow-access-only-from-the-hostname-that-it-autodetects-so-make-sure-that-whaterver-URL-you-normally-access-this-server-through-is-listed-in-GateOne’s-orgin-list"><a href="#4-You-will-see-some-messages-flash-by-wait-untill-you-see-something-like-GateOne-will-by-default-run-on-443-https-and-will-allow-access-only-from-the-hostname-that-it-autodetects-so-make-sure-that-whaterver-URL-you-normally-access-this-server-through-is-listed-in-GateOne’s-orgin-list" class="headerlink" title="4. You will see some messages flash by, wait untill you see something like:GateOne will by default run on 443(https), and will allow access only from the hostname that it autodetects, so make sure that whaterver URL you normally access, this server through is listed in GateOne’s orgin list:"></a>4. You will see some messages flash by, wait untill you see something like:GateOne will by default run on 443(https), and will allow access only from the hostname that it autodetects, so make sure that whaterver URL you normally access, this server through is listed in GateOne’s orgin list:</h5><pre><code>Listening on https://*:443/
vim /etc/gateone/conf.d/10server.conf
</code></pre>
<h5 id="5-Now-run"><a href="#5-Now-run" class="headerlink" title="5.Now run:"></a>5.Now run:</h5><pre><code>/etc/init.d/gateone start 
or
service gateone start
</code></pre>
<h5 id="6-Access-Webssh"><a href="#6-Access-Webssh" class="headerlink" title="6.Access Webssh:"></a>6.Access Webssh:</h5><pre><code>https://YOUR_SERVER_IP
</code></pre>
<h5 id="like-this"><a href="#like-this" class="headerlink" title="like this:"></a>like this:</h5><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/02/14876901172167-2.jpg">￼</p>
<h5 id="Sure-the-GateOne-has-Docker-Version-you-can"><a href="#Sure-the-GateOne-has-Docker-Version-you-can" class="headerlink" title="Sure ,the GateOne has Docker Version, you can:"></a>Sure ,the GateOne has Docker Version, you can:</h5><pre><code>docker pull gateone
docker run -d --name=gateone -p 443:8000 liftoff/gateone
or
git clone https://github.com/liftoff/GateOne.git
cd  GateOne/docker
docker build -t gateone .
</code></pre>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
      <tags>
        <tag>系统运维</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Use Mojo/webqq Send QQ Message</title>
    <url>/2017/04/06/how-to-use-mojowebqq-send-qq-message/</url>
    <content><![CDATA[<h4 id="1-get-docker-images"><a href="#1-get-docker-images" class="headerlink" title="1. get docker images"></a>1. get docker images</h4><pre><code>docker pull sjdy521/mojo-webqq
</code></pre>
<h4 id="2-run-it"><a href="#2-run-it" class="headerlink" title="2. run it"></a>2. run it</h4><pre><code>docker run -d -p 9999:5000 -v /tmp/:/tmp/ sjdy521/mojo-webqq
</code></pre>
<h4 id="3-check-logs"><a href="#3-check-logs" class="headerlink" title="3. check logs"></a>3. check logs</h4><pre><code>docker logs -f CONTAINER_ID
[17/04/07 15:57:13] [info] 当前正在使用 Mojo-Webqq v2.0.8
[17/04/07 15:57:13] [warn] 当前版本与1.x.x版本不兼容，改动详情参见更新日志
[17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::UploadQRcode ]
[17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::ShowMsg ]
[17/04/07 15:57:13] [info] 执行插件[ Mojo::Webqq::Plugin::Openqq ]
[17/04/07 15:57:13] [info] Listening at "http://0.0.0.0:5000"
Server available at http://0.0.0.0:5000
[17/04/07 15:57:13] [info] 初始化 smartqq 客户端参数...
[17/04/07 15:57:13] [info] 正在获取登录二维码...
[17/04/07 15:57:13] [info] 二维码已下载到本地[ /tmp/mojo_webqq_qrcode_default.png ]
[17/04/07 15:57:15] [info] 二维码已上传云存储[ https://ooo.0o0.ooo/2017/04/07/58e7465b89582.png ]
[17/04/07 15:57:15] [info] 等待手机QQ扫描二维码...
[17/04/07 15:58:01] [info] 手机QQ扫码成功，请在手机上点击[允许登录smartQQ]按钮...
[17/04/07 15:59:11] [info] 检查安全代码...
[17/04/07 15:59:11] [info] 获取数据验证参数...
[17/04/07 15:59:11] [info] 尝试进行登录(2)...
[17/04/07 15:59:12] [info] 帐号(xxxxxxxxxxxxx)登录成功
</code></pre>
<h4 id="4-scan-qrcode"><a href="#4-scan-qrcode" class="headerlink" title="4. scan qrcode"></a>4. scan qrcode</h4><pre><code>/tmp/mojo_webqq_qrcode_default.png
</code></pre>
<h4 id="5-sendmessage"><a href="#5-sendmessage" class="headerlink" title="5. sendmessage:"></a>5. sendmessage:</h4><pre><code>curl "http://YOUR_SERVER_IP:9999/openqq/send_friend_message?uid=friends_qq_num&amp;content=hello"
curl "http://YOUR_SERVER_IP:9999/openqq/send_group_message?uid=group_qq_number&amp;content=hehe"
</code></pre>
<h4 id="6-other-system-use"><a href="#6-other-system-use" class="headerlink" title="6. other system use:"></a>6. other system use:</h4><pre><code>def send_message(number, content):
  url = "http://YOUR_SERVER_IP:9999/openqq/send_friend_message"
  data = {
        "uid": number,
        "content": content,
  }
  requests.get(url=url,data=data)
  
if __name__ == "__main__":
  send_message('2399447849','hello')
</code></pre>
]]></content>
      <categories>
        <category>Monitor</category>
      </categories>
  </entry>
  <entry>
    <title>Jenkins配置令牌远程触发项目构建</title>
    <url>/2016/04/18/jenkins-e9-85-8d-e7-bd-ae-e4-bb-a4-e7-89-8c-e8-bf-9c-e7-a8-8b-e8-a7-a6-e5-8f-91-e9-a1-b9-e7-9b-ae-e6-9e-84-e5-bb-ba/</url>
    <content><![CDATA[<p>我们在执行Jenkins的项目构建的时候一般都是通过web管理界面中的”构建”来执行项目构建操作，但是除此之外我们还可以通过项目配置中的”构建触发器”来触发构建操作，其中”构建触发器”有一种方式是通过配置令牌远程触发项目构建；要启用Token(令牌)远程触发项目构建首先要保证Jenkins服务安装了<a href="https://wiki.jenkins-ci.org/display/JENKINS/Build+Token+Root+Plugin">build-token-root</a>&nbsp;插件，并且配置了Jenkins的身份验证(不是必须)。 该如何使用这个插件呢？ 打开一个Jenkins Project –&gt; 配置 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/04/11.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/04/11.jpg" alt="build"></a>然后在我们自己的工作机上编写一条远程执行的命令即可，其实就是一条POST请求，如： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/04/11111.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/04/11111.jpg" alt="11111"></a>  在我们工作机上直接执行该curl请求就可以执行项目的构建啦，而不再需要点击页面中的”构建”，用这种方式主要还是习惯吧，敲命令的感觉还是挺好的；一般为了方便我都是写到一个脚本里面去执行，如果有多个项目也可以通过传参啊，多个参数(项目)一起执行构建；看自己的工作环境而定吧，在解决问题的情况下，加快个人工作效率的情况下还是怎么简单怎么来吧。下图就是我执行了这条命令的结果，跟我们手动在web界面点击”构建”是一样的效果。 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/04/222.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/04/222.jpg" alt="222"></a></p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>K8s Yaml编写小技巧</title>
    <url>/2018/12/09/k8s-yaml-bian-xie-xiao-ji-qiao/</url>
    <content><![CDATA[<span id="more"></span>

<p>学习使用k8s的童鞋都知道我们在部署pod的时候有时候需要手动去编写一些yaml文件；比如我需要编写deployment,那除了在其他地方粘贴拷贝外有没有其他方法呢？答案是有的</p>
<h3 id="1-用run命令生成，然后作为模板进行编辑。"><a href="#1-用run命令生成，然后作为模板进行编辑。" class="headerlink" title="1.用run命令生成，然后作为模板进行编辑。"></a>1.用run命令生成，然后作为模板进行编辑。</h3><figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">kubectl <span class="built_in">run</span> <span class="attribute">--image</span>=nginx my-deploy -o yaml --dry-<span class="built_in">run</span> &gt; my-deploy.yaml </span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line"><span class="comment">### 2.用get命令导出，然后作为模板进行编辑。</span></span><br><span class="line">```  </span><br><span class="line"><span class="comment"># 注意: --export 是为了去除当前正在运行的这个deployment生成的一些状态，我们用不到就过滤掉</span></span><br><span class="line">kubectl <span class="built_in">get</span> deployment/nginx <span class="attribute">-o</span>=yaml --<span class="built_in">export</span>  &gt; new.yaml</span><br><span class="line">```      </span><br><span class="line"></span><br><span class="line"><span class="comment">### 3.Pod亲和性下面字段的拼写忘记了</span></span><br></pre></td></tr></tbody></table></figure>
<p>kubectl explain pod.spec.affinity.podAffinity</p>
<pre><code>
示例:
---

我想生成一个有三个副本的redis pod的yaml，然后我想把这三个pod 通过node亲和性调度到同一个node节点上面；

### 1\. 我这里用kubectl run来生成:

    kubectl run redis --image=redis --replicas=3 --dry-run -o yaml &gt; redis_node_affinity.yaml
    

### 2\. 手写亲和性策略：

额 问题来了亲和性策略的字段我记不住啊，怎么办？那就需要通过  
`kubectl explain RESOURCE [options]`来获取资源文档

怎么用?  
比如我这里是要为pod做node的亲和性，那么一定是这个api接口下面的配置文档:想看pod的资源文档:

    [root@k8s-m1 ~]# kubectl explain pod.spec.affinity
    KIND:     Pod
    VERSION:  v1
    
    RESOURCE: affinity &lt;Object&gt;
    
    DESCRIPTION:
         If specified, the pod's scheduling constraints
    
         Affinity is a group of affinity scheduling rules.
    
    FIELDS:
       nodeAffinity &lt;Object&gt;
         Describes node affinity scheduling rules for the pod.
    
       podAffinity  &lt;Object&gt;
         Describes pod affinity scheduling rules (e.g. co-locate this pod in the
         same node, zone, etc. as some other pod(s)).
    
       podAntiAffinity  &lt;Object&gt;
         Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod
         in the same node, zone, etc. as some other pod(s)).
    
    [root@k8s-m1 ~]#
    

上面我们通过 `pod.spec.affinity` 定位到了`nodeAffinity` 文档, 这些字段也是yaml种使用的字段，随后我通过一层一层的定位就大体上知道这些字段在yaml中是怎么使用的啦~

    [root@k8s-m1 ~]# kubectl explain pods.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchExpressions
    

最后快速生成并且编辑的deployment yaml就写好了。

    apiVersion: apps/v1beta1
    kind: Deployment
    metadata:
      creationTimestamp: null
      labels:
        run: redis
      name: redis
    spec:
      replicas: 3
      selector:
        matchLabels:
          run: redis
      strategy: {}
      template:
        metadata:
          creationTimestamp: null
          labels:
            run: redis
        spec:
          containers:
          - image: redis
            name: redis
            resources: {}
          # 以下内容就是我通过explain参数来查询到的我想要的字段写的
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                 nodeSelectorTerms:
                   - matchExpressions:
                       - key: kubernetes.io/hostname
                         operator: In
                         values:
                           - k8s-m1
    status: {}
    

![](https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/12/15443718852925.jpg)￼  
还是非常快速高效的。
</code></pre>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Jq命令-在命令行直接解析Json文档</title>
    <url>/2016/06/20/jq-ming-ling-zai-ming-ling-xing-zhi-jie-jie-xijson/</url>
    <content><![CDATA[<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><pre><code>yum install -y libtool &amp;&amp; \
wget https://github.com/stedolan/jq/releases/download/jq-1.5/jq-1.5.tar.gz &amp;&amp; \
tar -xf jq-1.5.tar.gz -C /usr/local  &amp;&amp; \
cd /usr/local/jq-1.5 &amp;&amp; \
./configure --disable-maintainer-mode &amp;&amp; \
make LDFLAGS=-all-static &amp;&amp; \
make install &amp;&amp; \
cp /usr/local/jq-1.5/jq /sbin/ \
echo "程序已安装: `which jq`"
</code></pre>
<h4 id="举个栗子，如："><a href="#举个栗子，如：" class="headerlink" title="举个栗子，如："></a>举个栗子，如：</h4><h5 id="我这里有个API请求，在终端中得到的结果是一串json字符串，但是看起来不是那么规整，一下子还很难判断是什么格式；"><a href="#我这里有个API请求，在终端中得到的结果是一串json字符串，但是看起来不是那么规整，一下子还很难判断是什么格式；" class="headerlink" title="我这里有个API请求，在终端中得到的结果是一串json字符串，但是看起来不是那么规整，一下子还很难判断是什么格式；"></a>我这里有个API请求，在终端中得到的结果是一串json字符串，但是看起来不是那么规整，一下子还很难判断是什么格式；</h5><pre><code>[root@localhost ~]# curl -s http://SERVER_IP/health/monitor/service
{"data":{"serCode":"0","serDesc":"服务正常"},"resultCode":1}
</code></pre>
<h5 id="于是通过jq这个命令我们格式化一下，终端里面看到的结果就是下面这样的啦"><a href="#于是通过jq这个命令我们格式化一下，终端里面看到的结果就是下面这样的啦" class="headerlink" title="于是通过jq这个命令我们格式化一下，终端里面看到的结果就是下面这样的啦"></a>于是通过jq这个命令我们格式化一下，终端里面看到的结果就是下面这样的啦</h5><pre><code>[root@salt-api ~]# curl -s http://SERVER_IP/health/monitor/service | jq
{
  "data": {
    "serCode": "0",
    "serDesc": "服务正常"
  },
  "resultCode": 1
}
</code></pre>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>jq</tag>
      </tags>
  </entry>
  <entry>
    <title>理解Kubernetes中的认证&amp;授权&amp;准入机制</title>
    <url>/2018/12/16/kubernetes-auth/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>首先需要了解这三种机制的区别：</p>
<ul>
<li>认证(Authenticating)是对客户端的认证，通俗点就是用户名密码验证，</li>
<li>授权(Authorization)是对资源的授权，k8s中的资源无非是容器，最终其实就是容器的计算，网络，存储资源，当一个请求经过认证后，需要访问某一个资源（比如创建一个pod），授权检查都会通过访问策略比较该请求上下文的属性，（比如用户，资源和Namespace），根据授权规则判定该资源（比如某namespace下的pod）是否是该客户可访问的。</li>
<li>准入(Admission Control)机制是一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。<br>在k8s中，这三种机制如下图：<br><img src="/2018/12/16/kubernetes-auth/k8s-authorition.png" alt="k8s-authorition"><br>k8s的整体架构也是一个微服务的架构，所有的请求都是通过一个GateWay，也就是kube-apiserver这个组件（对外提供REST服务），由图中可以看出，k8s中客户端有两类，一种是普通用户，一种是集群内的Pod，这两种客户端的认证机制略有不同，后文会详述。但无论是哪一种，都需要依次经过认证，授权，准入这三个机制。<br><img src="/2018/12/16/kubernetes-auth/auththentication&amp;authorization.png" alt="截图来自华为CCE云课堂"></li>
</ul>
<h1 id="kubernetes-中的认证机制"><a href="#kubernetes-中的认证机制" class="headerlink" title="kubernetes 中的认证机制"></a>kubernetes 中的认证机制</h1><p>需要注意的是，kubernetes虽然提供了多种认证机制，但并没有提供user 实体信息的存储，也就是说，账户体系需要我们自己去做维护。当然，也可以接入第三方账户体系（如谷歌账户），也可以使用开源的keystone去做整合。kubernetes 支持多种认证机制，可以配置成多个认证体制共存，这样，只要有一个认证通过，这个request就认证通过了。下面列举的是官网几种常见认证机制：</p>
<ul>
<li>X509 Client Certs</li>
<li>Static Token File</li>
<li>Bootstrap Tokens</li>
<li>Static Password File</li>
<li>Service Account Tokens</li>
<li>OpenID Connect Tokens</li>
</ul>
<p>这里我主要还是理解一下常用认证方式:</p>
<h2 id="X509-Client-Certs"><a href="#X509-Client-Certs" class="headerlink" title="X509 Client Certs"></a>X509 Client Certs</h2><p>也叫作双向数字证书认证，HTTPS证书认证，是基于CA根证书签名的双向数字证书认证方式，是所有认证方式中最严格的认证。默认在kubeadm创建的集群中是enabled的，可以在master node上查看kube-apiserver的pod配置文件：</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ cat /usr/lib/systemd/system/kube-apiserver.service</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">ExecStart</span>=/usr/local/bin/kube-apiserver \</span><br><span class="line">      <span class="attribute">--v</span>=2  \</span><br><span class="line">      <span class="attribute">--logtostderr</span>=<span class="literal">true</span>  \</span><br><span class="line">      <span class="attribute">--allow-privileged</span>=<span class="literal">true</span>  \</span><br><span class="line">      <span class="attribute">--bind-address</span>=0.0.0.0  \</span><br><span class="line">      <span class="attribute">--secure-port</span>=6443  \</span><br><span class="line">      <span class="attribute">--insecure-port</span>=0  \</span><br><span class="line">      <span class="attribute">--advertise-address</span>=192.168.56.110 \</span><br><span class="line">      <span class="attribute">--service-cluster-ip-range</span>=10.96.0.0/12  \</span><br><span class="line">      <span class="attribute">--service-node-port-range</span>=30000-32767  \</span><br><span class="line">      <span class="attribute">--etcd-servers</span>=https://192.168.56.111:2379,https://192.168.56.112:2379,https://192.168.56.113:2379 \</span><br><span class="line">      <span class="attribute">--etcd-cafile</span>=/etc/etcd/ssl/etcd-ca.pem  \</span><br><span class="line">      <span class="attribute">--etcd-certfile</span>=/etc/etcd/ssl/etcd.pem  \</span><br><span class="line">      <span class="attribute">--etcd-keyfile</span>=/etc/etcd/ssl/etcd-key.pem  \</span><br><span class="line">      <span class="attribute">--client-ca-file</span>=/etc/kubernetes/pki/ca.pem  \</span><br><span class="line">      <span class="attribute">--tls-cert-file</span>=/etc/kubernetes/pki/apiserver.pem  \</span><br><span class="line">      <span class="attribute">--tls-private-key-file</span>=/etc/kubernetes/pki/apiserver-key.pem  \</span><br><span class="line">      <span class="attribute">--kubelet-client-certificate</span>=/etc/kubernetes/pki/apiserver.pem  \</span><br><span class="line">      <span class="attribute">--kubelet-client-key</span>=/etc/kubernetes/pki/apiserver-key.pem  \</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br></pre></td></tr></tbody></table></figure>
<p>相关的三个启动参数：</p>
<ul>
<li>client-ca-file: 指定CA根证书文件为/etc/kubernetes/pki/ca.pem</li>
<li>tls-private-key-file: 指定ApiServer私钥文件为/etc/kubernetes/pki/apiserver-key.pem</li>
<li>tls-cert-file：指定ApiServer证书文件为/etc/kubernetes/pki/apiserver.pem</li>
</ul>
<p>请求中需要带有由该证书签名的证书，才能认证通过，客户端签署的证书里包含user,group信息，具体为证书的subject.CommonName(username)以及subject.Organization(group)</p>
<div style="width: 50%; margin: auto">![x509](kubernetes-auth/x509.png)</div>

<h2 id="Service-Account-Tokens"><a href="#Service-Account-Tokens" class="headerlink" title="Service Account Tokens"></a>Service Account Tokens</h2><p>Service Account Token 是一种比较特殊的认证机制，适用于上文中提到的pod内部服务需要访问apiserver的认证情况，默认enabled。<br>还是看上文中apiserver 的启动配置参数有–service-account-key-file，如果没有指明文件，默认使用–tls-private-key-file的值，即API Server的私钥。</p>
<div style="width: 50%; margin: auto">![ServiceAccount工作流程](kubernetes-auth/sa.png) </div>

<p>通过控制器ServiceAccountController会去list,watch k8s apiserver对于命名空间的创建、删除；<br>就会在新创建的名称空间下创建一个名为”default”的service account;</p>
<p>TokenController 根据创建的ServiceAccount下面关联生成一个带有Token的secret。<br>Admission 是在通过认证进行鉴权的一个阶段，那么他会默认给当前名称空间下的pod打上，当前名称空间的那个ServiceAccount。</p>
<p>service accout本身是作为一种资源在k8s集群中，我们可以通过命令行获取</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl get sa <span class="operator">-</span>-all-namespaces</span><br><span class="line"></span><br><span class="line">NAMESPACE     NAME                                 SECRETS   AGE</span><br><span class="line">default       default                              <span class="number">1</span>         <span class="number">16</span>d</span><br><span class="line">kube-system   default                              <span class="number">1</span>         <span class="number">16</span>d</span><br><span class="line">kube-public   default                              <span class="number">1</span>         <span class="number">16</span>d</span><br><span class="line">kube-system   attachdetach-controller              <span class="number">1</span>         <span class="number">16</span>d</span><br><span class="line">kube-system   bootstrap-signer                     <span class="number">1</span>         <span class="number">16</span>d</span><br><span class="line">kube-system   calico-node                          <span class="number">1</span>         <span class="number">16</span></span><br><span class="line">...........</span><br><span class="line">...........</span><br><span class="line"></span><br><span class="line">$ kubectl describe serviceaccount<span class="symbol">/default</span> <span class="operator">-</span>n kube-system</span><br><span class="line"><span class="params">Name:</span>                default</span><br><span class="line"><span class="params">Namespace:</span>           kube-system</span><br><span class="line"><span class="params">Labels:</span>              <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>         <span class="symbol">&lt;none&gt;</span></span><br><span class="line">Image pull <span class="params">secrets:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line">Mountable <span class="params">secrets:</span>   default-token-q9s9l</span><br><span class="line"><span class="params">Tokens:</span>              default-token-q9s9l</span><br><span class="line"><span class="params">Events:</span>              <span class="symbol">&lt;none&gt;</span></span><br><span class="line">$ kubectl get secret default-token-q9s9l <span class="operator">-</span>o yaml <span class="operator">-</span>n kube-system</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">data:</span></span><br><span class="line">  ca.<span class="params">crt:</span> LS0tLS1CRUdJ ...............略</span><br><span class="line">  <span class="params">namespace:</span> a3ViZS1zeXN0ZW0<span class="operator">=</span></span><br><span class="line">  <span class="params">token:</span> ZXlKaGJHY2lPaUpTVX- ...............略</span><br><span class="line"><span class="params">kind:</span> Secret</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">annotations:</span></span><br><span class="line">    kubernetes.io<span class="operator">/</span>service-account.<span class="params">name:</span> default</span><br><span class="line">    kubernetes.io<span class="operator">/</span>service-account.<span class="params">uid:</span> a5568634-f445-<span class="number">11</span>e8-b4c4-<span class="number">000</span>c295134cf</span><br><span class="line">  <span class="params">creationTimestamp:</span> <span class="number">201</span>8-<span class="number">1</span>1-<span class="number">30</span>T02:<span class="number">14</span>:<span class="number">19</span>Z</span><br><span class="line">  <span class="params">name:</span> default-token-q9s9l</span><br><span class="line">  <span class="params">namespace:</span> kube-system</span><br><span class="line">  <span class="params">resourceVersion:</span> <span class="string">"337"</span></span><br><span class="line">  <span class="params">selfLink:</span> <span class="symbol">/api/v1/namespaces/kube-system/secrets/default-token-q9s9l</span></span><br><span class="line">  <span class="params">uid:</span> a56521fd-f445-<span class="number">11</span>e8-aa44-<span class="number">000</span>c29fe5618</span><br><span class="line"><span class="params">type:</span> kubernetes.io<span class="symbol">/service-account-token</span></span><br><span class="line">[root@k8s-m1 kubelet]<span class="comment">#</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>可以看到service-account-token的secret资源包含的数据有三部分：</p>
<ul>
<li>ca.crt，这是API Server的CA公钥证书，用于Pod中的Process对API Server的服务端数字证书进行校验时使用的；</li>
<li>namespace，这是Secret所在namespace的值的base64编码：# echo -n “kube-system”|base64 =&gt; “a3ViZS1zeXN0ZW0=”</li>
<li>token：该token就是由service-account-key-file的值签署(sign)生成。</li>
</ul>
<h3 id="API-Server的service-account-authentication-身份验证"><a href="#API-Server的service-account-authentication-身份验证" class="headerlink" title="API Server的service account authentication(身份验证)"></a>API Server的service account authentication(身份验证)</h3><p>前面说过，service account为Pod中的Process提供了一种身份标识，在Kubernetes的身份校验(authenticating)环节，以某个service account提供身份的Pod的用户名为：</p>
<figure class="highlight vbnet"><table><tbody><tr><td class="code"><pre><span class="line"><span class="symbol">system:</span>serviceaccount:(<span class="keyword">NAMESPACE</span>):(SERVICEACCOUNT)</span><br></pre></td></tr></tbody></table></figure>
<p>以上面那个kube-system namespace下的<code>default</code> service account为例，使用它的Pod的username全称为：</p>
<figure class="highlight hsp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">system</span>:serviceaccount:kube-<span class="keyword">system</span>:<span class="keyword">default</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="默认的service-account"><a href="#默认的service-account" class="headerlink" title="默认的service account"></a>默认的service account</h3><p>Kubernetes会为每个cluster中的namespace自动创建一个默认的service account资源，并命名为”default”.</p>
<p>如果Pod中没有显式指定spec.serviceAccount字段值(自定义Admission)，那么Kubernetes会默认将该namespace下的default service account自动mount到在这个namespace中创建的Pod里，那这个操作就是通过上面所说的ServiceAccountAdmission来实现的。<br>我们以namespace “default”为例，我们查看一下其中的一个Pod的信息：</p>
<figure class="highlight lasso"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl describe pod nginx<span class="number">-64</span>f497f8fd<span class="params">-lkmdr</span></span><br><span class="line">Name:           nginx<span class="number">-64</span>f497f8fd<span class="params">-lkmdr</span></span><br><span class="line">Namespace:      default</span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line">Containers:</span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line"><span class="params">...</span><span class="params">...</span></span><br><span class="line">      Mounts:</span><br><span class="line">      /<span class="built_in">var</span>/run/secrets/kubernetes.io/serviceaccount from default<span class="params">-token</span><span class="params">-d5d8g</span> (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  <span class="keyword">Type</span>              Status</span><br><span class="line">  Initialized       <span class="literal">True</span></span><br><span class="line">  Ready             <span class="literal">False</span></span><br><span class="line">  ContainersReady   <span class="literal">False</span></span><br><span class="line">  PodScheduled      <span class="literal">True</span></span><br><span class="line">Volumes:</span><br><span class="line"> <span class="params">...</span><span class="params">...</span></span><br><span class="line"> <span class="params">...</span><span class="params">...</span></span><br><span class="line">Events:</span><br><span class="line">  <span class="keyword">Type</span>    Reason     Age   From               Message</span><br><span class="line">  ----    ------     ----  ----               -------</span><br><span class="line">  Normal  Scheduled  <span class="number">27</span>s   default<span class="params">-scheduler</span>  Successfully assigned default/nginx<span class="number">-64</span>f497f8fd<span class="params">-lkmdr</span> <span class="keyword">to</span> k8s<span class="params">-m3</span></span><br><span class="line">  Normal  Pulling    <span class="number">17</span>s   kubelet, k8s<span class="params">-m3</span>    pulling image <span class="string">"nginx"</span></span><br></pre></td></tr></tbody></table></figure>

<p>可以看到，kubernetes将default namespace中的service account “default”的service account token挂载(mount)到了Pod中容器的/var/run/secrets/kubernetes.io/serviceaccount路径下。</p>
<p>深入容器内部，查看mount的serviceaccount路径下的结构：</p>
<figure class="highlight stata"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl exec nginx-64f497f8fd-lkmdr -- <span class="keyword">ls</span> /<span class="keyword">var</span>/<span class="keyword">run</span>/secrets/kubernetes.io/serviceaccount</span><br><span class="line"><span class="keyword">ca</span>.crt</span><br><span class="line">namespace</span><br><span class="line"><span class="keyword">token</span></span><br><span class="line"></span><br><span class="line"># 这三个文件与上面提到的service account的<span class="keyword">token</span>中的数据是一一对应的。</span><br></pre></td></tr></tbody></table></figure>

<h3 id="如何创建一个ServiceAccount"><a href="#如何创建一个ServiceAccount" class="headerlink" title="如何创建一个ServiceAccount"></a>如何创建一个ServiceAccount</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 通过命令行直接创建</span></span><br><span class="line">$ kubectl create sa myk8ssa</span><br><span class="line">serviceaccount<span class="symbol">/myk8ssa</span> created</span><br><span class="line">$ kubectl describe sa<span class="symbol">/myk8ssa</span></span><br><span class="line"><span class="params">Name:</span>                mysa</span><br><span class="line"><span class="params">Namespace:</span>           default</span><br><span class="line"><span class="params">Labels:</span>              <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>         <span class="symbol">&lt;none&gt;</span></span><br><span class="line">Image pull <span class="params">secrets:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line">Mountable <span class="params">secrets:</span>   mysa-token-kcwqm</span><br><span class="line"><span class="params">Tokens:</span>              mysa-token-kcwqm</span><br><span class="line"><span class="params">Events:</span>              <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如何使用？下面我在讨论</span></span><br></pre></td></tr></tbody></table></figure>

<h1 id="kubernetes-中的鉴权机制"><a href="#kubernetes-中的鉴权机制" class="headerlink" title="kubernetes 中的鉴权机制"></a>kubernetes 中的鉴权机制</h1><p>目前，k8s 中一共有 4 种鉴权权限模式：</p>
<ul>
<li>Node: 一种特殊目的的授权模式，主要用来让 kubernetes 遵从 node 的编排规则，实际上是 RBAC 的一部分，相当于只定义了 node 这个角色以及它的权限；</li>
<li>ABAC: Attribute-based access control；</li>
<li>RBAC: Role-based access control；</li>
<li>Webhook: 以 HTTP Callback 的方式，利用外部授权接口来进行权限控制；</li>
</ul>
<p>这里我主要学习总结最常用的鉴权方式—<code>RBAC</code><br>RBAC的鉴权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。RBAC 在 Kubernetes 中被映射为 API 资源和操作。</p>
<div style="width: 50%; margin: auto">![RBAC](kubernetes-auth/RBAC.png)</div>

<p>需要理解 RBAC 一些基础的概念和思路，RBAC 是让用户能够访问 Kubernetes API 资源的授权方式。</p>
<p><em><strong>role</strong></em><br>角色是一系列权限的集合，例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限， ClusterRole 跟 Role 类似，但是可以在集群中到处使用（ Role 是 namespace 一级的）。<br><em><strong>role binding</strong></em><br>RoleBinding 把角色映射到用户，从而让这些用户继承角色在 namespace 中的权限。ClusterRoleBinding 让用户继承 ClusterRole 在整个集群中的权限。</p>
<p>另外还要考虑<code>cluster roles</code>和<code>cluster role binding</code>。cluster role和cluster role binding方法跟role和role binding一样，出了它们有更广的scope。</p>
<p><em><strong>Kubernetes中的RBAC</strong></em><br>RBAC 现在被 Kubernetes 深度集成，并使用他给系统组件进行授权。System Roles 一般具有前缀system:，很容易识别：</p>
<figure class="highlight x86asm"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl get clusterroles --namespace=kube-system</span><br><span class="line">NAME                                                                   AGE</span><br><span class="line">admin                                                                  <span class="number">16d</span></span><br><span class="line">anonymous-dashboard-proxy-role                                         <span class="number">16d</span></span><br><span class="line">calico-node                                                            <span class="number">16d</span></span><br><span class="line">calicoctl                                                              <span class="number">16d</span></span><br><span class="line">cluster-admin                                                          <span class="number">16d</span></span><br><span class="line">edit                                                                   <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>aggregate-to-admin                                              <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>aggregate-to-edit                                               <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>aggregate-to-view                                               <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>auth-delegator                                                  <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>aws-cloud-provider                                              <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>basic-user                                                      <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>certificates<span class="number">.</span>k8s<span class="number">.</span>io:certificatesigningrequests:nodeclient       <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>certificates<span class="number">.</span>k8s<span class="number">.</span>io:certificatesigningrequests:selfnodeclient   <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:attachdetach-controller                              <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:certificate-controller                               <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:clusterrole-aggregation-controller                   <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:cronjob-controller                                   <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:daemon-set-controller                                <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:deployment-controller                                <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:disruption-controller                                <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:endpoint-controller                                  <span class="number">16d</span></span><br><span class="line"><span class="symbol">system:</span>controller:expand-controller                                    <span class="number">16d</span></span><br><span class="line">......略</span><br></pre></td></tr></tbody></table></figure>

<p>示例：</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> role.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">kind:</span> Role</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line">  <span class="params">name:</span> pod-reader</span><br><span class="line"><span class="params">rules:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroups:</span> [<span class="string">""</span>] <span class="comment"># "" indicates the core API group</span></span><br><span class="line">    <span class="params">resources:</span> [<span class="string">"pods"</span>] <span class="comment"># 可用操作资源对象</span></span><br><span class="line">    <span class="params">verbs:</span> [<span class="string">"get"</span>, <span class="string">"watch"</span>, <span class="string">"list"</span>] <span class="comment"># 对上面定义资源有哪些操作权限</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> rolebinding.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">kind:</span> RoleBinding</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1beta1</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> pod-reader-binding</span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line"><span class="params">subjects:</span>  </span><br><span class="line"><span class="operator">-</span> <span class="params">kind:</span> ServiceAccount </span><br><span class="line">  <span class="params">name:</span> mysa  </span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line"><span class="params">roleRef:</span></span><br><span class="line">    <span class="params">kind:</span> Role</span><br><span class="line">    <span class="params">name:</span> pod-reader</span><br><span class="line">    <span class="params">apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行创建</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f role.yaml</span><br><span class="line">role.rbac.authorization.k8s.io<span class="symbol">/pod-reader</span> created</span><br><span class="line">$ kubectl create <span class="operator">-</span>f rolebinding.yaml</span><br><span class="line">rolebinding.rbac.authorization.k8s.io<span class="symbol">/pod-reader-binding</span> created</span><br><span class="line"></span><br><span class="line">[root@k8s-m1 ~]<span class="comment"># kubectl get role</span></span><br><span class="line">NAME         AGE</span><br><span class="line">pod-reader   <span class="number">36</span>s</span><br><span class="line">[root@k8s-m1 ~]<span class="comment"># kubectl describe role pod-reader</span></span><br><span class="line"><span class="params">Name:</span>         pod-reader</span><br><span class="line"><span class="params">Labels:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">PolicyRule:</span></span><br><span class="line">  Resources  Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  <span class="operator">-</span>--------  <span class="operator">-</span>----------------  <span class="operator">-</span>-------------  <span class="operator">-</span>----</span><br><span class="line">  pods       []                 []              [get watch list]</span><br><span class="line">$ kubectl describe rolebinding pod-reader-binding</span><br><span class="line"><span class="params">Name:</span>         pod-reader-binding</span><br><span class="line"><span class="params">Labels:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Role:</span></span><br><span class="line">  <span class="params">Kind:</span>  Role</span><br><span class="line">  <span class="params">Name:</span>  pod-reader</span><br><span class="line"><span class="params">Subjects:</span></span><br><span class="line">  Kind            Name  Namespace</span><br><span class="line">  <span class="operator">-</span>---            <span class="operator">-</span>---  <span class="operator">-</span>--------</span><br><span class="line">  ServiceAccount  mysa  default</span><br><span class="line"> </span><br></pre></td></tr></tbody></table></figure>

<p>上面，</p>
<ol>
<li>我定义了一个pod-reader的role,其中它的权限定义如上方定义的<code>get</code>,<code>watch</code>,<code>list</code>;</li>
<li>然后定义了一个pod-reader-binding的rolebinding<br>前面提到过；用户通过认证之后就是鉴权，通过SA的认证方式之后拿到userinfo,然后绑定到role里面；那么此时对应的用户权限就是role里面定义的权限，subjects应用的对象类型也可以是User,Group;总之此时这一步就是获取到userinfo信息，然后通过角色绑定。相对应的权限就会赋予该用户。那用户从何而来？下面就需要创建一个用户来访问我们的集群了。</li>
</ol>
<p>那如何使用我们自己创建的ServiceAccount来登录并且通过自定义的这个RBAC来鉴权呢？这就涉及到了kubeconfig文件生成的问题了</p>
<h2 id="创建kubeconfig"><a href="#创建kubeconfig" class="headerlink" title="创建kubeconfig"></a>创建kubeconfig</h2><h3 id="获取上面已创建的ServiceAccount的Seter名称"><a href="#获取上面已创建的ServiceAccount的Seter名称" class="headerlink" title="获取上面已创建的ServiceAccount的Seter名称"></a>获取上面已创建的ServiceAccount的Seter名称</h3><p>这里我是在默认namespace下面创建的。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl describe sa<span class="symbol">/myk8ssa</span></span><br><span class="line"><span class="params">Name:</span>                myk8ssa</span><br><span class="line"><span class="params">Namespace:</span>           default</span><br><span class="line"><span class="params">Labels:</span>              <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>         <span class="symbol">&lt;none&gt;</span></span><br><span class="line">Image pull <span class="params">secrets:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line">Mountable <span class="params">secrets:</span>   myk8ssa-token-<span class="number">2</span>kntd</span><br><span class="line"><span class="params">Tokens:</span>              myk8ssa-token-<span class="number">2</span>kntd</span><br><span class="line"><span class="params">Events:</span>              <span class="symbol">&lt;none&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="获取secret下的token，并base64解码获取token明文"><a href="#获取secret下的token，并base64解码获取token明文" class="headerlink" title="获取secret下的token，并base64解码获取token明文"></a>获取secret下的token，并base64解码获取token明文</h3><figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ <span class="attribute">token</span>=`kubectl <span class="built_in">get</span><span class="built_in"> secret </span>myk8ssa-token-2kntd -oyaml |grep token: | awk <span class="string">'{print $2}'</span> | xargs echo -n | base64 -d`</span><br><span class="line">$ echo <span class="variable">$token</span></span><br><span class="line">eyJhbGciOiJSUzI1<span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span>略</span><br></pre></td></tr></tbody></table></figure>

<h3 id="新增用户guomaoqiu"><a href="#新增用户guomaoqiu" class="headerlink" title="新增用户guomaoqiu"></a>新增用户guomaoqiu</h3><figure class="highlight dsconfig"><table><tbody><tr><td class="code"><pre><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-cluster</span> <span class="string">my-k8s</span> <span class="built_in">--server=https://192.168.56.110:8443</span> \</span><br><span class="line"><span class="built_in">--certificate-authority=/etc/kubernetes/pki/ca.pem</span> \</span><br><span class="line"><span class="built_in">--embed-certs=true</span></span><br><span class="line"><span class="string">Cluster</span> <span class="string">"my-k8s"</span> <span class="string">set</span>.</span><br><span class="line"></span><br><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-credentials</span> <span class="string">guomaoqiu</span> <span class="built_in">--token=$token</span></span><br><span class="line"><span class="string">User</span> <span class="string">"guomaoqiu"</span> <span class="string">set</span>.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置上下文</span></span><br><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-context</span> <span class="string">my-k8s</span> <span class="built_in">--cluster=kubernetes</span></span><br><span class="line"><span class="string">Context</span> <span class="string">"my-k8s"</span> <span class="string">created</span>.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认用户信息</span></span><br><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-context</span> <span class="string">my-k8s</span> <span class="built_in">--user=guomaoqiu</span></span><br><span class="line"><span class="string">Context</span> <span class="string">"my-k8s"</span> <span class="string">modified</span>.</span><br></pre></td></tr></tbody></table></figure>

<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE</span><br><span class="line">          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin</span><br><span class="line"><span class="operator">*</span>         my-k8s                        kubernetes   guomaoqiu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将其设置为当前默认上下文:</span></span><br><span class="line">$ kubectl config use-context my-k8s</span><br><span class="line">Switched to context <span class="string">"my-k8s"</span>.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取POD:</span></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                     READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-<span class="number">64</span>f497f8fd-<span class="number">4</span>qdnt   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">10</span>m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试删除POD:</span></span><br><span class="line">$ kubectl delete pod nginx-<span class="number">64</span>f497f8fd-<span class="number">4</span>qdnt</span><br><span class="line">Error from server (Forbidden): pods <span class="string">"nginx-64f497f8fd-4qdnt"</span> is <span class="params">forbidden:</span> User <span class="string">"system:serviceaccount:default:myk8ssa"</span> cannot delete pods <span class="keyword">in</span> the namespace <span class="string">"default"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可见，该用户的的权限只有可读，而没有删除的权限；</span></span><br><span class="line"><span class="comment"># 说明配置是成功了的。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 浏览一下当前我环境中有哪些kubeconfig?</span></span><br><span class="line">$ kubectl config view</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">clusters:</span></span><br><span class="line"><span class="operator">-</span> <span class="params">cluster:</span></span><br><span class="line">    <span class="params">certificate-authority-data:</span> REDACTED</span><br><span class="line">    <span class="params">server:</span> https:<span class="operator">//</span><span class="number">192.168</span>.<span class="number">56.110</span>:<span class="number">8443</span></span><br><span class="line">  <span class="params">name:</span> kubernetes</span><br><span class="line"><span class="operator">-</span> <span class="params">cluster:</span></span><br><span class="line">    <span class="params">certificate-authority-data:</span> REDACTED</span><br><span class="line">    <span class="params">server:</span> https:<span class="operator">//</span><span class="number">192.168</span>.<span class="number">56.110</span>:<span class="number">8443</span></span><br><span class="line">  <span class="params">name:</span> my-k8s</span><br><span class="line"><span class="params">contexts:</span></span><br><span class="line"><span class="operator">-</span> <span class="params">context:</span></span><br><span class="line">    <span class="params">cluster:</span> kubernetes</span><br><span class="line">    <span class="params">user:</span> kubernetes-admin</span><br><span class="line">  <span class="params">name:</span> kubernetes-admin@kubernetes</span><br><span class="line"><span class="operator">-</span> <span class="params">context:</span></span><br><span class="line">    <span class="params">cluster:</span> kubernetes</span><br><span class="line">    <span class="params">user:</span> guomaoqiu</span><br><span class="line">  <span class="params">name:</span> my-k8s</span><br><span class="line"><span class="params">current-context:</span> my-k8s  <span class="comment"># 当前环境</span></span><br><span class="line"><span class="params">kind:</span> Config</span><br><span class="line"><span class="params">preferences:</span> {}</span><br><span class="line"><span class="params">users:</span></span><br><span class="line"><span class="operator">-</span> <span class="params">name:</span> guomaoqiu</span><br><span class="line">  <span class="params">user:</span></span><br><span class="line">    token:......略</span><br><span class="line"><span class="operator">-</span> <span class="params">name:</span> kubernetes-admin</span><br><span class="line">  <span class="params">user:</span></span><br><span class="line">    <span class="params">client-certificate-data:</span> REDACTED</span><br><span class="line">    <span class="params">client-key-data:</span> REDACTED</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>通过namespace我们可以做到不同业务之间的隔离，如果再加上如上这种权限控制将会更加细化、以上实验主要是在默认的namespace下面操作，除此之外也可以新建namespace然后进行验证操作。<code>kubeconfig</code>切换上下文也是非常实用的一种功能。<br>常用命令：</p>
<figure class="highlight dsconfig"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">kubectl</span> - 使用<span class="string">kubectl</span>来管理<span class="string">Kubernetes</span>集群。</span><br><span class="line"><span class="string">kubectl</span> <span class="string">config</span> <span class="string">set</span> - 在<span class="string">kubeconfig</span>配置文件中设置一个单独的值。</span><br><span class="line"><span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-cluster</span> - 在<span class="string">kubeconfig</span>配置文件中设置一个集群项。</span><br><span class="line"><span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-context</span> - 在<span class="string">kubeconfig</span>配置文件中设置一个环境项。</span><br><span class="line"><span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-credentials</span> - 在<span class="string">kubeconfig</span>配置文件中设置一个用户项。</span><br><span class="line"><span class="string">kubectl</span> <span class="string">config</span> <span class="string">unset</span> - 在<span class="string">kubeconfig</span>配置文件中清除一个单独的值。</span><br><span class="line"><span class="string">kubectl</span> <span class="string">config</span> <span class="string">use-context</span> - 使用<span class="string">kubeconfig</span>中的一个环境项作为当前配置。</span><br><span class="line"><span class="string">kubectl</span> <span class="string">config</span> <span class="string">view</span> - 显示合并后的<span class="string">kubeconfig</span>设置，或者一个指定的<span class="string">kubeconfig</span>配置文件。</span><br></pre></td></tr></tbody></table></figure>

<h1 id="kubernetes-中的准入机制"><a href="#kubernetes-中的准入机制" class="headerlink" title="kubernetes 中的准入机制"></a>kubernetes 中的准入机制</h1><p>Kubernetes的Admission Control实际上是一个准入控制器(Admission Controller)插件列表，发送到APIServer的请求都需要经过这个列表中的每个准入控制器插件的检查，如果某一个控制器插件准入失败，就准入失败。<br>更多可查看: <a href="http://docs.kubernetes.org.cn/144.html">http://docs.kubernetes.org.cn/144.html</a><br>这里我们主要学习<code>PodSecurityPolicy</code></p>
<h2 id="安全上下文-Pod-SecurityContext"><a href="#安全上下文-Pod-SecurityContext" class="headerlink" title="安全上下文(Pod SecurityContext)"></a>安全上下文(Pod SecurityContext)</h2><p>分为Pod级别和容器级别，容器级别的会覆盖Pod级别的相同设置。<br>在有PodSecurityPolicy策略的情况下，两者需要配合使用;</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> pod-security-policy.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test-pod</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> test</span><br><span class="line">      <span class="params">emptyDir:</span> {}</span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> test-pod</span><br><span class="line">      <span class="params">image:</span> alpine</span><br><span class="line">      <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">      <span class="params">command:</span> ['sh', '-c', 'echo The app is running<span class="operator">!</span> <span class="operator">&amp;&amp;</span> sleep <span class="number">36000</span>']</span><br><span class="line">      <span class="params">volumeMounts:</span></span><br><span class="line">        <span class="operator">-</span> <span class="params">name:</span> test</span><br><span class="line">          <span class="params">mountPath:</span> <span class="symbol">/data/test</span></span><br><span class="line">      <span class="params">securityContext:</span>    <span class="comment"># 设置容器安全上下文</span></span><br><span class="line">        <span class="params">readOnlyRootFilesystem:</span> <span class="literal">false</span>  <span class="comment"># 容器文件系统是否是只读</span></span><br><span class="line">        <span class="params">privileged:</span> <span class="literal">false</span> <span class="comment"># 是否为特权容器</span></span><br><span class="line">        <span class="params">runAsUser:</span> <span class="number">1000</span>   <span class="comment"># 进程运行用户UID</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行创建:</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f pod-security-policy.yaml</span><br><span class="line">pod<span class="symbol">/test-pod</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入容器验证：</span></span><br><span class="line">$ kubectl exec <span class="operator">-</span>it test-pod sh</span><br><span class="line"><span class="symbol">/</span> $ id</span><br><span class="line">u<span class="attr">id</span><span class="operator">=</span><span class="number">1000</span> gid<span class="operator">=</span><span class="number">0</span>(root)</span><br><span class="line"><span class="symbol">/</span> $ ps <span class="operator">-</span>ef</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    <span class="number">1</span> <span class="number">1000</span>      <span class="number">0</span>:<span class="number">00</span> sleep <span class="number">36000</span></span><br><span class="line">    <span class="number">6</span> <span class="number">1000</span>      <span class="number">0</span>:<span class="number">00</span> sh</span><br><span class="line">   <span class="number">15</span> <span class="number">1000</span>      <span class="number">0</span>:<span class="number">00</span> sh</span><br><span class="line">   <span class="number">21</span> <span class="number">1000</span>      <span class="number">0</span>:<span class="number">00</span> ps <span class="operator">-</span>ef</span><br><span class="line"><span class="symbol">/</span> $ sysctl <span class="operator">-</span>w net.ipv4.tcp_recovery<span class="operator">=</span><span class="number">2</span></span><br><span class="line"><span class="params">sysctl:</span> error setting key 'net.ipv4.<span class="params">tcp_recovery':</span> Read-only file system</span><br><span class="line"><span class="symbol">/</span> $</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以上我设置限制了pod是否开启特权模式，并且运行用户uid为1000</span></span><br><span class="line"><span class="comment"># 那么如果在pod级别设置上下文的话，容器级别的会覆盖Pod级别的相同设置(已验证)</span></span><br></pre></td></tr></tbody></table></figure>
<p>其他更多参数参见: <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">https://kubernetes.io/docs/concepts/policy/pod-security-policy/</a></p>
<h1 id="运行态的安全控制—网络策略-NetworkPolicy"><a href="#运行态的安全控制—网络策略-NetworkPolicy" class="headerlink" title="运行态的安全控制—网络策略(NetworkPolicy)"></a>运行态的安全控制—网络策略(NetworkPolicy)</h1><p>Kubernetes要求集群中所有pod，无论是节点内还是跨节点，都可以直接通信，或者说所有pod工作在同一跨节点网络，此网络一般是二层虚拟网络，称为pod网络。在安装引导kubernetes时，由选择并安装的network plugin实现。默认情况下，集群中所有pod之间、pod与节点之间可以互通。</p>
<p>网络主要解决两个问题，一个是连通性，实体之间能够通过网络互通。另一个是隔离性，出于安全、限制网络流量的目的，又要控制实体之间的连通性。Network Policy用来实现隔离性，只有匹配规则的流量才能进入pod，同理只有匹配规则的流量才可以离开pod。</p>
<p>但请注意，kubernetes支持的用以实现pod网络的network plugin有很多种，并不是全部都支持Network Policy，为kubernetes选择network plugin时需要考虑到这点，是否需要隔离？可用network plugin及是否支持Network Policy请参考<a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy">这里</a>。</p>
<h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>Network Policy是kubernetes中的一种资源类型，它从属于某个namespace。其内容从逻辑上看包含两个关键部分，一是pod选择器，基于标签选择相同namespace下的pod，将其中定义的规则作用于选中的pod。另一个就是规则了，就是网络流量进出pod的规则，其采用的是白名单模式，符合规则的通过，不符合规则的拒绝。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">apiVersion:</span> networking.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">kind:</span> NetworkPolicy</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test-network-policy</span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">podSelector:</span></span><br><span class="line">    <span class="params">matchLabels:</span>   <span class="comment"># 规则选择器，选择匹配的POD</span></span><br><span class="line">      <span class="params">role:</span> db</span><br><span class="line">  <span class="params">policyTypes:</span></span><br><span class="line">  <span class="operator">-</span> Ingress</span><br><span class="line">  <span class="operator">-</span> Egress</span><br><span class="line">  <span class="params">ingress:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">from:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">ipBlock:</span>     <span class="comment"># 远端(访问端)IP白名单开放</span></span><br><span class="line">        <span class="params">cidr:</span> <span class="number">172.17</span>.<span class="number">0.0</span><span class="symbol">/16</span></span><br><span class="line">        <span class="params">except:</span></span><br><span class="line">        <span class="operator">-</span> <span class="number">172.17</span>.<span class="number">1.0</span><span class="symbol">/24</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">namespaceSelector:</span>  <span class="comment"># 远端(访问端)namespaces白名单开放</span></span><br><span class="line">        <span class="params">matchLabels:</span></span><br><span class="line">          <span class="params">project:</span> myproject</span><br><span class="line">    <span class="operator">-</span> <span class="params">podSelector:</span>        <span class="comment"># 远端(访问端)Pod白名单开放</span></span><br><span class="line">        <span class="params">matchLabels:</span></span><br><span class="line">          <span class="params">role:</span> frontend</span><br><span class="line">    <span class="params">ports:</span>  			   <span class="comment"># 本端(被访问端)允许被访问的端口和协议</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">protocol:</span> TCP</span><br><span class="line">      <span class="params">port:</span> <span class="number">6379</span></span><br><span class="line">  <span class="params">egress:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">to:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">ipBlock:</span></span><br><span class="line">        <span class="params">cidr:</span> <span class="number">10.0</span>.<span class="number">0.0</span><span class="symbol">/24</span></span><br><span class="line">    <span class="params">ports:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">protocol:</span> TCP</span><br></pre></td></tr></tbody></table></figure>
<p>对象创建方法与其它如ReplicaSet相同。apiVersion、kind、metadata与其它类型对象含义相同，不详细描述。</p>
<ul>
<li><p>.spec.PodSelector<br>顾名思义，它是pod选择器，基于标签选择与Network Policy处于同一namespace下的pod，如果pod被选中，则对其应用Network Policy中定义的规则。此为可选字段，当没有此字段时，表示选中所有pod。</p>
</li>
<li><p>.spec.PolicyTypes<br>Network Policy定义的规则可以分成两种，一种是入pod的Ingress规则，一种是出pod的Egress规则。本字段可以看作是一个开关，如果其中包含Ingress，则Ingress部分定义的规则生效，如果是Egress则Egress部分定义的规则生效，如果都包含则全部生效。当然此字段也可选，如果没有指定的话，则默认Ingress生效，如果Egress部分有定义的话，Egress才生效。怎么理解这句话，下文会提到，没有明确定义Ingress、Egress部分，它也是一种规则，默认规则而非没有规则。</p>
</li>
<li><p>.spec.ingress与.spec.egress<br>前者定义入pod规则，后者定义出pod规则，详细参考这里，这里只讲一下重点。上例中ingress与egress都只包含一条规则，两者都是数组，可以包含多条规则。当包含多条时，条目之间的逻辑关系是“或”，只要匹配其中一条就可以。.spec.ingress[].from<br>也是数组，数组成员对访问pod的外部source进行描述，符合条件的source才可以访问pod，有多种方法，如示例中的ip地址块、名称空间、pod标签等，数组中的成员也是逻辑或的关系。spec.ingress[].from.prots表示允许通过的协议及端口号。</p>
</li>
<li><p>.spec.egress.to 定义的是pod想要访问的外部destination，其它与ingress相同。</p>
</li>
</ul>
<h3 id="kubernetes-默认NetworkPolicy"><a href="#kubernetes-默认NetworkPolicy" class="headerlink" title="kubernetes 默认NetworkPolicy:"></a>kubernetes 默认NetworkPolicy:</h3><figure class="highlight nestedtext"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#默认禁止所有出口请求</span></span><br><span class="line"><span class="attribute">apiVersion</span><span class="punctuation">:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attribute">kind</span><span class="punctuation">:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attribute">metadata</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">name</span><span class="punctuation">:</span> <span class="string">default-deny</span></span><br><span class="line"><span class="attribute">spec</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">podSelector</span><span class="punctuation">:</span> <span class="string">{}</span></span><br><span class="line">  <span class="attribute">policyTypes</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认禁止所有入口请求</span></span><br><span class="line"><span class="attribute">apiVersion</span><span class="punctuation">:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attribute">kind</span><span class="punctuation">:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attribute">metadata</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">name</span><span class="punctuation">:</span> <span class="string">default-deny</span></span><br><span class="line"><span class="attribute">spec</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">podSelector</span><span class="punctuation">:</span> <span class="string">{}</span></span><br><span class="line">  <span class="attribute">policyTypes</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认允许所有出口请求</span></span><br><span class="line"><span class="attribute">apiVersion</span><span class="punctuation">:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attribute">kind</span><span class="punctuation">:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attribute">metadata</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">name</span><span class="punctuation">:</span> <span class="string">allow-all</span></span><br><span class="line"><span class="attribute">spec</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">podSelector</span><span class="punctuation">:</span> <span class="string">{}</span></span><br><span class="line">  <span class="attribute">egress</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">{}</span></span><br><span class="line">  <span class="attribute">policyTypes</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认允许所有入口请求</span></span><br><span class="line"><span class="attribute">apiVersion</span><span class="punctuation">:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attribute">kind</span><span class="punctuation">:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attribute">metadata</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">name</span><span class="punctuation">:</span> <span class="string">allow-all</span></span><br><span class="line"><span class="attribute">spec</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">podSelector</span><span class="punctuation">:</span> <span class="string">{}</span></span><br><span class="line">  <span class="attribute">egress</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">{}</span></span><br><span class="line">  <span class="attribute">policyTypes</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br></pre></td></tr></tbody></table></figure>

<p>默认策略 无需详解，但请注意，pod与所运行节点之间流量不受Network Policy限制。</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>下面通过一个真实示例展示Network Policy普通用法。</p>
<h4 id="用Deployment创建nginx-pod实例并用service暴露"><a href="#用Deployment创建nginx-pod实例并用service暴露" class="headerlink" title="用Deployment创建nginx pod实例并用service暴露"></a>用Deployment创建nginx pod实例并用service暴露</h4><figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl <span class="built_in">run</span> nginx <span class="attribute">--image</span>=nginx <span class="attribute">--replicas</span>=3</span><br><span class="line">deployment.apps/nginx created</span><br><span class="line">$ kubectl expose deployment nginx <span class="attribute">--port</span>=80</span><br><span class="line">service/nginx exposed</span><br></pre></td></tr></tbody></table></figure>

<h4 id="确认创建结果"><a href="#确认创建结果" class="headerlink" title="确认创建结果"></a>确认创建结果</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl get pod,svc</span><br><span class="line">NAME                         READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod<span class="symbol">/nginx-64f497f8fd-9mfd7</span>   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">2</span>m</span><br><span class="line">pod<span class="symbol">/nginx-64f497f8fd-nss94</span>   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">2</span>m</span><br><span class="line">pod<span class="symbol">/nginx-64f497f8fd-zs926</span>   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">2</span>m</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service<span class="symbol">/kubernetes</span>   ClusterIP   <span class="number">10.96</span>.<span class="number">0.1</span>        <span class="symbol">&lt;none&gt;</span>        <span class="number">443</span><span class="symbol">/TCP</span>   <span class="number">3</span>d</span><br><span class="line">service<span class="symbol">/nginx</span>        ClusterIP   <span class="number">10.111</span>.<span class="number">254.191</span>   <span class="symbol">&lt;none&gt;</span>        <span class="number">80</span><span class="symbol">/TCP</span>    <span class="number">14</span>s</span><br></pre></td></tr></tbody></table></figure>

<h4 id="测试nginx服务连通性"><a href="#测试nginx服务连通性" class="headerlink" title="测试nginx服务连通性"></a>测试nginx服务连通性</h4><figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line">$  kubectl run busybox <span class="params">--rm</span> -it <span class="params">--image=busybox</span> <span class="string">/bin/sh</span></span><br><span class="line">If you don't see a <span class="keyword">command</span> prompt, <span class="keyword">try</span> pressing enter.</span><br><span class="line">/ <span class="comment"># wget --spider --timeout=3 nginx</span></span><br><span class="line">Connecting to nginx <span class="params">(10.111.254.191:80)</span>  <span class="comment"># 说明通的</span></span><br><span class="line">/ <span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="通过创建Network-Policy对象添加隔离性"><a href="#通过创建Network-Policy对象添加隔离性" class="headerlink" title="通过创建Network Policy对象添加隔离性"></a>通过创建Network Policy对象添加隔离性</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> pod-network-policy.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">kind:</span> NetworkPolicy</span><br><span class="line"><span class="params">apiVersion:</span> networking.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> access-nginx</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">podSelector:</span></span><br><span class="line">    <span class="params">matchLabels:</span> <span class="comment"># 默认该pod就有run=app这个标签了，我这里无需在从新定义</span></span><br><span class="line">      <span class="params">run:</span> nginx  </span><br><span class="line">  <span class="params">ingress:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">from:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">podSelector:</span></span><br><span class="line">        <span class="params">matchLabels:</span></span><br><span class="line">          <span class="params">access:</span> <span class="string">"true"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行创建</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f pod-network-policy.yaml</span><br><span class="line">networkpolicy.networking.k8s.io<span class="symbol">/access-nginx</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看具体隔离策略</span></span><br><span class="line">$ kubectl describe networkpolicy access-nginx</span><br><span class="line"><span class="params">Name:</span>         access-nginx</span><br><span class="line"><span class="params">Namespace:</span>    default</span><br><span class="line">Created <span class="params">on:</span>   <span class="number">201</span>8-<span class="number">1</span>2-<span class="number">17</span> <span class="number">01</span>:<span class="number">16</span>:<span class="number">57</span> <span class="operator">-</span><span class="number">0500</span> EST</span><br><span class="line"><span class="params">Labels:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Spec:</span></span><br><span class="line">  <span class="params">PodSelector:</span>     run<span class="operator">=</span>nginx</span><br><span class="line">  Allowing ingress <span class="params">traffic:</span></span><br><span class="line">    To <span class="params">Port:</span> <span class="symbol">&lt;any&gt;</span> (traffic allowed to all ports)</span><br><span class="line">    <span class="params">From:</span></span><br><span class="line">      <span class="params">PodSelector:</span> access<span class="operator">=</span><span class="literal">true</span></span><br><span class="line">  Allowing egress <span class="params">traffic:</span></span><br><span class="line">    <span class="symbol">&lt;none&gt;</span> (Selected pods are isolated for egress connectivity)</span><br><span class="line">  Policy <span class="params">Types:</span> Ingress</span><br></pre></td></tr></tbody></table></figure>

<h4 id="测试隔离性"><a href="#测试隔离性" class="headerlink" title="测试隔离性"></a>测试隔离性</h4><figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl run busybox <span class="params">--rm</span> -it <span class="params">--image=busybox</span> <span class="string">/bin/sh</span></span><br><span class="line">If you don't see a <span class="keyword">command</span> prompt, <span class="keyword">try</span> pressing enter.</span><br><span class="line">/ <span class="comment"># wget --spider --timeout=3 nginx</span></span><br><span class="line">Connecting to nginx <span class="params">(10.111.254.191:80)</span></span><br><span class="line">wget: download timed out <span class="comment"># 已经超时，说明已经不能访问了</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="为pod添加access-“true”标签后再次测试连通性"><a href="#为pod添加access-“true”标签后再次测试连通性" class="headerlink" title="为pod添加access: “true”标签后再次测试连通性"></a>为pod添加access: “true”标签后再次测试连通性</h4><figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl run busybox <span class="params">--rm</span> -it <span class="params">--labels=</span><span class="string">"access=true"</span> <span class="params">--image=busybox</span> <span class="string">/bin/sh</span></span><br><span class="line">If you don't see a <span class="keyword">command</span> prompt, <span class="keyword">try</span> pressing enter.</span><br><span class="line">/ <span class="comment"># wget --spider --timeout=3 nginx</span></span><br><span class="line">Connecting to nginx <span class="params">(10.111.254.191:80)</span></span><br></pre></td></tr></tbody></table></figure>

<p>参考:<br><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</a><br><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a><br><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">https://kubernetes.io/docs/concepts/services-networking/network-policies/</a><br><a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/</a><br><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">https://kubernetes.io/docs/concepts/policy/pod-security-policy/</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>ServiceAccount</tag>
        <tag>Kubeconfig</tag>
        <tag>RBAC</tag>
        <tag>Admission</tag>
        <tag>PodSecurityContext</tag>
        <tag>NetworkPolicy</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Bootstrap Token完成TLS Bootstrapping</title>
    <url>/2018/12/30/kubernetes-bootstrapping/</url>
    <content><![CDATA[<h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><p>本文主要记录一下搭建一个kubelet搭建到加入集群到手动、自动(证书轮换的)证书认证过程, 那么我的环境是用的v1.12.4版本，有一台master,除了master上面的一些组件已经搭建完毕，主要重点是在kubelet上面，所以现在就是master已经工作正常的情况下，加入新的<code>kubelet(worker节点)</code>需要做一些什么样的操作。<br>然后需要实现的是:</p>
<ul>
<li>通过bootstrap token以及boostrap.kubeconfig来引导worker节点申请证书</li>
<li>如果第一步申请发出去之后，在手动在master端进行手动批准证书申请，然后发放kubelet证书</li>
<li>实现kubelet证书的自动批准</li>
<li>实现kubelet证书的自动轮换操作<br><img src="/2018/12/30/kubernetes-bootstrapping/start.png"></li>
</ul>
<p><strong>简单理一下TLS Bootstrapping的一个引导过程</strong></p>
<ol>
<li>master端创建API Server和Kubelet Client通信凭证即生成 <code>apiserver.pem/apiserver-key.pem/apiserver.csr</code>;</li>
<li>在集群内创建特定的 Bootstrap Token Secret，该 Secret 将替代以前的 token.csv 内置用户声明文件;</li>
<li>在集群内创建首次 TLS Bootstrap 申请证书的 ClusterRole 即<code>system:node-bootstrapper</code>,并将上面的bootstrap token secret进行绑定即 clusterrolebinding <code>kubelet-bootstrap</code>，这样通过绑定就能以这个内置用户组信息去发起CSR请求啦；</li>
<li>生成特定的包含 <code>TLS Bootstrapping Token</code> 的 <code>bootstrap.kubeconfig</code>;</li>
<li>调整kubelete启动参数，指定引导文件<code>bootstrap.kubeconfig</code>;</li>
<li>重载配置、重启服务，master端手动批准完成worker节点的CSR请求;</li>
<li>后续如果没有配置证书轮换的话，就会一直使用由controller-manager批准颁发的证书文件了，有效期是一年。(自动批准&amp;证书轮换下面再说)</li>
</ol>
<p>当然以我的理解要要实现证书轮换那么肯定是没有外界干预的情况下自动执行的，那这个功能也肯定是需要自动批准这个功能的。</p>
<p>那么自动批准需要做的一些事情却是在手动批准的基础上做了一些操作:</p>
<ol>
<li>创建CSR请求的 TLS Bootstrap 申请证书的renew Kubelet client/server 的 ClusterRole，以及其相关对应的 ClusterRoleBinding；并绑定到对应的组或用户</li>
<li>调整 Controller Manager 配置，以使其能自动签署相关证书和自动清理过期的 TLS Bootstrapping Token</li>
<li>可选的: 集群搭建成功后立即清除 Bootstrap Token Secret，或等待 Controller Manager 待其过期后删除，以防止被恶意利用</li>
</ol>
<h1 id="二、手动批准"><a href="#二、手动批准" class="headerlink" title="二、手动批准"></a>二、手动批准</h1><h2 id="2-1-创建API-Server和Kubelet-Client通信凭证"><a href="#2-1-创建API-Server和Kubelet-Client通信凭证" class="headerlink" title="2.1 创建API Server和Kubelet Client通信凭证"></a>2.1 创建API Server和Kubelet Client通信凭证</h2><p>我这里已经有ca根证书文件了，那就从创建apiserver证书开始</p>
<figure class="highlight powershell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="variable">$</span> <span class="built_in">cat</span> &gt;&gt; apiserver<span class="literal">-csr</span>.json &lt;&lt; EOF</span><br><span class="line">{</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"kube-apiserver"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [</span><br><span class="line">    <span class="string">"10.96.0.1"</span>,</span><br><span class="line">    <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"192.168.56.201"</span>,</span><br><span class="line">    <span class="string">"kubernetes"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc.cluster"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc.cluster.local"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"key"</span>: {</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: <span class="number">2048</span></span><br><span class="line">  },</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"Hangzhou"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"Hangzhou"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"Kubernetes"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"Kubernetes-manual"</span></span><br><span class="line">    }</span><br><span class="line">  ]</span><br><span class="line">}</span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p>我这里就单台master,所以就写了个这一个IP地址</p>
<figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 执行创建:</span></span><br><span class="line"><span class="variable">$cfssl</span> gencert \</span><br><span class="line">  -ca=<span class="regexp">/etc/</span>kubernetes<span class="regexp">/pki/</span>ca.pem \</span><br><span class="line">  -ca-key=<span class="regexp">/etc/</span>kubernetes<span class="regexp">/pki/</span>ca-key.pem \</span><br><span class="line">  -config=ca-config.json \</span><br><span class="line">  -profile=kubernetes \</span><br><span class="line">  apiserver-csr.json | cfssljson -bare <span class="variable">${PKI_DIR}</span>/apiserver</span><br><span class="line"></span><br><span class="line">ls <span class="regexp">/etc/</span>kubernetes<span class="regexp">/pki/</span>apiserver*.pem</span><br><span class="line"><span class="regexp">/etc/</span>kubernetes<span class="regexp">/pki/</span>apiserver-key.pem  <span class="regexp">/etc/</span>kubernetes<span class="regexp">/pki/</span>apiserver.pem</span><br></pre></td></tr></tbody></table></figure>
<h2 id="2-2-创建引导配置文件-bootstrap-kubelet-kubeconfig"><a href="#2-2-创建引导配置文件-bootstrap-kubelet-kubeconfig" class="headerlink" title="2.2 创建引导配置文件 bootstrap-kubelet.kubeconfig"></a>2.2 创建引导配置文件 bootstrap-kubelet.kubeconfig</h2><h3 id="2-2-1-生成bootsrap—token"><a href="#2-2-1-生成bootsrap—token" class="headerlink" title="2.2.1 生成bootsrap—token"></a>2.2.1 生成bootsrap—token</h3><figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> <span class="attribute">TOKEN_ID</span>=$(openssl rand 3 -hex)</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">TOKEN_SECRET</span>=$(openssl rand 8 -hex)</span><br><span class="line"><span class="built_in">export</span> <span class="attribute">BOOTSTRAP_TOKEN</span>=<span class="variable">${TOKEN_ID}</span>.${TOKEN_SECRET}</span><br></pre></td></tr></tbody></table></figure>
<p>关于这个token的格式要求参见<a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#token-format">这里</a></p>
<h3 id="2-2-2-创建boostrap-token-secret"><a href="#2-2-2-创建boostrap-token-secret" class="headerlink" title="2.2.2 创建boostrap token secret"></a>2.2.2 创建boostrap token secret</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> bootstrap-token-Secret.yml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Secret</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> bootstrap-token-{TOKEN_ID}</span><br><span class="line">  <span class="params">namespace:</span> kube-system</span><br><span class="line"><span class="params">type:</span> bootstrap.kubernetes.io<span class="symbol">/token</span></span><br><span class="line"><span class="params">stringData:</span></span><br><span class="line">  <span class="params">token-id:</span> {TOKEN_ID}</span><br><span class="line">  <span class="params">token-secret:</span> {TOKEN_SECRET}</span><br><span class="line">  <span class="params">usage-bootstrap-authentication:</span> <span class="string">"true"</span></span><br><span class="line">  <span class="params">usage-bootstrap-signing:</span> <span class="string">"true"</span></span><br><span class="line">  <span class="params">auth-extra-groups:</span> system:bootstrappers:default-node-token</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将以上变量赋值进去</span></span><br><span class="line">sed <span class="operator">-</span>ri <span class="string">"s#<span class="char escape_">\{</span>TOKEN_ID<span class="char escape_">\}</span>#<span class="subst">${TOKEN_ID}</span>#g"</span> bootstrap-token-Secret.yml</span><br><span class="line">sed <span class="operator">-</span>ri <span class="string">"/token-id/s#<span class="char escape_">\S</span>+<span class="char escape_">\$</span>#'&amp;'#"</span> resources<span class="symbol">/bootstrap-token-Secret.yml</span></span><br><span class="line">sed <span class="operator">-</span>ri <span class="string">"s#<span class="char escape_">\{</span>TOKEN_SECRET<span class="char escape_">\}</span>#<span class="subst">${TOKEN_SECRET}</span>#g"</span> resources<span class="symbol">/bootstrap-token-Secret.yml</span></span><br><span class="line">kubectl create <span class="operator">-</span>f resources<span class="symbol">/bootstrap-token-Secret.yml</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看详情，一定要放到kube-system这个namespace</span></span><br><span class="line">$ kubectl describe secret  bootstrap-token-b8cf79 <span class="operator">-</span>n kube-system</span><br><span class="line"><span class="params">Name:</span>         bootstrap-token-b8cf79</span><br><span class="line"><span class="params">Namespace:</span>    kube-system</span><br><span class="line"><span class="params">Labels:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span></span><br><span class="line"><span class="params">Type:</span>         bootstrap.kubernetes.io<span class="symbol">/token</span></span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line"><span class="operator">==</span><span class="operator">==</span></span><br><span class="line"><span class="params">auth-extra-groups:</span>               <span class="number">39</span> bytes</span><br><span class="line"><span class="params">token-id:</span>                        <span class="number">6</span> bytes</span><br><span class="line"><span class="params">token-secret:</span>                    <span class="number">16</span> bytes</span><br><span class="line"><span class="params">usage-bootstrap-authentication:</span>  <span class="number">4</span> bytes</span><br><span class="line"><span class="params">usage-bootstrap-signing:</span>         <span class="number">4</span> bytes</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-2-2-创建启动引导配置"><a href="#2-2-2-创建启动引导配置" class="headerlink" title="2.2.2 创建启动引导配置"></a>2.2.2 创建启动引导配置</h3><figure class="highlight dsconfig"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># bootstrap set cluster</span></span><br><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-cluster</span> <span class="string">kubernetes</span> \</span><br><span class="line">    <span class="built_in">--certificate-authority=/etc/kubernetes/pki/ca.pem</span> \</span><br><span class="line">    <span class="built_in">--embed-certs=true</span> \</span><br><span class="line">    <span class="built_in">--server=192.168.56.202</span> \</span><br><span class="line">    <span class="built_in">--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bootstrap set credentials</span></span><br><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-credentials</span> <span class="string">tls-bootstrap-token-user</span> \</span><br><span class="line">    <span class="built_in">--token=${BOOTSTRAP_TOKEN}</span> \</span><br><span class="line">    <span class="built_in">--kubeconfig=/etc/kubernetes//bootstrap-kubelet.kubeconfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bootstrap set context</span></span><br><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="built_in">set-context</span> <span class="string">tls-bootstrap-token-user</span>@<span class="string">kubernetes</span> \</span><br><span class="line">    <span class="built_in">--cluster=kubernetes</span> \</span><br><span class="line">    <span class="built_in">--user=tls-bootstrap-token-user</span> \</span><br><span class="line">    <span class="built_in">--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bootstrap use default context</span></span><br><span class="line">$ <span class="string">kubectl</span> <span class="string">config</span> <span class="string">use-context</span> <span class="string">tls-bootstrap-token-user</span>@<span class="string">kubernetes</span> \</span><br><span class="line">    <span class="built_in">--kubeconfig=/etc/kubernetes/bootstrap-kubelet.kubeconfig</span></span><br></pre></td></tr></tbody></table></figure>
<p>该文件生成之后需要拷贝至worker节点上面去，并且修改配置，指定该引导配置文件<br>这个流程在官方文档中也有描述:<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#bootstrap-initialization">bootstrap初始化过程</a></p>
<h3 id="2-2-3-调整worker节点kubelet启动配置文件加上这个引导文件-参数"><a href="#2-2-3-调整worker节点kubelet启动配置文件加上这个引导文件-参数" class="headerlink" title="2.2.3 调整worker节点kubelet启动配置文件加上这个引导文件(参数)"></a>2.2.3 调整worker节点kubelet启动配置文件加上这个引导文件(参数)</h3><figure class="highlight ini"><table><tbody><tr><td class="code"><pre><span class="line"><span class="section">[Service]</span></span><br><span class="line"><span class="attr">ExecStart</span>=/usr/local/bin/kubelet</span><br><span class="line">  <span class="attr">--kubeconfig</span>=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">  <span class="attr">--network-plugin</span>=cni \</span><br><span class="line">  <span class="attr">--cni-conf-dir</span>=/etc/cni/net.d \</span><br><span class="line">  <span class="attr">--cni-bin-dir</span>=/opt/cni/bin \</span><br><span class="line">  <span class="attr">--logtostderr</span>=<span class="literal">false</span> \</span><br><span class="line">  <span class="attr">--log-dir</span>=/etc/kubernetes/log \</span><br><span class="line">  <span class="attr">--config</span>=/etc/kubernetes/kubelet-conf.yml \</span><br><span class="line">  <span class="attr">--node-labels</span>=node-role.kubernetes.io/master=<span class="string">''</span> \</span><br><span class="line">  <span class="attr">--pod-infra-container-image</span>=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:<span class="number">3.1</span> \</span><br><span class="line">  <span class="attr">--v</span>=<span class="number">2</span></span><br></pre></td></tr></tbody></table></figure>
<p>此时如果启动kubelet服务的话在apiserver将会报错：<br><img src="/2018/12/30/kubernetes-bootstrapping/forbidden.jpg"></p>
<p>这是因为在默认情况下，kubelet 通过 bootstrap.kubeconfig 中的预设用户 Token 声明了自己的身份，然后创建 CSR 请求；但是不要忘记这个用户在我们不处理的情况下他没任何权限的，包括创建 CSR 请求；所以需要如下命令创建一个 ClusterRoleBinding，将预设用户 kubelet-bootstrap 与内置的 ClusterRole system:node-bootstrapper 绑定到一起，使其能够发起 CSR 请求</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> kubelet-bootstrap-clusterrolebinding.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> kubelet-bootstrap</span><br><span class="line"><span class="params">roleRef:</span></span><br><span class="line">  <span class="params">apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line">  <span class="params">kind:</span> ClusterRole</span><br><span class="line">  <span class="params">name:</span> system:node-bootstrapper</span><br><span class="line"><span class="params">subjects:</span></span><br><span class="line"><span class="operator">-</span> <span class="params">apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line">  <span class="params">kind:</span> Group</span><br><span class="line">  <span class="params">name:</span> system:bootstrappers:default-node-token</span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p>将上面创建后就可以看到我们通过手动批准之后worker节点就加入到集群当中来了<br><img src="/2018/12/30/kubernetes-bootstrapping/result.jpg"><br>而且在worker节点的/var/lib/kubelet/pki目录下已经获得controller-manager颁发下来的证书,并且还分发了一个kubelet.kubeconfig的文件，这个文件虽然配置中指定了，但这个也是由controller-manager分发给worker节点的。</p>
<figure class="highlight tap"><table><tbody><tr><td class="code"><pre><span class="line">[root@k8s-n0 kubernetes]<span class="comment"># ll /var/lib/kubelet/pki/</span></span><br><span class="line">total 12</span><br><span class="line">-rw-------<span class="number"> 1 </span>root root<span class="number"> 1293 </span>Dec<span class="number"> 30 </span>10:14 kubelet-client-2018-12-30-10-14-50.pem</span><br><span class="line">lrwxrwxrwx<span class="number"> 1 </span>root root  <span class="number"> 59 </span>Dec<span class="number"> 30 </span>10:14 kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2018-12-30-10-14-50.pem</span><br><span class="line">-rw-r--r--<span class="number"> 1 </span>root root<span class="number"> 2153 </span>Dec<span class="number"> 30 </span>10:14 kubelet.crt</span><br><span class="line">-rw-------<span class="number"> 1 </span>root root<span class="number"> 1679 </span>Dec<span class="number"> 30 </span>10:14 kubelet.key</span><br></pre></td></tr></tbody></table></figure>


<p>还有个问题就是，集群已经完成，在我执行exec想进入一个容器的时候却出现了forbidden的问题:</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">[root@k8s-m1 resources]# kubectl exec -it kubectl-terminal-ubuntu /bin/bash</span><br><span class="line">error: unable <span class="keyword">to</span><span class="built_in"> upgrade </span>connection: Forbidden (<span class="attribute">user</span>=kube-apiserver, <span class="attribute">verb</span>=create, <span class="attribute">resource</span>=nodes, <span class="attribute">subresource</span>=proxy)</span><br></pre></td></tr></tbody></table></figure>
<p>这是因为kube-apiserveruser并没有nodes的资源存取权限,属于正常。由于 API 权限,故需要建立一个 RBAC Role 来获取存取权限;</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> apiserver-to-kubelet-rbac.yml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">kind:</span> ClusterRole</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">annotations:</span></span><br><span class="line">    rbac.authorization.kubernetes.io<span class="operator">/</span><span class="params">autoupdate:</span> <span class="string">"true"</span></span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    kubernetes.io<span class="operator">/</span><span class="params">bootstrapping:</span> rbac-defaults</span><br><span class="line">  <span class="params">name:</span> system:kube-apiserver-to-kubelet</span><br><span class="line"><span class="params">rules:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroups:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">""</span></span><br><span class="line">    <span class="params">resources:</span></span><br><span class="line">      <span class="operator">-</span> nodes<span class="symbol">/proxy</span></span><br><span class="line">      <span class="operator">-</span> nodes<span class="symbol">/stats</span></span><br><span class="line">      <span class="operator">-</span> nodes<span class="symbol">/log</span></span><br><span class="line">      <span class="operator">-</span> nodes<span class="symbol">/spec</span></span><br><span class="line">      <span class="operator">-</span> nodes<span class="symbol">/metrics</span></span><br><span class="line">    <span class="params">verbs:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"*"</span></span><br><span class="line"><span class="operator">-</span>--</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> system:kube-apiserver</span><br><span class="line">  <span class="params">namespace:</span> <span class="string">""</span></span><br><span class="line"><span class="params">roleRef:</span></span><br><span class="line">  <span class="params">apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line">  <span class="params">kind:</span> ClusterRole</span><br><span class="line">  <span class="params">name:</span> system:kube-apiserver-to-kubelet</span><br><span class="line"><span class="params">subjects:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line">    <span class="params">kind:</span> User</span><br><span class="line">    <span class="params">name:</span> kube-apiserver</span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p>创建好之后就可以exec啦~</p>
<p>以上，TLS Bootstrapping的手动批准已经完成。</p>
<h1 id="三、自动批准"><a href="#三、自动批准" class="headerlink" title="三、自动批准"></a>三、自动批准</h1><p>上面已经完成了手动批准的操作，那接下来就是实现以下自动批准以及证书轮换的实现</p>
<p>上面需要的配置其实都差不多了，只需要再做一些配置即可完成；</p>
<p>kubelet 所发起的 CSR 请求是由 controller manager 签署的；如果想要是实现自动续期，就需要让 controller manager 能够在 kubelet 发起证书请求的时候自动帮助其签署证书；那么 controller manager 不可能对所有的 CSR 证书申请都自动签署，这时候就需要配置 RBAC 规则，保证 controller manager 只对 kubelet 发起的特定 CSR 请求自动批准即可；在 TLS bootstrapping 官方文档中，CSR有三种请求类型:</p>
<ul>
<li>nodeclient: kubelet 以 O=system:nodes 和 CN=system:node:(node name) 形式发起的 CSR 请求</li>
<li>selfnodeclient: kubelet client renew 自己的证书发起的 CSR 请求(与上一个证书就有相同的 O 和 CN)</li>
<li>selfnodeserver: kubelet server renew 自己的证书发起的 CSR 请求</li>
</ul>
<p>通俗点讲就是:<br>nodeclient 类型的 CSR 仅在第一次启动时会产生，selfnodeclient 类型的 CSR 请求实际上就是 kubelet renew 自己作为 client 跟 apiserver 通讯时使用的证书产生的，selfnodeserver 类型的 CSR 请求则是 kubelet 首次申请或后续 renew 自己的 10250 api 端口证书时产生的</p>
<p>那么针对以上3种CSR请求分别给出了3种对应的 ClusterRole，如下所示</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span></span><br><span class="line"><span class="comment"># node client credentials.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:nodeclient</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">"certificates.k8s.io"</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">"certificatesigningrequests/nodeclient"</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">"create"</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span></span><br><span class="line"><span class="comment"># own client credentials.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">"certificates.k8s.io"</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">"certificatesigningrequests/selfnodeclient"</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">"create"</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span></span><br><span class="line"><span class="comment"># serving cert matching its client cert.</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">"certificates.k8s.io"</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">"certificatesigningrequests/selfnodeserver"</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">"create"</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>其中<code>selfnodeclient</code>,<code>nodeclient</code> 已经是集群内置的了;</p>
<figure class="highlight asciidoc"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl get clusterrole</span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">system:certificates.k8s.io:certificatesigningrequests:nodeclient       28h</span></span><br><span class="line"><span class="code">system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   28h</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line">......</span><br></pre></td></tr></tbody></table></figure>

<p>然后是三个 ClusterRole 对应的 三个ClusterRoleBinding；需要注意的是 在使用 Bootstrap Token 进行引导时，Kubelet 组件使用 Token 发起的请求其用户名为<code>system:bootstrap:&lt;token id&gt;</code>，用户组为 system:bootstrappers；所以 我们在创建 ClusterRoleBinding 时要绑定到这个用户或者组上；这里我选择全部绑定到组上：</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">cat &gt;&gt; kubelet-bootstrap-rbac.yml &lt;&lt; EOF</span><br><span class="line"><span class="comment"># 允许 system:bootstrappers 组用户创建 CSR 请求</span></span><br><span class="line"><span class="comment"># (这一步我在上面已经做过啦)</span></span><br><span class="line">kubectl create clusterrolebinding kubelet-bootstrap <span class="attribute">--clusterrole</span>=system:node-bootstrapper <span class="attribute">--group</span>=system:bootstrappers</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span></span><br><span class="line">kubectl create clusterrolebinding node-client-auto-approve-csr <span class="attribute">--clusterrole</span>=system:certificates.k8s.io:certificatesigningrequests:nodeclient <span class="attribute">--group</span>=system:bootstrappers</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span></span><br><span class="line">kubectl create clusterrolebinding node-client-auto-renew-crt <span class="attribute">--clusterrole</span>=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient <span class="attribute">--group</span>=system:nodes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span></span><br><span class="line">kubectl create clusterrolebinding node-server-auto-renew-crt <span class="attribute">--clusterrole</span>=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver <span class="attribute">--group</span>=system:nodes</span><br></pre></td></tr></tbody></table></figure>
<p>即: kubelet worker节点证书手动或自动批准需要用到的角色以及角色绑定有:</p>
<figure class="highlight asciidoc"><table><tbody><tr><td class="code"><pre><span class="line"># ClusterRole</span><br><span class="line">$ kubectl get clusterrole</span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">system:certificates.k8s.io:certificatesigningrequests:nodeclient       2d</span></span><br><span class="line"><span class="code">system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2d</span></span><br><span class="line"><span class="code">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver   5m</span></span><br><span class="line"><span class="code">system:node-bootstrapper                                               2d</span></span><br><span class="line"><span class="code">system:kube-apiserver-to-kubelet                                       17h</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code"># ClusterRoleBinding</span></span><br><span class="line"><span class="code">$ kubectl get clusterrolebinding | grep auto</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">node-client-auto-approve-csr                           16h</span></span><br><span class="line"><span class="code">node-client-auto-renew-crt                             16h</span></span><br><span class="line"><span class="code">node-server-auto-renew-crt                             16h</span></span><br><span class="line"><span class="code">system:kube-apiserver                                  17h</span></span><br><span class="line"><span class="code">kubelet-bootstrap                                      18h</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line">......</span><br></pre></td></tr></tbody></table></figure>

<p>创建好对应的角色及角色绑定之后就可以修改这个证书的有效期啦；只要在<code>Description=Kubernetes Controller Manager </code>服务的启动配置参数中加入:</p>
<figure class="highlight ini"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">--experimental-cluster-signing-duration</span>=<span class="number">10</span>m0s \</span><br><span class="line"><span class="attr">--feature-gates</span>=RotateKubeletClientCertificate=<span class="literal">true</span> \</span><br><span class="line"><span class="attr">--feature-gates</span>=RotateKubeletServerCertificate=<span class="literal">true</span> </span><br></pre></td></tr></tbody></table></figure>
<p>以及kubelet服务启动配置参数中加入:</p>
<figure class="highlight ini"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">--feature-gates</span>=RotateKubeletClientCertificate=<span class="literal">true</span> \</span><br><span class="line"><span class="attr">--feature-gates</span>=RotateKubeletServerCertificate=<span class="literal">true</span></span><br></pre></td></tr></tbody></table></figure>
<p>通过文档了解到<code>--feature-gates</code> 这个是kubernetes中特有的一些功能，有些功能是处于开发版本的，具体可以参看<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">这里</a></p>
<p>以下就是kubelet运行一段时间后，通过在<code>controller-manager</code>设置的证书有效期快过期的时候通过自动申请并自动批准的结果；<br>可以看出来<code>node-csr-ysGrNbyioCNnj3UUFiQGn5WwLGF1P6wJ4DTIib1IcqQ</code> 这个CSR是第一次启动时的用户<code>system:bootstrap:fd2f4f</code>的证书请求。<br><img src="/2018/12/30/kubernetes-bootstrapping/result2.jpg"></p>
<p>并且在证书轮换的过程也可以通过日志发现</p>
<figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">Dec</span> <span class="number">31</span> <span class="number">04</span>:<span class="number">10</span>:<span class="number">06</span> k8s-n0 kubelet: I1231 <span class="number">17</span>:<span class="number">10</span>:<span class="number">06</span>.<span class="number">753064</span>   <span class="number">18294</span> transport.go:<span class="number">132</span>] certificate rotation detected, shutting down client connections to start using new credentials</span><br></pre></td></tr></tbody></table></figure>
<p>那么以上就是kubelet证书的整个批准以及证书轮换的过程。</p>
<h1 id="四、TLS-Bootstrapping总结"><a href="#四、TLS-Bootstrapping总结" class="headerlink" title="四、TLS Bootstrapping总结"></a>四、TLS Bootstrapping总结</h1><p>流程总结</p>
<ol>
<li><p>kubelet 首次启动通过加载<code> bootstrap.kubeconfig</code> 中的用户 Token 和 apiserver CA 证书发起首次 CSR 请求，这个 Token 被预先内置在 apiserver 节点的 token.csv(新版本中已经推荐使用Bootstrap Token Secert) 中，其身份为 <code>kubelet-bootstrap</code> 用户和 <code>system:bootstrappers</code> 用户组；想要首次 CSR 请求能成功(成功指的是不会被 apiserver 401 拒绝)，则需要先将 <code>kubelet-bootstrap</code> 用户和 <code>system:node-bootstrapper</code> 内置 ClusterRole 绑定；</p>
</li>
<li><p>对于首次 CSR 请求可以手动签发，也可以将 <code>system:bootstrappers</code> 用户组与 <code>approve-node-client-csr</code> ClusterRole 绑定实现自动签发(1.8 之前这个 ClusterRole 需要手动创建，1.8 后 apiserver 自动创建，并更名为 <code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code>)</p>
</li>
<li><p>默认签署的的证书只有 1 年有效期，如果想要调整证书有效期可以通过设置 kube-controller-manager 的 <code>--experimental-cluster-signing-duration</code> 参数实现，该参数默认值为 8760h0m0s</p>
</li>
<li><p>对于证书轮换，需要通过协调两个方面实现；第一，想要 kubelet 在证书到期后自动发起续期请求，则需要在 kubelet 启动时增加 <code>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> 来实现；第二，想要让 controller manager 自动批准续签的 CSR 请求需要在 controller manager 启动时增加 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，并绑定对应的 RBAC 规则</p>
</li>
</ol>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>TLS bootstrapping</tag>
        <tag>证书手动签发</tag>
        <tag>证书自动签发</tag>
        <tag>证书轮换(自动续期)</tag>
      </tags>
  </entry>
  <entry>
    <title>K8s集群中pause容器是干嘛的~</title>
    <url>/2018/12/07/k8s-ji-qun-zhongpause-rong-qi-shi-gan-ma-de/</url>
    <content><![CDATA[<p>当我们在检查k8s集群状态的时候会发现有很多 <code>pause</code> 容器运行于服务器上面，然后每次启动一个容器，都会伴随一个pause容器的启动。那它究竟是干啥子的？</p>
<p>Pause容器，又叫Infra容器，下面通过实验来理解它。</p>
<p>我们知道在搭建k8s集群的时候，kubelet的配置中有这样一个参数：</p>
<pre><code>[root@linux-node1 cfg]# more /usr/lib/systemd/system/kubelet.service
······
······
  --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \
······
······
</code></pre>
<p>我这里是直接将这些配置参数通过启动脚本来补充进去的。<br>Pause容器，是可以自己来定义，官方使用的<code>gcr.io/google_containers/pause-amd64:3.0</code>容器的代码见Github，使用C语言编写。</p>
<h2 id="Pause容器的作用"><a href="#Pause容器的作用" class="headerlink" title="Pause容器的作用"></a>Pause容器的作用</h2><p>我们检查nod节点的时候会发现每个node上都运行了很多的pause容器，例如如下。</p>
<pre><code>[root@linux-node1 cfg]# docker ps
······
······
CONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS              PORTS               NAMES
a007c18b8dc0        568c4670fa80                             "nginx -g 'daemon of…"   42 hours ago        Up 42 hours                             k8s_nginx_nginx-pod-7d9f9876cc-75sf7_default_a688bb46-f872-11e8-ae6b-000c29c6d12b_1
9866c08d1f4b        568c4670fa80                             "nginx -g 'daemon of…"   42 hours ago        Up 42 hours                             k8s_nginx_nginx-pod-7d9f9876cc-wpv4h_default_a6a899c0-f872-11e8-ae6b-000c29c6d12b_1
aafef6727026        mirrorgooglecontainers/pause-amd64:3.0   "/pause"                 42 hours ago        Up 42 hours                             k8s_POD_flask-app-6f5b6cc447-kbxks_flask-app-extions-stage_374b8aa0-f873-11e8-ae6b-000c29c6d12b_1
c4f48f90b27f        mirrorgooglecontainers/pause-amd64:3.0   "/pause"                 42 hours ago        Up 42 hours                             k8s_POD_flask-app-6f5b6cc447-f9wjn_flask-app-extions-stage_373be9db-f873-11e8-ae6b-000c29c6d12b_1
9f452e6961f6        mirrorgooglecontainers/pause-amd64:3.0   "/pause"                 42 hours ago        Up 42 hours                             k8s_POD_nginx-pod-7d9f9876cc-ccx94_default_a6a8c440-f872-11e8-ae6b-000c29c6d12b_1
7e68043469d1        mirrorgooglecontainers/pause-amd64:3.0   "/pause"                 42 hours ago        Up 42 hours                             k8s_POD_nginx-pod-7d9f9876cc-sskpk_default_a6ac43bd-f872-11e8-ae6b-000c29c6d12b_1
······
······
</code></pre>
<p>kubernetes中的pause容器主要为每个业务容器提供以下功能：</p>
<ul>
<li>在pod中担任Linux命名空间共享的基础；</li>
<li>启用pid命名空间，开启init进程。</li>
</ul>
<p>在<a href="https://www.ianlewis.org/en/almighty-pause-container">The Almighty Pause Container</a>这篇文章中做出了详细的说明，pause容器的作用可以从这个例子中看出，首先见下图：<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/12/15441701986547.png">￼</p>
<h5 id="1-我们首先在节点上运行一个pause容器。"><a href="#1-我们首先在节点上运行一个pause容器。" class="headerlink" title="1.我们首先在节点上运行一个pause容器。"></a>1.我们首先在节点上运行一个pause容器。</h5><pre><code>[root@k8s-node1 ~]# docker run -d --name pause -p 8880:80 registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1
38d2aa8366d5aa6fe4c57aa0d879de4b5259c67c83d17428dd4d9f8937205c02

[root@k8s-node1 ~]# docker ps | grep pause
38d2aa8366d5        registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1   "/pause"                 14 seconds ago      Up 13 seconds       0.0.0.0:8880-&gt;80/tcp   pause
</code></pre>
<h5 id="2-然后再运行一个nginx容器，nginx将为localhost-2368创建一个代理。"><a href="#2-然后再运行一个nginx容器，nginx将为localhost-2368创建一个代理。" class="headerlink" title="2.然后再运行一个nginx容器，nginx将为localhost:2368创建一个代理。"></a>2.然后再运行一个nginx容器，nginx将为localhost:2368创建一个代理。</h5><pre><code>[root@k8s-node1 ~]# cat &lt;&lt;EOF &gt;&gt; nginx.conf
error_log stderr;
events { worker_connections  1024; }
http {
    access_log /dev/stdout combined;
    server {
        listen 80 default_server;
        server_name example.com www.example.com;
        location / {
            proxy_pass http://127.0.0.1:2368;
        }
    }
}
EOF

[root@k8s-node1 ~]# docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf --net=container:pause --ipc=container:pause --pid=container:pause nginx
fa078473c01e040db795004ad16db525dea8a113893d3052cc6ab1c5e117ba10
</code></pre>
<h5 id="3-然后再为ghost创建一个应用容器，这是一款博客软件。"><a href="#3-然后再为ghost创建一个应用容器，这是一款博客软件。" class="headerlink" title="3.然后再为ghost创建一个应用容器，这是一款博客软件。"></a>3.然后再为ghost创建一个应用容器，这是一款博客软件。</h5><pre><code>[root@linux-node2 ~]# docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost


# 查看结果：
[root@k8s-node1 ~]# docker ps | grep -E "pause|nginx|ghost"
9b796efd95a5        ghost                                                                 "docker-entrypoint..."   47 seconds ago       Up 46 seconds                              ghost
fa078473c01e        nginx                                                                 "nginx -g 'daemon ..."   About a minute ago   Up About a minute                          nginx
38d2aa8366d5        registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1   "/pause"                 3 minutes ago        Up 3 minutes        0.0.0.0:8880-&gt;80/tcp   pause
[root@k8s-node1 ~]#



# 现在访问http://119.3.198.128:8880/就可以看到ghost博客的界面了吗
# 这里我直接curl 然后浏览器访问也是正常的
[root@k8s-node1 ~]# curl -I http://119.3.198.128:8880/
HTTP/1.1 200 OK
Server: nginx/1.15.5
Date: Fri, 07 Dec 2018 08:35:49 GMT
Content-Type: text/html; charset=utf-8
Content-Length: 17381
Connection: keep-alive
X-Powered-By: Express
Cache-Control: public, max-age=0
ETag: W/"43e5-ELHSnbaoapp3YOyz+PU502oJo5E"
Vary: Accept-Encoding

[root@k8s-node1 ~]#
</code></pre>
<h2 id="解析"><a href="#解析" class="headerlink" title="解析:"></a>解析:</h2><ol>
<li>pause 容器将内部的80端口映射到了宿主机的8880端口;</li>
<li>pause容器在宿主机上设置好了网络namespace后，nginx容器加入到该网络namespace中;</li>
<li>nginx容器启动的时候指定了–net=container:pause;</li>
<li>ghost容器启动的时候同样加入到了该网络namespace中;</li>
<li>这样三个容器就共享了网络，互相之间就可以使用localhost直接通信，</li>
<li>–ipc=contianer:pause –pid=container:pause就是三个容器处于同一个namespace中，init进程为pause;</li>
</ol>
<p>我们到ghost容器中查看一下:</p>
<pre><code>[root@k8s-node1 ~]# docker exec -it ghost /bin/bash
root@38d2aa8366d5:/var/lib/ghost# ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   1012     4 ?        Ss   08:27   0:00 /pause
root         5  0.0  0.0  32472  3168 ?        Ss   08:29   0:00 nginx: master process nginx -g daemon off;
systemd+     9  0.0  0.0  32932  1812 ?        S    08:29   0:00 nginx: worker process
node        10  0.5  2.1 1262748 84688 ?       Ssl  08:30   0:03 node current/index.js
root        83  0.2  0.0  20240  1912 pts/0    Ss   08:41   0:00 /bin/bash
root        87  0.0  0.0  17496  1148 pts/0    R+   08:41   0:00 ps aux
root@38d2aa8366d5:/var/lib/ghost#
</code></pre>
<p>在ghost容器中同时可以看到pause和nginx容器的进程，并且pause容器的PID是1。而在kubernetes中容器的PID=1的进程即为容器本身的业务进程。</p>
<h2 id="其他概念："><a href="#其他概念：" class="headerlink" title="其他概念："></a>其他概念：</h2><ul>
<li>PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID；</li>
<li>网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围；</li>
<li>IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信；</li>
<li>UTS命名空间：Pod中的多个容器共享一个主机名；Volumes（共享存储卷）：</li>
<li>Pod中的各个容器可以访问在Pod级别定义的Volumes；</li>
</ul>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes中deployment升级回滚</title>
    <url>/2018/12/10/kubernetes-deployment-rollingupdate/</url>
    <content><![CDATA[<p>在kubernetes中使用deployment管理rs时，可以利用deployment滚动升级的特性，达到服务零停止升级的目的</p>
<h5 id="命令行创建一个deploymen"><a href="#命令行创建一个deploymen" class="headerlink" title="命令行创建一个deploymen"></a>命令行创建一个deploymen</h5><figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">kubectl <span class="built_in">run</span> nginx <span class="attribute">--image</span>=nginx</span><br></pre></td></tr></tbody></table></figure>
<h5 id="多副本deployment"><a href="#多副本deployment" class="headerlink" title="多副本deployment"></a>多副本deployment</h5><figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">kubectl <span class="built_in">run</span> nginx <span class="attribute">--image</span>=nginx <span class="attribute">--replicas</span>=2 --record</span><br></pre></td></tr></tbody></table></figure>
<h5 id="滚动升级-（可以加上–record参数可以记录命令）"><a href="#滚动升级-（可以加上–record参数可以记录命令）" class="headerlink" title="滚动升级 （可以加上–record参数可以记录命令）"></a>滚动升级 （可以加上–record参数可以记录命令）</h5><figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">kubectl</span> set image deploy/nginx nginx=nginx:<span class="number">1</span>.<span class="number">9</span>.<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 滚动策略:</span></span><br><span class="line"><span class="attribute">kubectl</span> edit deployment nginx</span><br><span class="line">  <span class="attribute">strategy</span>:</span><br><span class="line">    <span class="attribute">rollingUpdate</span>:</span><br><span class="line">      <span class="attribute">maxSurge</span>: <span class="number">1</span> # 升级过程中可以比预设的 pod 的数量多出的个数，默认值是<span class="number">25</span>%</span><br><span class="line">      <span class="attribute">maxUnavailable</span>: <span class="number">1</span> # 最多有几个 pod 处于无法工作的状态，默认值是<span class="number">25</span>%</span><br></pre></td></tr></tbody></table></figure>
<h5 id="滚动升级状态查看"><a href="#滚动升级状态查看" class="headerlink" title="滚动升级状态查看"></a>滚动升级状态查看</h5><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">kubectl rollout status deploy/nginx </span><br></pre></td></tr></tbody></table></figure>
<h5 id="查看升级历史"><a href="#查看升级历史" class="headerlink" title="查看升级历史"></a>查看升级历史</h5><figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line">kubectl rollout <span class="keyword">history</span> <span class="keyword">deploy</span>/nginx</span><br><span class="line"><span class="comment"># 查看单个revision 的详细信息：</span></span><br><span class="line">kubectl rollout <span class="keyword">history</span> <span class="keyword">deploy</span>/nginx <span class="params">--revision=2</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="资源调整"><a href="#资源调整" class="headerlink" title="资源调整:"></a>资源调整:</h5><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">kubectl set resources deploy<span class="symbol">/nginx</span> <span class="operator">-</span>c<span class="operator">=</span>nginx  <span class="operator">-</span>-limits<span class="operator">=</span>cpu<span class="operator">=</span><span class="number">100</span>m,memory<span class="operator">=</span><span class="number">200</span>m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时在查看历史状态:</span></span><br><span class="line">$ kubectl rollout history deploy<span class="symbol">/nginx</span></span><br><span class="line">deployments <span class="string">"nginx"</span></span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line"><span class="number">1</span>         <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="number">2</span>         <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="number">3</span>         <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看revision 3 的内容,可以看出刚才调整了pod的资源记录了到了升级历史当中</span></span><br><span class="line">$ kubectl rollout history deploy<span class="symbol">/nginx</span> <span class="operator">-</span>-revision<span class="operator">=</span><span class="number">3</span></span><br><span class="line">eployments <span class="string">"nginx"</span> <span class="keyword">with</span> revision <span class="comment">#3</span></span><br><span class="line">Pod <span class="params">Template:</span></span><br><span class="line">  <span class="params">Labels:</span>       pod-template-hash<span class="operator">=</span><span class="number">440294079</span></span><br><span class="line">        <span class="attr">run</span><span class="operator">=</span>nginx</span><br><span class="line">  <span class="params">Containers:</span></span><br><span class="line">   <span class="params">nginx:</span></span><br><span class="line">    <span class="params">Image:</span>      nginx:<span class="number">1.9</span>.<span class="number">1</span></span><br><span class="line">    <span class="params">Port:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line">    <span class="params">Limits:</span></span><br><span class="line">      <span class="params">cpu:</span>      <span class="number">100</span>m</span><br><span class="line">      <span class="params">memory:</span>   <span class="number">200</span>m</span><br><span class="line">    <span class="params">Environment:</span>        <span class="symbol">&lt;none&gt;</span></span><br><span class="line">    <span class="params">Mounts:</span>     <span class="symbol">&lt;none&gt;</span></span><br><span class="line">  <span class="params">Volumes:</span>      <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h5 id="回滚操作"><a href="#回滚操作" class="headerlink" title="回滚操作:"></a>回滚操作:</h5><figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 回滚到第二个版本去</span></span><br><span class="line">$ kubectl rollout undo <span class="keyword">deploy</span> nginx <span class="params">--to-revision=2</span></span><br><span class="line">deployment <span class="string">"nginx"</span> </span><br></pre></td></tr></tbody></table></figure>
<h5 id="应用弹性伸缩"><a href="#应用弹性伸缩" class="headerlink" title="应用弹性伸缩:"></a>应用弹性伸缩:</h5><figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl scale <span class="keyword">deploy</span> nginx <span class="params">--replicas=10</span></span><br><span class="line">deployment <span class="string">"nginx"</span> scaled</span><br></pre></td></tr></tbody></table></figure>
<p>如果集群中对接了heapster，和HPA联动后，可以通过<code>autoscale</code>自动弹性伸缩</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl autoscale deployment nginx <span class="attribute">--min</span>=10 <span class="attribute">--max</span>=15 <span class="attribute">--cpu-percent</span>=80</span><br></pre></td></tr></tbody></table></figure>


<h5 id="暂停和恢复Deployment"><a href="#暂停和恢复Deployment" class="headerlink" title="暂停和恢复Deployment"></a>暂停和恢复Deployment</h5><p>您可以在发出一次或多次更新前暂停一个 Deployment，然后再恢复它。这样您就能多次暂停和恢复 Deployment，在此期间进行一些更新，修复工作，而不会发出不必要的 rollout。</p>
<p>例如使用刚刚创建 Deployment：</p>
<figure class="highlight pgsql"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl <span class="keyword">get</span> deploy nginx</span><br><span class="line"><span class="type">NAME</span>      DESIRED   <span class="keyword">CURRENT</span>   UP-<span class="keyword">TO</span>-<span class="type">DATE</span>   AVAILABLE   AGE</span><br><span class="line">nginx     <span class="number">3</span>         <span class="number">3</span>         <span class="number">3</span>            <span class="number">3</span>           <span class="number">27</span>s</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>使用以下命令暂停 Deployment：</p>
<figure class="highlight autohotkey"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl rollout <span class="keyword">pause</span> deployment/nginx</span><br><span class="line">deployment <span class="string">"nginx"</span> paused</span><br></pre></td></tr></tbody></table></figure>

<p>然后更新 Deplyment中的镜像：</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl <span class="built_in">set</span> image deploy/nginx <span class="attribute">nginx</span>=nginx:1.9.1</span><br><span class="line">deployment <span class="string">"nginx-deployment"</span> image updated</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>您可以进行任意多次更新，例如更新使用的资源：</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl <span class="built_in">set</span> resources deployment nginx <span class="attribute">-c</span>=nginx <span class="attribute">--limits</span>=cpu=200m,memory=256Mi</span><br><span class="line">deployment <span class="string">"nginx"</span><span class="built_in"> resource </span>requirements updated</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h5 id="Deployment-暂停前的初始状态将继续它的功能，而不会对-Deployment-的更新产生任何影响，只要-Deployment是暂停的。"><a href="#Deployment-暂停前的初始状态将继续它的功能，而不会对-Deployment-的更新产生任何影响，只要-Deployment是暂停的。" class="headerlink" title="Deployment 暂停前的初始状态将继续它的功能，而不会对 Deployment 的更新产生任何影响，只要 Deployment是暂停的。"></a>Deployment 暂停前的初始状态将继续它的功能，而不会对 Deployment 的更新产生任何影响，只要 Deployment是暂停的。</h5><p>那么新的更新或者改动的结果将是在暂停期间所有做的操作都是顺序生效取相同操作的最后一步，<br>比如,暂停后我做了以下操作:<br>1.更新了版本: 1.9.1<br>2.更新了资源: memory=222Mi<br>3.又更新了版本: 1.9.3<br>4.又调整了资源: memory=211Mi</p>
<p>最后，恢复这个 Deployment，观察完成更新的 ReplicaSet 已经创建出来了：</p>
<p><img src="/2018/12/10/kubernetes-deployment-rollingupdate/rolling.jpg"><br>注意： 在恢复 Deployment 之前您无法回退一个已经暂停的 Deployment。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>deployment</tag>
        <tag>rolling</tag>
        <tag>update</tag>
        <tag>pausing</tag>
        <tag>resuming</tag>
      </tags>
  </entry>
  <entry>
    <title>理解Kubernetes安全的持久化保存键值-Etcd</title>
    <url>/2018/12/23/kubernetes-etcd-secret/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>etcd是CoreOS团队于2013年6月发起的开源项目，它的目标是构建一个高可用的分布式键值(key-value)数据库。etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。</p>
<p>etcd作为服务发现系统，有以下的特点：</p>
<p>简单：安装配置简单，而且提供了HTTP API进行交互，使用也很简单<br>安全：支持SSL证书验证<br>快速：根据官方提供的benchmark数据，单实例支持每秒2k+读操作<br>可靠：采用raft算法，实现分布式系统数据的可用性和一致性</p>
<p>etcd项目地址：<a href="https://github.com/coreos/etcd/">https://github.com/coreos/etcd/</a></p>
<h2 id="kubernetes中的使用"><a href="#kubernetes中的使用" class="headerlink" title="kubernetes中的使用"></a>kubernetes中的使用</h2><p>目前etcd是作为kubernetes集群当中的存储后端</p>
<p>在kuernetes中etcd涉及到的安全相关的主要有:</p>
<ul>
<li>etcd支持备份恢复机制，防止数据被误删导致数据丢失</li>
<li>用户的敏感信息建议放在secret类型的资源中，该类型资源是加密存储在etcd中的</li>
<li>etcd支持https, kube-apiserver访问etcd使用https协议</li>
</ul>
<p><img src="/2018/12/23/kubernetes-etcd-secret/etcd.png"></p>
<p>在kubernetes中的配置:</p>
<h3 id="Client-Server"><a href="#Client-Server" class="headerlink" title="Client -> Server"></a>Client -&gt; Server</h3><figure class="highlight nsis"><table><tbody><tr><td class="code"><pre><span class="line">client-transport-security:</span><br><span class="line">  <span class="comment"># 通道以TLS协议加密</span></span><br><span class="line">  ca-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd-ca.pem'</span></span><br><span class="line">  cert-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd.pem'</span> </span><br><span class="line">  key-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd-key.pem'</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 服务端会认证客户端证书是否受信任CA签发</span></span><br><span class="line">  client-cert-auth: <span class="literal">true</span></span><br><span class="line">  trusted-ca-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd-ca.pem'</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 是否系统自动生成证书</span></span><br><span class="line">  <span class="literal">auto</span>-tls: <span class="literal">true</span> </span><br></pre></td></tr></tbody></table></figure>
<h3 id="Server-Server"><a href="#Server-Server" class="headerlink" title="Server -> Server"></a>Server -&gt; Server</h3><figure class="highlight nsis"><table><tbody><tr><td class="code"><pre><span class="line">peer-transport-security:</span><br><span class="line">  <span class="comment"># 通道以TLS协议加密</span></span><br><span class="line">  ca-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd-ca.pem'</span></span><br><span class="line">  cert-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd.pem'</span></span><br><span class="line">  key-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd-key.pem'</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 服务端会认证客户端证书是否受信任CA签发</span></span><br><span class="line">  peer-client-cert-auth: <span class="literal">true</span></span><br><span class="line">  trusted-ca-<span class="keyword">file</span>: <span class="string">'/etc/etcd/ssl/etcd-ca.pem'</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 是否系统自动生成证书</span></span><br><span class="line">  <span class="literal">auto</span>-tls: <span class="literal">true</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="etcd的备份"><a href="#etcd的备份" class="headerlink" title="etcd的备份"></a>etcd的备份</h3><p>对于 API 3 备份与恢复方法<br>官方 v3 admin guide<br>在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。<br>在命令行设置：</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> <span class="attribute">ETCDCTL_API</span>=3</span><br></pre></td></tr></tbody></table></figure>

<p>备份数据：</p>
<figure class="highlight stylus"><table><tbody><tr><td class="code"><pre><span class="line">$ etcdctl <span class="attr">--endpoints</span>=<span class="selector-attr">[localhost:2379]</span> snapshot save snapshot<span class="selector-class">.db</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>恢复：</p>
<figure class="highlight pgsql"><table><tbody><tr><td class="code"><pre><span class="line">$ etcdctl <span class="keyword">snapshot</span> restore <span class="keyword">snapshot</span>.db <span class="comment">--name m3 --data-dir=/home/etcd_data</span></span><br></pre></td></tr></tbody></table></figure>

<p>恢复后的文件需要修改权限为 etcd:etcd<br>–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>–data-dir：指定数据目录<br>建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>
<p>etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>etcd cluster</tag>
        <tag>etcd secret</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes调度之NodeSelector</title>
    <url>/2018/12/03/kubernetes-diao-du-zhinodeselector-trashed/</url>
    <content><![CDATA[<h2 id="1-NodeName"><a href="#1-NodeName" class="headerlink" title="1 NodeName"></a>1 NodeName</h2><p>Pod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是<code>强制匹配</code>。<br>例子：</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      nodeName: 192.168.56.13 # 指定pod调度到该节点
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent # 镜像拉取策略
        ports:
        - containerPort: 80
</code></pre>
<h2 id="2-NodeSelector"><a href="#2-NodeSelector" class="headerlink" title="2 NodeSelector"></a>2 NodeSelector</h2><p>Pod.spec.nodeSelector是通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是<code>强制约束</code>。启用节点选择器的步骤为：</p>
<p>Node添加label标记</p>
<h4 id="标记规则："><a href="#标记规则：" class="headerlink" title="标记规则："></a>标记规则：</h4><pre><code>kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;
kubectl label nodes 192.168.56.13 server_type=game_server
</code></pre>
<h4 id="确认标记"><a href="#确认标记" class="headerlink" title="确认标记"></a>确认标记</h4><pre><code>[root@linux-node1 ~]#  kubectl get nodes 192.168.56.14 --show-labels
NAME            STATUS    ROLES     AGE       VERSION   LABELS
192.168.56.14   Ready     &lt;none&gt;    81d       v1.10.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.56.14,server_type=game_server
Pod定义中添加nodeSelector
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
      nodeSelector: 
        server_type: game_server #指定调度节点为带有label标记为server_type=game_server
</code></pre>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>管理Kubernetes日志</title>
    <url>/2018/12/13/kubernetes-logs/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>日志是我们在运维部署当中非常重要的一个东西，对于我个人工作经验而言，一般出现问题，第一步事情就是查看日志，其次是服务器资源查看这样去定位问题，那么在kubernets集群当中呢，我主要给其分了两种类型:</p>
<ul>
<li>kubernetes组件日志</li>
<li>运行于kubernetes上的容器应用日志</li>
</ul>
<h2 id="kubernetes组件日志"><a href="#kubernetes组件日志" class="headerlink" title="kubernetes组件日志"></a>kubernetes组件日志</h2><p>我们知道在kubernetes集群是有多个组件组成，协同为我们提供一个运行容器的环境。当运行当中出现问题，也是需要查看对应组件的日志，进行问题排查、处理。那么常见的日志如下：</p>
<figure class="highlight arcade"><table><tbody><tr><td class="code"><pre><span class="line">/<span class="keyword">var</span>/<span class="built_in">log</span>/kube-apiserver.<span class="built_in">log</span></span><br><span class="line">/<span class="keyword">var</span>/<span class="built_in">log</span>/kube-proxy.<span class="built_in">log</span></span><br><span class="line">/<span class="keyword">var</span>/<span class="built_in">log</span>/kube-controller-manager.<span class="built_in">log</span></span><br><span class="line">/<span class="keyword">var</span>/<span class="built_in">log</span>/kube-scheduler.<span class="built_in">log</span></span><br><span class="line">/<span class="keyword">var</span>/<span class="built_in">log</span>/kubelet.<span class="built_in">log</span></span><br></pre></td></tr></tbody></table></figure>
<p>当然根据搭建集群的方式不同，我们配置的日志目录也不尽相同，所以只是列举一下；<br>如果组件的安装方式由systemd来管理的话 我们还可以通过以下命令进行排错</p>
<figure class="highlight fortran"><table><tbody><tr><td class="code"><pre><span class="line">journalctl -u kubelet</span><br><span class="line">或</span><br><span class="line">systemctl <span class="keyword">status</span> kubelet -l</span><br></pre></td></tr></tbody></table></figure>

<p>如果使用的是kuernetes插件式方式部署的组件则使用以下命令</p>
<figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line">kubectl logs -f <span class="tag">&lt;<span class="name">组件名称</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="运行于kubernetes上的容器应用日志"><a href="#运行于kubernetes上的容器应用日志" class="headerlink" title="运行于kubernetes上的容器应用日志"></a>运行于kubernetes上的容器应用日志</h2><p>运行于kubernetes上的，比如一个nginx容器；我们如何查看这个应用的日志呢？</p>
<h3 id="从容器标准输出截获"><a href="#从容器标准输出截获" class="headerlink" title="从容器标准输出截获"></a>从容器标准输出截获</h3><p>用法类似于docker</p>
<figure class="highlight dust"><table><tbody><tr><td class="code"><pre><span class="line"><span class="language-xml">kubectl logs -f </span><span class="template-variable">{POD_NAME}</span><span class="language-xml"> -c </span><span class="template-variable">{Container_NAME}</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="进入容器进行查看"><a href="#进入容器进行查看" class="headerlink" title="进入容器进行查看"></a>进入容器进行查看</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it {POD_NAME} -c {Container_NAME} /bin/sh</span><br><span class="line">docker <span class="built_in">exec</span> -it {Container_NAME} /bin/sh</span><br></pre></td></tr></tbody></table></figure>
<h3 id="将日志文件挂载到主机目录"><a href="#将日志文件挂载到主机目录" class="headerlink" title="将日志文件挂载到主机目录"></a>将日志文件挂载到主机目录</h3><p>比如我要把nginx的日志挂载到运行于该POD的宿主机的某个目录</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 编写yaml文件</span></span><br><span class="line"><span class="comment"># 这里我们知道手写一个yaml是很烦的，而且那么多关键词不是那么容易记住，</span></span><br><span class="line"><span class="comment"># 那么此时就需要在现有环境中找一个pod然后把他的yaml导出再做修改</span></span><br><span class="line"><span class="comment"># 如果环境中还是没有pod，那就直接run一个</span></span><br><span class="line"></span><br><span class="line">$ kubectl get pod nginx-<span class="number">67</span>ccc95d8c-fd5nk <span class="operator">-</span>o yaml <span class="operator">-</span>-export <span class="operator">&gt;</span> nginx.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除一些不必要的字段</span></span><br><span class="line">$ vim nginx.yaml </span><br><span class="line"></span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> nginx</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">run:</span> nginx</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">image:</span> nginx:latest</span><br><span class="line">    <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">    <span class="params">name:</span> nginx</span><br><span class="line">    <span class="params">volumeMounts:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">mountPath:</span> <span class="symbol">/var/log/nginx</span>  <span class="comment"># nginx应用默认日志目录</span></span><br><span class="line">      <span class="params">name:</span> nginx-log-volume     </span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> nginx-log-volume</span><br><span class="line">    <span class="params">hostPath:</span></span><br><span class="line">      <span class="params">path:</span> <span class="symbol">/var/k8s/log</span>         <span class="comment"># 宿主机目录</span></span><br><span class="line">      </span><br><span class="line">$ kubectl create <span class="operator">-</span>f nginx.yaml</span><br><span class="line">pod <span class="string">"nginx"</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看宿主机，这样容器的日志目录我就挂到了宿主</span></span><br><span class="line"><span class="comment"># 而不用在进入容器中查看了</span></span><br><span class="line">$ ll <span class="operator">/</span>var<span class="operator">/</span>k8s<span class="operator">/</span>log<span class="symbol">/</span> </span><br><span class="line">total <span class="number">0</span></span><br><span class="line"><span class="operator">-</span>rw-r----- <span class="number">1</span> root root <span class="number">0</span> Dec <span class="number">18</span> <span class="number">15</span>:<span class="number">17</span> access.log</span><br><span class="line"><span class="operator">-</span>rw-r----- <span class="number">1</span> root root <span class="number">0</span> Dec <span class="number">18</span> <span class="number">15</span>:<span class="number">17</span> error.log</span><br></pre></td></tr></tbody></table></figure>
<p>注意点: 写yaml的时候一定要养成导出现有资源类型的yaml的习惯再去修改；这样能避免手动编写时格式的一些问题，如果关键字记不住那就一定要用<code>kubectl explain</code>获取资源文档</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes Session亲和性设置</title>
    <url>/2018/07/15/kubernetes-session-bao-chi-deng-she-zhi/</url>
    <content><![CDATA[<p>当我们在部署了多个pod，以及一个Service后，就可以在集群内部通过ServiceIP访问pod提供的服务了；<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15317143688914.jpg">￼<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15317151806742.jpg">￼</p>
<h3 id="当不设置session保持时，service向后台pod转发规则是轮询"><a href="#当不设置session保持时，service向后台pod转发规则是轮询" class="headerlink" title="当不设置session保持时，service向后台pod转发规则是轮询:"></a>当不设置session保持时，service向后台pod转发规则是轮询:</h3><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15317146734082.jpg">￼<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15317146968972.jpg">￼<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15317147222424.jpg">￼<br>以上我通过点击页面请求，可以就看出service将我的请求分发到了后面的三个pod;</p>
<h3 id="k8s会根据访问的ip来把请求转发给他以前访问过的pod，这样session就保持住了。"><a href="#k8s会根据访问的ip来把请求转发给他以前访问过的pod，这样session就保持住了。" class="headerlink" title="k8s会根据访问的ip来把请求转发给他以前访问过的pod，这样session就保持住了。"></a>k8s会根据访问的ip来把请求转发给他以前访问过的pod，这样session就保持住了。</h3><p>查看创建service时的yaml文件内容，如果没有设置的话 该项是为None的<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/07/15317149817174.jpg">￼</p>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes容器存活探测&amp;应用自恢复</title>
    <url>/2018/12/18/kubernetes-liveness/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>当你使用kuberentes的时候，有没有遇到过Pod在启动后一会就挂掉然后又重新启动这样的恶性循环？<br>你有没有想过kubernetes是如何检测pod是否还存活？<br>虽然容器已经启动，但是kubernetes如何知道容器的进程是否准备好对外提供服务了呢？</p>
<p>本博文主要记录实践如何配置容器的存活和就绪探针。</p>
<p><strong>liveness probe（存活探针）</strong><br>用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。</p>
<p><strong>readiness probe（就绪探针）</strong><br>用于判断容器是否启动完成，即容器的Ready是否为True，可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。</p>
<p><strong>每类探针都支持三种探测方法</strong></p>
<ul>
<li>exec：通过执行命令来检查服务是否正常，针对复杂检测或无HTTP接口的服务，命令返回值为0则表示容器健康。</li>
<li>httpGet：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康。</li>
<li>tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康。</li>
</ul>
<p>每种方式都可以定义在readiness 或者liveness 中。比如定义readiness 中http get 就是意思说如果我定义的这个path的http get 请求返回200-400以外的http code 就把我从所有有我的服务里面删了吧，如果定义在liveness里面就是把我kill 了。<br>注意，liveness不会重启pod，pod是否会重启由你的restart policy 控制。</p>
<p><strong>探针探测的结果有以下三者之一</strong></p>
<ul>
<li>Success：Container通过了检查。</li>
<li>Failure：Container未通过检查。</li>
<li>Unknown：未能执行检查，因此不采取任何措施。</li>
</ul>
<p><strong>重启策略</strong></p>
<ul>
<li>Always: 总是重启</li>
<li>OnFailure: 如果失败就重启</li>
<li>Never: 永远不重启</li>
</ul>
<h2 id="LivenessProbe探针配置"><a href="#LivenessProbe探针配置" class="headerlink" title="LivenessProbe探针配置"></a>LivenessProbe探针配置</h2><h3 id="示例一-通过exec方式做健康探测"><a href="#示例一-通过exec方式做健康探测" class="headerlink" title="示例一: 通过exec方式做健康探测"></a>示例一: 通过exec方式做健康探测</h3><figure class="highlight nestedtext"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">exec-liveness.yaml  </span></span><br><span class="line"><span class="attribute">apiVersion</span><span class="punctuation">:</span> <span class="string">v1</span></span><br><span class="line"><span class="attribute">kind</span><span class="punctuation">:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attribute">metadata</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">labels</span><span class="punctuation">:</span></span><br><span class="line">    <span class="attribute">test</span><span class="punctuation">:</span> <span class="string">liveness</span></span><br><span class="line">  <span class="attribute">name</span><span class="punctuation">:</span> <span class="string">liveness-exec</span></span><br><span class="line"><span class="attribute">spec</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">containers</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name: liveness</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">k8s.gcr.io/busybox  </span></span><br><span class="line">    <span class="attribute">args</span><span class="punctuation">:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600</span></span><br><span class="line">    <span class="attribute">livenessProbe</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">exec</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">command</span><span class="punctuation">:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">cat</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/tmp/healthy</span></span><br><span class="line">      <span class="attribute">initialDelaySeconds</span><span class="punctuation">:</span> <span class="string">5</span></span><br><span class="line">      <span class="attribute">periodSeconds</span><span class="punctuation">:</span> <span class="string">5</span></span><br></pre></td></tr></tbody></table></figure>
<p>在该配置文件中，对容器执行livenessProbe检查，periodSeconds字段指定kubelet每5s执行一次检查，检查的命令为cat /tmp/healthy，initialDelaySeconds字段告诉kubelet应该在执行第一次检查之前等待5秒，<br>如果命令执行成功，则返回0，那么kubelet就认为容器是健康的，如果为非0，则Kubelet会Kill掉容器并根据重启策略来决定是否需要重启(kubernetes默认为POD配置的重启策略为Always)</p>
<p>当容器启动时，它会执行以下命令：</p>
<figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line"><span class="regexp">/bin/</span>sh -c <span class="string">"touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"</span></span><br></pre></td></tr></tbody></table></figure>
<p>对于容器的前30秒，有一个/tmp/healthy文件。因此，在前30秒内，该命令cat /tmp/healthy返回成功代码。30秒后，cat /tmp/healthy返回失败代码。</p>
<p>在30秒内，查看Pod事件：</p>
<figure class="highlight sql"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl <span class="keyword">describe</span> pod liveness<span class="operator">-</span><span class="keyword">exec</span></span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age               <span class="keyword">From</span>               Message</span><br><span class="line">  <span class="comment">----     ------     ----              ----               -------</span></span><br><span class="line">  Normal   Scheduled  <span class="number">15</span>m               <span class="keyword">default</span><span class="operator">-</span>scheduler  Successfully assigned <span class="keyword">default</span><span class="operator">/</span>liveness<span class="operator">-</span><span class="keyword">exec</span> <span class="keyword">to</span> k8s<span class="operator">-</span>m3</span><br><span class="line">  Normal   Pulled     <span class="number">3</span>m (x3 <span class="keyword">over</span> <span class="number">5</span>m)   kubelet, k8s<span class="operator">-</span>m3    Successfully pulled image "k8s.gcr.io/busybox"</span><br><span class="line">  Normal   Created    <span class="number">3</span>m (x3 <span class="keyword">over</span> <span class="number">5</span>m)   kubelet, k8s<span class="operator">-</span>m3    Created container</span><br><span class="line">  Normal   Started    <span class="number">3</span>m (x3 <span class="keyword">over</span> <span class="number">5</span>m)   kubelet, k8s<span class="operator">-</span>m3    Started container</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>在30秒后，查看Pod事件：</p>
<figure class="highlight sql"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl <span class="keyword">describe</span> pod liveness<span class="operator">-</span><span class="keyword">exec</span></span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age              <span class="keyword">From</span>               Message</span><br><span class="line">  <span class="comment">----     ------     ----             ----               -------</span></span><br><span class="line">  Normal   Scheduled  <span class="number">16</span>m              <span class="keyword">default</span><span class="operator">-</span>scheduler  Successfully assigned <span class="keyword">default</span><span class="operator">/</span>liveness<span class="operator">-</span><span class="keyword">exec</span> <span class="keyword">to</span> k8s<span class="operator">-</span>m3</span><br><span class="line">  Normal   Pulled     <span class="number">5</span>m (x3 <span class="keyword">over</span> <span class="number">7</span>m)  kubelet, k8s<span class="operator">-</span>m3    Successfully pulled image "k8s.gcr.io/busybox"</span><br><span class="line">  Normal   Created    <span class="number">5</span>m (x3 <span class="keyword">over</span> <span class="number">7</span>m)  kubelet, k8s<span class="operator">-</span>m3    Created container</span><br><span class="line">  Normal   Started    <span class="number">5</span>m (x3 <span class="keyword">over</span> <span class="number">7</span>m)  kubelet, k8s<span class="operator">-</span>m3    Started container</span><br><span class="line">  Warning  Unhealthy  <span class="number">4</span>m (x9 <span class="keyword">over</span> <span class="number">7</span>m)  kubelet, k8s<span class="operator">-</span>m3    Liveness probe failed: cat: can<span class="string">'t open '</span><span class="operator">/</span>tmp<span class="operator">/</span>healthy<span class="string">': No such file or directory</span></span><br><span class="line"><span class="string">  Normal   Pulling    4m (x4 over 7m)  kubelet, k8s-m3    pulling image "k8s.gcr.io/busybox"</span></span><br><span class="line"><span class="string">  Normal   Killing    2m (x4 over 6m)  kubelet, k8s-m3    Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated.</span></span><br></pre></td></tr></tbody></table></figure>
<p>再等30秒，确认Container已重新启动, 下面输出中RESTARTS的次数已增加：</p>
<figure class="highlight sql"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl <span class="keyword">get</span> pod liveness<span class="operator">-</span><span class="keyword">exec</span></span><br><span class="line">NAME            READY     STATUS    RESTARTS   AGE</span><br><span class="line">liveness<span class="operator">-</span><span class="keyword">exec</span>   <span class="number">1</span><span class="operator">/</span><span class="number">1</span>       <span class="keyword">Running</span>   <span class="number">1</span>          <span class="number">1</span>m</span><br></pre></td></tr></tbody></table></figure>


<h3 id="示例二-通过HTTP方式做健康探测"><a href="#示例二-通过HTTP方式做健康探测" class="headerlink" title="示例二: 通过HTTP方式做健康探测"></a>示例二: 通过HTTP方式做健康探测</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">test:</span> liveness</span><br><span class="line">  <span class="params">name:</span> liveness-http</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> liveness</span><br><span class="line">    <span class="params">image:</span> k8s.gcr.io<span class="symbol">/liveness</span> <span class="comment"># 官方用户测试的demo镜像</span></span><br><span class="line">    <span class="params">args:</span></span><br><span class="line">    <span class="operator">-</span> <span class="symbol">/server</span></span><br><span class="line">    <span class="params">livenessProbe:</span></span><br><span class="line">      <span class="params">httpGet:</span></span><br><span class="line">        <span class="params">path:</span> <span class="symbol">/healthz</span></span><br><span class="line">        <span class="params">port:</span> <span class="number">8080</span></span><br><span class="line">        <span class="params">httpHeaders:</span></span><br><span class="line">        <span class="operator">-</span> <span class="params">name:</span> X-Custom-Header</span><br><span class="line">          <span class="params">value:</span> Awesome</span><br><span class="line">      <span class="params">initialDelaySeconds:</span> <span class="number">3</span> </span><br><span class="line">      <span class="params">periodSeconds:</span> <span class="number">3</span></span><br></pre></td></tr></tbody></table></figure>

<p>在配置文件中，使用k8s.gcr.io/liveness镜像，创建出一个Pod，其中periodSeconds字段指定kubelet每3秒执行一次探测，initialDelaySeconds字段告诉kubelet延迟等待3秒，探测方式为向容器中运行的服务发送HTTP GET请求，请求8080端口下的/healthz, 任何大于或等于200且小于400的代码表示成功。任何其他代码表示失败。</p>
<p>10秒后，查看Pod事件以验证liveness探测失败并且Container已重新启动：</p>
<figure class="highlight fortran"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl describe pod liveness-http</span><br><span class="line"><span class="keyword">NAME</span>            READY     <span class="keyword">STATUS</span>             RESTARTS   AGE</span><br><span class="line">liveness-http   <span class="number">1</span>/<span class="number">1</span>       RUNNING            <span class="number">1</span>          <span class="number">1</span>m</span><br></pre></td></tr></tbody></table></figure>

<p>httpGet探测方式有如下可选的控制字段</p>
<ul>
<li>host：要连接的主机名，默认为Pod IP，可以在http request head中设置host头部。</li>
<li>scheme: 用于连接host的协议，默认为HTTP。</li>
<li>path：http服务器上的访问URI。</li>
<li>httpHeaders：自定义HTTP请求headers，HTTP允许重复headers。</li>
<li>port： 容器上要访问端口号或名称。</li>
</ul>
<h3 id="示例三：通过TCP方式做健康探测"><a href="#示例三：通过TCP方式做健康探测" class="headerlink" title="示例三：通过TCP方式做健康探测"></a>示例三：通过TCP方式做健康探测</h3><p>Kubelet将尝试在指定的端口上打开容器上的套接字，如果能建立连接，则表明容器健康。</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">goproxy</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">goproxy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">goproxy</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/goproxy:0.1</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">20</span></span><br></pre></td></tr></tbody></table></figure>
<p>TCP检查方式和HTTP检查方式非常相似，示例中两种探针都使用了，在容器启动5秒后，kubelet将发送第一个readinessProbe探针，这将连接到容器的8080端口，如果探测成功，则该Pod将被标识为ready，10秒后，kubelet将进行第二次连接。<br>除此之后，此配置还包含了livenessProbe探针，在容器启动15秒后，kubelet将发送第一个livenessProbe探针，仍然尝试连接容器的8080端口，如果连接失败则重启容器。</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">ports:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness-port</span></span><br><span class="line">  <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">  <span class="attr">hostPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">httpGet:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">    <span class="attr">port:</span> <span class="string">liveness-port</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="ReadinessProbe探针配置："><a href="#ReadinessProbe探针配置：" class="headerlink" title="ReadinessProbe探针配置："></a>ReadinessProbe探针配置：</h2><p>ReadinessProbe探针的使用场景livenessProbe稍有不同，有的时候应用程序可能暂时无法接受请求，比如Pod已经Running了，但是容器内应用程序尚未启动成功，在这种情况下，如果没有ReadinessProbe，则Kubernetes认为它可以处理请求了，然而此时，我们知道程序还没启动成功是不能接收用户请求的，所以不希望kubernetes把请求调度给它，则使用ReadinessProbe探针。<br>ReadinessProbe和livenessProbe可以使用相同探测方式，只是对Pod的处置方式不同，ReadinessProbe是将Pod IP:Port从对应的EndPoint列表中删除，而livenessProbe则Kill容器并根据Pod的重启策略来决定作出对应的措施。</p>
<p><strong>ReadinessProbe</strong>探针探测容器是否已准备就绪，如果未准备就绪则kubernetes不会将流量转发给此Pod。</p>
<p>ReadinessProbe探针与livenessProbe一样也支持exec、httpGet、TCP的探测方式，配置方式相同，只不过是将livenessProbe字段修改为ReadinessProbe。</p>
<figure class="highlight nestedtext"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">readinessProbe</span><span class="punctuation">:</span></span><br><span class="line"> <span class="attribute">exec</span><span class="punctuation">:</span></span><br><span class="line">   <span class="attribute">command</span><span class="punctuation">:</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">cat</span></span><br><span class="line">   <span class="bullet">-</span> <span class="string">/tmp/healthy</span></span><br><span class="line"> <span class="attribute">initialDelaySeconds</span><span class="punctuation">:</span> <span class="string">5</span></span><br><span class="line"> <span class="attribute">periodSeconds</span><span class="punctuation">:</span> <span class="string">5</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="示例一-ReadinessProbe示例"><a href="#示例一-ReadinessProbe示例" class="headerlink" title="示例一: ReadinessProbe示例"></a>示例一: ReadinessProbe示例</h3><p>现在来看一个加入ReadinessProbe探针和一个没有ReadinessProbe探针的示例：<br>该示例中，创建了一个deploy，名为gogs，启动的容器运行一个类似于gitlab的应用程序，程序监听端口为3000。<br>这里为了模拟效果我这里原镜像做了一下修改，主要是为了延迟他的启动时间为40s后再去启动gogs的应用程序，此时就会开启3000端口，</p>
<p>(感兴趣的同学可以了解一下，非常爽的一个自助gitweb平台<a href="https://gogs.io/">Gogs</a>)</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">      <span class="attr">test:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">3000</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">test:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">         <span class="attr">test:</span> <span class="string">gogs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">test</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">gogs</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">3000</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">                  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">                    <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                    <span class="attr">values:</span></span><br><span class="line">                      <span class="bullet">-</span> <span class="string">k8s-m1</span></span><br></pre></td></tr></tbody></table></figure>
<div style="width: 50%; margin: auto">![Not add readinessProbe result](kubernetes-liveness/image1.png)</div>
从上图可以看出来，当我创建部署之后，Pod启动18s，自身状态已Running，其READ字段，1/1 表示1个容器状态已准备就绪了，此时，对于kubernetes而言，它已经可以接收请求了，而实际上我在去访问的时候服务还无法访问，因为Gogo程序还尚启动起来，40s之后方可正常访问，所以针对于服务启动慢或者其他原因的此类程序，必须配置ReadinessProbe。

<p>下面我加入readinessProbe</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">      <span class="attr">test:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">3000</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">test:</span> <span class="string">gogs</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">         <span class="attr">test:</span> <span class="string">gogs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">test</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">gogs</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">3000</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">tcpSocket:</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">3000</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">10</span> <span class="comment"># 启动后10秒开始探测</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">5</span>        <span class="comment"># 每5秒探测一次</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">                  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">                    <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                    <span class="attr">values:</span></span><br><span class="line">                      <span class="bullet">-</span> <span class="string">k8s-m1</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<div style="width: 50%; margin: auto">![Add readinessProbe result](kubernetes-liveness/image1.png)</div>
上图可以看出Pod虽然已处于Runnig状态，但是由于第一次探测时间未到，所以READY字段为0/1，即容器的状态为未准备就绪，在未准备就绪的情况下，其Pod对应的Service下的Endpoint也为空，所以不会有任何请求被调度进来。
当通过第一次探测的检查通过后，容器的状态自然会转为READ状态。
此后根据指定的间隔时间10s后再次探测，如果不通过，则kubernetes就会将Pod IP从EndPoint列表中移除。

<h2 id="配置探针-Probe-相关属性"><a href="#配置探针-Probe-相关属性" class="headerlink" title="配置探针(Probe)相关属性"></a>配置探针(Probe)相关属性</h2><p>探针(Probe)有许多可选字段，可以用来更加精确的控制Liveness和Readiness两种探针的行为(Probe)：</p>
<p><code>initialDelaySeconds</code>：Pod启动后延迟多久才进行检查，单位：秒。<br><code>periodSeconds</code>：检查的间隔时间，默认为10，单位：秒。<br><code>timeoutSeconds</code>：探测的超时时间，默认为1，单位：秒。<br><code>successThreshold</code>：探测失败后认为成功的最小连接成功次数，默认为1，在Liveness探针中必须为1，最小值为1。<br><code>failureThreshold</code>：探测失败的重试次数，重试一定次数后将认为失败，在readiness探针中，Pod会被标记为未就绪，默认为3，最小值为1。</p>
<p>参考:<br><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>liveness probe</tag>
        <tag>readiness probe</tag>
        <tag>restartPolicy</tag>
      </tags>
  </entry>
  <entry>
    <title>理解kubernetes中的Secret</title>
    <url>/2018/12/20/kubernetes-secret/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Secret对象与ConfigMap对象类似，但它主要用于存储以下敏感信息，例如密码，OAuth token和SSH key等等。将这些信息存储在secret中，和直接存储在Pod的定义中，或Docker镜像定义中相比，更加安全和灵活。</p>
<p>kuberntes中内置了三种secret类型:</p>
<ul>
<li><code>Opaque</code>：使用base64编码存储信息，可以通过base64 –decode解码获得原始数据，因此安全性弱。</li>
<li><code>kubernetes.io/dockerconfigjson</code>：用于存储docker registry的认证信息。</li>
<li><code>kubernetes.io/service-account-token</code>：用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中。(<a href="https://blog.sctux.com/2018/12/16/kubernetes-auth/">前面博文</a>记录过实践后的食用方法)</li>
</ul>
<h1 id="Opaque-Secret"><a href="#Opaque-Secret" class="headerlink" title="Opaque Secret"></a>Opaque Secret</h1><p>Opaque类型的Secret，其value为base64编码后的值。</p>
<h2 id="创建方式"><a href="#创建方式" class="headerlink" title="创建方式"></a>创建方式</h2><h3 id="从文件中创建Secret"><a href="#从文件中创建Secret" class="headerlink" title="从文件中创建Secret"></a>从文件中创建Secret</h3><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">echo</span> -n <span class="string">"admin"</span> &gt; ./username.txt</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">echo</span> -n <span class="string">"1f2d1e2e67df"</span> &gt; ./password.txt</span></span><br></pre></td></tr></tbody></table></figure>
<p>使用kubectl create secret命令创建secret：</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 创建</span></span><br><span class="line">$ kubectl create secret generic db-user-passwd <span class="operator">-</span>-from-file<span class="operator">=</span><span class="symbol">./username.txt</span> <span class="operator">-</span>-from-file<span class="operator">=</span><span class="symbol">./password.txt</span></span><br><span class="line">secret <span class="string">"db-user-passwd"</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看创建结果</span></span><br><span class="line">$ kubectl get secrets db-user-passwd</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">data:</span></span><br><span class="line">  password.<span class="params">txt:</span> MWYyZDFlMmU2N2Rm</span><br><span class="line">  username.<span class="params">txt:</span> YWRtaW4<span class="operator">=</span></span><br><span class="line"><span class="params">kind:</span> Secret</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">creationTimestamp:</span> <span class="number">201</span>8-<span class="number">1</span>2-<span class="number">21</span>T08:<span class="number">58</span>:<span class="number">33</span>Z</span><br><span class="line">  <span class="params">name:</span> db-user-passwd</span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line">  <span class="params">resourceVersion:</span> <span class="string">"57310"</span></span><br><span class="line">  <span class="params">selfLink:</span> <span class="symbol">/api/v1/namespaces/default/secrets/db-user-passwd</span></span><br><span class="line">  <span class="params">uid:</span> <span class="number">9848894</span>7-<span class="number">04</span>fe-<span class="number">11</span>e9-<span class="number">97</span>cd-<span class="number">00505621</span>dd5b</span><br><span class="line"><span class="params">type:</span> Opaque</span><br></pre></td></tr></tbody></table></figure>
<h3 id="使用描述文件创建Secret"><a href="#使用描述文件创建Secret" class="headerlink" title="使用描述文件创建Secret"></a>使用描述文件创建Secret</h3><p>首先使用base64对数据进行编码：</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">echo</span> -n <span class="string">"admin"</span> | <span class="built_in">base64</span></span></span><br><span class="line">YWRtaW4=</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">echo</span> -n <span class="string">"1f2d1e2e67df"</span> | <span class="built_in">base64</span></span></span><br><span class="line">MWYyZDFlMmU2N2Rm</span><br></pre></td></tr></tbody></table></figure>
<p>创建一个类型为Secret的描述文件：</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> secret.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Secret</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> mysecret</span><br><span class="line"><span class="params">type:</span> Opaque</span><br><span class="line"><span class="params">data:</span></span><br><span class="line">  <span class="params">username:</span> YWRtaW4<span class="operator">=</span></span><br><span class="line">  <span class="params">password:</span> MWYyZDFlMmU2N2Rm</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f secret.yaml </span><br><span class="line">secret<span class="symbol">/mysecret</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看创建结果</span></span><br><span class="line">$ kubectl get secrets mysecret <span class="operator">-</span>o yaml</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">data:</span></span><br><span class="line">  <span class="params">password:</span> MWYyZDFlMmU2N2Rm</span><br><span class="line">  <span class="params">username:</span> YWRtaW4<span class="operator">=</span></span><br><span class="line"><span class="params">kind:</span> Secret</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">creationTimestamp:</span> <span class="number">201</span>8-<span class="number">1</span>2-<span class="number">21</span>T09:<span class="number">03</span>:<span class="number">52</span>Z</span><br><span class="line">  <span class="params">name:</span> mysecret</span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line">  <span class="params">resourceVersion:</span> <span class="string">"57767"</span></span><br><span class="line">  <span class="params">selfLink:</span> <span class="symbol">/api/v1/namespaces/default/secrets/mysecret</span></span><br><span class="line">  <span class="params">uid:</span> <span class="number">564</span>bd4da-<span class="number">04</span>ff-<span class="number">11</span>e9-<span class="number">97</span>cd-<span class="number">00505621</span>dd5b</span><br><span class="line"><span class="params">type:</span> Opaque</span><br></pre></td></tr></tbody></table></figure>
<h2 id="食用方式"><a href="#食用方式" class="headerlink" title="食用方式"></a>食用方式</h2><p>创建好Secret之后，可以通过两种方式食用：</p>
<ul>
<li>以Volume方式</li>
<li>以环境变量方式</li>
</ul>
<h3 id="将-Secret-挂载到-Volume-中"><a href="#将-Secret-挂载到-Volume-中" class="headerlink" title="将 Secret 挂载到 Volume 中"></a>将 Secret 挂载到 Volume 中</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> redis-pod.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> redis</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">app:</span> redis</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">nodeName:</span> k8s-m1</span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> container-<span class="number">0</span></span><br><span class="line">    <span class="params">image:</span> redis</span><br><span class="line">    <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">    <span class="params">volumeMounts:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">name:</span> foo</span><br><span class="line">        <span class="params">mountPath:</span> <span class="symbol">/etc/foo</span></span><br><span class="line">        <span class="params">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> foo</span><br><span class="line">      <span class="params">secret:</span></span><br><span class="line">        <span class="params">secretName:</span> db-user-passwd</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ 创建:</span><br><span class="line">$ kubectl create <span class="operator">-</span>f redis-pod.yaml</span><br><span class="line">pod<span class="symbol">/redis</span> created</span><br></pre></td></tr></tbody></table></figure>

<p>进入容器检查</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l /etc/foo/</span></span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 19 Dec 21 09:14 password.txt -&gt; ..data/password.txt</span><br><span class="line">lrwxrwxrwx 1 root root 19 Dec 21 09:14 username.txt -&gt; ..data/username.txt</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> /etc/foo/username.txt</span> </span><br><span class="line">admin</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> /etc/foo/password.txt</span> </span><br><span class="line">1f2d1e2e67df </span><br></pre></td></tr></tbody></table></figure>
<p>也可以只挂载Secret中特定的key：</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> redis-pod2.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> redis2</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">app:</span> redis</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">nodeName:</span> k8s-m1</span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> redis</span><br><span class="line">    <span class="params">image:</span> redis</span><br><span class="line">    <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">    <span class="params">volumeMounts:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">name:</span> foo</span><br><span class="line">        <span class="params">mountPath:</span> <span class="symbol">/etc/foo</span></span><br><span class="line">        <span class="params">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> foo</span><br><span class="line">      <span class="params">secret:</span></span><br><span class="line">        <span class="params">secretName:</span> mysecret</span><br><span class="line">        <span class="params">items:</span></span><br><span class="line">          <span class="operator">-</span> <span class="params">key:</span> username</span><br><span class="line">            <span class="params">path:</span> my-group<span class="symbol">/my-username</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f redis-pod2.yaml </span><br><span class="line">pod<span class="symbol">/redis2</span> created</span><br></pre></td></tr></tbody></table></figure>

<p>进入容器检查</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it redis5 /bin/bash</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l /etc/foo/my-group/my-username</span> </span><br><span class="line">-rw-r--r-- 1 root root 5 Dec 21 10:26 /etc/foo/my-group/my-username</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> /etc/foo/my-group/my-username</span></span><br><span class="line">admin</span><br></pre></td></tr></tbody></table></figure>
<p>在这种情况下：</p>
<p>username 存储在/etc/foo/my-group/my-username中<br>password未被挂载<br><code>注意</code>: 指定key的这种挂载方式只适用于是通过使用描述文件创建的Secret,从文件中创建的那种Secret挂载会报错:</p>
<figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">$ 当我挂载<span class="built_in"> Secret </span>db-user-passwd 的时候pod创建事件:</span><br><span class="line">Events:</span><br><span class="line"> <span class="built_in"> Type </span>    Reason       Age               <span class="keyword">From</span>             Message</span><br><span class="line">  ----     ------       ----              ----             -------</span><br><span class="line">  <span class="built_in">Warning</span>  FailedMount  5s (x5 over 12s)  kubelet, k8s-m1  MountVolume.SetUp failed <span class="keyword">for</span> volume <span class="string">"foo"</span> : references non-existent<span class="built_in"> secret </span>key</span><br></pre></td></tr></tbody></table></figure>
<p>具体原因目前无解<del>,有知道的大哥可以评论留言告知一二，谢谢</del></p>
<h3 id="将-Secret-导出到环境变量中"><a href="#将-Secret-导出到环境变量中" class="headerlink" title="将 Secret 导出到环境变量中"></a>将 Secret 导出到环境变量中</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> redis3.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> redis3</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">app:</span> redis</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">nodeName:</span> k8s-m1</span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> redis</span><br><span class="line">    <span class="params">image:</span> redis</span><br><span class="line">    <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">    <span class="params">env:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> SECRET_USERNAME</span><br><span class="line">      <span class="params">valueFrom:</span></span><br><span class="line">        <span class="params">secretKeyRef:</span></span><br><span class="line">          <span class="params">name:</span> mysecret</span><br><span class="line">          <span class="params">key:</span> username</span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> SECRET_PASSWORD</span><br><span class="line">      <span class="params">valueFrom:</span></span><br><span class="line">        <span class="params">secretKeyRef:</span></span><br><span class="line">          <span class="params">name:</span> mysecret</span><br><span class="line">          <span class="params">key:</span> password</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入容器查看结果</span></span><br><span class="line">$ kubectl exec <span class="operator">-</span>it redis3 <span class="symbol">/bin/bash</span></span><br><span class="line">$ echo $SECRET_USERNAME</span><br><span class="line">admin</span><br><span class="line">$ echo $SECRET_PASSWORD</span><br><span class="line"><span class="number">1</span>f2d1e2e67df</span><br></pre></td></tr></tbody></table></figure>

<p>ok , 通过不同方式挂载进容器中，我们的应用程序就可以拿来用啦，具体选择哪种方式挂载还是需要看实际环境；<br>好啦，以上就是Secret的简单食用方法，更多可以看<a href="https://kubernetes.io/docs/concepts/configuration/secret/">官方文档</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kuberntes</tag>
        <tag>secret</tag>
      </tags>
  </entry>
  <entry>
    <title>理解kubernetes中的静态Pod</title>
    <url>/2018/12/22/kubernetes-static-pod/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>今儿是冬至，尽管如此，学习的脚步还是不能停下，今天学习实践一下kubernetes中的静态pod是什么？</p>
<p>我们知道在前面Pod的声明周期管理都是通过像DaemonSet、StatefulSet、Deployment这种方式创建管理的，而官方文档介绍了一种特殊的pod就是静态Pod，</p>
<h2 id="什么是静态Pod"><a href="#什么是静态Pod" class="headerlink" title="什么是静态Pod"></a>什么是静态Pod</h2><p>静态Pod是由kubelet进行管理，仅存在于特定Node上的Pod。它们不能通过API Server进行管理，无法与ReplicationController、Deployment或DaemonSet进行关联，并且kubelet也无法对其健康检查。</p>
<h3 id="静态Pod的创建"><a href="#静态Pod的创建" class="headerlink" title="静态Pod的创建:"></a>静态Pod的创建:</h3><p>静态pod可以通过两种方式创建：使用配置文件或HTTP。</p>
<h4 id="通过配置文件创建"><a href="#通过配置文件创建" class="headerlink" title="通过配置文件创建"></a>通过配置文件创建</h4><p>配置文件只是特定目录中json或yaml格式的标准pod定义。他通过在kubelet守护进程中添加配置参数<code>--pod-manifest-path=&lt;the directory&gt;</code> 来运行静态Pod，kubelet经常会它定期扫描目录；</p>
<p>例如，如何将一个简单web服务作为静态pod启动</p>
<p>选择运行静态pod的节点服务器<br>不一定是node节点，只要有kubelet进程所在的节点都可以运行静态pod</p>
<p>我在某个节点上创建一个放置一个Web服务器pod定义的描述文件文件夹，例如/etc/kubelet.d/static-web.yaml</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ mkdir <span class="operator">/</span>etc<span class="operator">/</span>kubelet.d<span class="symbol">/</span></span><br><span class="line">$ cat <span class="operator">&lt;</span><span class="operator">&lt;</span>EOF <span class="operator">&gt;</span><span class="symbol">/etc/kubelet.d/static-web.yaml</span></span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> static-web</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">role:</span> myrole</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> web</span><br><span class="line">      <span class="params">image:</span> nginx</span><br><span class="line">      <span class="params">ports:</span></span><br><span class="line">        <span class="operator">-</span> <span class="params">name:</span> web</span><br><span class="line">          <span class="params">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="params">protocol:</span> TCP</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ ls <span class="operator">/</span>etc<span class="operator">/</span>kubelet.d<span class="symbol">/</span></span><br><span class="line">static-web.yaml</span><br></pre></td></tr></tbody></table></figure>

<p>通过使用–pod-manifest-path=/etc/kubelet.d/参数运行它，在节点上配置我的kubelet守护程序以使用此目录。<br>比如我这里kubelet启动参数位于/etc/systemd/system/kubelet.service.d/10-kubelet.conf, 修改配置，然后将参数加入到现有参数配置项中(安装方式不尽相同，但是道理一样)</p>
<figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line">$ <span class="keyword">vim</span> /etc/systemd/<span class="built_in">system</span>/kubelet.service.d/<span class="number">10</span>-kubelet.<span class="keyword">conf</span></span><br><span class="line">······</span><br><span class="line">······</span><br><span class="line">Environment=<span class="string">"KUBELET_EXTRA_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local --pod-manifest-path=/etc/kubelet.d/"</span></span><br><span class="line">······</span><br><span class="line">······</span><br></pre></td></tr></tbody></table></figure>
<p>保存退出，reload一下systemd daeomon ,重启kubelet服务进程</p>
<figure class="highlight crystal"><table><tbody><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>systemctl daemon-reload</span><br><span class="line"><span class="variable">$ </span>systemctl restart kubelet</span><br></pre></td></tr></tbody></table></figure>

<p>前面说啦，当kubelet启动时，它会自动启动在指定的目录–pod-manifest-path=或–manifest-url=参数中定义的所有pod ，即我们的static-web。<br>再该节点上检查是否创建成功：</p>
<figure class="highlight crmsh"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                READY     STATUS    RESTARTS   AGE       IP            <span class="keyword">NODE</span>    </span><br><span class="line"><span class="title">static-web-k8s-m1</span>   <span class="number">1</span>/<span class="number">1</span>       Running   <span class="number">0</span>          <span class="number">2m</span>        <span class="number">10.244</span>.<span class="number">2.32</span>   k8s-m1</span><br></pre></td></tr></tbody></table></figure>

<p>上面也提到了，他不归任何部署方式来管理，即使我们尝试kubelet命令去删除</p>
<figure class="highlight crmsh"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl delete pod static-web-k8s-m1</span><br><span class="line">pod <span class="string">"static-web-k8s-m1"</span> deleted</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                READY     STATUS    RESTARTS   AGE       IP        <span class="keyword">NODE</span>      <span class="title">NOMINATED</span> <span class="keyword">NODE</span></span><br><span class="line"><span class="title">static-web-k8s-m1</span>   <span class="number">0</span>/<span class="number">1</span>       Pending   <span class="number">0</span>          <span class="number">2s</span>        <span class="tag">&lt;none&gt;</span>    k8s-m1    <span class="tag">&lt;none&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以看出静态pod通过这种方式是没法删除的</p>
<p>那我如何去删除或者说是动态的添加一个pod呢？<br>这种机制已经知道，kubelet进程会定期扫描配置的目录（/etc/kubelet.d在我的示例）以进行更改，并在文件出现/消失在此目录中时添加/删除pod。</p>
<h4 id="通过url来创建"><a href="#通过url来创建" class="headerlink" title="通过url来创建:"></a>通过url来创建:</h4><p>例如，我在这个<a href="https://blog.sctux.com/kubernetes/yaml/static-pod.json">url</a>放置了一个pod创建的描述文件</p>
<p>那么上面提到了如何修改？通过url来创建的？就是kubelet启动配置参数中配置<code>--manifest-url</code>指向即可</p>
<p><img src="/2018/12/22/kubernetes-static-pod/static-pod-from-url.png" alt="static-pod-from-url"></p>
<p>ok 以上就是如何创建静态Pod的具体食用方法，那什么场景需要用到它，这个我个人觉得还是因人或因环境而已，毕竟kubernetes开发出了这个功能，就说明有用武之地的。</p>
<p>同时在kubelet-conf.yml可以指定直接指定这个StaticPod的描述文件存放目录:</p>
<figure class="highlight asciidoc"><table><tbody><tr><td class="code"><pre><span class="line">$ cat kubelet-conf.yml</span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line"><span class="code">staticPodPath: /etc/kubernetes/manifests</span></span><br><span class="line"><span class="code">......</span></span><br><span class="line">......</span><br></pre></td></tr></tbody></table></figure>
<p>但是还是需要在启动参数中配置 <code>--pod-manifest-path</code></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>Static Pod</tag>
      </tags>
  </entry>
  <entry>
    <title>理解kubernetes中的有状态服务StatefulSet</title>
    <url>/2018/12/14/kubernetes-statefulset/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>StatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状态服务而设计），其应用场景包括</p>
<ul>
<li>稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现</li>
<li>稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有 Cluster IP 的 Service）来实现</li>
<li>有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依序进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers 来实现</li>
<li>有序收缩，有序删除（即从 N-1 到 0）</li>
</ul>
<p>从上面的应用场景可以发现，StatefulSet 由以下几个部分组成：</p>
<ul>
<li>用于定义网络标志（DNS domain）的 Headless Service</li>
<li>用于创建 PersistentVolumes 的 volumeClaimTemplates</li>
<li>定义具体应用的 StatefulSet</li>
</ul>
<p>StatefulSet 中每个 Pod 的 DNS 格式为 statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local，其中</p>
<ul>
<li>serviceName 为 Headless Service 的名字</li>
<li>0..N-1 为 Pod 所在的序号，从 0 开始到 N-1</li>
<li>statefulSetName 为 StatefulSet 的名字</li>
<li>namespace 为服务所在的 namespace，Headless Service 和 StatefulSet 必须在相同的 namespace</li>
<li>.cluster.local 为 Cluster Domain</li>
</ul>
<h2 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h2><ul>
<li>给定 Pod 的存储必须由 PersistentVolume Provisioner 根据请求的 storage class 进行配置，或由管理员预先配置。</li>
<li>删除或 scale StatefulSet 将不会删除与 StatefulSet 相关联的 volume。 这样做是为了确保数据安全性，这通常比自动清除所有相关 StatefulSet 资源更有价值。</li>
<li>StatefulSets 目前要求 Headless Service 负责 Pod 的网络身份。 您有责任创建此服务。</li>
</ul>
<h1 id="部署Statefulset服务"><a href="#部署Statefulset服务" class="headerlink" title="部署Statefulset服务"></a>部署Statefulset服务</h1><p>首先我们下面使用的是用前面<a href="https://blog.sctux.com/2018/12/15/kubernetes-storage/#NFS%E4%BD%9C%E4%B8%BA%E5%8A%A8%E6%80%81%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E5%8D%B7">博文</a>中使用到的NFS做动态供给PVC作为持久化存储。那么他的创建步骤这里不在赘述，我们在编排yaml文件中申请即可。</p>
<h2 id="获取动态卷信息"><a href="#获取动态卷信息" class="headerlink" title="获取动态卷信息"></a>获取动态卷信息</h2><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl describe storageclass managed-nfs-storage</span><br><span class="line"><span class="params">Name:</span>                  managed-nfs-storage</span><br><span class="line"><span class="params">IsDefaultClass:</span>        No</span><br><span class="line"><span class="params">Annotations:</span>           <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Provisioner:</span>           fuseim.pri<span class="symbol">/ifs</span></span><br><span class="line"><span class="params">Parameters:</span>            archiveOnDelete<span class="operator">=</span><span class="literal">false</span></span><br><span class="line"><span class="params">AllowVolumeExpansion:</span>  <span class="symbol">&lt;unset&gt;</span></span><br><span class="line"><span class="params">MountOptions:</span>          <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">ReclaimPolicy:</span>         Delete</span><br><span class="line"><span class="params">VolumeBindingMode:</span>     Immediate</span><br><span class="line"><span class="params">Events:</span>                <span class="symbol">&lt;none&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="编写service"><a href="#编写service" class="headerlink" title="编写service"></a>编写service</h2><p>一个名为 nginx 的 headless service，用于控制网络域。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> statefulset_service.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Service</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> nginx</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">app:</span> nginx</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">ports:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="params">name:</span> web</span><br><span class="line">  <span class="params">clusterIP:</span> None</span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">app:</span> nginx</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行创建</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f statefulset_service.yaml</span><br><span class="line">service<span class="symbol">/nginx</span> created</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h2 id="编写statefulset-yaml"><a href="#编写statefulset-yaml" class="headerlink" title="编写statefulset yaml"></a>编写statefulset yaml</h2><ul>
<li>一个名为 web 的 StatefulSet，它的 Spec 中指定在有 2 个运行 nginx 容器的 Pod。</li>
<li><code>volumeClaimTemplates</code> 使用 PersistentVolume Provisioner 提供的 <code>PersistentVolumes</code> 作为稳定存储。</li>
</ul>
<p>volumeClaimTemplates: 表示一类PVC的模板，系统会根据Statefulset配置的replicas数量，创建相应数量的PVC。这些PVC除了名字不一样之外其他配置都是一样的。</p>
<p>下面storageClassName配置为我之前通过NFS创建的。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> statefulset.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> apps<span class="symbol">/v1beta1</span></span><br><span class="line"><span class="params">kind:</span> StatefulSet</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> web</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">serviceName:</span> <span class="string">"nginx"</span></span><br><span class="line">  <span class="params">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="params">template:</span></span><br><span class="line">    <span class="params">metadata:</span></span><br><span class="line">      <span class="params">labels:</span></span><br><span class="line">        <span class="params">app:</span> nginx</span><br><span class="line">    <span class="params">spec:</span></span><br><span class="line">      <span class="params">containers:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">name:</span> nginx</span><br><span class="line">        <span class="params">image:</span> nginx</span><br><span class="line">        <span class="params">ports:</span></span><br><span class="line">        <span class="operator">-</span> <span class="params">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="params">name:</span> web</span><br><span class="line">        <span class="params">volumeMounts:</span></span><br><span class="line">        <span class="operator">-</span> <span class="params">name:</span> www-disk</span><br><span class="line">          <span class="params">mountPath:</span> <span class="symbol">/usr/share/nginx/html</span></span><br><span class="line">  <span class="params">volumeClaimTemplates:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">metadata:</span></span><br><span class="line">      <span class="params">name:</span> www-disk</span><br><span class="line">      <span class="params">annotations:</span></span><br><span class="line">        volume.beta.kubernetes.io<span class="operator">/</span><span class="params">storage-class:</span> <span class="string">"managed-nfs-storage"</span></span><br><span class="line">    <span class="params">spec:</span></span><br><span class="line">      <span class="params">accessModes:</span> [ <span class="string">"ReadWriteOnce"</span> ]</span><br><span class="line">      <span class="params">storageClassName:</span> <span class="string">"managed-nfs-storage"</span></span><br><span class="line">      <span class="params">resources:</span></span><br><span class="line">        <span class="params">requests:</span></span><br><span class="line">          <span class="params">storage:</span> <span class="number">1</span>Gi</span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>

<h2 id="验证服务伸缩性"><a href="#验证服务伸缩性" class="headerlink" title="验证服务伸缩性"></a>验证服务伸缩性</h2><h3 id="创建Statefulset服务："><a href="#创建Statefulset服务：" class="headerlink" title="创建Statefulset服务："></a>创建Statefulset服务：</h3><figure class="highlight subunit"><table><tbody><tr><td class="code"><pre><span class="line"># 执行创建</span><br><span class="line">$ kubectl create -f statefulset.yaml</span><br><span class="line">statefulset.apps/web created</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod/nfs-client-provisioner<span class="string">-6</span>c6997c674-z7m9r   1/1       Running   0          18m</span><br><span class="line">pod/web<span class="string">-0</span>                                     1/1       Running   0          13m</span><br><span class="line">pod/web<span class="string">-1</span>                                     1/1       Running   0          12m</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME                                   STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-0</span>   Bound     pvc-a440540e<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   13m</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-1</span>   Bound     pvc-af5c7006<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   12m</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h3 id="扩容服务到3个Pod，显示会创建新的云盘卷："><a href="#扩容服务到3个Pod，显示会创建新的云盘卷：" class="headerlink" title="扩容服务到3个Pod，显示会创建新的云盘卷："></a>扩容服务到3个Pod，显示会创建新的云盘卷：</h3><figure class="highlight subunit"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl scale sts web --replicas=3</span><br><span class="line">statefulset.apps/web scaled</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">nfs-client-provisioner<span class="string">-6</span>c6997c674-z7m9r   1/1       Running   0          19m</span><br><span class="line">web<span class="string">-0</span>                                     1/1       Running   0          14m</span><br><span class="line">web<span class="string">-1</span>                                     1/1       Running   0          14m</span><br><span class="line">web<span class="string">-2</span>                                     1/1       Running   0          40s</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">www-disk-web<span class="string">-0</span>   Bound     pvc-a440540e<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   14m</span><br><span class="line">www-disk-web<span class="string">-1</span>   Bound     pvc-af5c7006<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   14m</span><br><span class="line">www-disk-web<span class="string">-2</span>   Bound     pvc<span class="string">-97</span>afce59<span class="string">-01</span>df<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   38s</span><br></pre></td></tr></tbody></table></figure>
<h3 id="缩容服务到1个Pod，显示pvc-pv并不会一同删除："><a href="#缩容服务到1个Pod，显示pvc-pv并不会一同删除：" class="headerlink" title="缩容服务到1个Pod，显示pvc/pv并不会一同删除："></a>缩容服务到1个Pod，显示pvc/pv并不会一同删除：</h3><figure class="highlight subunit"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl scale sts web --replicas=1</span><br><span class="line">statefulset.apps/web scaled</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">nfs-client-provisioner<span class="string">-6</span>c6997c674-z7m9r   1/1       Running   0          21m</span><br><span class="line">web<span class="string">-0</span>                                     1/1       Running   0          16m</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">www-disk-web<span class="string">-0</span>   Bound     pvc-a440540e<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   16m</span><br><span class="line">www-disk-web<span class="string">-1</span>   Bound     pvc-af5c7006<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   16m</span><br><span class="line">www-disk-web<span class="string">-2</span>   Bound     pvc<span class="string">-97</span>afce59<span class="string">-01</span>df<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   2m</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h3 id="再次扩容到3个Pod，新的pod会复用原来的PVC-PV："><a href="#再次扩容到3个Pod，新的pod会复用原来的PVC-PV：" class="headerlink" title="再次扩容到3个Pod，新的pod会复用原来的PVC/PV："></a>再次扩容到3个Pod，新的pod会复用原来的PVC/PV：</h3><figure class="highlight subunit"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl scale sts web --replicas=3</span><br><span class="line">statefulset.apps/web scaled</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod/nfs-client-provisioner<span class="string">-6</span>c6997c674-z7m9r   1/1       Running   1          46m</span><br><span class="line">pod/web<span class="string">-0</span>                                     1/1       Running   0          3m</span><br><span class="line">pod/web<span class="string">-1</span>                                     1/1       Running   0          1m</span><br><span class="line">pod/web<span class="string">-2</span>                                     1/1       Running   0          18s</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME                                   STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-0</span>   Bound     pvc-a440540e<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   41m</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-1</span>   Bound     pvc-af5c7006<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   40m</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-2</span>   Bound     pvc<span class="string">-97</span>afce59<span class="string">-01</span>df<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   27m</span><br></pre></td></tr></tbody></table></figure>
<h3 id="删除一个pod-web0前，Pod引用PVC：www-disk-web-0"><a href="#删除一个pod-web0前，Pod引用PVC：www-disk-web-0" class="headerlink" title="删除一个pod/web0前，Pod引用PVC：www-disk-web-0"></a>删除一个pod/web0前，Pod引用PVC：www-disk-web-0</h3><figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">kubectl</span> describe pod/web-<span class="number">0</span> | grep ClaimName</span><br><span class="line">    <span class="attribute">ClaimName</span>:  www-disk-web-<span class="number">0</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="删除Pod后，重新创建的Pod名字与删除的一致，且使用同一个PVC："><a href="#删除Pod后，重新创建的Pod名字与删除的一致，且使用同一个PVC：" class="headerlink" title="删除Pod后，重新创建的Pod名字与删除的一致，且使用同一个PVC："></a>删除Pod后，重新创建的Pod名字与删除的一致，且使用同一个PVC：</h3><figure class="highlight subunit"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl delete pod/web<span class="string">-0</span></span><br><span class="line">pod "web<span class="string">-0</span>" deleted</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod/nfs-client-provisioner<span class="string">-6</span>c6997c674-z7m9r   1/1       Running   1          49m</span><br><span class="line">pod/web<span class="string">-0</span>                                     1/1       Running   0          10s</span><br><span class="line">pod/web<span class="string">-1</span>                                     1/1       Running   0          4m</span><br><span class="line">pod/web<span class="string">-2</span>                                     1/1       Running   0          3m</span><br><span class="line"></span><br><span class="line">$ kubectl describe pod/web<span class="string">-0</span> | grep ClaimName</span><br><span class="line">    ClaimName:  www-disk-web<span class="string">-0</span></span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME                                   STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-0</span>   Bound     pvc-a440540e<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   44m</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-1</span>   Bound     pvc-af5c7006<span class="string">-01</span>dd<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   43m</span><br><span class="line">persistentvolumeclaim/www-disk-web<span class="string">-2</span>   Bound     pvc<span class="string">-97</span>afce59<span class="string">-01</span>df<span class="string">-11</span>e9-b006<span class="string">-00505621</span>dd5b   1Gi        RWO            managed-nfs-storage   30m</span><br></pre></td></tr></tbody></table></figure>

<h2 id="验证服务高可用性"><a href="#验证服务高可用性" class="headerlink" title="验证服务高可用性"></a>验证服务高可用性</h2><h3 id="共享持久卷中新建文件"><a href="#共享持久卷中新建文件" class="headerlink" title="共享持久卷中新建文件"></a>共享持久卷中新建文件</h3><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> web-1 <span class="built_in">ls</span> /usr/share/nginx/html</span></span><br><span class="line">statefulset</span><br></pre></td></tr></tbody></table></figure>
<h3 id="删除Pod，验证数据持久性："><a href="#删除Pod，验证数据持久性：" class="headerlink" title="删除Pod，验证数据持久性："></a>删除Pod，验证数据持久性：</h3><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete pod web-1</span></span><br><span class="line">pod "web-1" deleted</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">待新的pod创建之后再次检查文件是否还在</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> web-1 <span class="built_in">ls</span> /usr/share/nginx/html</span></span><br><span class="line">statefulset</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">以上说明删除pod对pvc没有任何的影响，我们的数据还是保存着的。</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="数据保存"><a href="#数据保存" class="headerlink" title="数据保存:"></a>数据保存:</h3><p>上面也说啦, 删除或 scale StatefulSet 将不会删除与 StatefulSet 相关联的 volume。 这样做是为了确保数据安全性，这通常比自动清除所有相关 StatefulSet 资源更有价值。</p>
<p>所以我到NFS Server上面check一下:<br><img src="/2018/12/14/kubernetes-statefulset/nfs.png"></p>
<p>可见 我们在pod/web-1上创建的文件还是保存着的。</p>
<h3 id="附"><a href="#附" class="headerlink" title="附:"></a>附:</h3><p>该yaml为华为云那边的PV申请，以及StatefulSet应用的创建</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># PVC存储申请</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-evs-auto-example</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">volume.beta.kubernetes.io/storage-class:</span> <span class="string">sata</span></span><br><span class="line">    <span class="attr">volume.beta.kubernetes.io/storage-provisioner:</span> <span class="string">flexvolume-huawei.com/fuxivol</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">failure-domain.beta.kubernetes.io/region:</span> <span class="string">cn-north-1</span></span><br><span class="line">    <span class="attr">failure-domain.beta.kubernetes.io/zone:</span> <span class="string">cn-north-1a</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># StatefulSet应用创建</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cce21days-app11-guomaoqiu</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podManagementPolicy:</span> <span class="string">OrderedReady</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">cce21days-app11-guomaoqiu</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span>  </span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">cce21days-app11-guomaoqiu</span></span><br><span class="line">      <span class="attr">failure-domain.beta.kubernetes.io/region:</span> <span class="string">cn-north-1</span></span><br><span class="line">      <span class="attr">failure-domain.beta.kubernetes.io/zone:</span> <span class="string">cn-north-1a</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">cce21days-app11-guomaoqiu</span></span><br><span class="line">        <span class="attr">failure-domain.beta.kubernetes.io/region:</span> <span class="string">cn-north-1</span></span><br><span class="line">        <span class="attr">failure-domain.beta.kubernetes.io/zone:</span> <span class="string">cn-north-1a</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span> {}</span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="number">100.125</span><span class="number">.0</span><span class="number">.198</span><span class="string">:20202/guomaoqiu/tank:1.0.1</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">container-0</span></span><br><span class="line">        <span class="attr">resources:</span> {}</span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/tmp</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">pvc-evs-example</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">default-secret</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line">      <span class="attr">securityContext:</span> {}</span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pvc-evs-example</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">pvc-evs-auto-example</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>


]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>StatefulSet</tag>
        <tag>有状态</tag>
        <tag>持久化存储</tag>
      </tags>
  </entry>
  <entry>
    <title>理解kubernetes中的Storage</title>
    <url>/2018/12/15/kubernetes-storage/</url>
    <content><![CDATA[<h1 id="序、为何需要存储卷"><a href="#序、为何需要存储卷" class="headerlink" title="序、为何需要存储卷"></a>序、为何需要存储卷</h1><p>容器部署过程中一般有以下三种数据:</p>
<ul>
<li>启动时需要的初始数据，可以是配置文件</li>
<li>启动过程中产生的临时数据，该临时数据需要多个容器间共享</li>
<li>启动过程中产生的持久化数据</li>
</ul>
<p>以上三种数据都不希望在容器重启时就消失，存储卷由此而来，它可以根据不同场景提供不同类型的存储能力。<br>各类卷:</p>
<div style="width: 50%; margin: auto">![截图来源华为云cce课堂](kubernetes-storage/volume.png)</div>

<ul>
<li>spec.volumes：通过此字段提供指定的存储卷</li>
<li>spec.containers.volumeMounts：通过此字段将存储卷挂接到容器中</li>
</ul>
<h1 id="一、普通存储卷的用法"><a href="#一、普通存储卷的用法" class="headerlink" title="一、普通存储卷的用法:"></a>一、普通存储卷的用法:</h1><h2 id="容器启动时依赖数据"><a href="#容器启动时依赖数据" class="headerlink" title="容器启动时依赖数据"></a>容器启动时依赖数据</h2><h3 id="1-ConfigMap"><a href="#1-ConfigMap" class="headerlink" title="1. ConfigMap"></a>1. ConfigMap</h3><p>上图也说了它是在容器启动的时候以来的数据来源</p>
<h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 1.创建configmap预制数据卷</span></span><br><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> configmap.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">data:</span></span><br><span class="line">  <span class="params">guomaoqiu:</span> hello-world</span><br><span class="line"><span class="params">kind:</span> ConfigMap</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test</span><br><span class="line">EOF</span><br><span class="line">$ kubectl create <span class="operator">-</span>f configmap.yaml</span><br><span class="line">configmap<span class="symbol">/test</span> created</span><br><span class="line"><span class="comment"># 2. 创建pod来使用这个configmap</span></span><br><span class="line"><span class="comment"># 这里我创建了一个nginx pod 让他启动是挂载这个数据</span></span><br><span class="line">$ cat nginx-deployment.yaml</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">    <span class="params">spec:</span></span><br><span class="line">      <span class="params">containers:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">image:</span> nginx</span><br><span class="line">        <span class="params">name:</span> nginx</span><br><span class="line">        <span class="params">resources:</span> {}</span><br><span class="line">        <span class="params">volumeMounts:</span></span><br><span class="line">        <span class="operator">-</span> <span class="params">name:</span> test</span><br><span class="line">          <span class="params">mountPath:</span> <span class="symbol">/tmp</span>  <span class="comment"># 挂载点</span></span><br><span class="line">      <span class="params">volumes:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">name:</span> test</span><br><span class="line">        <span class="params">configMap:</span> </span><br><span class="line">          <span class="params">name:</span> test       <span class="comment"># ConfigMap name</span></span><br><span class="line">          <span class="params">defaultMode:</span> <span class="number">420</span> <span class="comment"># 指定挂载到pod文件的权限</span></span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.检查数据是否挂载进pod</span></span><br><span class="line">kubectl exec  nginx-<span class="number">7</span>d8bb9c4fc-bxxkt <span class="operator">-</span>- ls <span class="symbol">/tmp</span></span><br><span class="line">guomaoqiu</span><br><span class="line">kubectl exec  nginx-<span class="number">7</span>d8bb9c4fc-bxxkt <span class="operator">-</span>- cat <span class="symbol">/tmp/guomaoqiu</span></span><br><span class="line">hello-world                                                                                                                                                                                                                                 $</span><br><span class="line"><span class="comment"># 以上可以看到将configmap的key作为了文件名，将value作为了文件的内容。</span></span><br><span class="line"><span class="comment"># 他的作用也就是允许您将配置文件从容器镜像中解耦，从而增强容器应用的可移植性</span></span><br><span class="line"><span class="comment"># 更多可查看: https://k8smeetup.github.io/docs/tasks/configure-pod-container/configmap/</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-Serect"><a href="#2-Serect" class="headerlink" title="2. Serect"></a>2. Serect</h3><p>Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。</p>
<p>用户可以创建 secret，同时系统也创建了一些 secret。<br>要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 volume 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。</p>
<h4 id="示例：-1"><a href="#示例：-1" class="headerlink" title="示例："></a>示例：</h4><p>假设有些 pod 需要访问数据库。这些 pod 需要使用的用户名和密码在您本地机器的 ./username.txt 和 ./password.txt 文件里。</p>
<figure class="highlight jboss-cli"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Create files needed for rest of example.</span></span><br><span class="line"><span class="keyword">echo</span> -n <span class="string">"admin"</span> &gt; <span class="string">./username.txt</span></span><br><span class="line"><span class="keyword">echo</span> -n <span class="string">"1f2d1e2e67df"</span> &gt; <span class="string">./password.txt</span></span><br></pre></td></tr></tbody></table></figure>
<p>kubectl create secret 命令将这些文件打包到一个 Secret 中并在 API server 中创建了一个对象。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl create secret generic db-user-pass <span class="operator">-</span>-from-file<span class="operator">=</span><span class="symbol">./username.txt</span> <span class="operator">-</span>-from-file<span class="operator">=</span><span class="symbol">./password.txt</span></span><br><span class="line">secret<span class="symbol">/db-user-pass</span> created</span><br><span class="line"></span><br><span class="line">$ kubectl describe secret db-user-pass</span><br><span class="line"><span class="params">Name:</span>         db-user-pass</span><br><span class="line"><span class="params">Namespace:</span>    default</span><br><span class="line"><span class="params">Labels:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Annotations:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="params">Type:</span>  Opaque</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line"><span class="operator">==</span><span class="operator">==</span></span><br><span class="line">password.<span class="params">txt:</span>  <span class="number">12</span> bytes</span><br><span class="line">username.<span class="params">txt:</span>  <span class="number">5</span> bytes</span><br><span class="line"><span class="comment"># 默认情况下，get 和 describe 命令都不会显示文件的内容。</span></span><br><span class="line"><span class="comment"># 这是为了防止将 secret 中的内容被意外暴露给从终端日志记录中刻意寻找它们的人。</span></span><br></pre></td></tr></tbody></table></figure>
<p>在 Pod 中使用 Secret 文件<br>在挂载的 secret volume 的容器内，secret key 将作为文件，并且 secret 的值使用 base-64 解码并存储在这些文件中。这是在上面的示例容器内执行的命令的结果：</p>
<figure class="highlight swift"><table><tbody><tr><td class="code"><pre><span class="line"> kubectl exec <span class="operator">-</span>it mypod <span class="regexp">/bin/</span>bash</span><br><span class="line">root<span class="meta">@mypod</span>:<span class="regexp">/# cat /</span>etc<span class="regexp">/foo/</span>username.txt</span><br><span class="line">admin</span><br><span class="line">root<span class="meta">@mypod</span>:<span class="regexp">/# cat /</span>etc<span class="regexp">/foo/</span>password.txt</span><br><span class="line">1f2d1e2e67df</span><br></pre></td></tr></tbody></table></figure>
<p>容器中的程序负责从文件中读取 secret。<br>关于secret挂载使用方法 更多：<a href="https://kubernetes.io/zh/docs/concepts/configuration/secret/#secret-%E4%B8%8E-pod-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%9A%84%E8%81%94%E7%B3%BB">https://kubernetes.io/zh/docs/concepts/configuration/secret/#secret-%E4%B8%8E-pod-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%9A%84%E8%81%94%E7%B3%BB</a></p>
<h2 id="临时数据存储"><a href="#临时数据存储" class="headerlink" title="临时数据存储"></a>临时数据存储</h2><h3 id="1-emptryDir"><a href="#1-emptryDir" class="headerlink" title="1. emptryDir"></a>1. emptryDir</h3><p>当 Pod 被分配给节点时，首先创建 emptyDir 卷，并且只要该 Pod 在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod 中的容器可以读取和写入 emptyDir 卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除 Pod 时，emptyDir 中的数据将被永久删除。</p>
<p>注意：容器崩溃不会从节点中移除 pod，因此 emptyDir 卷中的数据在容器崩溃时是安全的。</p>
<p>缺省情况下，EmptyDir 是使用主机磁盘进行存储的，也可以设置emptyDir.medium 字段的值为Memory，来提高运行速度，但是这种设置，对该卷的占用会消耗容器的内存份额。</p>
<p>emptyDir 的用法有：</p>
<ul>
<li>暂存空间，例如用于基于磁盘的合并排序</li>
<li>用作长时间计算崩溃恢复时的检查点</li>
<li>Web服务器容器提供数据时，保存内容管理器容器提取的文件</li>
<li>可以在同一 Pod 内的不同容器之间共享工作过程中产生的文件。</li>
</ul>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test-emptydir-pod</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">image:</span> nginx</span><br><span class="line">    <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">    <span class="params">name:</span> test-emptydir-pod</span><br><span class="line">    <span class="params">volumeMounts:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">mountPath:</span> <span class="symbol">/temp-data</span>      <span class="comment"># 挂载点</span></span><br><span class="line">      <span class="params">name:</span> temp-data</span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> temp-data</span><br><span class="line">    <span class="params">emptyDir:</span> {}</span><br></pre></td></tr></tbody></table></figure>

<p>创建pod</p>
<figure class="highlight crmsh"><table><tbody><tr><td class="code"><pre><span class="line"> kubectl create -f test-emptydir-pod.yaml</span><br><span class="line">pod/test-emptydir-pod created</span><br><span class="line">$ kubectl get pods -o wide</span><br><span class="line">NAME                READY     STATUS    RESTARTS   AGE       IP            <span class="keyword">NODE</span>      <span class="title">NOMINATED</span> <span class="keyword">NODE</span></span><br><span class="line"><span class="title">test-emptydir-pod</span>   <span class="number">1</span>/<span class="number">1</span>       Running   <span class="number">0</span>          <span class="number">18s</span>       <span class="number">10.244</span>.<span class="number">0.23</span>   k8s-m3    <span class="tag">&lt;none&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<p>此时Emptydir已经创建成功，在宿主机上的访问路径为/var/lib/kubelet/pods/<pod uid="">/volumes/kubernetes.io~empty-dir/temp-data,如果在此目录中创建删除文件，都将对容器中的/data目录有影响，如果删除Pod，文件将全部删除，即使是在宿主机上创建的文件也是如此，在宿主机上删除容器则k8s会再自动创建一个容器，此时文件仍然存在。</pod></p>
<figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 获取 pod uid</span></span><br><span class="line"><span class="attribute">kubectl</span> get pod test-emptydir-pod -o yaml | grep -n uid</span><br><span class="line"><span class="attribute">11</span>:  uid: <span class="number">3328</span>c9e1-ff8f-<span class="number">11</span>e8-b6d6-<span class="number">00505621</span>dd5b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录到节点k8s-m3 查看</span></span><br><span class="line"> <span class="attribute">ll</span> /var/lib/kubelet/pods/<span class="number">3328</span>c9e1-ff8f-<span class="number">11</span>e8-b6d6-<span class="number">00505621</span>dd5b/volumes/kubernetes.io~empty-dir/</span><br><span class="line"><span class="attribute">total</span> <span class="number">0</span></span><br><span class="line"><span class="attribute">drwxrwxrwx</span> <span class="number">2</span> root root <span class="number">6</span> Dec <span class="number">14</span> <span class="number">05</span>:<span class="number">58</span> temp-data</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-hostPath"><a href="#2-hostPath" class="headerlink" title="2. hostPath"></a>2. hostPath</h3><p>hostPath volume映射node文件系统中的文件或者目录到pod里。大多数Pod都不需要这个功能，但对于一些特定的场景，该特性还是很有作用的。这些场景包括： </p>
<ul>
<li>运行的容器需要访问Docker内部结构：使用hostPath映射/var/lib/docker </li>
<li>在容器中运行cAdvisor，使用hostPath映射/dev/cgroups</li>
</ul>
<p>不过，使用这种volume要小心，因为： </p>
<ul>
<li>配置相同的pod（如通过podTemplate创建），可能在不同的Node上表现不同，因为不同节点上映射的文件内容不同 </li>
<li>当Kubernetes增加了资源敏感的调度程序，hostPath使用的资源不会被计算在内 </li>
<li>宿主机下创建的目录只有root有写权限。你需要让你的程序运行在privileged container上，或者修改宿主机上的文件权限。</li>
</ul>
<p>　</p>
<h4 id="示例：-2"><a href="#示例：-2" class="headerlink" title="示例："></a>示例：</h4><p>假设我们在创建docker镜像的时候忘记了更改容器中的时区文件；这时候想把宿主机的/usr/share/zoneinfo/Asia/Shanghai挂到pod中的容器内(这里为了学习挂载方式，暂且不谈不同系统或不同宿主机的一些配置或路径)</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test-emptydir-pod</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">image:</span> nginx</span><br><span class="line">    <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">    <span class="params">name:</span> test-emptydir-pod</span><br><span class="line">    <span class="params">volumeMounts:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> container-time</span><br><span class="line">      <span class="params">mountPath:</span> <span class="symbol">/etc/localtime</span>  <span class="comment"># 挂载点，container启动后直接挂载覆盖此文件</span></span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> container-time</span><br><span class="line">    <span class="params">hostPath:</span> </span><br><span class="line">      <span class="params">path:</span> <span class="symbol">/usr/share/zoneinfo/Asia/Shanghai</span>  <span class="comment"># 挂载时区文件到container中</span></span><br></pre></td></tr></tbody></table></figure>
<p>通过创建pod后登录检查时区正常.</p>
<h1 id="二、持久存储卷的用法"><a href="#二、持久存储卷的用法" class="headerlink" title="二、持久存储卷的用法:"></a>二、持久存储卷的用法:</h1><p>学习持久化存储之前需要学习一下以下概念：<br>Volume 提供了非常好的数据持久化方案，不过在可管理性上还有不足。<br>拿前面 AWS EBS 的例子来说，要使用 Volume，Pod 必须事先知道如下信息：</p>
<ul>
<li>当前 Volume 来自 AWS EBS。</li>
<li>EBS Volume 已经提前创建，并且知道确切的 volume-id。</li>
</ul>
<p>Pod 通常是由应用的开发人员维护，而 Volume 则通常是由存储系统的管理员维护。开发人员要获得上面的信息：</p>
<ul>
<li>要么询问管理员。</li>
<li>要么自己就是管理员。</li>
</ul>
<p>这样就带来一个管理上的问题：应用开发人员和系统管理员的职责耦合在一起了。如果系统规模较小或者对于开发环境这样的情况还可以接受。但当集群规模变大，特别是对于生成环境，考虑到效率和安全性，这就成了必须要解决的问题。</p>
<p>Kubernetes 给出的解决方案是 PersistentVolume 和 PersistentVolumeClaim。</p>
<p>PersistentVolume (PV) 是外部存储系统中的一块存储空间，由管理员创建和维护。与 Volume 一样，PV 具有持久性，生命周期独立于 Pod。</p>
<p>PersistentVolumeClaim (PVC) 是对 PV 的申请 (Claim)。PVC 通常由普通用户创建和维护。需要为 Pod 分配存储资源时，用户可以创建一个 PVC，指明存储资源的容量大小和访问模式（比如只读）等信息，Kubernetes 会查找并提供满足条件的 PV。</p>
<p>有了 PersistentVolumeClaim，用户只需要告诉 Kubernetes 需要什么样的存储资源，而不必关心真正的空间从哪里分配，如何访问等底层细节信息。这些 Storage Provider 的底层信息交给管理员来处理，只有管理员才应该关心创建 PersistentVolume 的细节信息。</p>
<p>Kubernetes 支持多种类型的 PersistentVolume，比如 AWS EBS、Ceph、NFS 等，完整列表请参考 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes</a></p>
<p>主要包括:<code>NFS</code>/<code>GlusterFS</code>/<code>CephFS</code>/<code>AWS</code>/<code>GCE</code>等等<br>作为一个容器集群，支持网络存储自然是重中之重了,Kubernetes支持为数众多的云提供商和网络存储方案。<br>不同公司选择的方案也是不相同。</p>
<p>下节我用 NFS 来体会它存储的使用方法。</p>
<h2 id="NFS"><a href="#NFS" class="headerlink" title="NFS"></a>NFS</h2><h3 id="NFS的搭建"><a href="#NFS的搭建" class="headerlink" title="NFS的搭建"></a>NFS的搭建</h3><h4 id="1-在NFS存储的服务器创建存储目录"><a href="#1-在NFS存储的服务器创建存储目录" class="headerlink" title="1. 在NFS存储的服务器创建存储目录:"></a>1. 在NFS存储的服务器创建存储目录:</h4><figure class="highlight autohotkey"><table><tbody><tr><td class="code"><pre><span class="line"># 这里先创建三个共享目录稍后会用到.</span><br><span class="line">$ mkdir /{nfs_dat<span class="built_in">a_1</span>,nfs_dat<span class="built_in">a_2</span>,nfs_dat<span class="built_in">a_3</span>}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="2-安装nfs"><a href="#2-安装nfs" class="headerlink" title="2. 安装nfs:"></a>2. 安装nfs:</h4><figure class="highlight cmake"><table><tbody><tr><td class="code"><pre><span class="line">$ yum -y <span class="keyword">install</span> nfs-utils rpcbind</span><br></pre></td></tr></tbody></table></figure>
<h4 id="3-写入配置"><a href="#3-写入配置" class="headerlink" title="3. 写入配置"></a>3. 写入配置</h4><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"/nfs_data_1 *(rw,sync,no_subtree_check,no_root_squash)"</span> &gt; /etc/exports</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"/nfs_data_2 *(rw,sync,no_subtree_check,no_root_squash)"</span> &gt;&gt; /etc/exports</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"/nfs_data_3 *(rw,sync,no_subtree_check,no_root_squash)"</span> &gt;&gt; /etc/exports</span><br></pre></td></tr></tbody></table></figure>
<h4 id="4-重启nfs服务验证"><a href="#4-重启nfs服务验证" class="headerlink" title="4. 重启nfs服务验证"></a>4. 重启nfs服务验证</h4><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">systemctl restart nfs-server</span><br><span class="line">showmount -e 192.168.56.113  <span class="comment"># NFS Server</span></span><br><span class="line">Export list <span class="keyword">for</span> 192.168.56.113:</span><br><span class="line">/nfs_data_3 *</span><br><span class="line">/nfs_data_2 *</span><br><span class="line">/nfs_data_1 *</span><br><span class="line"></span><br><span class="line"><span class="comment"># nfs安装并且共享目录已经创建完毕</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="NFS作为普通存储卷"><a href="#NFS作为普通存储卷" class="headerlink" title="NFS作为普通存储卷"></a>NFS作为普通存储卷</h3><p>在Kubernetes中，可以通过nfs类型的存储卷将现有的NFS（网络文件系统）到的挂接到Pod中。在移除Pod时，NFS存储卷中的内容被不会被删除，只是将存储卷卸载而已。这意味着在NFS存储卷总可以预先填充数据，并且可以在Pod之间共享数据。NFS可以被同时挂接到多个Pod中，并能同时进行写入。需要注意的是：在使用nfs存储卷之前，必须已正确部署和运行NFS服务器，并已经设置了共享目录。</p>
<h4 id="创建一个pod，让其挂载上面创建的共享目录"><a href="#创建一个pod，让其挂载上面创建的共享目录" class="headerlink" title="创建一个pod，让其挂载上面创建的共享目录"></a>创建一个pod，让其挂载上面创建的共享目录</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> nfs-busybox <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> nfs-busybox-pod</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> nfs-busybox-pod</span><br><span class="line">    <span class="params">image:</span> busybox</span><br><span class="line">    <span class="params">imagePullPolicy:</span> IfNotPresent</span><br><span class="line">    <span class="params">command:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"/bin/sh"</span></span><br><span class="line">    <span class="params">args:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"-c"</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1"</span></span><br><span class="line">    <span class="params">volumeMounts:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">name:</span> nfs-busybox-storage</span><br><span class="line">        <span class="params">mountPath:</span> <span class="string">"/mnt"</span></span><br><span class="line">  <span class="params">restartPolicy:</span> <span class="string">"Never"</span></span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> nfs-busybox-storage</span><br><span class="line">      <span class="params">nfs:</span></span><br><span class="line">        <span class="params">path:</span> <span class="symbol">/nfs_data_1</span>       <span class="comment"># NFS 共享目录</span></span><br><span class="line">        <span class="params">server:</span> <span class="number">192.168</span>.<span class="number">56.113</span>  <span class="comment"># NFS Server</span></span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p>启动POD，一会儿POD就是completed状态，说明执行完毕。</p>
<figure class="highlight processing"><table><tbody><tr><td class="code"><pre><span class="line">kubectl apply -f <span class="built_in">nfs</span>-busybox.<span class="property">yaml</span></span><br><span class="line">pod/<span class="built_in">nfs</span>-busybox-pod created</span><br><span class="line">$ kubectl <span class="built_in">get</span> pods</span><br><span class="line">NAME              READY     STATUS      RESTARTS   AGE</span><br><span class="line"><span class="built_in">nfs</span>-busybox-pod   <span class="number">0</span>/<span class="number">1</span>       Completed   <span class="number">0</span>          <span class="number">7</span>s</span><br></pre></td></tr></tbody></table></figure>
<p>我们去NFS Server共享目录查看有没有SUCCESS文件。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">ls</span> /nfs_data_1/</span><br><span class="line">SUCCESS</span><br></pre></td></tr></tbody></table></figure>
<p>说明当NFS作为普通存储卷时我们挂载到pod容器中产生数据将会保存到这个存储卷当中，并且删除pod不会影响数据。</p>
<hr>
<h3 id="如何基于NFS文件系统创建持久化存储？"><a href="#如何基于NFS文件系统创建持久化存储？" class="headerlink" title="如何基于NFS文件系统创建持久化存储？"></a>如何基于NFS文件系统创建持久化存储？</h3><p>Provisioning: PV的预制创建有两种模式：<code>静态</code>和<code>动态</code>供给模式，他们的含义是：</p>
<p><code>静态供给模式</code>: 需要先手动创建PV, 然后通过 PVC 申请 PV 并在 Pod 中使用，这种方式叫做静态供给（Static Provision）。<br><code>动态供给模式</code>: 只需要创建PVC，系统根据PVC创建PV, 如果没有满足 PVC 条件的 PV，会动态创建 PV。相比静态供给，动态供给(Dynamical Provision）有明显的优势：不需要提前创建 PV，减少了管理员的工作量，效率高.</p>
<p>动态供给是通过 StorageClass 实现的，StorageClass 定义了如何创建 PV。<br><img src="/2018/12/15/kubernetes-storage/k8s-pvc.png" alt="k8s-pv"></p>
<p>下面就分别实践这两种模式的创建跟使用:</p>
<h3 id="NFS作为静态供给模式持久化存储卷"><a href="#NFS作为静态供给模式持久化存储卷" class="headerlink" title="NFS作为静态供给模式持久化存储卷"></a>NFS作为静态供给模式持久化存储卷</h3><figure class="highlight brainfuck"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">手动创建PV</span><span class="literal">---</span>&gt;<span class="comment">手动创建PVC</span><span class="literal">---</span>&gt;<span class="comment">POD挂载使用</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="1-创建PV"><a href="#1-创建PV" class="headerlink" title="1. 创建PV"></a>1. 创建PV</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> nfs-test-pv.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> PersistentVolume</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> nfs-test-pv</span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">app:</span> nfs-test-pv</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">capacity:</span></span><br><span class="line">    <span class="params">storage:</span> <span class="number">5</span>Gi   <span class="comment"># 指定PV容量为5G</span></span><br><span class="line">  <span class="params">accessModes:</span></span><br><span class="line">    <span class="operator">-</span> ReadWriteOnce <span class="comment">#  指定访问模式为 ReadWriteOnce</span></span><br><span class="line">  <span class="params">persistentVolumeReclaimPolicy:</span> Recycle <span class="comment"># 指定当 PV 的回收策略为 Recycle</span></span><br><span class="line">  <span class="params">storageClassName:</span> nfs   <span class="comment"># 定 PV 的 class 为 nfs。相当于为 PV 设置了一个分类，PVC 可以指定 class 申请相应 class 的 PV。</span></span><br><span class="line">  <span class="params">nfs:</span></span><br><span class="line">    <span class="params">path:</span> <span class="symbol">/nfs_data_2</span> <span class="comment"># 指定 PV 在 NFS 服务器上对应的目录</span></span><br><span class="line">    <span class="params">server:</span> <span class="number">192.168</span>.<span class="number">56.113</span> <span class="comment"># NFS Server地址</span></span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2018/12/15/kubernetes-storage/create_pv.png" alt="create_pv"></p>
<p>STATUS 为 Available，表示 nfs-test-pv 就绪，可以被 PVC 申请。<br>接下来创建 PVC nfs-test-pv-claim：</p>
<h4 id="2-创建PVC"><a href="#2-创建PVC" class="headerlink" title="2. 创建PVC"></a>2. 创建PVC</h4><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat <span class="operator">&gt;</span><span class="operator">&gt;</span> nfs-test-pv-claim.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> PersistentVolumeClaim</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> nfs-test-pv-claim</span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">accessModes:</span> <span class="comment"># 存储访问模式，此能力依赖存储厂商能力</span></span><br><span class="line">    <span class="operator">-</span> ReadWriteOnce</span><br><span class="line">  <span class="params">resources:</span></span><br><span class="line">    <span class="params">requests:</span></span><br><span class="line">      <span class="params">storage:</span> <span class="number">2</span>Gi <span class="comment"># 请求获得的pvc存储大小</span></span><br><span class="line">  <span class="params">storageClassName:</span> nfs </span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2018/12/15/kubernetes-storage/create_pvc.png" alt="create_pvc"></p>
<p>从 kubectl get pvc 和 kubectl get pv 的输出可以看到 nfs-test-pv-claim 已经 Bound 到 nfs-test-pv，申请成功。<br>接下来就可以在 Pod 中使用存储了：</p>
<h4 id="3-创建Pod"><a href="#3-创建Pod" class="headerlink" title="3. 创建Pod"></a>3. 创建Pod</h4><figure class="highlight nestedtext"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">cat &gt;&gt; nfs-test-pod.yaml &lt;&lt; EOF</span></span><br><span class="line"><span class="attribute">apiVersion</span><span class="punctuation">:</span> <span class="string">v1</span></span><br><span class="line"><span class="attribute">kind</span><span class="punctuation">:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attribute">metadata</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">name</span><span class="punctuation">:</span> <span class="string"> nfs-test-pod</span></span><br><span class="line"><span class="attribute">spec</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">containers</span><span class="punctuation">:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name:  nfs-test-pod</span></span><br><span class="line">    <span class="attribute">image</span><span class="punctuation">:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attribute">args</span><span class="punctuation">:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sleep 30000</span></span><br><span class="line">    <span class="attribute">volumeMounts</span><span class="punctuation">:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">mountPath: "/nfs-data"</span></span><br><span class="line">      <span class="attribute">name</span><span class="punctuation">:</span> <span class="string">nfs-data</span></span><br><span class="line">  <span class="attribute">volumes</span><span class="punctuation">:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">name: nfs-data</span></span><br><span class="line">      <span class="attribute">persistentVolumeClaim</span><span class="punctuation">:</span></span><br><span class="line">        <span class="attribute">claimName</span><span class="punctuation">:</span> <span class="string"> nfs-test-pv-claim</span></span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p>与使用普通 Volume 的格式类似，在 volumes 中通过 persistentVolumeClaim 指定使用 nfs-test-pv-claim 申请的 Volume。<br><img src="/2018/12/15/kubernetes-storage/create_pod.png" alt="create_pod"><br>验证 PV 是否可用：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> nfs-test-pod <span class="built_in">touch</span> /nfs-data/SUCCESS</span><br></pre></td></tr></tbody></table></figure>
<div style="width: 50%; margin: auto">![create_result](kubernetes-storage/create_result.png)</div>

<p>可见，在 Pod 中创建的文件 /nfs-data/SUCCESS 确实已经保存到了 NFS 服务器目录 /nfs_data_2/中。<br>如果不再需要使用 PV，可用删除 PVC 回收 PV:</p>
<h4 id="4-回收PVC"><a href="#4-回收PVC" class="headerlink" title="4. 回收PVC"></a>4. 回收PVC</h4><p><strong>持久化卷声明的保护</strong><br>PVC 保护的目的是确保由 pod 正在使用的 PVC 不会从系统中移除，因为如果被移除的话可能会导致数据丢失。<br>注意：当 pod 状态为 Pending 并且 pod 已经分配给节点或 pod 为 Running 状态时，PVC 处于活动状态。<br>当启用<a href="https://k8smeetup.github.io/docs/tasks/administer-cluster/pvc-protection/#lichuqiang">PVC保护</a>功能时，如果用户删除了一个 pod 正在使用的 PVC，则该 PVC 不会被立即删除。PVC 的删除将被推迟，直到 PVC 不再被任何 pod 使用。<br>您可以看到，当我直接删除上面POD正在使用的PVC时命令直接hang住了，此时虽然为 Teminatiing，但PVC 受到保护，<code>Finalizers</code> 列表中包含 kubernetes.io/pvc-protection：</p>
<figure class="highlight maxima"><table><tbody><tr><td class="code"><pre><span class="line">kubectl <span class="built_in">delete</span> pvc nfs-test-<span class="built_in">pv</span>-claim</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2018/12/15/kubernetes-storage/pvc_delete_status.png" alt="pvc_delete_status"><br>等待 pod 状态变为 Terminated（删除 pod 或者等到它结束），然后检查，确认 PVC 被移除。</p>
<p>反之，如果一个PVC没有被pod使用则可以直接删除。</p>
<p>用户用完 volume 后，可以从允许回收资源的 API 中删除 PVC 对象。PersistentVolume 的回收策略告诉集群在存储卷声明释放后应如何处理该卷。目前，volume 的处理策略有:</p>
<ul>
<li>Retain，不清理, 保留 Volume（需要手动清理）</li>
<li>Recycle，删除数据，即 rm -rf /thevolume/*（只有 NFS 和 HostPath 支持）</li>
<li>Delete，删除存储资源，比如删除 AWS EBS 卷（只有 AWS EBS, GCE PD, Azure Disk 和 Cinder 支持）</li>
</ul>
<p>OK 以上是NFS创建静态模式创建PVC,以及PVC跟POD生命周期的一些实践，下面实践动态模式</p>
<h3 id="NFS作为动态持久化存储卷"><a href="#NFS作为动态持久化存储卷" class="headerlink" title="NFS作为动态持久化存储卷"></a>NFS作为动态持久化存储卷</h3><p>利用NFS client provisioner动态提供Kubernetes后端存储卷</p>
<p>想要动态生成PV，需要运行一个NFS-Provisioner服务，将已配置好的NFS系统相关参数录入，并向用户提供创建PV的服务。官方推荐使用Deployment运行一个replica来实现，当然也可以使用Daemonset等其他方式，这些都在官方文档中提供了。</p>
<p>前提条件是有已经安装好的NFS服务器，并且NFS服务器与Kubernetes的Slave节点都能网络连通。 所有下文用到的文件来自于<code>git clone https://github.com/kubernetes-incubator/external-storage.git</code> 的nfs-client目录</p>
<p>nfs-client-provisioner 是一个Kubernetes的简易NFS的外部provisioner，本身不提供NFS，需要现有的NFS服务器提供存储</p>
<ul>
<li>PV以 <code>${namespace}-${pvcName}-${pvName}</code>的命名格式提供（在NFS服务器上）</li>
<li>PV回收的时候以 <code>archieved-${namespace}-${pvcName}-${pvName}</code> 的命名格式（在NFS服务器上）</li>
</ul>
<h4 id="1-获取nfs-client-provisioner配置文件"><a href="#1-获取nfs-client-provisioner配置文件" class="headerlink" title="1. 获取nfs-client-provisioner配置文件"></a>1. 获取nfs-client-provisioner配置文件</h4><figure class="highlight crmsh"><table><tbody><tr><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/kubernetes-incubator/external-storage.git</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="2-安装部署"><a href="#2-安装部署" class="headerlink" title="2. 安装部署:"></a>2. 安装部署:</h4><h5 id="1-修改deployment文件并部署-deploy-deployment-yaml"><a href="#1-修改deployment文件并部署-deploy-deployment-yaml" class="headerlink" title="1. 修改deployment文件并部署 deploy/deployment.yaml"></a>1. 修改deployment文件并部署 deploy/deployment.yaml</h5><p>需要修改的地方只有NFS服务器所在的IP地址（192.168.56.113），以及NFS服务器共享的路径（/nfs_data_3），两处都需要修改为你实际的NFS服务器和共享目录</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">deploy/deployment.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/external_storage/nfs-client-provisioner:latest</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/persistentvolumes</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PROVISIONER_NAME</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">fuseim.pri/ifs</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_SERVER</span></span><br><span class="line">              <span class="attr">value:</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.113</span> </span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_PATH</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">/nfs_data_3</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">          <span class="attr">nfs:</span></span><br><span class="line">            <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.56</span><span class="number">.113</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/nfs_data_3</span></span><br><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">deploy/deployment.yaml</span> <span class="comment"># 执行部署</span></span><br><span class="line"><span class="string">serviceaccount/nfs-client-provisioner</span> <span class="string">created</span></span><br><span class="line"><span class="string">deployment.extensions/nfs-client-provisioner</span> <span class="string">created</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="2-修改StorageClass文件并部署-deploy-class-yaml"><a href="#2-修改StorageClass文件并部署-deploy-class-yaml" class="headerlink" title="2. 修改StorageClass文件并部署 deploy/class.yaml"></a>2. 修改StorageClass文件并部署 deploy/class.yaml</h5><p>此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat deploy<span class="symbol">/class.yaml</span></span><br><span class="line"><span class="params">apiVersion:</span> storage.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">kind:</span> StorageClass</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> managed-nfs-storage</span><br><span class="line"><span class="params">provisioner:</span> fuseim.pri<span class="symbol">/ifs</span> <span class="comment"># or choose another name, must match deployment's env PROVISIONER_NAME'</span></span><br><span class="line"><span class="params">parameters:</span></span><br><span class="line">  <span class="params">archiveOnDelete:</span> <span class="string">"false"</span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 执行部署 </span></span><br><span class="line">kubectl apply <span class="operator">-</span>f deploy<span class="symbol">/class.yaml</span></span><br><span class="line">storageclass.storage.k8s.io<span class="symbol">/managed-nfs-storage</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看StorageClass</span></span><br><span class="line">kubectl get sc</span><br><span class="line">NAME                  PROVISIONER      AGE</span><br><span class="line">managed-nfs-storage   fuseim.pri<span class="symbol">/ifs</span>   <span class="number">16</span>m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置这个managed-nfs-storage名字的SC为Kubernetes的默认存储后端</span></span><br><span class="line">kubectl patch storageclass managed-nfs-storage <span class="operator">-</span>p '{<span class="string">"metadata"</span>: {<span class="string">"annotations"</span>:{<span class="string">"storageclass.kubernetes.io/is-default-class"</span>:<span class="string">"true"</span>}}}'</span><br><span class="line">$ kubectl get sc</span><br><span class="line">NAME                            PROVISIONER      AGE</span><br><span class="line">managed-nfs-storage (default)   fuseim.pri<span class="symbol">/ifs</span>   <span class="number">19</span>m</span><br></pre></td></tr></tbody></table></figure>
<h5 id="3-授权"><a href="#3-授权" class="headerlink" title="3. 授权"></a>3. 授权</h5><p>如果您的集群启用了RBAC，或者您正在运行OpenShift，则必须授权provisioner。 如果你在非默认的“default”名称空间/项目之外部署，可以编辑deploy/rbac.yaml或编辑`oadm policy“指令。</p>
<p><em>如果启用了RBAC</em><br>需要执行如下的命令来授权。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat deploy<span class="symbol">/rbac.yaml</span></span><br><span class="line"><span class="params">kind:</span> ClusterRole</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> nfs-client-provisioner-runner</span><br><span class="line"><span class="params">rules:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="params">resources:</span> [<span class="string">"persistentvolumes"</span>]</span><br><span class="line">    <span class="params">verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"delete"</span>]</span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="params">resources:</span> [<span class="string">"persistentvolumeclaims"</span>]</span><br><span class="line">    <span class="params">verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"update"</span>]</span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroups:</span> [<span class="string">"storage.k8s.io"</span>]</span><br><span class="line">    <span class="params">resources:</span> [<span class="string">"storageclasses"</span>]</span><br><span class="line">    <span class="params">verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="params">resources:</span> [<span class="string">"events"</span>]</span><br><span class="line">    <span class="params">verbs:</span> [<span class="string">"create"</span>, <span class="string">"update"</span>, <span class="string">"patch"</span>]</span><br><span class="line"><span class="operator">-</span>--</span><br><span class="line"><span class="params">kind:</span> ClusterRoleBinding</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> run-nfs-client-provisioner</span><br><span class="line"><span class="params">subjects:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">kind:</span> ServiceAccount</span><br><span class="line">    <span class="params">name:</span> nfs-client-provisioner</span><br><span class="line">    <span class="params">namespace:</span> default</span><br><span class="line"><span class="params">roleRef:</span></span><br><span class="line">  <span class="params">kind:</span> ClusterRole</span><br><span class="line">  <span class="params">name:</span> nfs-client-provisioner-runner</span><br><span class="line">  <span class="params">apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="operator">-</span>--</span><br><span class="line"><span class="params">kind:</span> Role</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> leader-locking-nfs-client-provisioner</span><br><span class="line"><span class="params">rules:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">apiGroups:</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="params">resources:</span> [<span class="string">"endpoints"</span>]</span><br><span class="line">    <span class="params">verbs:</span> [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"update"</span>, <span class="string">"patch"</span>]</span><br><span class="line"><span class="operator">-</span>--</span><br><span class="line"><span class="params">kind:</span> RoleBinding</span><br><span class="line"><span class="params">apiVersion:</span> rbac.authorization.k8s.io<span class="symbol">/v1</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> leader-locking-nfs-client-provisioner</span><br><span class="line"><span class="params">subjects:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">kind:</span> ServiceAccount</span><br><span class="line">    <span class="params">name:</span> nfs-client-provisioner</span><br><span class="line">    <span class="comment"># replace with namespace where provisioner is deployed</span></span><br><span class="line">    <span class="params">namespace:</span> default</span><br><span class="line"><span class="params">roleRef:</span></span><br><span class="line">  <span class="params">kind:</span> Role</span><br><span class="line">  <span class="params">name:</span> leader-locking-nfs-client-provisioner</span><br><span class="line">  <span class="params">apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line">kubectl create <span class="operator">-</span>f deploy<span class="symbol">/rbac.yaml</span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io<span class="symbol">/nfs-client-provisioner-runner</span> created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io<span class="symbol">/run-nfs-client-provisioner</span> created</span><br><span class="line">role.rbac.authorization.k8s.io<span class="symbol">/leader-locking-nfs-client-provisioner</span> created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io<span class="symbol">/leader-locking-nfs-client-provisioner</span> created</span><br></pre></td></tr></tbody></table></figure>
<h5 id="4-执行部署"><a href="#4-执行部署" class="headerlink" title="4. 执行部署:"></a>4. 执行部署:</h5><h6 id="测试创建PVC"><a href="#测试创建PVC" class="headerlink" title="测试创建PVC"></a>测试创建PVC</h6><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat deploy<span class="symbol">/test-claim.yaml</span></span><br><span class="line"><span class="params">kind:</span> PersistentVolumeClaim</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test-claim</span><br><span class="line">  <span class="params">annotations:</span></span><br><span class="line">    volume.beta.kubernetes.io<span class="operator">/</span><span class="params">storage-class:</span> <span class="string">"managed-nfs-storage"</span></span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">accessModes:</span></span><br><span class="line">    <span class="operator">-</span> ReadWriteMany</span><br><span class="line">  <span class="params">resources:</span></span><br><span class="line">    <span class="params">requests:</span></span><br><span class="line">      <span class="params">storage:</span> <span class="number">1</span>Mi</span><br><span class="line"></span><br><span class="line">$ kubectl create <span class="operator">-</span>f deploy<span class="symbol">/test-claim.yaml</span></span><br><span class="line">persistentvolumeclaim<span class="symbol">/test-claim</span> created</span><br><span class="line"></span><br><span class="line">$ kubectl get pv,pvc</span><br><span class="line">NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                STORAGECLASS          REASON    AGE</span><br><span class="line">persistentvolume<span class="symbol">/pvc-137f0450-0048-11e9-af7a-00505621dd5b</span>   <span class="number">1</span>Mi        RWX            Delete           Bound     default<span class="symbol">/test-claim</span>   managed-nfs-storage             <span class="number">9</span>m</span><br><span class="line"></span><br><span class="line">NAME                               STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">persistentvolumeclaim<span class="symbol">/test-claim</span>   Bound     pvc-<span class="number">137</span>f0450-<span class="number">004</span>8-<span class="number">11</span>e9-af7a-<span class="number">00505621</span>dd5b   <span class="number">1</span>Mi        RWX            managed-nfs-storage   <span class="number">9</span>m</span><br><span class="line"><span class="comment"># 以上可以看到我们的pvc已经申请成功，等待POD挂载使用</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="测试创建POD"><a href="#测试创建POD" class="headerlink" title="测试创建POD"></a>测试创建POD</h6><p>POD文件如下，作用就是在test-claim的PV里touch一个SUCCESS文件。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">cat deploy<span class="symbol">/test-pod.yaml</span></span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> test-pod</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> test-pod</span><br><span class="line">    <span class="params">image:</span> gcr.io<span class="operator">/</span>google_containers<span class="operator">/</span>busybox:<span class="number">1.24</span></span><br><span class="line">    <span class="params">command:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"/bin/sh"</span></span><br><span class="line">    <span class="params">args:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"-c"</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1"</span></span><br><span class="line">    <span class="params">volumeMounts:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">name:</span> nfs-pvc</span><br><span class="line">        <span class="params">mountPath:</span> <span class="string">"/mnt"</span></span><br><span class="line">  <span class="params">restartPolicy:</span> <span class="string">"Never"</span></span><br><span class="line">  <span class="params">volumes:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">name:</span> nfs-pvc</span><br><span class="line">      <span class="params">persistentVolumeClaim:</span></span><br><span class="line">        <span class="params">claimName:</span> test-claim</span><br><span class="line">        </span><br><span class="line">kubectl create <span class="operator">-</span>f deploy<span class="symbol">/test-pod.yaml</span></span><br><span class="line">pod<span class="symbol">/test-pod</span> created</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动POD，一会儿POD就是completed状态，说明执行完毕。</span></span><br><span class="line">kubectl get pod | grep test-pod</span><br><span class="line">NAME                                      READY     STATUS      RESTARTS   AGE</span><br><span class="line">test-pod                                  <span class="number">0</span><span class="symbol">/1</span>       Completed   <span class="number">0</span>          <span class="number">18</span>s</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h6 id="在NFS服务器上的共享目录下的卷子目录中检查创建的NFS-PV卷下是否有”SUCCESS”-文件。"><a href="#在NFS服务器上的共享目录下的卷子目录中检查创建的NFS-PV卷下是否有”SUCCESS”-文件。" class="headerlink" title="在NFS服务器上的共享目录下的卷子目录中检查创建的NFS PV卷下是否有”SUCCESS” 文件。"></a>在NFS服务器上的共享目录下的卷子目录中检查创建的NFS PV卷下是否有”SUCCESS” 文件。</h6><p><img src="/2018/12/15/kubernetes-storage/SUCCESS.png" alt="SUCCESS"><br>以上，说明我们部署正常，并且可以通过动态分配NFS的持久共享卷</p>
<p><strong>参考</strong><br><a href="https://k8smeetup.github.io/docs/concepts/storage/volumes/">https://k8smeetup.github.io/docs/concepts/storage/volumes/</a><br><a href="https://k8smeetup.github.io/docs/concepts/storage/persistent-volumes/#%E5%9B%9E%E6%94%B6-1">https://k8smeetup.github.io/docs/concepts/storage/persistent-volumes/#回收-1</a><br><a href="https://k8smeetup.github.io/docs/tasks/administer-cluster/pvc-protection/#lichuqiang">https://k8smeetup.github.io/docs/tasks/administer-cluster/pvc-protection/#lichuqiang</a><br><a href="https://jimmysong.io/kubernetes-handbook/practice/using-nfs-for-persistent-storage.html">https://jimmysong.io/kubernetes-handbook/practice/using-nfs-for-persistent-storage.html</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernets</tag>
        <tag>持久化存储</tag>
        <tag>Storage</tag>
        <tag>NFS PV静态供给</tag>
        <tag>NFS PV动态供给</tag>
      </tags>
  </entry>
  <entry>
    <title>理解kubernetes中Pod访问方式</title>
    <url>/2018/12/12/kubernetes_pod_access/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Pod的IP是在docker0网段动态分配的，当发生重启，扩容等操作时，IP地址会随之变化。当某个Pod(frontend)需要去访问其依赖的另外一组Pod(backend)时，如果backend的IP发生变化时，如何保证fronted到backend的正常通信变的非常重要。由此，引出了Service的概念。<br>这里docker0是一个网桥，docker daemon启动container时会根据docker0的网段来划粉container的IP地址Docker网络<br>在实际生产环境中，对Service的访问可能会有两种来源：Kubernetes集群内部的程序（Pod）和Kubernetes集群外部，为了满足上述的场景，Kubernetes service有以下三种类型：</p>
<ul>
<li>ClusterIP:提供一个集群内部的虚拟IP以供Pod访问。</li>
<li>NodePort:在每个Node上打开一个端口以供外部访问。</li>
<li>LoadBalancer:通过外部的负载均衡器来访问。</li>
</ul>
<p>那么它是怎么实现的，通过下面的示例来理解。(创建方式可以是通过yaml文件或者是命令行方式，这里为了理解我先用命令行方式创建，如果不合适的地方我们通过<code>kubectl edit (RESOURCE/NAME | -f FILENAME) [options]</code>这种方式先修改)</p>
<p>其次，还需要理解<code>NodePort</code>,<code>TargetPort</code>以及<code>port</code>他们的区别:<br><strong>NodePort</strong><br>外部机器可访问的端口。<br>比如一个Web应用需要被其他用户访问，那么需要配置type=NodePort，而且配置nodePort=30001，那么其他机器就可以通过浏览器访问scheme://node:30001访问到该服务，例如<a href="http://node:30001。">http://node:30001。</a><br>例如MySQL数据库可能不需要被外界访问，只需被内部服务访问，那么不必设置NodePort</p>
<p><strong>TargetPort</strong><br>容器的端口（最根本的端口入口），与制作容器时暴露的端口一致（DockerFile中EXPOSE），例如docker.io官方的nginx暴露的是80端口。<br>docker.io官方的nginx容器的DockerFile参考<a href="https://github.com/nginxinc/docker-nginx">https://github.com/nginxinc/docker-nginx</a></p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Service</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line"> <span class="params">name:</span> nginx-service</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line"> <span class="params">type:</span> NodePort         <span class="comment"># 有配置NodePort，外部流量可访问k8s中的服务</span></span><br><span class="line"> <span class="params">ports:</span></span><br><span class="line"> <span class="operator">-</span> <span class="params">port:</span> <span class="number">30080</span>          <span class="comment"># 服务访问端口</span></span><br><span class="line">   <span class="params">targetPort:</span> <span class="number">80</span>       <span class="comment"># 容器端口</span></span><br><span class="line">   <span class="params">nodePort:</span> <span class="number">30001</span>      <span class="comment"># NodePort</span></span><br><span class="line"> <span class="params">selector:</span></span><br><span class="line">  <span class="params">name:</span> nginx-pod</span><br></pre></td></tr></tbody></table></figure>
<p>　</p>
<p><strong>port</strong><br>　kubernetes中的服务之间访问的端口，尽管mysql容器暴露了3306端口（参考<a href="">https://github.com/docker-library/mysql/</a>的DockerFile），但是集群内其他容器需要通过33306端口访问该服务，外部机器不能访问mysql服务，因为他没有配置NodePort类型</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">mysql-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">ports:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">33306</span></span><br><span class="line">   <span class="attr">targetPort:</span> <span class="number">3306</span></span><br><span class="line"> <span class="attr">selector:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-pod</span></span><br></pre></td></tr></tbody></table></figure>
<h1 id="一、Create-service-type-ClusterIP"><a href="#一、Create-service-type-ClusterIP" class="headerlink" title="一、Create service (type: ClusterIP)"></a>一、Create service (type: ClusterIP)</h1><p>此模式会提供一个集群内部的虚拟IP（与Pod不在同一网段)，以供集群内部的pod之间通信使用。<br>ClusterIP也是Kubernetes service的默认类型。</p>
<p>为了实现图上的功能主要需要以下几个组件的协同工作</p>
<p>apiserver 用户通过kubectl命令向apiserver发送创建service的命令，apiserver接收到请求以后将数据存储到etcd中。</p>
<p>kube-proxy kubernetes的每个节点中都有一个叫做kube-proxy的进程，这个进程负责感知service，pod的变化，并将变化的信息写入本地的iptables中。</p>
<p>iptables 使用NAT等技术将virtualIP的流量转至endpoint中。</p>
<p>下面我们实际发布一个Service，能够更清晰的了解到Service是如何工作的。</p>
<h3 id="1-通过命令行方式创建service-类型为clusterip"><a href="#1-通过命令行方式创建service-类型为clusterip" class="headerlink" title="1. 通过命令行方式创建service,类型为clusterip"></a>1. 通过命令行方式创建service,类型为clusterip</h3><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl create service clusterip my-svc-cp <span class="operator">-</span>-tcp<span class="operator">=</span><span class="number">8080</span>:<span class="number">80</span></span><br><span class="line"></span><br><span class="line">$ kubectl describe service<span class="symbol">/my-svc-cp</span></span><br><span class="line"><span class="params">Name:</span>              my-svc-cp</span><br><span class="line"><span class="params">Namespace:</span>         default</span><br><span class="line"><span class="params">Labels:</span>            app<span class="operator">=</span>my-svc-cp</span><br><span class="line"><span class="params">Annotations:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Selector:</span>          app<span class="operator">=</span>my-svc-cp</span><br><span class="line"><span class="params">Type:</span>              ClusterIP          <span class="comment"># 类型</span></span><br><span class="line"><span class="params">IP:</span>                <span class="number">10.247</span>.<span class="number">52.210</span></span><br><span class="line"><span class="params">Port:</span>              <span class="number">8</span>0-<span class="number">8080</span>  <span class="number">8080</span><span class="symbol">/TCP</span>  <span class="comment"># 映射到集群的端口</span></span><br><span class="line"><span class="params">TargetPort:</span>        <span class="number">80</span><span class="symbol">/TCP</span>             <span class="comment"># 目标pod暴露端口</span></span><br><span class="line"><span class="params">Endpoints:</span>         <span class="symbol">&lt;none&gt;</span>             <span class="comment"># 此时还没有后端容器</span></span><br><span class="line">Session <span class="params">Affinity:</span>  None</span><br><span class="line"><span class="params">Events:</span>            <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line">$ kubectl get svc <span class="operator">-</span>o wide</span><br><span class="line">my-svc-cp      ClusterIP   <span class="number">10.247</span>.<span class="number">52.210</span>   <span class="symbol">&lt;none&gt;</span>        <span class="number">80</span><span class="symbol">/TCP</span>    <span class="number">6</span>s        app<span class="operator">=</span>my-svc-cp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以上我创建了一个svc, 那么他是为pod服务的,</span></span><br><span class="line"><span class="comment"># 那么 pod跟service是通过`selector label`来做关联的, 所以我们还需要对这个svc做下调整</span></span><br><span class="line"></span><br><span class="line">$ kubectl edit svc my-svc-cp</span><br><span class="line"><span class="comment"># Please edit the object below. Lines beginning with a '#' will be ignored,</span></span><br><span class="line"><span class="comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span></span><br><span class="line"><span class="comment"># reopened with the relevant failures.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Service</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">creationTimestamp:</span> <span class="number">201</span>8-<span class="number">1</span>2-<span class="number">12</span>T17:<span class="number">41</span>:<span class="number">54</span>Z</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">app:</span> my-svc-cp</span><br><span class="line">  <span class="params">name:</span> my-svc-cp</span><br><span class="line">  <span class="params">namespace:</span> default</span><br><span class="line">  <span class="params">resourceVersion:</span> <span class="string">"32742"</span></span><br><span class="line">  <span class="params">selfLink:</span> <span class="symbol">/api/v1/namespaces/default/services/my-svc-cp</span></span><br><span class="line">  <span class="params">uid:</span> <span class="number">372</span>f6f2c-fe35-<span class="number">11</span>e8-b967-fa163e874e90</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">clusterIP:</span> <span class="number">10.247</span>.<span class="number">52.210</span></span><br><span class="line">  <span class="params">ports:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> <span class="number">8</span>0-<span class="number">8080</span></span><br><span class="line">    <span class="params">port:</span> <span class="number">8080</span></span><br><span class="line">    <span class="params">protocol:</span> TCP</span><br><span class="line">    <span class="params">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">app:</span> my-svc-cp-pod  <span class="comment"># 这里我修改了selector ，需要与pod name与之对应起来</span></span><br><span class="line">  <span class="params">sessionAffinity:</span> None</span><br><span class="line">  <span class="params">type:</span> ClusterIP</span><br><span class="line"><span class="params">status:</span></span><br><span class="line">  <span class="params">loadBalancer:</span> {}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># :wq 保存退出</span></span><br><span class="line"><span class="comment"># kubectl describe service/my-svc-cp 查看修改是否成功</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="2-创建pod"><a href="#2-创建pod" class="headerlink" title="2.创建pod"></a>2.创建pod</h2><p>注意<code>selector label</code></p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ vim my-svc-cp-pod.yaml</span><br><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">kind:</span> Pod</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> my-svc-cp-pod</span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">app:</span> my-svc-cp-pod</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">containers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">image:</span> nginx:latest</span><br><span class="line">    <span class="params">imagePullPolicy:</span> Always</span><br><span class="line">    <span class="params">name:</span> nginx</span><br><span class="line">  <span class="comment"># 注意这里的亲和性是为了将pod调度至有外网的node节点便于pull镜像</span></span><br><span class="line">  <span class="params">affinity:</span></span><br><span class="line">    <span class="params">nodeAffinity:</span></span><br><span class="line">      <span class="params">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="params">nodeSelectorTerms:</span></span><br><span class="line">          <span class="operator">-</span> <span class="params">matchExpressions:</span></span><br><span class="line">              <span class="operator">-</span> <span class="params">key:</span> kubernetes.io<span class="symbol">/hostname</span></span><br><span class="line">                <span class="params">operator:</span> In</span><br><span class="line">                <span class="params">values:</span></span><br><span class="line">                  <span class="operator">-</span> <span class="number">192.168</span>.<span class="number">253.183</span></span><br><span class="line">  <span class="params">restartPolicy:</span> Always</span><br><span class="line">  <span class="params">schedulerName:</span> default-scheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行:</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f my-svc-cp-pod.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl get pods,svc,endpoints,deployment <span class="operator">-</span>o wide</span><br><span class="line">NAME               READY     STATUS    RESTARTS   AGE       IP            NODE</span><br><span class="line">po<span class="symbol">/my-svc-cp-pod</span>   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">36</span>m       <span class="number">172.16</span>.<span class="number">0.36</span>   <span class="number">192.168</span>.<span class="number">253.183</span></span><br><span class="line"></span><br><span class="line">NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE   SELECTOR</span><br><span class="line">svc<span class="symbol">/kubernetes</span>   ClusterIP   <span class="number">10.247</span>.<span class="number">0.1</span>      <span class="symbol">&lt;none&gt;</span>        <span class="number">443</span><span class="symbol">/TCP</span>    <span class="number">1</span>h    <span class="symbol">&lt;none&gt;</span></span><br><span class="line">svc<span class="symbol">/my-svc-cp</span>    ClusterIP   <span class="number">10.247</span>.<span class="number">52.210</span>   <span class="symbol">&lt;none&gt;</span>        <span class="number">8080</span><span class="symbol">/TCP</span>   <span class="number">3</span>m    app<span class="operator">=</span>my-svc-cp-pod</span><br><span class="line"></span><br><span class="line">NAME            ENDPOINTS                                                      AGE</span><br><span class="line">ep<span class="symbol">/kubernetes</span>   <span class="number">192.168</span>.<span class="number">103.50</span>:<span class="number">5444</span>,<span class="number">192.168</span>.<span class="number">174.46</span>:<span class="number">5444</span>,<span class="number">192.168</span>.<span class="number">236.124</span>:<span class="number">5444</span>   <span class="number">1</span>h</span><br><span class="line">ep<span class="symbol">/my-svc-cp</span>    <span class="number">172.16</span>.<span class="number">0.36</span>:<span class="number">80</span>                                                 <span class="number">3</span>m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看service是否关联到了pod</span></span><br><span class="line">$ kubectl describe ep<span class="symbol">/my-svc-cp</span></span><br><span class="line"><span class="params">Name:</span>         my-svc-cp</span><br><span class="line"><span class="params">Namespace:</span>    default</span><br><span class="line"><span class="params">Labels:</span>       app<span class="operator">=</span>my-svc-cp</span><br><span class="line"><span class="params">Annotations:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Subsets:</span>					</span><br><span class="line">  <span class="params">Addresses:</span>          <span class="number">172.16</span>.<span class="number">0.36</span> <span class="comment"># 可见endpoints中已经有了后端的那个pod</span></span><br><span class="line">  <span class="params">NotReadyAddresses:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line">  <span class="params">Ports:</span></span><br><span class="line">    Name     Port  Protocol</span><br><span class="line">    <span class="operator">-</span>---     <span class="operator">-</span>---  <span class="operator">-</span>-------</span><br><span class="line">    <span class="number">8</span>0-<span class="number">8080</span>  <span class="number">80</span>    TCP</span><br><span class="line"></span><br><span class="line"><span class="params">Events:</span>  <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群内部通过Cluster IP访问nginx服务</span></span><br><span class="line"><span class="comment"># 指定了端口映射到Cluster IP</span></span><br><span class="line">$ curl <span class="operator">-</span>I <span class="number">10.247</span>.<span class="number">52.210</span>:<span class="number">8080</span></span><br><span class="line">HTTP<span class="symbol">/1.1</span> <span class="number">200</span> OK</span><br><span class="line"><span class="params">Server:</span> nginx<span class="symbol">/1.15.5</span></span><br><span class="line"><span class="params">Date:</span> Wed, <span class="number">12</span> Dec <span class="number">2018</span> <span class="number">17</span>:<span class="number">48</span>:<span class="number">56</span> GMT</span><br><span class="line"><span class="params">Content-Type:</span> text<span class="symbol">/html</span></span><br><span class="line"><span class="params">Content-Length:</span> <span class="number">612</span></span><br><span class="line"><span class="params">Last-Modified:</span> Tue, <span class="number">02</span> Oct <span class="number">2018</span> <span class="number">14</span>:<span class="number">49</span>:<span class="number">27</span> GMT</span><br><span class="line"><span class="params">Connection:</span> keep-alive</span><br><span class="line"><span class="params">ETag:</span> <span class="string">"5bb38577-264"</span></span><br><span class="line"><span class="params">Accept-Ranges:</span> bytes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群内部通过POD IP访问nginx服务</span></span><br><span class="line"><span class="comment"># 因为pod暴露的是80 默认不用加端口</span></span><br><span class="line">$ curl <span class="operator">-</span>I <span class="number">172.16</span>.<span class="number">0.36</span></span><br><span class="line">HTTP<span class="symbol">/1.1</span> <span class="number">200</span> OK</span><br><span class="line"><span class="params">Server:</span> nginx<span class="symbol">/1.15.5</span></span><br><span class="line"><span class="params">Date:</span> Wed, <span class="number">12</span> Dec <span class="number">2018</span> <span class="number">17</span>:<span class="number">47</span>:<span class="number">52</span> GMT</span><br><span class="line"><span class="params">Content-Type:</span> text<span class="symbol">/html</span></span><br><span class="line"><span class="params">Content-Length:</span> <span class="number">612</span></span><br><span class="line"><span class="params">Last-Modified:</span> Tue, <span class="number">02</span> Oct <span class="number">2018</span> <span class="number">14</span>:<span class="number">49</span>:<span class="number">27</span> GMT</span><br><span class="line"><span class="params">Connection:</span> keep-alive</span><br><span class="line"><span class="params">ETag:</span> <span class="string">"5bb38577-264"</span></span><br><span class="line"><span class="params">Accept-Ranges:</span> bytes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过这种方式也只是集群内部能够访问到，如果集群外部要访问我们的pod又该如何做呢？</span></span><br><span class="line"><span class="comment"># 下面继续</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>


<h1 id="二、Create-service-type-NodePort"><a href="#二、Create-service-type-NodePort" class="headerlink" title="二、Create service (type: NodePort)"></a>二、Create service (type: NodePort)</h1><h2 id="1-通过命令行方式创建service-类型为nodeport"><a href="#1-通过命令行方式创建service-类型为nodeport" class="headerlink" title="1.通过命令行方式创建service,类型为nodeport"></a>1.通过命令行方式创建service,类型为nodeport</h2><p>Cluster service 的 IP 地址是虚拟的，因此，只能从node节点上使用该IP 地址访问应用。为了从集群外访问应用，K8S 提供了使用 node 节点的IP 地址访问应用的方式。</p>
<p>基本上，NodePort 服务与普通的 “ClusterIP” 服务 YAML 定义有两点区别。 首先，type 是 “NodePort”。还有一个称为 nodePort 的附加端口，指定在节点上打开哪个端口。 如果你不指定这个端口，它会选择一个随机端口。</p>
<p>下图中是 32591. 该端口号的范围是 kube-apiserver 的启动参数 –service-node-port-range指定的，在当前测试环境中其值是 30000-32767。</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl create service nodeport my-svc-np <span class="operator">-</span>-tcp<span class="operator">=</span><span class="number">8080</span>:<span class="number">80</span></span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]<span class="comment"># kubectl describe service my-svc-np</span></span><br><span class="line"><span class="params">Name:</span>                     my-svc-np</span><br><span class="line"><span class="params">Namespace:</span>                default</span><br><span class="line"><span class="params">Labels:</span>                   app<span class="operator">=</span>my-svc-np</span><br><span class="line"><span class="params">Annotations:</span>              <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Selector:</span>                 app<span class="operator">=</span>my-svc-np</span><br><span class="line"><span class="params">Type:</span>                     NodePort            <span class="comment"># 类型</span></span><br><span class="line"><span class="params">IP:</span>                       <span class="number">10.103</span>.<span class="number">115.169</span>      <span class="comment"># 分配的ClusterIP</span></span><br><span class="line"><span class="params">Port:</span>                     <span class="number">808</span>0-<span class="number">80</span>  <span class="number">8080</span><span class="symbol">/TCP</span>   <span class="comment"># 容器映射到node节点的端口</span></span><br><span class="line"><span class="params">TargetPort:</span>               <span class="number">80</span><span class="symbol">/TCP</span>              <span class="comment"># 容器将要暴露的端口</span></span><br><span class="line"><span class="params">NodePort:</span>                 <span class="number">808</span>0-<span class="number">80</span>  <span class="number">32591</span><span class="symbol">/TCP</span>  <span class="comment"># 附加端口</span></span><br><span class="line"><span class="params">Endpoints:</span>                <span class="symbol">&lt;none&gt;</span>		        <span class="comment"># 此时后端没有pod</span></span><br><span class="line">Session <span class="params">Affinity:</span>         None</span><br><span class="line">External Traffic <span class="params">Policy:</span>  Cluster</span><br><span class="line"><span class="params">Events:</span>                   <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有了svc我们就需要创建一个或者一组pod来跟这个svc建立连接</span></span><br><span class="line"><span class="comment"># 像前面一样我们需要更改这个svc的选择器：</span></span><br><span class="line">$ kubectl edit svc my-svc-np</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">  <span class="params">externalTrafficPolicy:</span> Cluster</span><br><span class="line">  <span class="params">ports:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">name:</span> <span class="number">808</span>0-<span class="number">80</span></span><br><span class="line">    <span class="params">nodePort:</span> <span class="number">32591</span></span><br><span class="line">    <span class="params">port:</span> <span class="number">8080</span></span><br><span class="line">    <span class="params">protocol:</span> TCP</span><br><span class="line">    <span class="params">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">app:</span> my-svc-np-pod  <span class="comment"># 与即将创建的pod对应上</span></span><br><span class="line">  <span class="params">sessionAffinity:</span> None</span><br><span class="line">  <span class="params">type:</span> NodePort</span><br><span class="line"><span class="params">status:</span></span><br><span class="line">  <span class="params">loadBalancer:</span> {}</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line"><span class="comment"># :wq保存退出</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="2-创建pod与之建立关系"><a href="#2-创建pod与之建立关系" class="headerlink" title="2.创建pod与之建立关系"></a>2.创建pod与之建立关系</h2><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 我这里通过命令行方式创建一个deployment的3个副本的nginx pod</span></span><br><span class="line"><span class="comment"># 1. 首先通过命令行生成yaml文件</span></span><br><span class="line">$ kubectl run nginx-deployment <span class="operator">-</span>-image<span class="operator">=</span>nginx <span class="operator">-</span>-replicas<span class="operator">=</span><span class="number">3</span> <span class="operator">-</span>-dry-run <span class="operator">-</span>o yaml <span class="operator">&gt;</span> nginx-deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 进行调整</span></span><br><span class="line">$ cat <span class="operator">&gt;</span><span class="operator">&gt;</span> nginx-deployment.yaml <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF</span><br><span class="line"><span class="params">apiVersion:</span> apps<span class="symbol">/v1beta1</span></span><br><span class="line"><span class="params">kind:</span> Deployment</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">run:</span> nginx-deployment</span><br><span class="line">  <span class="params">name:</span> nginx-deployment</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">matchLabels:</span></span><br><span class="line">      <span class="params">app:</span> my-svc-np-pod    		<span class="comment"># 此处修改为svc selector相同</span></span><br><span class="line">  <span class="params">strategy:</span> {}</span><br><span class="line">  <span class="params">template:</span></span><br><span class="line">    <span class="params">metadata:</span></span><br><span class="line">      <span class="params">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="params">labels:</span></span><br><span class="line">        <span class="params">app:</span> my-svc-np-pod		    <span class="comment"># 此处修改为svc selector相同</span></span><br><span class="line">    <span class="params">spec:</span></span><br><span class="line">      <span class="params">containers:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">image:</span> nginx</span><br><span class="line">        <span class="params">name:</span> nginx-deployment</span><br><span class="line">        <span class="params">resources:</span> {}</span><br><span class="line"><span class="params">status:</span> {}</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># 3. 执行</span></span><br><span class="line">$ kubectl create <span class="operator">-</span>f nginx-deployment.yaml</span><br><span class="line">deployment.apps<span class="symbol">/nginx-deployment</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 查看状态:</span></span><br><span class="line">$ kubectl get pod <span class="operator">-</span>o wide</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE    IP            NODE      NOMINATED NODE</span><br><span class="line">nginx-deployment-<span class="number">6794</span>c779fb-<span class="number">8</span>x92g   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">4</span>m     <span class="number">10.244</span>.<span class="number">1.19</span>   k8s-m2    <span class="symbol">&lt;none&gt;</span></span><br><span class="line">nginx-deployment-<span class="number">6794</span>c779fb-rf8qj   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">4</span>m     <span class="number">10.244</span>.<span class="number">0.19</span>   k8s-m3    <span class="symbol">&lt;none&gt;</span></span><br><span class="line">nginx-deployment-<span class="number">6794</span>c779fb-tdscx   <span class="number">1</span><span class="symbol">/1</span>       Running   <span class="number">0</span>          <span class="number">4</span>m     <span class="number">10.244</span>.<span class="number">2.35</span>   k8s-m1    <span class="symbol">&lt;none&gt;</span></span><br><span class="line">$ kubectl describe svc my-svc-np</span><br><span class="line"><span class="params">Name:</span>                     my-svc-np</span><br><span class="line"><span class="params">Namespace:</span>                default</span><br><span class="line"><span class="params">Labels:</span>                   app<span class="operator">=</span>my-svc-np</span><br><span class="line"><span class="params">Annotations:</span>              <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Selector:</span>                 app<span class="operator">=</span>my-svc-np-pod</span><br><span class="line"><span class="params">Type:</span>                     NodePort</span><br><span class="line"><span class="params">IP:</span>                       <span class="number">10.103</span>.<span class="number">115.169</span></span><br><span class="line"><span class="params">Port:</span>                     <span class="number">808</span>0-<span class="number">80</span>  <span class="number">8080</span><span class="symbol">/TCP</span></span><br><span class="line"><span class="params">TargetPort:</span>               <span class="number">80</span><span class="symbol">/TCP</span></span><br><span class="line"><span class="params">NodePort:</span>                 <span class="number">808</span>0-<span class="number">80</span>  <span class="number">32591</span><span class="symbol">/TCP</span></span><br><span class="line"><span class="params">Endpoints:</span>                <span class="number">10.244</span>.<span class="number">0.19</span>:<span class="number">80</span>,<span class="number">10.244</span>.<span class="number">1.19</span>:<span class="number">80</span>,<span class="number">10.244</span>.<span class="number">2.35</span>:<span class="number">80</span>   <span class="comment"># 可以看到我们的三个endpoint</span></span><br><span class="line">Session <span class="params">Affinity:</span>         None</span><br><span class="line">External Traffic <span class="params">Policy:</span>  Cluster</span><br><span class="line"><span class="params">Events:</span>                   <span class="symbol">&lt;none&gt;</span></span><br><span class="line">$ kubectl get svc my-svc-np</span><br><span class="line">NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">my-svc-np    NodePort    <span class="number">10.103</span>.<span class="number">115.169</span>   <span class="symbol">&lt;none&gt;</span>        <span class="number">8080</span>:<span class="number">32591</span><span class="symbol">/TCP</span>   <span class="number">42</span>m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.如何访问?</span></span><br><span class="line"><span class="comment"># a.集群内部访问pod ip</span></span><br><span class="line">$ curl <span class="operator">-</span>I <span class="number">10.244</span>.<span class="number">0.19</span></span><br><span class="line">HTTP<span class="symbol">/1.1</span> <span class="number">200</span> OK</span><br><span class="line"><span class="params">Server:</span> nginx<span class="symbol">/1.15.7</span></span><br><span class="line"><span class="params">Date:</span> Thu, <span class="number">13</span> Dec <span class="number">2018</span> <span class="number">08</span>:<span class="number">43</span>:<span class="number">25</span> GMT</span><br><span class="line"><span class="params">Content-Type:</span> text<span class="symbol">/html</span></span><br><span class="line"><span class="params">Content-Length:</span> <span class="number">612</span></span><br><span class="line"><span class="params">Last-Modified:</span> Tue, <span class="number">27</span> Nov <span class="number">2018</span> <span class="number">12</span>:<span class="number">31</span>:<span class="number">56</span> GMT</span><br><span class="line"><span class="params">Connection:</span> keep-alive</span><br><span class="line"><span class="params">ETag:</span> <span class="string">"5bfd393c-264"</span></span><br><span class="line"><span class="params">Accept-Ranges:</span> bytes</span><br><span class="line"></span><br><span class="line"><span class="comment"># b.集群内部clusterip+端口</span></span><br><span class="line">$ curl <span class="operator">-</span>I <span class="number">10.103</span>.<span class="number">115.169</span>:<span class="number">8080</span></span><br><span class="line">HTTP<span class="symbol">/1.1</span> <span class="number">200</span> OK</span><br><span class="line"><span class="params">Server:</span> nginx<span class="symbol">/1.15.7</span></span><br><span class="line"><span class="params">Date:</span> Thu, <span class="number">13</span> Dec <span class="number">2018</span> <span class="number">08</span>:<span class="number">42</span>:<span class="number">53</span> GMT</span><br><span class="line"><span class="params">Content-Type:</span> text<span class="symbol">/html</span></span><br><span class="line"><span class="params">Content-Length:</span> <span class="number">612</span></span><br><span class="line"><span class="params">Last-Modified:</span> Tue, <span class="number">27</span> Nov <span class="number">2018</span> <span class="number">12</span>:<span class="number">31</span>:<span class="number">56</span> GMT</span><br><span class="line"><span class="params">Connection:</span> keep-alive</span><br><span class="line"><span class="params">ETag:</span> <span class="string">"5bfd393c-264"</span></span><br><span class="line"><span class="params">Accept-Ranges:</span> bytes</span><br><span class="line"></span><br><span class="line"><span class="comment"># c.集群外部访问nodeip+附加端口</span></span><br><span class="line">curl <span class="operator">-</span>I <span class="number">192.168</span>.<span class="number">56.111</span>:<span class="number">32591</span></span><br><span class="line">HTTP<span class="symbol">/1.1</span> <span class="number">200</span> OK</span><br><span class="line"><span class="params">Server:</span> nginx<span class="symbol">/1.15.7</span></span><br><span class="line"><span class="params">Date:</span> Thu, <span class="number">13</span> Dec <span class="number">2018</span> <span class="number">08</span>:<span class="number">45</span>:<span class="number">39</span> GMT</span><br><span class="line"><span class="params">Content-Type:</span> text<span class="symbol">/html</span></span><br><span class="line"><span class="params">Content-Length:</span> <span class="number">612</span></span><br><span class="line"><span class="params">Last-Modified:</span> Tue, <span class="number">27</span> Nov <span class="number">2018</span> <span class="number">12</span>:<span class="number">31</span>:<span class="number">56</span> GMT</span><br><span class="line"><span class="params">Connection:</span> keep-alive</span><br><span class="line"><span class="params">ETag:</span> <span class="string">"5bfd393c-264"</span></span><br><span class="line"><span class="params">Accept-Ranges:</span> bytes</span><br></pre></td></tr></tbody></table></figure>
<p>以上就是nodeport网络类型的简单实现</p>
<h1 id="三、Create-service-type-HeadlessClusterIP"><a href="#三、Create-service-type-HeadlessClusterIP" class="headerlink" title="三、Create service (type: HeadlessClusterIP)"></a>三、Create service (type: HeadlessClusterIP)</h1><p>所谓的HeadlessClusterIP 就是我们在创建这种网络类型的时候将 spec.clusterIP 设置成 None,这样k8s就不会给service分配clusterIp了。但指定了selector，那么endpoints controller还是会创建Endpoints的，会创建一个新的DNS记录直接指向这个service描述的后端pod。否则，不会创建Endpoints记录。这种ClusterIP，kube-proxy 并不处理此类服务，因为没有load balancing或 proxy 代理设置，在访问服务的时候回返回后端的全部的Pods IP地址，主要用于开发者自己根据pods进行负载均衡器的开发(设置了selector)。</p>
<p>下面通过实践理解</p>
<h2 id="1-通过命令行方式创建svc-类型为headlessCluserIP"><a href="#1-通过命令行方式创建svc-类型为headlessCluserIP" class="headerlink" title="1.通过命令行方式创建svc,类型为headlessCluserIP"></a>1.通过命令行方式创建svc,类型为headlessCluserIP</h2><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ kubectl create svc clusterip my-svc-headless <span class="operator">-</span>-clusterip<span class="operator">=</span><span class="string">"None"</span> <span class="operator">-</span>-tcp<span class="operator">=</span><span class="number">8080</span>:<span class="number">80</span></span><br><span class="line">service<span class="symbol">/my-svc-headless</span> created</span><br><span class="line"></span><br><span class="line">$ kubectl describe svc my-svc-headless</span><br><span class="line"><span class="params">Name:</span>              my-svc-headless</span><br><span class="line"><span class="params">Namespace:</span>         default</span><br><span class="line"><span class="params">Labels:</span>            app<span class="operator">=</span>my-svc-headless</span><br><span class="line"><span class="params">Annotations:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Selector:</span>          app<span class="operator">=</span>my-svc-headless</span><br><span class="line"><span class="params">Type:</span>              ClusterIP</span><br><span class="line"><span class="params">IP:</span>                None</span><br><span class="line">Session <span class="params">Affinity:</span>  None</span><br><span class="line"><span class="params">Events:</span>            <span class="symbol">&lt;none&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="2-创建pod与之关联："><a href="#2-创建pod与之关联：" class="headerlink" title="2. 创建pod与之关联："></a>2. 创建pod与之关联：</h2><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">$ cat nginx-deployment.yaml</span><br><span class="line"><span class="params">apiVersion:</span> apps<span class="symbol">/v1beta1</span></span><br><span class="line"><span class="params">kind:</span> Deployment</span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="params">labels:</span></span><br><span class="line">    <span class="params">run:</span> nginx-deployment</span><br><span class="line">  <span class="params">name:</span> nginx-deployment</span><br><span class="line"><span class="params">spec:</span></span><br><span class="line">  <span class="params">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="params">selector:</span></span><br><span class="line">    <span class="params">matchLabels:</span></span><br><span class="line">      <span class="params">app:</span> my-svc-headless <span class="comment"># 对应到svc</span></span><br><span class="line">  <span class="params">strategy:</span> {}</span><br><span class="line">  <span class="params">template:</span></span><br><span class="line">    <span class="params">metadata:</span></span><br><span class="line">      <span class="params">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="params">labels:</span></span><br><span class="line">        <span class="params">app:</span> my-svc-headless <span class="comment"># 对应到svc</span></span><br><span class="line"></span><br><span class="line">    <span class="params">spec:</span></span><br><span class="line">      <span class="params">containers:</span></span><br><span class="line">      <span class="operator">-</span> <span class="params">image:</span> nginx</span><br><span class="line">        <span class="params">name:</span> nginx-deployment</span><br><span class="line">        <span class="params">resources:</span> {}</span><br><span class="line"><span class="params">status:</span> {}</span><br><span class="line"></span><br><span class="line">$ kubectl create <span class="operator">-</span>f nginx-deployment.yaml</span><br><span class="line">deployment.apps<span class="symbol">/nginx-deployment</span> created</span><br><span class="line"></span><br><span class="line">$ [root@k8s-m1 ~]<span class="comment"># kubectl get pod -o wide </span></span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE</span><br><span class="line">nginx-deployment-<span class="number">54</span>bfc79477-b78c6   <span class="number">1</span><span class="symbol">/1</span>     Running   <span class="number">0</span>          <span class="number">40</span>s   <span class="number">10.244</span>.<span class="number">1.21</span>   k8s-m2    <span class="symbol">&lt;none&gt;</span></span><br><span class="line">nginx-deployment-<span class="number">54</span>bfc79477-dvp2t   <span class="number">1</span><span class="symbol">/1</span>     Running   <span class="number">0</span>          <span class="number">40</span>s   <span class="number">10.244</span>.<span class="number">0.22</span>   k8s-m3    <span class="symbol">&lt;none&gt;</span></span><br><span class="line">nginx-deployment-<span class="number">54</span>bfc79477-x6v7s   <span class="number">1</span><span class="symbol">/1</span>     Running   <span class="number">0</span>          <span class="number">40</span>s   <span class="number">10.244</span>.<span class="number">2.38</span>   k8s-m1    <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ kubectl describe svc my-svc-headless</span><br><span class="line"><span class="params">Name:</span>              my-svc-headless</span><br><span class="line"><span class="params">Namespace:</span>         default</span><br><span class="line"><span class="params">Labels:</span>            app<span class="operator">=</span>my-svc-headless</span><br><span class="line"><span class="params">Annotations:</span>       <span class="symbol">&lt;none&gt;</span></span><br><span class="line"><span class="params">Selector:</span>          app<span class="operator">=</span>my-svc-headless</span><br><span class="line"><span class="params">Type:</span>              ClusterIP</span><br><span class="line"><span class="params">IP:</span>                None</span><br><span class="line"><span class="params">Port:</span>              <span class="number">808</span>0-<span class="number">80</span>  <span class="number">8080</span><span class="symbol">/TCP</span></span><br><span class="line"><span class="params">TargetPort:</span>        <span class="number">80</span><span class="symbol">/TCP</span></span><br><span class="line"><span class="params">Endpoints:</span>         <span class="number">10.244</span>.<span class="number">0.20</span>:<span class="number">80</span>,<span class="number">10.244</span>.<span class="number">1.20</span>:<span class="number">80</span>,<span class="number">10.244</span>.<span class="number">2.36</span>:<span class="number">80</span></span><br><span class="line">Session <span class="params">Affinity:</span>  None</span><br><span class="line"><span class="params">Events:</span>            <span class="symbol">&lt;none&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以上可以看出后端的pod列表已经加到该svc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们看通过内部域名是否能够解析访问到pod</span></span><br><span class="line"><span class="comment"># 1.我们先获取到dns服务的IP</span></span><br><span class="line"><span class="comment">#kubectl get svc -n kube-system | grep dns</span></span><br><span class="line">kube-dns               ClusterIP   <span class="number">10.96</span>.<span class="number">0.10</span>    <span class="symbol">&lt;none&gt;</span>    <span class="number">53</span><span class="operator">/</span>UDP,<span class="number">53</span><span class="symbol">/TCP</span>   <span class="number">13</span>d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.登录到pod 内部获取dns域</span></span><br><span class="line">$ kubectl exec <span class="operator">-</span>it nginx-deployment-<span class="number">54</span>bfc79477-b78c6 <span class="operator">-</span>- cat <span class="symbol">/etc/resolv.conf</span></span><br><span class="line">nameserver <span class="number">10.96</span>.<span class="number">0.10</span></span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.指定dns服务器并查询该域名：`nginx-deployment.default.svc.cluster.local`</span></span><br><span class="line">$ dig @<span class="number">10.96</span>.<span class="number">0.10</span> my-svc-headless.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">; <span class="operator">&lt;</span><span class="operator">&lt;</span><span class="operator">&gt;</span><span class="operator">&gt;</span> DiG <span class="number">9.9</span>.<span class="number">4</span><span class="operator">-</span>RedHat-<span class="number">9.9</span>.<span class="number">4</span><span class="operator">-</span><span class="number">72</span>.el7 <span class="operator">&lt;</span><span class="operator">&lt;</span><span class="operator">&gt;</span><span class="operator">&gt;</span> @<span class="number">10.96</span>.<span class="number">0.10</span> my-svc-headless.default.svc.cluster.local</span><br><span class="line">; (<span class="number">1</span> server found)</span><br><span class="line">;; global <span class="params">options:</span> <span class="operator">+</span>cmd</span><br><span class="line">;; Got <span class="params">answer:</span></span><br><span class="line">;; <span class="operator">-&gt;</span><span class="operator">&gt;</span>HEADER<span class="operator">&lt;</span><span class="operator">&lt;</span><span class="operator">-</span> <span class="params">opcode:</span> QUERY, <span class="params">status:</span> NOERROR, <span class="params">id:</span> <span class="number">41743</span></span><br><span class="line">;; <span class="params">flags:</span> qr aa rd ra; <span class="params">QUERY:</span> <span class="number">1</span>, <span class="params">ANSWER:</span> <span class="number">3</span>, <span class="params">AUTHORITY:</span> <span class="number">0</span>, <span class="params">ADDITIONAL:</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">;; QUESTION <span class="params">SECTION:</span></span><br><span class="line">;my-svc-headless.default.svc.cluster.local. IN A</span><br><span class="line"></span><br><span class="line">;; ANSWER <span class="params">SECTION:</span></span><br><span class="line">my-svc-headless.default.svc.cluster.local. <span class="number">30</span> IN A <span class="number">10.244</span>.<span class="number">0.22</span></span><br><span class="line">my-svc-headless.default.svc.cluster.local. <span class="number">30</span> IN A <span class="number">10.244</span>.<span class="number">1.21</span></span><br><span class="line">my-svc-headless.default.svc.cluster.local. <span class="number">30</span> IN A <span class="number">10.244</span>.<span class="number">2.38</span></span><br><span class="line"></span><br><span class="line">;; Query <span class="params">time:</span> <span class="number">29</span> msec</span><br><span class="line">;; <span class="params">SERVER:</span> <span class="number">10.96</span>.<span class="number">0.10</span><span class="comment">#53(10.96.0.10)</span></span><br><span class="line">;; <span class="params">WHEN:</span> Thu Dec <span class="number">13</span> <span class="number">04</span>:<span class="number">37</span>:<span class="number">13</span> EST <span class="number">2018</span></span><br><span class="line">;; MSG SIZE  <span class="params">rcvd:</span> <span class="number">107</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="由上可见-dns服务为service和pod生成不同格式的DNS记录"><a href="#由上可见-dns服务为service和pod生成不同格式的DNS记录" class="headerlink" title="由上可见 dns服务为service和pod生成不同格式的DNS记录"></a>由上可见 dns服务为service和pod生成不同格式的DNS记录</h5><h6 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h6><ul>
<li><p>A记录：生成my-svc.my-namespace.svc.cluster.local域名，解析成 IP 地址，分为两种情况：<br> 1.普通 Service：解析成 ClusterIP<br> 2.Headless Service：解析为指定 Pod的IP列表(上述示例就是headless)</p>
</li>
<li><p>SRV记录：为命名的端口（普通 Service 或 Headless Service）生成<code>_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local</code>的域名</p>
</li>
</ul>
<h6 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h6><ul>
<li>A记录：生成域名 <code>pod-ip.my-namespace.pod.cluster.local</code></li>
</ul>
<p>上述示例个人理解是 这种无头ClusterIP类型 在其集群内部直接可以通过该域名去访问pod，<br>并且该域名也起到了通过dns做负载的能力。</p>
<p>其他访问法方式可查阅官网学习,此篇博文主要是再次学习巩固一下知识~<br>诸如ingress这种访问方式之前也在学习实践过程中已经实现过了，感兴趣可以<a href="https://github.com/guomaoqiu/flask_kubernetes">撩我</a></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>ClusterIP</tag>
        <tag>NodePort</tag>
        <tag>TargetPort</tag>
        <tag>Port</tag>
      </tags>
  </entry>
  <entry>
    <title>蓝鲸平台实践及应用...</title>
    <url>/2017/05/31/lan-jing-ping-tai-shi-jian-ji-ying-yong-2/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言:"></a>前言:</h4><p>在今年三月份蓝鲸推出了一套免费的蓝鲸DevOps技能培训，借助蓝鲸平台致力于让每个运维屌丝都具备一定的开发能力；有幸在此次培训课程坚持了下来，也收获了关于腾讯蓝鲸运维的一些理念、思维；同时自己的工作方式、学习方式也有了一定的提升、改变；再此感谢各位导师的孜孜不倦的教授—–授人以鱼不如授人以渔。</p>
<hr>
<h4 id="一、如何学习它？"><a href="#一、如何学习它？" class="headerlink" title="一、如何学习它？"></a>一、如何学习它？</h4><ol>
<li>Linux基础(必备)；</li>
<li>需要有一定的Python基础知识；</li>
<li>前端方面也需要多写；</li>
<li>学习能力 + 学习方法；</li>
</ol>
<hr>
<h4 id="二、学习途径："><a href="#二、学习途径：" class="headerlink" title="二、学习途径："></a>二、学习途径：</h4><p>链接: <a href="http://bbs.bk.tencent.com/forum.php?mod=viewthread&amp;tid=261&amp;extra=page=1">蓝鲸DevOps技能培训</a></p>
<hr>
<h4 id="三、我的学习记录"><a href="#三、我的学习记录" class="headerlink" title="三、我的学习记录:"></a>三、我的学习记录:</h4><p>链接: <a href="http://bbs.bk.tencent.com/forum.php?mod=viewthread&amp;tid=261&amp;page=21&amp;authorid=704">作业记录</a></p>
<hr>
<h4 id="四、目前学习成果"><a href="#四、目前学习成果" class="headerlink" title="四、目前学习成果:"></a>四、目前学习成果:</h4><ol>
<li>蓝鲸paas平台页面截图: <img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/05/14962014386437-14.jpg"></li>
<li>蓝鲸开发者中心: ￼<a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/05/WechatIMG2.jpeg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/05/WechatIMG2.jpeg"></a></li>
<li>自主开发App权限控制平台: <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/05/WechatIMG5.jpeg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/05/WechatIMG5.jpeg"></a>￼</li>
<li>自主开发运维管控平台: <img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/05/14962014574341-16.jpg">￼</li>
</ol>
<hr>
<h4 id="五、小结"><a href="#五、小结" class="headerlink" title="五、小结:"></a>五、小结:</h4><h5 id="上面的两个应用已经应用在生产环境-其他各个部门的平台开发ing"><a href="#上面的两个应用已经应用在生产环境-其他各个部门的平台开发ing" class="headerlink" title="上面的两个应用已经应用在生产环境;其他各个部门的平台开发ing;"></a>上面的两个应用已经应用在生产环境;其他各个部门的平台开发ing;</h5><h5 id="现在小伙伴儿们如果去学习的话；应该非常的顺利，毕竟坑差不多都被填完了。"><a href="#现在小伙伴儿们如果去学习的话；应该非常的顺利，毕竟坑差不多都被填完了。" class="headerlink" title="现在小伙伴儿们如果去学习的话；应该非常的顺利，毕竟坑差不多都被填完了。"></a>现在小伙伴儿们如果去学习的话；应该非常的顺利，毕竟坑差不多都被填完了。</h5><h5 id="不想说太多；此篇博文记录一下这三个月以来的一些收获吧"><a href="#不想说太多；此篇博文记录一下这三个月以来的一些收获吧" class="headerlink" title="不想说太多；此篇博文记录一下这三个月以来的一些收获吧;"></a>不想说太多；此篇博文记录一下这三个月以来的一些收获吧;</h5><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2017/06/14962839951327.jpg">￼</p>
<h5 id="你的对手在看书，"><a href="#你的对手在看书，" class="headerlink" title="你的对手在看书，"></a>你的对手在看书，</h5><h5 id="你的仇人在磨刀，"><a href="#你的仇人在磨刀，" class="headerlink" title="你的仇人在磨刀，"></a>你的仇人在磨刀，</h5><h5 id="你的闺密在减肥，"><a href="#你的闺密在减肥，" class="headerlink" title="你的闺密在减肥，"></a>你的闺密在减肥，</h5><h5 id="隔壁老王在练腰，"><a href="#隔壁老王在练腰，" class="headerlink" title="隔壁老王在练腰，"></a>隔壁老王在练腰，</h5><h5 id="我们必须不断学习，"><a href="#我们必须不断学习，" class="headerlink" title="我们必须不断学习，"></a>我们必须不断学习，</h5><h5 id="才能让自己变得更强。"><a href="#才能让自己变得更强。" class="headerlink" title="才能让自己变得更强。"></a>才能让自己变得更强。</h5>]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>系统运维</tag>
      </tags>
  </entry>
  <entry>
    <title>理解Kubernetes的亲和性调度</title>
    <url>/2018/12/04/li-jiekubernetes-de-qin-he-xing-diao-du/</url>
    <content><![CDATA[<p><code>NodeName、nodeSelector、nodeAffinity、podAffinity、Taints以及Tolerations用法</code></p>
<h2 id="1-NodeName"><a href="#1-NodeName" class="headerlink" title="1. NodeName"></a>1. NodeName</h2><p>Pod.spec.nodeName用于强制约束将Pod调度到指定的Node节点上，这里说是“调度”，但其实指定了nodeName的Pod会直接跳过Scheduler的调度逻辑，直接写入PodList列表，该匹配规则是<code>强制匹配</code>。<br>例子：<br>我的预期是将该pod运行于节点名称为 192.168.56.11 这个节点：</p>
<h6 id="1-1-例如-test-nodename-yaml"><a href="#1-1-例如-test-nodename-yaml" class="headerlink" title="1.1 例如(test-nodename.yaml)"></a>1.1 例如(test-nodename.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    app: with-nodename-busybox-pod
  name: with-nodename
spec:
  nodeName: 192.168.56.11 #通过这里指定
  containers:
  - command:
    - sleep
    - "3600"
    image: busybox
    imagePullPolicy: IfNotPresent
    name: test-busybox
</code></pre>
<h6 id="1-2-查看创建事件、与预期相符"><a href="#1-2-查看创建事件、与预期相符" class="headerlink" title="1.2 查看创建事件、与预期相符:"></a>1.2 查看创建事件、与预期相符:</h6><pre><code>Events:
  Type    Reason                 Age   From                    Message
  ----    ------                 ----  ----                    -------
  Normal  Scheduled              3m    default-scheduler       Successfully assigned with-pod-affinity to 192.168.56.11
  Normal  SuccessfulMountVolume  3m    kubelet, 192.168.56.11  MountVolume.SetUp succeeded for volume "default-token-5htws"
  Normal  Pulling                3m    kubelet, 192.168.56.11  pulling image "nginx"
  Normal  Pulled                 1m    kubelet, 192.168.56.11  Successfully pulled image "nginx"
  Normal  Created                1m    kubelet, 192.168.56.11  Created container
  Normal  Started                1m    kubelet, 192.168.56.11  Started container
</code></pre>
<h2 id="2-NodeSelector"><a href="#2-NodeSelector" class="headerlink" title="2. NodeSelector"></a>2. NodeSelector</h2><p>Pod.spec.nodeSelector是通过kubernetes的label-selector机制进行节点选择，由scheduler调度策略MatchNodeSelector进行label匹配，调度pod到目标节点，该匹配规则是<code>强制约束</code>。使用节点选择器的步骤为：</p>
<h6 id="2-1-Node添加label标记-标记规则："><a href="#2-1-Node添加label标记-标记规则：" class="headerlink" title="2.1 Node添加label标记,标记规则："></a>2.1 Node添加label标记,标记规则：</h6><pre><code>kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;
kubectl label nodes 192.168.56.14 server_type=game_server
</code></pre>
<h6 id="2-2-确认标记"><a href="#2-2-确认标记" class="headerlink" title="2.2 确认标记"></a>2.2 确认标记</h6><pre><code>[root@linux-node1 ~]# kubectl get nodes 192.168.56.14 --show-labels
NAME            STATUS    ROLES     AGE       VERSION   LABELS
192.168.56.14   Ready     node      82d       v1.10.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=192.168.56.14,node-role.kubernetes.io/node=true,server_type=game_server
</code></pre>
<h6 id="2-3-例如-test-nodeselector-yaml"><a href="#2-3-例如-test-nodeselector-yaml" class="headerlink" title="2.3 例如(test-nodeselector.yaml)"></a>2.3 例如(test-nodeselector.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    app: with-nodeselector-busybox-pod
  name: with-node-selector
spec:
  containers:
  - command:
    - sleep
    - "3600"
    image: busybox
    imagePullPolicy: IfNotPresent
    name: test-busybox
  # 通过节点标签进行预设该pod将会运行于有此标签的节点上面，
  # 如果多个节点拥有此标签，调度器将会择优进行调度
  nodeSelector:
    server_type: game_server
</code></pre>
<h5 id="2-4-查看创建事件"><a href="#2-4-查看创建事件" class="headerlink" title="2.4 查看创建事件:"></a>2.4 查看创建事件:</h5><pre><code>Events:
  Type    Reason                 Age   From                    Message
  ----    ------                 ----  ----                    -------
  Normal  Scheduled              16s   default-scheduler       Successfully assigned with-node-selector to 192.168.56.14
  Normal  SuccessfulMountVolume  16s   kubelet, 192.168.56.14  MountVolume.SetUp succeeded for volume "default-token-5htws"
  Normal  Pulling                11s   kubelet, 192.168.56.14  pulling image "busybox"
</code></pre>
<p>通过上面的例子我们可以感受到nodeSelector的方式比较直观，但是还够灵活，控制粒度偏大，下面我们再看另外一种更加灵活的方式：<code>nodeAffinity</code>。</p>
<h2 id="3-NodeAffinity"><a href="#3-NodeAffinity" class="headerlink" title="3. NodeAffinity"></a>3. NodeAffinity</h2><p><code>nodeAffinity</code>就是节点亲和性，相对应的是<code>Anti-Affinity</code>，就是反亲和性，这种方法比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。<br>调度可以分成软策略和硬策略两种方式，软策略就是如果你没有满足调度要求的节点的话，POD 就会忽略这条规则，继续完成调度过程。 <code>nodeAffinity</code>就有两上面两种策略：</p>
<pre><code># 软策略:(满足条件最好了，没有的话也无所谓了的策略)
preferredDuringSchedulingIgnoredDuringExecution
# 硬策略:(你必须满足我的要求，不然我就不干)
requiredDuringSchedulingIgnoredDuringExecution
</code></pre>
<h6 id="3-1-例如-test-node-affinity-yaml"><a href="#3-1-例如-test-node-affinity-yaml" class="headerlink" title="3.1 例如(test-node-affinity.yaml)"></a>3.1 例如(test-node-affinity.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
  labels:
    app: node-affinity-pod
spec:
  containers:
  - name: with-node-affinity
    image: nginx
    imagePullPolicy: IfNotPresent
  affinity:
    nodeAffinity:
      # 硬性策略
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: NotIn
            values:
            - 192.168.56.11
            - 192.168.56.12
      # 软性策略
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: server_type
            operator: In
            values:
              - game_server
</code></pre>
<p>上面的Pod我们进行一下解读:<br>1、该pod 通过硬性策略约束后将禁止调度到节点11，12上面去<br>2、排除节点11，12后，选择其他节点含有label为<code>server_type:game_server</code>的节点运行<br>3、如果其他节点也没有含有label <code>server_type:game_server</code>, 那么将会调度到任意节点运行<br>同样的我们可以使用descirbe命令查看具体的调度情况是否满足我们的要求。这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种：</p>
<ul>
<li>In：label 的值在某个列表中</li>
<li>NotIn：label 的值不在某个列表中</li>
<li>Gt：label 的值大于某个值</li>
<li>Lt：label 的值小于某个值</li>
<li>Exists：某个 label 存在</li>
<li>DoesNotExist：某个 label 不存在</li>
</ul>
<p>如果<code>nodeSelectorTerms</code>下面有多个选项的话，满足任何一个条件就可以了；如果<code>matchExpressions</code>有多个选项的话，则必须同时满足这些条件才能正常调度 POD。</p>
<h2 id="4-PodAffinity"><a href="#4-PodAffinity" class="headerlink" title="4. PodAffinity"></a>4. PodAffinity</h2><p>上面三种方式都是让POD去选择节点的，有的时候我们也希望能够根据 POD 之间的关系进行调度，Kubernetes在1.4版本引入的podAffinity概念就可以实现我们这个需求。</p>
<p>和<code>nodeAffinity</code>类似，<code>podAffinity</code>也有<code>requiredDuringSchedulingIgnoredDuringExecution(硬性策略)</code>和 <code>preferredDuringSchedulingIgnoredDuringExecution(软性策略)</code>两种调度策略，唯一不同的是如果要使用互斥性，我们需要使用<code>podAntiAffinity</code>字段。 如下例子，我们希望【with-pod-affinity】和【with-nodename-busybox-pod】能够就近部署，而不希望和【node-affinity-pod】部署在同一个拓扑域下面：</p>
<h6 id="4-1-例如-test-pod-affinity-yaml"><a href="#4-1-例如-test-pod-affinity-yaml" class="headerlink" title="4.1 例如(test-pod-affinity.yaml)"></a>4.1 例如(test-pod-affinity.yaml)</h6><pre><code>apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
  labels:
    app: pod-affinity-pod
spec:
  containers:
  - name: with-pod-affinity
    image: nginx
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - with-nodename-busybox-pod
        topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - node-affinity-pod
          topologyKey: kubernetes.io/hostname
</code></pre>
<p>上面的pod我们解读一下:<br>1、POD 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 POD：这个 POD 有一个<code>app=busybox-pod</code>的 label。<br>2、<code>podAntiAffinity</code>则是希望最好不要调度到这样的节点：这个节点上运行了某个 POD，而这个 POD 有app=node-affinity-pod的 label。<br>3、根据前面两个 POD 的定义，我们可以预见上面这个 POD 应该会被调度到192.168.56.11的节点上，因为前面实践的时候【with-nodename-busybox-pod】被调度到了192.168.56.11节点，而【node-affinity-pod】被调度到了192.168.56.11以为的节点，正好满足上面的需求。<br>通过describe查看：</p>
<pre><code>Events:
  Type    Reason                 Age   From                    Message
  ----    ------                 ----  ----                    -------
  Normal  Scheduled              53m   default-scheduler       Successfully assigned with-pod-affinity to 192.168.56.11
  Normal  SuccessfulMountVolume  52m   kubelet, 192.168.56.11  MountVolume.SetUp succeeded for volume "default-token-5htws"
  Normal  Pulling                52m   kubelet, 192.168.56.11  pulling image "nginx"
  Normal  Pulled                 51m   kubelet, 192.168.56.11  Successfully pulled image "nginx"
  Normal  Created                51m   kubelet, 192.168.56.11  Created container
  Normal  Started                51m   kubelet, 192.168.56.11  Started container
</code></pre>
<p>在labelSelector和 topologyKey的同级，还可以定义 namespaces 列表，表示匹配哪些 namespace 里面的 pod，默认情况下，会匹配定义的 pod 所在的 namespace；如果定义了这个字段，但是它的值为空，则匹配所有的 namespaces。</p>
<p>so，以上我们创建的四个例子结果为下:</p>
<pre><code>[root@linux-node1 affinity_study]# kubectl get pods -o wide
NAME                          READY     STATUS    RESTARTS   AGE       IP           NODE
with-node-affinity            1/1       Running   2          10h       10.2.70.16   192.168.56.14
with-node-selector            1/1       Running   0          8m        10.2.70.15   192.168.56.14
with-nodename                 1/1       Running   1          11h       10.2.88.9    192.168.56.11
with-pod-affinity             1/1       Running   0          10h       10.2.88.10   192.168.56.11
</code></pre>
<h2 id="5-污点（Taints）与容忍（tolerations）"><a href="#5-污点（Taints）与容忍（tolerations）" class="headerlink" title="5. 污点（Taints）与容忍（tolerations）"></a>5. 污点（Taints）与容忍（tolerations）</h2><p>对于nodeAffinity无论是硬策略还是软策略方式，都是调度 POD 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 POD 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度pod。</p>
<p>比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 POD，则污点就很有用了，POD 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：</p>
<h6 id="5-1-设置污点"><a href="#5-1-设置污点" class="headerlink" title="5.1 设置污点"></a>5.1 设置污点</h6><pre><code> kubectl taint node [node] key=value[effect]   
      其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ]
       NoSchedule ：一定不能被调度。POD 不会被调度到标记为 taints 节点。
       PreferNoSchedule：尽量不要调度。NoSchedule 的软策略版本。
       NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。
  示例：kubectl taint node 192.168.56.11 key1=value1:NoSchedule
</code></pre>
<h6 id="5-2-去除污点"><a href="#5-2-去除污点" class="headerlink" title="5.2 去除污点"></a>5.2 去除污点</h6><pre><code>#比如设置污点：
    kubectl taint nodes 192.168.56.11 key1=value1:NoSchedule
    kubectl taint nodes 192.168.56.11 key2=value2:NoExecute
    kubectl taint nodes 192.168.56.11 key3=value3:PreferNoSchedule
 #去除指定key及其effect：
    kubectl taint nodes 192.168.56.11 key:[effect]-    #(这里的key不用指定value)
                
 #去除指定key所有的effect: 
     kubectl taint nodes 192.168.56.11 key-
 
 #示例：
     kubectl taint node 192.168.56.11 key1:NoSchedule-
     kubectl taint node 192.168.56.11 key2:NoExecute-
     kubectl taint node 192.168.56.11 key3:PreferNoSchedule-
     kubectl taint node 192.168.56.11 key3- (去除指定key的所有effct)
</code></pre>
<h6 id="5-3-实践"><a href="#5-3-实践" class="headerlink" title="5.3 实践"></a>5.3 实践</h6><h6 id="5-3-1-给节点设置污点"><a href="#5-3-1-给节点设置污点" class="headerlink" title="# 5.3.1 给节点设置污点"></a># 5.3.1 给节点设置污点</h6><p>首先我的环境中还没有设置污点，我们执行以下例子(nginx-deployment.yaml):</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-pod
  labels:
    app: nginx-pod
spec:
  replicas: 10 # 这里为了实践效果特意设定了10个副本
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        
# 部署结果:
[root@linux-node1 ~]# kubectl get pods -o wide | grep nginx
nginx-pod-7d9f9876cc-95fj2   1/1       Running   0          1m        10.2.70.26   192.168.56.14
nginx-pod-7d9f9876cc-jclvf   1/1       Running   0          1m        10.2.44.28   192.168.56.13
nginx-pod-7d9f9876cc-k2m6x   1/1       Running   0          1m        10.2.44.31   192.168.56.13
nginx-pod-7d9f9876cc-l74hf   1/1       Running   0          1m        10.2.88.15   192.168.56.11
nginx-pod-7d9f9876cc-n8g6c   1/1       Running   0          1m        10.2.44.29   192.168.56.13
nginx-pod-7d9f9876cc-p2cft   1/1       Running   0          1m        10.2.88.14   192.168.56.11
nginx-pod-7d9f9876cc-p7hvt   1/1       Running   0          1m        10.2.69.25   192.168.56.12
nginx-pod-7d9f9876cc-rjj97   1/1       Running   0          1m        10.2.70.27   192.168.56.14
nginx-pod-7d9f9876cc-t2wlw   1/1       Running   0          1m        10.2.69.26   192.168.56.12
nginx-pod-7d9f9876cc-whkx9   1/1       Running   0          1m        10.2.44.30   192.168.56.13
</code></pre>
<p>以上例子可以看出在我的环境中已经分配到了每个节点包括我们接下来准备设置污点的节点192.168.56.11。</p>
<p>下面我把master节点192.168.56.11 设置污点(这里我用的effect是NoSchedule)</p>
<pre><code>[root@linux-node1 affinity_study]# kubectl taint nodes 192.168.56.11 server_type=k8s_system:NoSchedule
node "192.168.56.11" tainted

# 然后部署下面这个设置例子:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-pod-taints
  labels:
    app: nginx-pod-taints
spec:
  replicas: 10 # 这里为了实践效果特意设定了10个副本
  selector:
    matchLabels:
      app: nginx-taints
  template:
    metadata:
      labels:
        app: nginx-taints
    spec:
      containers:
      - name: nginx-taints
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
# 部署结果:
[root@linux-node1 ~]# kubectl get pods -o wide | grep nginx-pod-taints
nginx-pod-taints-57757d7677-5h6nj   1/1       Running   0          2m        10.2.69.31   192.168.56.12
nginx-pod-taints-57757d7677-dkd8h   1/1       Running   0          2m        10.2.70.34   192.168.56.14
nginx-pod-taints-57757d7677-f2vdn   1/1       Running   0          2m        10.2.44.38   192.168.56.13
nginx-pod-taints-57757d7677-fnx2n   1/1       Running   0          2m        10.2.44.36   192.168.56.13
nginx-pod-taints-57757d7677-gsc5g   1/1       Running   0          2m        10.2.70.32   192.168.56.14
nginx-pod-taints-57757d7677-kjl89   1/1       Running   0          2m        10.2.44.35   192.168.56.13
nginx-pod-taints-57757d7677-mqx27   1/1       Running   0          2m        10.2.70.33   192.168.56.14
nginx-pod-taints-57757d7677-skdd4   1/1       Running   0          2m        10.2.69.32   192.168.56.12
nginx-pod-taints-57757d7677-spwfh   1/1       Running   0          2m        10.2.69.30   192.168.56.12
nginx-pod-taints-57757d7677-tpcm8   1/1       Running   0          2m        10.2.44.37   192.168.56.13
</code></pre>
<p>以上例子可以看出我们将master节点设置污点之后调度器并没有把pod调度到master节点上。<br>这里我选择的effect是NoSchedule，也就是一定不要调度到该节点。</p>
<h6 id="5-3-2-给部署配置设置容忍性"><a href="#5-3-2-给部署配置设置容忍性" class="headerlink" title="# 5.3.2 给部署配置设置容忍性"></a># 5.3.2 给部署配置设置容忍性</h6><p>首先 我们上面设置了污点<br>即:</p>
<pre><code>  taints:
  - effect: NoSchedule
    key: server_type
    value: k8s_system
</code></pre>
<p>然后我们部署以下例子(test-tolertations.yaml)</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-pod-taints-tolerations
  labels:
    app: nginx-pod-taints-tolerations
spec:
  replicas: 10 # 这里为了实践效果特意设定了10个副本
  selector:
    matchLabels:
      app: nginx-taints-tolerations
  template:
    metadata:
      labels:
        app: nginx-taints-tolerations
    spec:
      # 设置容忍性
      tolerations:
      - key: "server_type"
        operator: "Equal"
        value: "k8s_system"
        effect: "NoSchedule"
      containers:
      - name: nginx-taints-tolerations
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80

# 部署结果:
[root@linux-node1 affinity_study]# kubectl get pods -o wide  | grep nginx-pod-taints-tolerations
nginx-pod-taints-tolerations-5b4988bd4-2pk46   1/1       Running   0          29m       10.2.69.33   192.168.56.12
nginx-pod-taints-tolerations-5b4988bd4-854xx   1/1       Running   0          29m       10.2.70.35   192.168.56.14
nginx-pod-taints-tolerations-5b4988bd4-88xrt   1/1       Running   0          29m       10.2.69.34   192.168.56.12
nginx-pod-taints-tolerations-5b4988bd4-8czpg   1/1       Running   0          29m       10.2.88.17   192.168.56.11
nginx-pod-taints-tolerations-5b4988bd4-czdss   1/1       Running   0          21m       10.2.44.41   192.168.56.13
nginx-pod-taints-tolerations-5b4988bd4-dj6mw   1/1       Running   0          29m       10.2.88.16   192.168.56.11
nginx-pod-taints-tolerations-5b4988bd4-g4ltd   1/1       Running   0          29m       10.2.44.39   192.168.56.13
nginx-pod-taints-tolerations-5b4988bd4-npthj   1/1       Running   0          29m       10.2.44.40   192.168.56.13
nginx-pod-taints-tolerations-5b4988bd4-ptlqg   1/1       Running   0          29m       10.2.88.18   192.168.56.11
nginx-pod-taints-tolerations-5b4988bd4-t9gs6   1/1       Running   0          29m       10.2.69.35   192.168.56.12
</code></pre>
<p>以上实践我们解读一下:<br>1. 在节点master 192.168.56.11上面设置了污点；但期望该节点能容忍调度;<br>2. 于是通过设置tolerations来实现，其中Pod要容忍的有污点的Node的key是server_type Equal k8s_system,效果是NoSchedule.<br>3. 通过设置容忍机制结果与预期相符。</p>
<p>对于tolerations属性的写法：</p>
<pre><code>其中的key、value、effect 与Node的Taint设置需保持一致， 还有以下几点说明：
     1、如果operator的值是Exists，则value属性可省略。
     2、如果operator的值是Equal，则表示其key与value之间的关系是equal(等于)。
     3、如果不指定operator属性，则默认值为Equal。
另外，还有两个特殊值：
     1、空的key 如果再配合Exists 就能匹配所有的key与value ，也是是能容忍所有node的所有Taints。
     2、空的effect 匹配所有的effect。
</code></pre>
<h6 id="5-3-3-其他"><a href="#5-3-3-其他" class="headerlink" title="# 5.3.3 其他"></a># 5.3.3 其他</h6><p>一个node上可以有多个污点：</p>
<pre><code>[root@linux-node1 affinity_study]# kubectl describe node/192.168.56.11
........
........
Taints:             server_type=k8s_system:NoSchedule
                    test=wahaha:PreferNoSchedule
........
........
</code></pre>
<p>如果按照我上面的例子来的话结果将不会有任何pod调度到该节点，因为上述我只容忍了一个污点；所以可以新加一个污点容忍:</p>
<pre><code>[root@linux-node1 affinity_study]# more test-tolertations.yaml
........
........      
    tolerations:
      - key: "server_type"
        operator: "Equal"
        value: "k8s_system"
        effect: "NoSchedule"
      - key: "test"
        operator: "Equal"
        value: "wahaha"
        effect: "PreferNoSchedule"
........
........
</code></pre>
<p>重新部署后结果也可以预见 只要两个容忍满足就可以调度过去啦~</p>
<p>设置容忍的效果：</p>
<ul>
<li>如果在设置node的Taints(污点)之前，就已经运行了一些Pod，那么这些Pod是否还能继续在此Node上运行？ 这就要看设置Taints污点时的effect(效果)了。</li>
<li>如果effect的值是NoSchedule或PreferNoSchedule，那么已运行的Pod仍然可以运行，只是新Pod(如果没有容忍)不会再往上调度。</li>
<li>如果 pod 不能忍受effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐</li>
<li>如果 pod 能够忍受effect 值为 NoExecute 的 taint，但是在 toleration 定义中没有指定 tolerationSeconds，则 pod 还会一直在这个节点上运行。</li>
<li>如果 pod 能够忍受effect 值为 NoExecute 的 taint，而且指定了 tolerationSeconds，则 pod 还能在这个节点上继续运行这个指定的时间长度。 虽然是立刻被驱逐，但是K8S为了彰显人性化，又给具有NoExecute效果的污点， 在容忍属性中有一个可选的tolerationSeconds字段，用来设置这些Pod还可以在这个Node之上运行多久，给它们一点宽限的时间，到时间才驱逐。</li>
</ul>
<p>不同的部署启动方式：</p>
<ul>
<li><p>如果是以Pod来启动的，那么Pod被驱逐后， 将不会再被运行，就等于把它删除了。</p>
</li>
<li><p>如果是deployment/rc，那么删除的pod会再其它节点运行。</p>
</li>
<li><p>如果是DaemonSet在此Node上启动的Pod，那么也不会再被运行，直到Node上的NoExecute污被去除或者Pod容忍。</p>
<p>#设置Pod的宽限时间<br>spec:<br>  tolerations: #设置容忍性</p>
<ul>
<li>key: “test”<br>operator: “Equal” #如果操作符为Exists，那么value属性可省略<br>value: “16”<br>effect: “wahaha”<br>tolerationSeconds: 180 <h1 id="如果运行此Pod的Node，被设置了具有NoExecute效果的Taint-污点-，这个Pod将在存活180s后才被驱逐。"><a href="#如果运行此Pod的Node，被设置了具有NoExecute效果的Taint-污点-，这个Pod将在存活180s后才被驱逐。" class="headerlink" title="如果运行此Pod的Node，被设置了具有NoExecute效果的Taint(污点)，这个Pod将在存活180s后才被驱逐。"></a>如果运行此Pod的Node，被设置了具有NoExecute效果的Taint(污点)，这个Pod将在存活180s后才被驱逐。</h1><h1 id="如果没有设置tolerationSeconds字段，将永久运行。"><a href="#如果没有设置tolerationSeconds字段，将永久运行。" class="headerlink" title="如果没有设置tolerationSeconds字段，将永久运行。"></a>如果没有设置tolerationSeconds字段，将永久运行。</h1></li>
</ul>
</li>
</ul>
<p>通过对Taints和Tolerations的了解，可以知道，通过它们可以让某些特定应用，独占一个Node，给特定的Node设置一个Taint，只让某些特定的应用来容忍这些污点，容忍后就有可能会被调度到此特定Node，但是也不一定会调度给此特定Node，设置容忍并不阻止调度器调度给其它Node，那么如何让特定应用的Node，只能被调度到此特定的Node呢，这就要结合NodeAffinity节点亲和性，给Node打个标签，然后在Pod属性里设置NodeAffinity到Node。如此就能达到要求了。</p>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux上ip归属查看神器</title>
    <url>/2014/04/12/linux-e4-b8-8aip-e5-bd-92-e5-b1-9e-e6-9f-a5-e7-9c-8b-e7-a5-9e-e5-99-a8/</url>
    <content><![CDATA[<p>nali取名”哪里”的拼音。 nali包含一组命令行程序，其主要功能就是把一些网络工具的输出的IP字符串，附加上地理位置信息(使用纯真数据库) 例如218.65.137.1会变成218.65.137.1[广西南宁市 电信]。 查询是在本地进行，并不会进行联网查询，所以效率方面不会有什么影响。 目前包含以下几个命令：</p>
<p>nali nali-dig nali-nslookup nali-traceroute nali-tracepath nali-ping</p>
<p>使用这些命令的前提是，他们对应的命令必须存在。例如你要用nali-dig，必须保证dig是存在的。他们的用法和原始命令是一样的。例如nali-dig，用法就和dig一样。 大家可能注意到了nali这个命令，它可以对标准输出的IP串附加上地理信息。nali-*系列工具都是基于这个来实现的。 下载 wget <a href="http://www.sctux.com/package/nali-0.1.tar.gz">http://www.sctux.com/package/nali-0.1.tar.gz</a> 安装</p>
<p>./configure –prefix=/usr &amp;&amp; make &amp;&amp; make instal</p>
<p>使用 1.统计nginx的访问记录</p>
<p>[root@sctux ~]# awk -F ‘ ‘ ‘{print $1}’ /var/log/access_sctux.log | sort -n | uniq -cd | sort -rn | nali</p>
<p>结果如下： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/06/nali.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/06/nali.png" alt="nali"></a> 2.使用traceroute <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/06/nali-traceroute.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/06/nali-traceroute.png" alt="nali-traceroute"></a> 也就是说，nali这个命令，可以对标准输出的ip，附加上地理信息。同理，如果你不喜欢用nali-dig，那么也可以用dig ip|nali这样的命令。 如果你觉得输入nali-xxx麻烦，那么可以做一些alias，例如：</p>
<p>vim /root/.bashrc<br>alias traceroute=’nali-traceroute’<br>alias dig=’nali-dig’</p>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
  </entry>
  <entry>
    <title>Linux系统初始化脚本</title>
    <url>/2015/06/19/linux-e7-b3-bb-e7-bb-9f-e5-88-9d-e5-a7-8b-e5-8c-96/</url>
    <content><![CDATA[<p>我们在安装好操作系统后一般都需要对系统做一些针对于自己环境的情况做一下系统初始化，这个我们一般用shell脚本来跑一遍，把相关的参数、配置调整一下就好了。使用下面这个脚本就可以初始化我们系统啦。</p>
<p>#!/bin/bash<br>#Version 1.9<br>#Auth: guomaoqiu<br>#For CentOS_mini<br>#Made on 2015-06-19</p>
<p>echo “   “<br>echo “#############################################################################”<br>echo “#  Initialize for the CentOS 6.4/6.5 mini_installed.                        #”<br>echo “#                                                                           #”<br>echo “#  Please affirm this OS connected net already before running this script ! #”<br>echo “#                                                                           #”<br>echo “#  must first connect to the Internet. because yum need it.                  #”<br>echo “#############################################################################”<br>echo “   “</p>
<p>format() {<br>          #echo -e “\033[42;37m ########### Finished ########### \033[0m”<br>          sleep 5<br>          echo -e “\033[42;37m ########### Finished ########### \033[0m”<br>          echo “  “<br>         }</p>
<p>##########################################################################<br># Set time 时区/时间同步设置<br>echo “Set time.”	<br>/bin/cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&gt; /dev/null<br>yum -y install ntpdate &amp;&gt; /dev/null<br>ntpdate  0.centos.pool.ntp.org &amp;&gt; /dev/null<br>hwclock -w<br>format</p>
<p>####################################################################################<br>#Set network 网卡开机自启动<br>#sed -i “s/ONBOOT=no/ONBOOT=yes/g”  /etc/sysconfig/network-scripts/ifcfg-eth0<br>#sed -i “s/NM_CONTROLLED=no/NM_CONTROLLED=no/g”/etc/sysconfig/network-scripts/ifcfg-eth0<br>#/etc/init.d/network restart &amp;&gt;/dev/null<br>#echo “==================================” &gt;&gt; $LOG<br>#format</p>
<p>##########################################################################<br># Create Log 创建该脚本运行记录日志<br>echo “Create log file.”<br>DATE1=`date +”%F %H:%M”`<br>LOG=/var/log/sysinitinfo.log<br>echo $DATE1 &gt;&gt; $LOG<br>echo “——————————————“ &gt;&gt; $LOG<br>echo “For CentOS_mini” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>echo “Set timezone is Shanghai” &gt;&gt; $LOG<br>echo “Finished ntpdate” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Disabled Selinux 禁用Selinux<br>echo “Disabled SELinux.”<br>sed -i ‘s/^SELINUX=enforcing/SELINUX=disabled/‘ /etc/sysconfig/selinux<br>echo “==================================================”<br>echo “Disabled SELinux.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Stop iptables 禁用iptables<br>echo “Stop iptables.”<br>service iptables stop &amp;&gt; /dev/null<br>chkconfig –level 35 iptables off<br>echo “Stop iptables.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Disable ipv6 禁用IPV6<br>echo “Disable ipv6.”<br>echo “alias net-pf-10 off” &gt;&gt; /etc/modprobe.conf<br>echo “alias ipv6 off” &gt;&gt; /etc/modprobe.conf<br>chkconfig –level 35 ip6tables off<br>echo “Disable ipv6.”&gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>##########################################################################<br>#Set history commands  设置命令历史记录参数<br>echo “Set history commands.”<br>echo “HISTFILESIZE=4000” &gt;&gt; /etc/bashrc<br>echo “HISTSIZE=4000” &gt;&gt; /etc/bashrc<br>echo “HISTTIMEFORMAT=’%F/%T’” &gt;&gt; /etc/bashrc<br>source /etc/bashrc<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>##########################################################################<br># Epel 升级epel源<br>echo “Install epel”<br>rpm -Uvh <a href="http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm">http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</a> &amp;&gt; /dev/null<br>sed -i “s/^#base/base/g” /etc/yum.repos.d/epel.repo<br>sed -i “s/^mirr/#mirr/g” /etc/yum.repos.d/epel.repo<br>echo “==================================================”<br>echo “Install epel” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>##########################################################################<br>#Yum install Development tools  安装开发包组及必备软件<br>echo “Install Development tools(It will be a moment,wait……)”<br>yum groupinstall -y “Development tools” &amp;&gt; /dev/null<br>yum groupinstall -y “Server Platform Development” &amp;&gt; /dev/null<br>yum groupinstall -y “Desktop Platform Development” &amp;&gt; /dev/null<br>yum groupinstall -y “chinese-support” &amp;&gt;/dev/null<br>for I in bind-utils lrzsz wget gcc gcc-c++ vim htop ;do<br>	yum install -y $I<br>done<br>echo “==================================================”<br>echo “Install Development tools” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>##########################################################################<br># Yum update bash and openssl  升级bash/openssl<br>echo “Update bash and openssl”<br>yum -y update bash &amp;&gt; /dev/null<br>yum -y update openssl &amp;&gt; /dev/null<br>echo “==================================================”<br>echo “Update bash and openssl” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Set ssh 设置ssh登录策略<br>echo “Disabled EmptyPassword.”<br>echo “Disabled SSH-DNS.”<br>echo “Set timeout is 6m.”<br>sed -i “s/^#PermitEmptyPasswords/PermitEmptyPasswords/“ /etc/ssh/sshd_config<br>sed -i “s/^#LoginGraceTime 2m/LoginGraceTime 6m/“ /etc/ssh/sshd_config<br>echo “UseDNS no” &gt;&gt; /etc/ssh/sshd_config<br>echo “==================================================”<br>echo “Disabled EmptyPassword.” &gt;&gt; $LOG<br>echo “Disabled SSH-DNS.” &gt;&gt; $LOG<br>echo “Set timeout is 6m.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Set default init 3  设置系统默认初始化<br>echo “Default init 3.”<br>sed -i ‘s/^id:5:initdefault:/id:3:initdefault:/‘ /etc/inittab<br>echo “==================================================”<br>echo “Default init 3.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Stop Service  关闭不必要的服务<br>echo “Some services are turned off now.”<br>for SER in rpcbind postfix portreserve certmonger mdmonitor blk-availability lvm2-monitor udev-post cups dhcpd firstboot gpm haldaemon hidd ip6tables ipsec isdn kudzu lpd mcstrans messagebus microcode_ctl netfs nfs nfslock nscd acpid anacron apmd atd auditd autofs avahi-daemon avahi-dnsconfd bluetooth cpuspeed pcscd portmap readahead_early restorecond rpcgssd rpcidmapd rstatd sendmail setroubleshoot snmpd sysstat xfs xinetd yppasswdd ypserv yum-updatesd<br> do<br>    /sbin/chkconfig –list $SER &amp;&gt; /dev/null<br>  if [ $? -eq 0 ]<br>    then<br>      chkconfig –level 35  $SER off<br>    echo “$SER” &gt;&gt; $LOG<br>  fi<br> done<br>echo “==================================================”<br>echo “Some services are turned off now:” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Del unnecessary users 删除不必要的用户<br>echo “Del unnecessary users.”<br>for USERS in adm lp sync shutdown halt mail news uucp operator games gopher<br> do<br>  grep $USERS /etc/passwd &amp;&gt;/dev/null<br>  if [ $? -eq 0 ]<br>   then<br>    userdel $USERS &amp;&gt; /dev/null<br>    echo $USERS &gt;&gt; $LOG<br>  fi<br> done<br>echo “==================================================”<br>echo “Del unnecessary users.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Del unnecessary groups 删除不必要的用户组<br>echo “Del unnecessary groups.”<br>for GRP in adm lp mail news uucp games gopher mailnull floppy dip pppusers popusers slipusers daemon<br> do<br>  grep $GRP /etc/group &amp;&gt; /dev/null<br>  if [ $? -eq 0 ]<br>   then<br>    groupdel $GRP &amp;&gt; /dev/null<br>    echo $GRP &gt;&gt; $LOG<br>  fi<br> done<br>echo “==================================================”<br>echo “Del unnecessary groups.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Disabled reboot by keys ctlaltdelete 禁用ctlaltdelete重启功能<br>echo “Disabled reboot by keys ctlaltdelete”<br>sed -i ‘s/^exec/#exec/‘ /etc/init/control-alt-delete.conf<br>echo “==================================================”<br>echo “Disabled reboot by keys ctlaltdelete” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Set ulimit  设置文件句柄数<br>echo “Set ulimit 1000000”<br>echo “*    soft    nofile  1000000” &gt;&gt; /etc/security/limits.conf<br>echo “*    hard    nofile  1000000” &gt;&gt; /etc/security/limits.conf<br>echo “*    soft    nproc 102400” &gt;&gt; /etc/security/limits.conf<br>echo “*    hard    nproc 102400” &gt;&gt; /etc/security/limits.conf<br>sed -i ‘s/102400/1000000/‘ /etc/security/limits.d/90-nproc.conf<br>echo “==================================================”<br>echo “Set ulimit 1000000” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Set login message 设置登录时显示的信息<br>echo “Set login message.”<br>echo “This is not a public Server” &gt; /etc/issue<br>echo “This is not a public Server” &gt; /etc/redhat-release<br>echo “==================================================”<br>echo “Set login message.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Record SUID and SGID files<br>DATE2=`date +”%F”`<br>echo “Record SUID and SGID files.”<br>echo “SUID — “ &gt; /var/log/SuSg_”$DATE2”.log<br>find / -path ‘/proc’  -prune -o -perm -4000 &gt;&gt; /var/log/SuSg_”$DATE2”.log<br>echo “—————————————————— “ &gt;&gt; /var/log/SuSg_”$DATE2”.log<br>echo “SGID — “ &gt;&gt; /var/log/SuSg_”$DATE2”.log<br>find / -path ‘/proc’  -prune -o -perm -2000 &gt;&gt; /var/log/SuSg_”$DATE2”.log<br>echo “==================================================”<br>echo “Record SUID and SGID.” &gt;&gt; $LOG<br>echo “Record is in /var/log/SuSg_”$DATE2”.log” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Disabled crontab send mail 禁用执行任务计划时向root发送邮件<br>echo “Disable crontab send mail.”<br>sed -i ‘s/^MAILTO=root/MAILTO=””/‘ /etc/crontab<br>sed -i ‘s/^mail\.\*/mail\.err/‘ /etc/rsyslog.conf<br>echo “==================================================”<br>echo “Disable crontab send mail.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Set ntp client 设置时间服务客户端<br>echo “Set ntp client.”<br>SED() {<br>    cp -p /etc/ntp.conf /etc/ntp.conf.bak<br>    sed -i ‘/^server/d’ /etc/ntp.conf<br>    sed -i ‘/^includefile/ i\server 0.centos.pool.ntp.org iburst’ /etc/ntp.conf<br>    sed -i ‘/0.centos.pool.ntp.org/ a\server 1.centos.pool.ntp.org iburst’ /etc/ntp.conf<br>    sed -i ‘/1.centos.pool.ntp.org/ a\server 2.centos.pool.ntp.org iburst’ /etc/ntp.conf<br>    sed -i ‘/2.centos.pool.ntp.org/ a\server 3.centos.pool.ntp.org iburst’ /etc/ntp.conf<br>    chkconfig –level 35 ntpd on &amp;&gt; /dev/null<br>    echo “==================================================”<br>}<br>rpm -q ntp &amp;&gt; /dev/null<br>if [ $? -eq 0 ]<br>  then<br>    SED<br>  else<br>   yum -y install ntp &amp;&gt; /dev/null<br>   SED<br>fi<br>echo “Set ntp client.” &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format</p>
<p>###########################################################################<br># Set sysctl.conf 设置内核参数<br>echo “Set sysctl.conf”<br>#web应用中listen函数的backlog默认会将内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认是511，所以必须调整,一般调整为2048<br>echo “net.core.somaxconn = 2048” &gt;&gt; /etc/sysctl.conf</p>
<p>echo “net.core.rmem_default = 262144” &gt;&gt; /etc/sysctl.conf<br>echo “net.core.wmem_default = 262144” &gt;&gt; /etc/sysctl.conf<br>echo “net.core.rmem_max = 16777216” &gt;&gt; /etc/sysctl.conf<br>echo “net.core.wmem_max = 16777216” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_rmem = 4096 4096 16777216” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_wmem = 4096 4096 16777216” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_mem = 786432 2097152 3145728” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_max_syn_backlog = 16384” &gt;&gt; /etc/sysctl.conf<br>echo “net.core.netdev_max_backlog = 20000” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_fin_timeout = 15” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_tw_reuse = 1” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_tw_recycle = 1” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.tcp_max_orphans = 131072” &gt;&gt; /etc/sysctl.conf<br>echo “net.ipv4.ip_local_port_range = 1024 65535” &gt;&gt; /etc/sysctl.conf<br>echo “Set sysctl.conf —- “ &gt;&gt; $LOG<br>/sbin/sysctl  -p &gt;&gt; $LOG<br>echo “==================================” &gt;&gt; $LOG<br>format<br>###########################################################################<br># Done<br>echo “Finished,You can check infomations in $LOG .”<br>#echo “System will reboot in 60s.”<br>#shutdown -r 1</p>
<p>如果有不对的地方，各位大婶可以提出来，大家一起进步哈；谢谢您的意见。 Download Script: <a href="http://www.sctux.com/project/initsystem.sh">initsystem.sh</a></p>
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title>Lvm的创建</title>
    <url>/2016/03/29/lvm-chuang-jian/</url>
    <content><![CDATA[<h4 id="1-查看磁盘："><a href="#1-查看磁盘：" class="headerlink" title="1. 查看磁盘："></a>1. 查看磁盘：</h4><pre><code>[root@localhost ~]# fdisk -l
......
磁盘 /dev/sdb：21.5 GB, 21474836480 字节，41943040 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节

磁盘 /dev/sdc：21.5 GB, 21474836480 字节，41943040 个扇区
Units = 扇区 of 1 * 512 = 512 bytes
扇区大小(逻辑/物理)：512 字节 / 512 字节
I/O 大小(最小/最佳)：512 字节 / 512 字节
......
</code></pre>
<h4 id="2-对两块磁盘进行lvm格式分区"><a href="#2-对两块磁盘进行lvm格式分区" class="headerlink" title="2. 对两块磁盘进行lvm格式分区"></a>2. 对两块磁盘进行lvm格式分区</h4><pre><code>fdisk /dev/sdb/ --&gt; n --&gt; p --&gt; 分区号(默认) --&gt; 起始扇区(默认) --&gt; last扇区(默认)
# 修改分区格式
t --&gt; 8e Linux LVM(分区格式为lvm) --&gt; w (保存)

# 退出后对磁盘/dev/sdc 执行相同操作即可
</code></pre>
<h4 id="3-创建物理卷-PV"><a href="#3-创建物理卷-PV" class="headerlink" title="3. 创建物理卷(PV)"></a>3. 创建物理卷(PV)</h4><pre><code>[root@localhost ~]# pvcreate /dev/sdb1
  Physical volume "/dev/sdb1" successfully created
[root@localhost ~]# pvcreate /dev/sdc1
  Physical volume "/dev/sdc1" successfully created
[root@localhost ~]#
# pvdisplay 显示详细PV信息
  "/dev/sdb1" is a new physical volume of "20.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb1
  VG Name
  PV Size               20.00 GiB
  Allocatable           NO
  PE Size               0
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               LcLg0U-gofo-MapY-eilj-OpkQ-TYMO-nSDfh3

  "/dev/sdc1" is a new physical volume of "20.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdc1
  VG Name
  PV Size               20.00 GiB
  Allocatable           NO
  PE Size               0
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               r9Dp6u-S1hL-EmXb-6M7c-yefR-Bzts-B0Rdtd
</code></pre>
<h4 id="4-将PV加入到卷组中-VG"><a href="#4-将PV加入到卷组中-VG" class="headerlink" title="4. 将PV加入到卷组中(VG)"></a>4. 将PV加入到卷组中(VG)</h4><pre><code>[root@localhost ~]# vgcreate vg-group /dev/sdb1 /dev/sdc1
  Volume group "vg-group" successfully created
</code></pre>
<h4 id="5-创建一个10G大小的lvm"><a href="#5-创建一个10G大小的lvm" class="headerlink" title="5. 创建一个10G大小的lvm"></a>5. 创建一个10G大小的lvm</h4><pre><code>[root@localhost ~]# vgs
  VG       #PV #LV #SN Attr   VSize  VFree
  centos     1   2   0 wz--n- 19.51g     0
  vg-group   2   0   0 wz--n- 39.99g 39.99g
[root@localhost ~]# lvcreate -L 10G -n data vg-group
  Logical volume "data" created
[root@localhost ~]# lvs
  LV   VG       Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
  root centos   -wi-ao---- 17.51g
  swap centos   -wi-ao----  2.00g
  data vg-group -wi-a----- 10.00g
</code></pre>
<h4 id="6-格式化这个lvm成xfs文件系统"><a href="#6-格式化这个lvm成xfs文件系统" class="headerlink" title="6. 格式化这个lvm成xfs文件系统"></a>6. 格式化这个lvm成xfs文件系统</h4><pre><code>[root@localhost ~]# mkfs.xfs /dev/vg-group/data   # 注意这个路径是/dev/卷组名/lvm名
meta-data=/dev/vg-group/data     isize=256    agcount=4, agsize=655360 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[root@localhost ~]# lvs
  LV   VG       Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
  root centos   -wi-ao---- 17.51g
  swap centos   -wi-ao----  2.00g
  data vg-group -wi-a----- 10.00g
[root@localhost ~]#
</code></pre>
<h4 id="7-逻辑卷扩容"><a href="#7-逻辑卷扩容" class="headerlink" title="7. 逻辑卷扩容"></a>7. 逻辑卷扩容</h4><pre><code>使用 lvextend 命令进行逻辑卷扩容。我把所有剩余空间都分配给了data
[root@localhost ~]# lvextend -l +100%FREE /dev/vg-group/data
  Extending logical volume data to 39.99 GiB
  Logical volume data successfully resized
[root@localhost ~]# lvs
  LV   VG       Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
  root centos   -wi-ao---- 17.51g
  swap centos   -wi-ao----  2.00g
  data vg-group -wi-a----- 39.99g

[root@localhost ~]# mkdir /data
[root@localhost ~]# mount /dev/vg-group/data /data/
[root@localhost ~]# df -hT | grep "data"   # 可以看到这时候的data 只有10G，可是我们的lvm已经有40G
/dev/mapper/vg--group-data xfs        10G   33M   10G    1% /data
</code></pre>
<h4 id="8-使用xfs-growfs命令在线调整xfs格式文件系统大小"><a href="#8-使用xfs-growfs命令在线调整xfs格式文件系统大小" class="headerlink" title="8. 使用xfs_growfs命令在线调整xfs格式文件系统大小"></a>8. 使用xfs_growfs命令在线调整xfs格式文件系统大小</h4><pre><code>[root@localhost ~]# xfs_growfs /dev/vg-group/data
meta-data=/dev/mapper/vg--group-data isize=256    agcount=4, agsize=655360 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 2621440 to 10483712
[root@localhost ~]# df -hT | grep "data"
/dev/mapper/vg--group-data xfs        40G   33M   40G    1% /data
</code></pre>
<h4 id="9-加入现有的卷组中："><a href="#9-加入现有的卷组中：" class="headerlink" title="9 .加入现有的卷组中："></a>9 .加入现有的卷组中：</h4><pre><code>(1)# 查看现有卷组
[root@localhost ~]# vgs
  VG       #PV #LV #SN Attr   VSize  VFree
  centos     1   2   0 wz--n- 19.51g    0
  vg-group   2   1   0 wz--n- 39.99g    0

(2)#查看新加磁盘,还是先分区，然后更改lvm格式(略)
[root@localhost ~]# fdisk -l | grep sdd
磁盘 /dev/sdd：21.5 GB, 21474836480 字节，41943040 个扇区
/dev/sdd1            2048    41943039    20970496   8e  Linux LVM

(3)#使用 vgextend 命令把/dev/sdd1加入到centos
[root@localhost ~]# vgextend centos /dev/sdd1
  Volume group "centos" successfully extended
[root@localhost ~]# vgs
  VG       #PV #LV #SN Attr   VSize  VFree
  centos     2   2   0 wz--n- 39.50g 20.00g
  vg-group   2   1   0 wz--n- 39.99g     0
  
(4)使用 lvextend 命令进行逻辑卷root扩容。我把所有剩余空间都分配给了root
[root@localhost ~]# lvextend -l +100%FREE /dev/centos/root
  Extending logical volume root to 37.50 GiB
  Logical volume root successfully resized

[root@localhost ~]# xfs_growfs /dev/centos/root
meta-data=/dev/mapper/centos-root isize=256    agcount=4, agsize=1147392 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0
data     =                       bsize=4096   blocks=4589568, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal               bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 4589568 to 9831424
[root@localhost ~]#
# 此时我们的根分区就得到了扩容啦；
[root@localhost ~]# df -hT | grep "root"
/dev/mapper/centos-root    xfs        38G  929M   37G    3% /
</code></pre>
<h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><pre><code>pvs -- vgs -- lvs
</code></pre>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>lvm</tag>
      </tags>
  </entry>
  <entry>
    <title>LVS+Keepalived</title>
    <url>/2014/10/31/lvskeepalived/</url>
    <content><![CDATA[<p>说明： 本文档仅围绕lvs+keepalived如何实现负载均衡、故障剔除、后端realserver健康监测、主备切换邮件通知;而防火墙、网络(路由交换)、后端数据存储、内外网暂未考虑; <strong>一、环境准备：</strong> 1.操作系统</p>
<p>CentOS6.4-x86_64</p>
<p>2.软件版本：</p>
<p>ipvsadm-1.25-10.el6.x86_64<br>keepalived-1.2.7-3.el6.x86_64<br>httpd-2.2.15-26.el6.centos.x86_64</p>
<p>3.实验拓扑： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/1.png" alt="1"></a> 4.时间同步：</p>
<figure class="highlight autoit"><table><tbody><tr><td class="code"><pre><span class="line">node1:</span><br><span class="line">\[root<span class="symbol">@node1</span> ~\]<span class="meta"># ntpdate 203.117.180.36</span></span><br><span class="line">\[root<span class="symbol">@node1</span> ~\]<span class="meta"># echo <span class="string">"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36"</span> &gt;&gt; /etc/crontab</span></span><br><span class="line">node2:</span><br><span class="line">\[root<span class="symbol">@node2</span> ~\]<span class="meta"># ntpdate 203.117.180.36</span></span><br><span class="line">\[root<span class="symbol">@node1</span> ~\]<span class="meta"># echo <span class="string">"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36"</span> &gt;&gt; /etc/crontab</span></span><br><span class="line">master:</span><br><span class="line">\[root<span class="symbol">@master</span> ~\]<span class="meta"># ntpdate 203.117.180.36</span></span><br><span class="line">\[root<span class="symbol">@master</span> ~\]<span class="meta"># echo <span class="string">"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36"</span> &gt;&gt; /etc/crontab</span></span><br><span class="line">Slave:</span><br><span class="line">\[root<span class="symbol">@slave</span> ~\]<span class="meta"># ntpdate 203.117.180.36</span></span><br><span class="line">\[root<span class="symbol">@slave</span> ~\]<span class="meta"># echo <span class="string">"*/10 * * * * /usr/sbin/ntpdate 203.117.180.36"</span> &gt;&gt; /etc/crontab</span></span><br></pre></td></tr></tbody></table></figure>
<p>5.主机名相互解析：</p>
<figure class="highlight accesslog"><table><tbody><tr><td class="code"><pre><span class="line">node1:</span><br><span class="line">\<span class="string">[root@node1 ~\]</span># cat /etc/hosts</span><br><span class="line"><span class="number">127.0.0.1</span>&nbsp;&nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4&nbsp;</span><br><span class="line">::<span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6&nbsp;</span><br><span class="line"><span class="number">192.168.254.201</span>&nbsp;&nbsp;&nbsp; node1.test.com&nbsp;&nbsp;&nbsp; node1&nbsp;</span><br><span class="line"><span class="number">192.168.254.202</span>&nbsp;&nbsp;&nbsp; node2.test.com&nbsp;&nbsp;&nbsp; node2</span><br><span class="line">node2:</span><br><span class="line">\<span class="string">[root@node2 ~\]</span># cat /etc/hosts</span><br><span class="line"><span class="number">127.0.0.1</span>&nbsp;&nbsp; localhost localhost.localdomain localhost4 localhost4.localdomain4&nbsp;</span><br><span class="line">::<span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; localhost localhost.localdomain localhost6 localhost6.localdomain6&nbsp;</span><br><span class="line"><span class="number">192.168.254.201</span>&nbsp;&nbsp;&nbsp; node1.test.com&nbsp;&nbsp;&nbsp; node1&nbsp;</span><br><span class="line"><span class="number">192.168.254.202</span>&nbsp;&nbsp;&nbsp; node2.test.com&nbsp;&nbsp;&nbsp; node2</span><br></pre></td></tr></tbody></table></figure>
<p>6.安装yum源：(其他三台主机上面同样执行以下两条命令即可，前提:能上网)</p>
<figure class="highlight crystal"><table><tbody><tr><td class="code"><pre><span class="line"><span class="symbol">node1:</span></span><br><span class="line">\[root<span class="variable">@node1</span>~\]<span class="comment">#</span></span><br><span class="line">rpm -ivh <span class="symbol">http:</span>/<span class="regexp">/download.fedoraproject.org/pub</span><span class="regexp">/epel/</span><span class="number">6</span>/x86_64/epel-release-<span class="number">6</span>-<span class="number">8</span>.noarch.rpm</span><br><span class="line">\[root<span class="variable">@node1</span> ~\]<span class="comment"># rpm -ivh http://elrepo.org/elrepo-release-6-5.el6.elrepo.noarch.rpm</span></span><br></pre></td></tr></tbody></table></figure>
<p>*<em>二、<strong><strong>web</strong></strong>节点安装配置：</em>* 安装web服务并执行realserver.sh,为lo:0绑定VIP地址192.168.254.200，抑制ARP广播; node1/node2配置：</p>
<figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line">\[root@node1 ~\]<span class="comment"># yum install -y httpd&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;</span></span><br><span class="line">\[root@node1 ~\]<span class="comment"># echo "&lt;h1&gt;Rs1.test.com&lt;/h1&gt;" &gt; /var/www/html/index.html</span></span><br><span class="line">\[root@node1 ~\]<span class="comment"># service httpd start</span></span><br><span class="line">\[root@node1 ~\]<span class="comment"># chkconfig httpd on</span></span><br><span class="line">\[root@node1 ~\]<span class="comment"># mkdir script</span></span><br><span class="line">\[root@node1 ~\]<span class="comment"># cd script/</span></span><br><span class="line">\[root@node1 ~\]<span class="comment"># vim script/realserver.sh</span></span><br><span class="line"><span class="comment">#-&gt;LVS客户端配置脚本realserver.sh：</span></span><br><span class="line"><span class="comment">#!/bin/bash&nbsp;</span></span><br><span class="line">\<span class="comment"># Script to start LVS DR real server.&nbsp;&nbsp;</span></span><br><span class="line">\<span class="comment"># description: LVS DR real server&nbsp;&nbsp;</span></span><br><span class="line"></span><br><span class="line">.&nbsp; <span class="regexp">/etc/</span>rc.d<span class="regexp">/init.d/</span>functions</span><br><span class="line">VIP=<span class="number">192.168</span>.<span class="number">254.200</span>&nbsp;</span><br><span class="line">host=`<span class="regexp">/bin/</span>hostname`</span><br><span class="line">case <span class="string">"$1"</span> <span class="keyword">in</span>&nbsp;</span><br><span class="line">start)&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment"># Start LVS-DR real server on this machine.&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="regexp">/sbin/i</span>fconfig lo down&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="regexp">/sbin/i</span>fconfig lo up&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">1</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>lo/arp_ignore&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">2</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>lo/arp_announce&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">1</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>all/arp_ignore&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">2</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>all/arp_announce</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="regexp">/sbin/i</span>fconfig lo:<span class="number">0</span> <span class="variable">$VIP</span> broadcast <span class="variable">$VIP</span> netmask <span class="number">255.255</span>.<span class="number">255.255</span> up&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="regexp">/sbin/</span>route add -host <span class="variable">$VIP</span> dev lo:<span class="number">0</span></span><br><span class="line">;;&nbsp;</span><br><span class="line"></span><br><span class="line">stop)</span><br><span class="line"></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment"># Stop LVS-DR real server loopback device(s).&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="regexp">/sbin/i</span>fconfig lo:<span class="number">0</span> down&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">0</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>lo/arp_ignore&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">0</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>lo/arp_announce&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">0</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>all/arp_ignore&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="number">0</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/i</span>pv4<span class="regexp">/conf/</span>all/arp_announce</span><br><span class="line">;;&nbsp;</span><br><span class="line"></span><br><span class="line">status)</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment"># Status of LVS-DR real server.&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; islothere=`<span class="regexp">/sbin/i</span>fconfig lo:<span class="number">0</span> | grep <span class="variable">$VIP</span>`&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; isrothere=\`netstat -rn | grep <span class="string">"lo:0"</span> | grep <span class="variable">$VIP</span>\`&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="keyword">if</span> \[ ! <span class="string">"$islothere"</span> -o ! <span class="string">"isrothere"</span> \];then&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment"># Either the route or the lo:0 device&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment"># not found.&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="string">"LVS-DR real server Stopped."</span>&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="keyword">else</span>&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="string">"LVS-DR real server Running."</span>&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; fi&nbsp;&nbsp;</span><br><span class="line">;;&nbsp;&nbsp;</span><br><span class="line">*)&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment"># Invalid entry.&nbsp;&nbsp;</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; echo <span class="string">"$0: Usage: $0 {start|status|stop}"</span>&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="keyword">exit</span> <span class="number">1</span>&nbsp;&nbsp;</span><br><span class="line"></span><br><span class="line">;;&nbsp;&nbsp;</span><br><span class="line">esac</span><br><span class="line">\[root@node1 ~\]<span class="comment"># chmod +x script/realserver.sh</span></span><br><span class="line">\[root@node1 ~\]<span class="comment"># ./script/realserver.sh start</span></span><br></pre></td></tr></tbody></table></figure>
<p>#-&gt;将该脚本scp到node2节点执行./script/realserver.sh start即可</p>
<p>#-&gt;如果服务器重启，那还需要手动的去执行这个脚本，服务便不可使用了，所以可以将realserver.sh加入到开机启动项中;</p>
<figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line">\[root@node1 ~\]<span class="comment"># vim /etc/rc.local</span></span><br><span class="line"><span class="regexp">/bin/</span>bash <span class="regexp">/root/</span>script/realserver.sh start</span><br><span class="line">\[root@node1 ~\]<span class="comment"># scp /script/realserver 192.168.254.46:/root/script</span></span><br><span class="line"><span class="comment">#-&gt;效果如下：</span></span><br><span class="line">[![<span class="number">2</span>](https:<span class="regexp">//</span>qcloud.coding.net<span class="regexp">/u/gu</span>omaoqiu<span class="regexp">/p/gu</span>omaoqiu<span class="regexp">/git/</span>raw<span class="regexp">/master/u</span>ploads<span class="regexp">/2014/</span><span class="number">10</span><span class="regexp">/2.png)](https:/</span><span class="regexp">/qcloud.coding.net/u</span><span class="regexp">/guomaoqiu/</span>p<span class="regexp">/guomaoqiu/gi</span>t<span class="regexp">/raw/m</span>aster<span class="regexp">/uploads/</span><span class="number">2014</span><span class="regexp">/10/</span><span class="number">2</span>.png) [![<span class="number">3</span>](https:<span class="regexp">//</span>qcloud.coding.net<span class="regexp">/u/gu</span>omaoqiu<span class="regexp">/p/gu</span>omaoqiu<span class="regexp">/git/</span>raw<span class="regexp">/master/u</span>ploads<span class="regexp">/2014/</span><span class="number">10</span><span class="regexp">/3.png)](https:/</span><span class="regexp">/qcloud.coding.net/u</span><span class="regexp">/guomaoqiu/</span>p<span class="regexp">/guomaoqiu/gi</span>t<span class="regexp">/raw/m</span>aster<span class="regexp">/uploads/</span><span class="number">2014</span><span class="regexp">/10/</span><span class="number">3</span>.png)</span><br><span class="line"><span class="comment">#-&gt;客户端访问测试：</span></span><br><span class="line">[![<span class="number">4</span>](https:<span class="regexp">//</span>qcloud.coding.net<span class="regexp">/u/gu</span>omaoqiu<span class="regexp">/p/gu</span>omaoqiu<span class="regexp">/git/</span>raw<span class="regexp">/master/u</span>ploads<span class="regexp">/2014/</span><span class="number">10</span><span class="regexp">/4.png)](https:/</span><span class="regexp">/qcloud.coding.net/u</span><span class="regexp">/guomaoqiu/</span>p<span class="regexp">/guomaoqiu/gi</span>t<span class="regexp">/raw/m</span>aster<span class="regexp">/uploads/</span><span class="number">2014</span><span class="regexp">/10/</span><span class="number">4</span>.png)</span><br></pre></td></tr></tbody></table></figure>
<p>*<em>三、<strong><strong>LVS-DR-Master/Slave</strong></strong>安装配置：</em>* <strong>3.1.master/slave<strong><strong>都安装</strong></strong>keepalived<strong><strong>和</strong></strong>ipvsadm :</strong></p>
<figure class="highlight less"><table><tbody><tr><td class="code"><pre><span class="line">master:</span><br><span class="line">\[root@master ~\]# yum install -<span class="attribute">y ipvsadm keepalived</span></span><br><span class="line"><span class="attribute">\[root@master ~\]# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak&nbsp; #-&gt;修改配置文件之前最好将该配置文件备份。避免后续出现问题</span></span><br><span class="line"><span class="attribute">Slave</span>:</span><br><span class="line">\[root<span class="variable">@slave</span> ~\]# yum install -y ipvsadm keepalived</span><br><span class="line">\[root<span class="variable">@slave</span> ~\]# cp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak</span><br><span class="line"></span><br><span class="line">**<span class="number">3.2</span>.****在****Lvs-DR-Master****上面的配置：**</span><br><span class="line"></span><br><span class="line">\[root<span class="variable">@master</span> ~\]# vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs {</span><br><span class="line">&nbsp;&nbsp; notification_email {</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="number">2399447849</span><span class="variable">@qq</span>.com&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#-&gt;#设置报警邮件地址，可以设置多个，每行一个</span><br><span class="line">&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp; <span class="selector-tag">notification</span>\<span class="selector-tag">_email</span>\<span class="selector-tag">_from</span> <span class="selector-tag">root</span>@<span class="selector-tag">localhost</span><span class="selector-class">.localdomain</span></span><br><span class="line">&nbsp;&nbsp; <span class="selector-tag">smtp_server</span> <span class="number">127.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="selector-id">#-</span>&gt;设置<span class="selector-tag">smtp</span> <span class="selector-tag">server</span>的地址</span><br><span class="line">&nbsp;&nbsp; <span class="selector-tag">smtp</span>\<span class="selector-tag">_connect</span>\<span class="selector-tag">_timeout</span> <span class="number">30</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="selector-id">#-</span>&gt;设置连接<span class="selector-tag">smtp</span> <span class="selector-tag">server</span>的超时时间</span><br><span class="line">&nbsp;&nbsp; <span class="selector-tag">router</span>\<span class="selector-tag">_id</span> <span class="selector-tag">LVS</span>\<span class="selector-tag">_DEVEL</span>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="selector-id">#-</span>&gt;表示运行<span class="selector-tag">keepalived</span>服务器的一个标识。发邮件时显示在邮件主题的信息</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">vrrp</span>\<span class="selector-tag">_instance</span> <span class="selector-tag">VI</span>\<span class="selector-tag">_1</span> {</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">state</span> <span class="selector-tag">MASTER</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;指定<span class="selector-tag">keepalived</span>的角色，<span class="selector-tag">MASTER</span>表示此主机是主服务器，<span class="selector-tag">BACKUP</span>表示此主机是备用服务器</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">interface</span> <span class="selector-tag">eth0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;指定<span class="selector-tag">HA</span>监测网络的接口</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">virtual</span>\<span class="selector-tag">_router</span>\<span class="selector-tag">_id</span> <span class="number">60</span> &nbsp;<span class="selector-id">#-</span>&gt;虚拟路由标识，这个标识是一个数字,同一个<span class="selector-tag">vrrp</span>实例使用唯一的标识。即同一<span class="selector-tag">vrrp_instance</span>下,<span class="selector-tag">MASTER</span>和<span class="selector-tag">BACKUP</span>&gt;必须是一致的</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">priority</span> <span class="number">101</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;定义优先级，数字越大，优先级越高，在同一个<span class="selector-tag">vrrp_instance</span>下，<span class="selector-tag">MASTER</span>的优先级必须大于<span class="selector-tag">BACKUP</span>的优先级</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">advert_int</span> <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设定<span class="selector-tag">MASTER</span>与<span class="selector-tag">BACKUP</span>负载均衡器之间同步检查的时间间隔，单位是秒</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">authentication</span> {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设置验证类型和密码</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">auth_type</span> <span class="selector-tag">PASS</span>&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设置验证类型，主要有<span class="selector-tag">PASS</span>和<span class="selector-tag">AH</span>两种</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">auth</span>\<span class="selector-tag">_pass</span> <span class="number">1111</span>&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设置验证密码，在同一个<span class="selector-tag">vrrp</span>\<span class="selector-tag">_instance</span>下，<span class="selector-tag">MASTER</span>与<span class="selector-tag">BACKUP</span>必须使用相同的密码才能正常通信</span><br><span class="line">&nbsp;&nbsp; }</span><br><span class="line"></span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">virtual_ipaddress</span> {&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设置虚拟<span class="selector-tag">IP</span>地址，可以设置多个虚拟<span class="selector-tag">IP</span>地址，每行一个</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">192.168</span><span class="selector-class">.254</span><span class="selector-class">.200</span>&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;客户端通过访问的就是该<span class="selector-tag">IP</span>地址</span><br><span class="line">&nbsp;&nbsp;&nbsp; }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">virtual_server</span> <span class="number">192.168</span><span class="selector-class">.254</span><span class="selector-class">.200</span> <span class="number">80</span> {&nbsp; <span class="selector-id">#-</span>&gt;设置虚拟服务器，需要指定虚拟<span class="selector-tag">IP</span>地址和服务端口，<span class="selector-tag">IP</span>与端口之间用空格隔开</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">delay_loop</span> <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设置运行情况检查时间，单位是秒</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">lb_algo</span> <span class="selector-tag">rr</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设置负载调度算法，这里设置为<span class="selector-tag">rr</span>，即轮询算法</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">lb_kind</span> <span class="selector-tag">DR</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;设置<span class="selector-tag">LVS</span>实现负载均衡的机制，有<span class="selector-tag">NAT</span>、<span class="selector-tag">TUN</span>、<span class="selector-tag">DR</span>三个模式可选</span><br><span class="line"><span class="selector-tag">nat_mask</span> <span class="number">255.255</span><span class="selector-class">.255</span><span class="selector-class">.0</span></span><br><span class="line"><span class="selector-id">#persistence_timeout</span> <span class="number">50</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;会话保持时间，单位是秒。这个选项对动态网页是非常有用的，为集群系统中的<span class="selector-tag">session</span>共享提供了一个很好&gt;的解决方案。</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;有了这个会话保持功能，用户的请求会被一直分发到某个服务节点，直到超过这个会话的保持时间。</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;需要注意的是，这个会话保持时间是最大无响应超时时间，也就是说，用户在操作动态页面时，如果<span class="number">50</span>秒内没</span><br><span class="line">有执行任何操作，</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">protocol</span> <span class="selector-tag">TCP</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;指定转发协议类型，有<span class="selector-tag">TCP</span>和<span class="selector-tag">UDP</span>两种</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">real_server</span> <span class="number">192.168</span><span class="selector-class">.254</span><span class="selector-class">.45</span> <span class="number">80</span>&nbsp; {&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;配置服务节点<span class="number">1</span>，需要指定<span class="selector-tag">real</span> <span class="selector-tag">server</span>的真实<span class="selector-tag">IP</span>地址和端口，<span class="selector-tag">IP</span>与端口之间用空格隔开</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">weight</span> <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;配置服务节点的权值，权值大小用数字表示，数字越大，权值越高，设置权值大小可以为不同性能的服务器</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;分配不同的负载，可以为性能高的服务器设置较高的权值，而为性能较低的服务器设置相对较低的权值，这样</span><br><span class="line">才能合理地利用和分配系统资源</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">HTTP_GET</span> {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;<span class="selector-tag">realserver</span>的状态检测设置部分，单位是秒</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">url</span> {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">path</span> /</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">status_code</span> <span class="number">200</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;状态码定义</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">connect_timeout</span> <span class="number">3</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;表示<span class="number">3</span>秒无响应超时</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">nb</span>\<span class="selector-tag">_get</span>\<span class="selector-tag">_retry</span> <span class="number">3</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;表示重试次数</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">delay</span>\<span class="selector-tag">_before</span>\<span class="selector-tag">_retry</span> <span class="number">3</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-id">#-</span>&gt;表示重试间隔</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line"></span><br><span class="line">&nbsp;&nbsp;&nbsp; }</span><br><span class="line"></span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="selector-tag">real_server</span> <span class="number">192.168</span><span class="selector-class">.254</span><span class="selector-class">.46</span> <span class="number">80</span> {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">weight</span> <span class="number">1</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">HTTP_GET</span> {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">url</span> {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">path</span> /</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">status_code</span> <span class="number">200</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">connect_timeout</span> <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">nb</span>\<span class="selector-tag">_get</span>\<span class="selector-tag">_retry</span> <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="selector-tag">delay</span>\<span class="selector-tag">_before</span>\<span class="selector-tag">_retry</span> <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp; }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>#-&gt;根据实验的拓扑可知后端的realserver有两台，所以上面定义了两个realserver的配置</p>
<p><strong>3.3<strong><strong>在</strong></strong>Lvs-DR-Slave****上面的配置：</strong></p>
<p>[root@master ~]# scp /etc/keepalived/keepalived.conf 192.168.254.48:/etc/keepalived/&nbsp; #-&gt;将master上面的配置复制至slave,然后稍作修改：<br>[root@slave ~]# vim /etc/keepalived/keepalived.conf<br>#-&gt;配置文件各个参数上面以解释，这里只对需要修改的作说明：</p>
<figure class="highlight livescript"><table><tbody><tr><td class="code"><pre><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">global_defs {</span><br><span class="line">&nbsp;&nbsp; notification_email {</span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="number">2399447849</span>@qq.com</span><br><span class="line">&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp; notification<span class="string">\_email\_from</span> root</span><br><span class="line">&nbsp;&nbsp; smtp_server <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line">&nbsp;&nbsp; smtp<span class="string">\_connect\_timeout</span> <span class="number">30</span></span><br><span class="line">&nbsp;&nbsp; router<span class="string">\_id</span> LVS<span class="string">\_DEVEL</span></span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">&nbsp;</span><br><span class="line"></span><br><span class="line">vrrp<span class="string">\_instance</span> VI<span class="string">\_1</span> {</span><br><span class="line">&nbsp;&nbsp;&nbsp; state BACKUP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;指定该服务器的keepalived角色为BACKUP(备用服务器)</span></span><br><span class="line">&nbsp;&nbsp;  interface eth0</span><br><span class="line">&nbsp;&nbsp;  virtual<span class="string">\_router\_id</span> <span class="number">60</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp;&nbsp;&nbsp; priority <span class="number">100</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;在同一个vrrp_instance下，MASTER的优先级必须大于BACKUP的优先级</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; advert_int <span class="number">1</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; authentication {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; auth_type PASS</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; auth_pass <span class="number">1111</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; }</span><br><span class="line"></span><br><span class="line">&nbsp;&nbsp;&nbsp; virtual_ipaddress {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">192.168</span>.<span class="number">254.200</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">&nbsp;</span><br><span class="line"></span><br><span class="line">virtual_server <span class="number">192.168</span>.<span class="number">254.200</span> <span class="number">80</span> {</span><br><span class="line">&nbsp;&nbsp;&nbsp; delay_loop <span class="number">6</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; lb_algo rr</span><br><span class="line">&nbsp;&nbsp;&nbsp; lb_kind DR</span><br><span class="line">&nbsp;&nbsp; nat_mask <span class="number">255.255</span>.<span class="number">255.0</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; <span class="comment">#persistence_timeout 50</span></span><br><span class="line">protocol TCP</span><br><span class="line">&nbsp;&nbsp; real_server <span class="number">192.168</span>.<span class="number">254.45</span> <span class="number">80</span>&nbsp; {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; weight <span class="number">1</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HTTP_GET {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; url {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path /</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; status_code <span class="number">200</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; connect_timeout <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp; real_server <span class="number">192.168</span>.<span class="number">254.46</span> <span class="number">80</span> {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; weight <span class="number">1</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; HTTP_GET {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; url {</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path /</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; status_code <span class="number">200</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; connect_timeout <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span><br><span class="line">&nbsp;&nbsp;&nbsp; }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>*<em>3.4.<strong><strong>启动</strong></strong>keepalived (<strong><strong>启动过程中观察日志</strong></strong>)</em>*</p>
<figure class="highlight livescript"><table><tbody><tr><td class="code"><pre><span class="line">master:</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> service keepalived start &amp;&amp; tail -f /<span class="keyword">var</span>/log/messages</span><br><span class="line">...........</span><br><span class="line">...........</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">14</span> master Keepalived_healthcheckers<span class="string">\[31358\]:</span> Opening file <span class="string">'/etc/keepalived/keepalived.conf'</span>.</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">14</span> master Keepalived_healthcheckers<span class="string">\[31358\]:</span> Configuration <span class="keyword">is</span> <span class="keyword">using</span> : <span class="number">16384</span> Bytes</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">14</span> master Keepalived_healthcheckers<span class="string">\[31358\]:</span> Using LinkWatch kernel netlink reflector...</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">14</span> master Keepalived_healthcheckers<span class="string">\[31358\]:</span> Activating healthchecker <span class="keyword">for</span> service <span class="string">\[192.168.254.45\]:80</span></span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">14</span> master Keepalived_healthcheckers<span class="string">\[31358\]:</span> Activating healthchecker <span class="keyword">for</span> service <span class="string">\[192.168.254.46\]:80</span></span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">15</span> master Keepalived<span class="string">\_vrrp\[31359\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> Transition <span class="keyword">to</span> MASTER STATE</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">16</span> master Keepalived<span class="string">\_vrrp\[31359\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> Entering MASTER STATE&nbsp; <span class="comment">#-&gt;主服务器状态</span></span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">16</span> master Keepalived<span class="string">\_vrrp\[31359\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> setting protocol VIPs.</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">16</span> master Keepalived<span class="string">\_vrrp\[31359\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> Sending gratuitous ARPs <span class="literal">on</span> eth0 <span class="keyword">for</span> <span class="number">192.168</span>.<span class="number">254.200</span></span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">16</span> master Keepalived_healthcheckers<span class="string">\[31358\]:</span> Netlink reflector reports IP <span class="number">192.168</span>.<span class="number">254.200</span> added</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">21</span> master Keepalived<span class="string">\_vrrp\[31359\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> Sending gratuitous ARPs <span class="literal">on</span> eth0 <span class="keyword">for</span> <span class="number">192.168</span>.<span class="number">254.200</span></span><br><span class="line">...........</span><br><span class="line">...........</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> ipvsadm -L –n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;LVS状态</span></span><br><span class="line">IP Virtual Server version <span class="number">1.2</span>.<span class="number">1</span> (size=<span class="number">4096</span>)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">&nbsp; -<span class="string">\&gt;</span> RemoteAddress:Port&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Forward Weight ActiveConn InActConn</span><br><span class="line">TCP&nbsp; <span class="number">192.168</span>.<span class="number">254.200</span>:<span class="number">80</span> rr</span><br><span class="line">&nbsp; -<span class="string">\&gt;</span> <span class="number">192.168</span>.<span class="number">254.45</span>:<span class="number">80</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp; -<span class="string">\&gt;</span> <span class="number">192.168</span>.<span class="number">254.46</span>:<span class="number">80</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> ip a</span><br><span class="line"><span class="number">1</span>: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class="number">16436</span> qdisc noqueue state UNKNOWN</span><br><span class="line">&nbsp;&nbsp;&nbsp; link/loopback <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> brd <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; inet <span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">8</span> scope host lo</span><br><span class="line">&nbsp;&nbsp;&nbsp; inet6 ::<span class="number">1</span>/<span class="number">128</span> scope host</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; valid<span class="string">\_lft</span> forever preferred<span class="string">\_lft</span> forever</span><br><span class="line"><span class="number">2</span>: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER<span class="string">\_UP&gt;</span> mtu <span class="number">1500</span> qdisc pfifo<span class="string">\_fast</span> state UP qlen <span class="number">1000</span></span><br><span class="line">&nbsp;&nbsp; link/ether <span class="number">00</span>:<span class="number">0c</span>:<span class="number">29</span>:<span class="number">5d</span>:<span class="number">7d</span>:<span class="number">94</span> brd ff:ff:ff:ff:ff:ff</span><br><span class="line">&nbsp;&nbsp;&nbsp; inet <span class="number">192.168</span>.<span class="number">254.47</span>/<span class="number">24</span> brd <span class="number">192.168</span>.<span class="number">254.255</span> scope global eth0</span><br><span class="line">&nbsp;&nbsp;&nbsp; inet <span class="number">192.168</span>.<span class="number">254.200</span>/<span class="number">32</span> scope global eth0&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;此时VIP在master上面</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; inet6 fe80::<span class="number">20c</span>:<span class="number">29ff</span>:fe5d:<span class="number">7d</span>94/<span class="number">64</span> scope link</span><br><span class="line">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; valid<span class="string">\_lft</span> forever preferred<span class="string">\_lft</span> forever</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span></span><br><span class="line">Slave:</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> service keepalived start &amp;&amp; tail -f /<span class="keyword">var</span>/log/messages</span><br><span class="line">...........</span><br><span class="line">...........</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_vrrp<span class="string">\[31389\]:</span> Opening file <span class="string">'/etc/keepalived/keepalived.conf'</span>.</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_vrrp<span class="string">\[31389\]:</span> Configuration <span class="keyword">is</span> <span class="keyword">using</span> : <span class="number">62845</span> Bytes</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_vrrp<span class="string">\[31389\]:</span> Using LinkWatch kernel netlink reflector...</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived<span class="string">\_vrrp\[31389\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> Entering BACKUP STATE&nbsp; <span class="comment">#-&gt;备用服务器状态</span></span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_vrrp<span class="string">\[31389\]:</span> VRRP sockpool: <span class="string">\[ifindex(2),</span> proto(<span class="number">112</span>), fd(<span class="number">10</span>,<span class="number">11</span>)<span class="string">\]</span></span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_healthcheckers<span class="string">\[31388\]:</span> Opening file <span class="string">'/etc/keepalived/keepalived.conf'</span>.</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_healthcheckers<span class="string">\[31388\]:</span> Configuration <span class="keyword">is</span> <span class="keyword">using</span> : <span class="number">16384</span> Bytes</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_healthcheckers<span class="string">\[31388\]:</span> Using LinkWatch kernel netlink reflector...</span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_healthcheckers<span class="string">\[31388\]:</span> Activating healthchecker <span class="keyword">for</span> service <span class="string">\[192.168.254.45\]:80</span></span><br><span class="line">Oct <span class="number">29</span> <span class="number">21</span>:<span class="number">42</span>:<span class="number">34</span> slave Keepalived_healthcheckers<span class="string">\[31388\]:</span> Activating healthchecker <span class="keyword">for</span> service <span class="string">\[192.168.254.46\]:80</span></span><br><span class="line">...........</span><br><span class="line">...........</span><br><span class="line"></span><br><span class="line">**<span class="number">3.5</span>.****测试：** [![<span class="number">5</span>](https:<span class="regexp">//qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/5.png)](https://</span>qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/<span class="number">2014</span>/<span class="number">10</span>/<span class="number">5.png</span>) **<span class="number">3.6</span>.****模拟故障**： (<span class="number">1</span>)停掉node1节点的web服务：</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@node1</span> ~<span class="string">\]#</span> service httpd stop</span><br><span class="line">停止 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@node1</span> ~<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">(<span class="number">2</span>)查看一下报警邮件： [![<span class="number">6</span>](https:<span class="regexp">//qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/6.png)](https://</span>qcloud.coding.net<span class="regexp">/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/6.png) (3)再在前端调度器上查看一下LVS状态： [![7](https:/</span><span class="regexp">/qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/7.png)](https:/</span>/qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/<span class="number">2014</span>/<span class="number">10</span>/<span class="number">7.png</span>) 很明显那台出现问题的realserver条目已经被剔除了 (<span class="number">4</span>)恢复node1节点上的web服务：</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@node1</span> ~<span class="string">\]#</span> service httpd start</span><br><span class="line">启动 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@node1</span> ~<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">(<span class="number">5</span>) 查看一下报警邮件： [![<span class="number">8</span>](https:<span class="regexp">//qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/8.png)](https://</span>qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/<span class="number">2014</span>/<span class="number">10</span>/<span class="number">8.png</span>) (<span class="number">6</span>)关闭master上面的keepalived：</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> service keepalived stop</span><br><span class="line">停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> ipvsadm -L -n</span><br><span class="line">IP Virtual Server version <span class="number">1.2</span>.<span class="number">1</span> (size=<span class="number">4096</span>)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">&nbsp; -<span class="string">\&gt;</span> RemoteAddress:Port&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Forward Weight ActiveConn InActConn</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">(<span class="number">7</span>)查看slave状态：</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> ip a</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">&nbsp;&nbsp;&nbsp; inet <span class="number">192.168</span>.<span class="number">254.200</span>/<span class="number">32</span> scope global eth0&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;可见VIP已经转移到了slave上面；并且通过客户端访问仍然正常！</span></span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">&nbsp;<span class="string">\[root@slave</span> ~<span class="string">\]#</span> ipvsadm -L -n</span><br><span class="line">IP Virtual Server version <span class="number">1.2</span>.<span class="number">1</span> (size=<span class="number">4096</span>)</span><br><span class="line"></span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"></span><br><span class="line">&nbsp; -<span class="string">\&gt;</span> RemoteAddress:Port&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Forward Weight ActiveConn InActConn</span><br><span class="line"></span><br><span class="line">TCP&nbsp; <span class="number">192.168</span>.<span class="number">254.200</span>:<span class="number">80</span> rr</span><br><span class="line">&nbsp; -<span class="string">\&gt;</span> <span class="number">192.168</span>.<span class="number">254.45</span>:<span class="number">80</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br><span class="line">&nbsp; -<span class="string">\&gt;</span> <span class="number">192.168</span>.<span class="number">254.46</span>:<span class="number">80</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Route&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">**通过上面的演示现在的****LVS****的高可用即前端负载均衡调度器的高可用，同时实现了对后端****realserver****监控，也实现了后端****realserver****宕机时会给管理员发送邮件；但是目前还面临几个问题：**</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>  **如果所有的****realserver****都宕机，如何处理，用户打不开就等它打不开，还是友善的提示一下？**</span><br><span class="line"><span class="number">2.</span>  **怎么完成维护模式****keepalived****切换？**</span><br><span class="line"><span class="number">3.</span>  **如何在****keepalived****主备切换时向管理员发送邮件？**</span><br><span class="line"></span><br><span class="line">**四、****LVS+Keepalived****后续延伸：** **<span class="number">4.1</span>.****所有****realserver****都宕机如何处理？** 在集群中如果所有real server全部宕机了，客户端访问时就会出现错误页面，这样是很不友好的，我们得提供一个维护页面来提醒用户，服务器正在维护，什么时间可以访问等，下面就来解决一下这个问题。解决方案有两种，一种是提供一台备用的real server当所有的服务器宕机时，提供维护页面，但这样做有点浪费服务器。另一种就是在负载均衡器上提供维护页面，这样是比较靠谱的，也比较常用。下面就来具体操作一下。 (<span class="number">1</span>)在master和slave上面安装httpd</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> yum install -y httpd</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> yum install -y httpd</span><br><span class="line"></span><br><span class="line">(<span class="number">2</span>)提供维护页面文件</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> echo <span class="string">"Oops ... you visit the page does not exist, the server may be maintained?"</span> &gt; /<span class="keyword">var</span>/www/html/index.html</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> service httpd start</span><br><span class="line">启动 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> echo <span class="string">"Oops ... you visit the page does not exist, the server may be maintained?"</span> &gt; /<span class="keyword">var</span>/www/html/index.html</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> service httpd start</span><br><span class="line">启动 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"></span><br><span class="line">(<span class="number">3</span>)测试： [![<span class="number">9</span>](https:<span class="regexp">//qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/9.png)](https://</span>qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/<span class="number">2014</span>/<span class="number">10</span>/<span class="number">9.png</span>) (<span class="number">4</span>)修改master/slave的keepalived配置文件：</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs {</span><br><span class="line">notification_email {</span><br><span class="line"><span class="number">2399447849</span>@qq.com</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">notification<span class="string">\_email\_from</span> root</span><br><span class="line">smtp_server <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line">smtp<span class="string">\_connect\_timeout</span> <span class="number">30</span></span><br><span class="line">router<span class="string">\_id</span> LVS<span class="string">\_DEVEL</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">vrrp<span class="string">\_instance</span> VI<span class="string">\_1</span> {</span><br><span class="line">state MASTER</span><br><span class="line">interface eth0</span><br><span class="line">       virtual<span class="string">\_router\_id</span> <span class="number">60</span></span><br><span class="line">       priority <span class="number">101</span></span><br><span class="line">       advert_int <span class="number">1</span></span><br><span class="line">      authentication {</span><br><span class="line">       auth_type PASS</span><br><span class="line">      auth_pass <span class="number">1111</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">virtual_ipaddress {</span><br><span class="line">     <span class="number">192.168</span>.<span class="number">254.200</span></span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">virtual_server <span class="number">192.168</span>.<span class="number">254.200</span> <span class="number">80</span> {</span><br><span class="line">delay_loop <span class="number">6</span></span><br><span class="line">lb_algo rr</span><br><span class="line">lb_kind DR</span><br><span class="line">nat_mask <span class="number">255.255</span>.<span class="number">255.0</span></span><br><span class="line"><span class="comment">#persistence_timeout 50</span></span><br><span class="line">protocol TCP</span><br><span class="line"></span><br><span class="line">real_server <span class="number">192.168</span>.<span class="number">254.45</span> <span class="number">80</span>&amp;nbsp; {</span><br><span class="line">weight <span class="number">1</span></span><br><span class="line">HTTP_GET {</span><br><span class="line"></span><br><span class="line">  url {</span><br><span class="line">   path /</span><br><span class="line">   status_code <span class="number">200</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">connect_timeout <span class="number">3</span></span><br><span class="line">nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">   }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">real_server <span class="number">192.168</span>.<span class="number">254.46</span> <span class="number">80</span> {</span><br><span class="line">weight <span class="number">1</span></span><br><span class="line">HTTP_GET {</span><br><span class="line">url {</span><br><span class="line">path /</span><br><span class="line">status_code <span class="number">200</span></span><br><span class="line">}</span><br><span class="line">connect_timeout <span class="number">3</span></span><br><span class="line">nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">sorry_server <span class="number">127.0</span>.<span class="number">0.1</span>&amp;nbsp;&amp;nbsp; <span class="comment">#-&gt;增加该配置参数，slave上面也需要添加，此处略。</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">(<span class="number">5</span>)关闭所有的realserver web服务,重新启动master/slave 的keepalived: &nbsp;</span><br><span class="line"></span><br><span class="line">node1:</span><br><span class="line"><span class="string">\[root@node1</span> ~<span class="string">\]#</span> service httpd stop</span><br><span class="line">停止 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@node1</span> ~<span class="string">\]#</span></span><br><span class="line">node2:</span><br><span class="line"><span class="string">\[root@node2</span> ~<span class="string">\]#</span> service httpd stop</span><br><span class="line">停止 httpd：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@node2</span> ~<span class="string">\]#</span></span><br><span class="line">master:</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> service keepalived restart</span><br><span class="line">停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line">正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span></span><br><span class="line">slave:</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> service keepalived restart</span><br><span class="line">停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line">正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">(<span class="number">6</span>)查看一下LVS状态： [![<span class="number">10</span>](https:<span class="regexp">//qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/10.png)](https://</span>qcloud.coding.net<span class="regexp">/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/10.png) (7)访问测试： [![11](https:/</span><span class="regexp">/qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/11.png)](https:/</span>/qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/<span class="number">2014</span>/<span class="number">10</span>/<span class="number">11.png</span>) **<span class="number">4.2</span>.****如何完成维护模式****keepalived** **切换？** 一般我们在测试主从切换的过程当中要么是手动停止keepalived服务，要么是手动关闭网卡，那还有其他方法实现维护模式的切换，这就是vrrp_script功能； (<span class="number">1</span>)master/slave配置：**(****注****:****这里演示主服务器的配置，添加上去的在****slave****上面也需要添加以红色标注内容****)**</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> vim /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs {</span><br><span class="line"></span><br><span class="line">notification_email {</span><br><span class="line"><span class="number">2399447849</span>@qq.com</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">notification<span class="string">\_email\_from</span> root</span><br><span class="line">smtp_server <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line">smtp<span class="string">\_connect\_timeout</span> <span class="number">30</span></span><br><span class="line">router<span class="string">\_id</span> LVS<span class="string">\_DEVEL</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">vrrp<span class="string">\_script</span> chk<span class="string">\_schedown</span> {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;定义vrrp执行脚本</span></span><br><span class="line">&nbsp;&nbsp; script <span class="string">"\[ -e /etc/keepalived/down \] &amp;&amp; exit 1 || exit 0"</span> &nbsp;<span class="comment">#-&gt;查看是否有down文件，有就进入维护模式</span></span><br><span class="line">&nbsp;&nbsp; interval <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;监控间隔时间</span></span><br><span class="line">&nbsp;&nbsp; weight -<span class="number">5</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;降低优先级,即priority参数</span></span><br><span class="line">&nbsp;&nbsp; fall <span class="number">2</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;失败次数</span></span><br><span class="line">&nbsp;&nbsp; rise <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="comment">#-&gt;成功次数</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">vrrp<span class="string">\_instance</span> VI<span class="string">\_1</span> {</span><br><span class="line">state MASTER</span><br><span class="line">interface eth0</span><br><span class="line">virtual<span class="string">\_router\_id</span> <span class="number">60</span></span><br><span class="line">priority <span class="number">101</span></span><br><span class="line">advert_int <span class="number">1</span></span><br><span class="line">authentication {</span><br><span class="line">auth_type PASS</span><br><span class="line">auth_pass <span class="number">1111</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">virtual_ipaddress {</span><br><span class="line"><span class="number">192.168</span>.<span class="number">254.200</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">track_script {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="comment">#-&gt;脚本追踪</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; chk_schedown&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;<span class="comment">#-&gt;上面自定义的vrrp脚本名称</span></span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">virtual_server <span class="number">192.168</span>.<span class="number">254.200</span> <span class="number">80</span> {</span><br><span class="line">delay_loop <span class="number">6</span></span><br><span class="line">lb_algo rr</span><br><span class="line">lb_kind DR</span><br><span class="line">nat_mask <span class="number">255.255</span>.<span class="number">255.0</span></span><br><span class="line"><span class="comment">#persistence_timeout 50</span></span><br><span class="line">protocol TCP</span><br><span class="line">&nbsp;</span><br><span class="line">real_server <span class="number">192.168</span>.<span class="number">254.45</span> <span class="number">80</span>&nbsp; {</span><br><span class="line">weight <span class="number">1</span></span><br><span class="line">HTTP_GET {</span><br><span class="line">url {</span><br><span class="line">path /</span><br><span class="line">status_code <span class="number">200</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">connect_timeout <span class="number">3</span></span><br><span class="line">nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">real_server <span class="number">192.168</span>.<span class="number">254.46</span> <span class="number">80</span> {</span><br><span class="line">weight <span class="number">1</span></span><br><span class="line">HTTP_GET {</span><br><span class="line">url {</span><br><span class="line">path /</span><br><span class="line">status_code <span class="number">200</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">connect_timeout <span class="number">3</span></span><br><span class="line">nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">sorry_server <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">(<span class="number">2</span>)测试： </span><br><span class="line"></span><br><span class="line">master:</span><br><span class="line"><span class="string">\[root@master</span> keepalived<span class="string">\]#</span> touch down &nbsp;<span class="comment">#-&gt;新建一个down文件，进入维护模式</span></span><br><span class="line"><span class="string">\[root@master</span> keepalived<span class="string">\]#</span> ll</span><br><span class="line">总用量 <span class="number">4</span></span><br><span class="line">-rw-r--r--. <span class="number">1</span> root root&nbsp;&nbsp;&nbsp; <span class="number">0</span> <span class="number">10</span>月 <span class="number">30</span> <span class="number">00</span>:<span class="number">16</span> down</span><br><span class="line">-rw-r--r--. <span class="number">1</span> root root <span class="number">1513</span> <span class="number">10</span>月 <span class="number">30</span> <span class="number">00</span>:<span class="number">08</span> keepalived.conf</span><br><span class="line"><span class="string">\[root@master</span> keepalived<span class="string">\]#</span> tail -f /<span class="keyword">var</span>/log/messages</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">Oct <span class="number">30</span> <span class="number">00</span>:<span class="number">16</span>:<span class="number">43</span> node3 Keepalived<span class="string">\_vrrp\[31993\]:</span> VRRP<span class="string">\_Script(chk_schedown)</span> failed</span><br><span class="line">Oct <span class="number">30</span> <span class="number">00</span>:<span class="number">16</span>:<span class="number">44</span> node3 Keepalived<span class="string">\_vrrp\[31993\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> Received higher prio advert</span><br><span class="line">Oct <span class="number">30</span> <span class="number">00</span>:<span class="number">16</span>:<span class="number">44</span> node3 Keepalived<span class="string">\_vrrp\[31993\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> Entering BACKUP STATE</span><br><span class="line">Oct <span class="number">30</span> <span class="number">00</span>:<span class="number">16</span>:<span class="number">44</span> node3 Keepalived<span class="string">\_vrrp\[31993\]:</span> VRRP<span class="string">\_Instance(VI_1)</span> removing protocol VIPs.</span><br><span class="line">Oct <span class="number">30</span> <span class="number">00</span>:<span class="number">16</span>:<span class="number">44</span> node3 Keepalived_healthcheckers<span class="string">\[31992\]:</span> Netlink reflector reports IP <span class="number">192.168</span>.<span class="number">254.200</span> removed&nbsp; <span class="comment">#-&gt;该VIP已转移到slave.</span></span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line"><span class="string">\[root@master</span> keepalived<span class="string">\]#</span> ip a&nbsp;&nbsp; <span class="comment">#-&gt;VIP 已转移到slave</span></span><br><span class="line"><span class="number">1</span>: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class="number">16436</span> qdisc noqueue state UNKNOWN</span><br><span class="line">link/loopback <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> brd <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">inet <span class="number">127.0</span>.<span class="number">0.1</span>/<span class="number">8</span> scope host lo</span><br><span class="line">inet6 ::<span class="number">1</span>/<span class="number">128</span> scope host</span><br><span class="line">valid<span class="string">\_lft</span> forever preferred<span class="string">\_lft</span> forever</span><br><span class="line"><span class="number">2</span>: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER<span class="string">\_UP&gt;</span> mtu <span class="number">1500</span> qdisc pfifo<span class="string">\_fast</span> state UP qlen <span class="number">1000</span></span><br><span class="line">link/ether <span class="number">00</span>:<span class="number">0c</span>:<span class="number">29</span>:<span class="number">5d</span>:<span class="number">7d</span>:<span class="number">94</span> brd ff:ff:ff:ff:ff:ff</span><br><span class="line">inet <span class="number">192.168</span>.<span class="number">254.47</span>/<span class="number">24</span> brd <span class="number">192.168</span>.<span class="number">254.255</span> scope global eth0</span><br><span class="line">inet6 fe80::<span class="number">20c</span>:<span class="number">29ff</span>:fe5d:<span class="number">7d</span>94/<span class="number">64</span> scope link</span><br><span class="line">valid<span class="string">\_lft</span> forever preferred<span class="string">\_lft</span> forever</span><br><span class="line"><span class="string">\[root@master</span> keepalived<span class="string">\]#</span></span><br><span class="line">slave:</span><br><span class="line"><span class="string">\[root@slave</span> keepalived<span class="string">\]#</span> ip a</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">inet <span class="number">192.168</span>.<span class="number">254.200</span>/<span class="number">32</span> scope global eth0&nbsp;&nbsp; <span class="comment">#-&gt;VIP 已转移过来</span></span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line"><span class="string">\[root@slave</span> keepalived<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">&nbsp; **至此自写监测脚本，完成维护模式切换，已经完成；下面来解决最后一个问题：** **<span class="number">4.3</span>.****如何在****Keepalived****主从切换时向管理员发送通知邮件？** (<span class="number">1</span>)Keepalived通知脚本进阶示例：</span><br><span class="line"></span><br><span class="line">下面的脚本可以接受选项，其中：</span><br><span class="line">-s, --service SERVICE,...：指定服务脚本名称，当状态切换时可自动启动、重启或关闭此服务；</span><br><span class="line">-a, --address VIP: 指定相关虚拟路由器的VIP地址；</span><br><span class="line">-m, --mode {mm|mb}：指定虚拟路由的模型，mm表示主主，mb表示主备；它们表示相对于同一种服务而方，其VIP的工作类型；</span><br><span class="line">-n, --notify {master|backup|fault}：指定通知的类型，即vrrp角色切换的目标角色；</span><br><span class="line">-h, --help：获取脚本的使用帮助；</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="string">\#</span> Author: Tux</span><br><span class="line"><span class="string">\#</span> description: An example <span class="keyword">of</span> notify script</span><br><span class="line"><span class="string">\#</span> Usage: notify.sh -m|--mode {mm|mb} -s|--service SERVICE1,... -a|--address VIP&amp;nbsp; -n|--notify {master|backup|falut} -h|--help</span><br><span class="line"></span><br><span class="line">&amp;nbsp;</span><br><span class="line"></span><br><span class="line">helpflag=<span class="number">0</span></span><br><span class="line">serviceflag=<span class="number">0</span></span><br><span class="line">modeflag=<span class="number">0</span></span><br><span class="line">addressflag=<span class="number">0</span></span><br><span class="line">notifyflag=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">contact=<span class="string">'2399447849@qq.com'</span>&amp;nbsp;&amp;nbsp; <span class="comment">#-&gt;指定联系人;可以有多个，用”,”分隔开来</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;br&gt;</span><br><span class="line">Usage() {</span><br><span class="line">  echo <span class="string">"Usage: notify.sh \[-m|--mode {mm|mb}\] \[-s|--service SERVICE1,...\] &lt;-a|--address VIP&gt;  &lt;-n|--notify {master|backup|falut}&gt;"</span> </span><br><span class="line">  echo <span class="string">"Usage: notify.sh -h|--help"</span></span><br><span class="line">}</span><br><span class="line"><span class="comment">########################################################################################</span></span><br><span class="line">ParseOptions() {</span><br><span class="line">  local I=<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span> <span class="string">\[</span> $<span class="comment"># -gt 0 \]; then</span></span><br><span class="line">    <span class="keyword">while</span> <span class="string">\[</span> $I -le $<span class="comment"># \]; do</span></span><br><span class="line">      <span class="keyword">case</span> $<span class="number">1</span> <span class="keyword">in</span></span><br><span class="line">	  -s|--service)</span><br><span class="line">		<span class="string">\[</span> $<span class="comment"># -lt 2 \] &amp;&amp; return 3</span></span><br><span class="line"> 	    serviceflag=<span class="number">1</span></span><br><span class="line"> 		services=(<span class="string">\`echo</span> $<span class="number">2</span>|awk -F<span class="string">","</span> <span class="string">'{for(i=1;i&lt;=NF;i++) print $i}'</span><span class="string">\`)</span></span><br><span class="line">		shift <span class="number">2</span> ;;</span><br><span class="line">	  -h|--help)</span><br><span class="line"> 		helpflag=<span class="number">1</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        shift</span><br><span class="line">		;;</span><br><span class="line">	  -a|--address)</span><br><span class="line">		<span class="string">\[</span> $<span class="comment"># -lt 2 \] &amp;&amp; return 3</span></span><br><span class="line">	    addressflag=<span class="number">1</span></span><br><span class="line">		vip=$<span class="number">2</span></span><br><span class="line">		shift <span class="number">2</span></span><br><span class="line">		;;</span><br><span class="line">	  -m|--mode)</span><br><span class="line">		<span class="string">\[</span> $<span class="comment"># -lt 2 \] &amp;&amp; return 3</span></span><br><span class="line">		mode=$<span class="number">2</span></span><br><span class="line">		shift <span class="number">2</span></span><br><span class="line">		;;</span><br><span class="line">	  -n|--notify)</span><br><span class="line">		<span class="string">\[</span> $<span class="comment"># -lt 2 \] &amp;&amp; return 3</span></span><br><span class="line">		notifyflag=<span class="number">1</span></span><br><span class="line">		notify=$<span class="number">2</span></span><br><span class="line">		shift <span class="number">2</span></span><br><span class="line">		;;</span><br><span class="line">	  *)</span><br><span class="line">		echo <span class="string">"Wrong options..."</span></span><br><span class="line">		Usage</span><br><span class="line">		<span class="keyword">return</span> <span class="number">7</span></span><br><span class="line">		;;</span><br><span class="line">       esac</span><br><span class="line">    done</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  fi</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">#workspace=$(dirname $0)</span></span><br><span class="line"></span><br><span class="line">RestartService() {</span><br><span class="line">  <span class="keyword">if</span> <span class="string">\[</span> ${<span class="comment">#@} -gt 0 \]; then</span></span><br><span class="line">    <span class="keyword">for</span> I <span class="keyword">in</span> $@; <span class="keyword">do</span></span><br><span class="line">      <span class="keyword">if</span> <span class="string">\[</span> -x <span class="regexp">/etc/rc.d/init.d/</span>$I <span class="string">\];</span> <span class="keyword">then</span></span><br><span class="line">        <span class="regexp">/etc/rc.d/init.d/</span>$I restart</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        echo <span class="string">"$I is not a valid service..."</span></span><br><span class="line">      fi</span><br><span class="line">    done</span><br><span class="line">  fi</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">StopService() {</span><br><span class="line">  <span class="keyword">if</span> <span class="string">\[</span> ${<span class="comment">#@} -gt 0 \]; then</span></span><br><span class="line">    <span class="keyword">for</span> I <span class="keyword">in</span> $@; <span class="keyword">do</span></span><br><span class="line">      <span class="keyword">if</span> <span class="string">\[</span> -x <span class="regexp">/etc/rc.d/init.d/</span>$I <span class="string">\];</span> <span class="keyword">then</span></span><br><span class="line">        <span class="regexp">/etc/rc.d/init.d/</span>$I stop</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        echo <span class="string">"$I is not a valid service..."</span></span><br><span class="line">      fi</span><br><span class="line">    done</span><br><span class="line">  fi</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Notify() {</span><br><span class="line">    mailsubject=<span class="string">"\`hostname\` to be $1: $vip floating"</span></span><br><span class="line">    mailbody=<span class="string">"\`date '+%F %H:%M:%S'\`, vrrp transition, \`hostname\` changed to be $1."</span></span><br><span class="line">    echo $mailbody | mail -s <span class="string">"$mailsubject"</span> $contact</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">\#</span> Main <span class="built_in">Function</span></span><br><span class="line">ParseOptions $@</span><br><span class="line"><span class="string">\[</span> $? -ne <span class="number">0</span> <span class="string">\]</span> &amp;&amp; Usage &amp;&amp; exit <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="string">\[</span> $helpflag -eq <span class="number">1</span> <span class="string">\]</span> &amp;&amp; Usage &amp;&amp; exit <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="string">\[</span> $addressflag -ne <span class="number">1</span> -o $notifyflag -ne <span class="number">1</span> <span class="string">\];</span> <span class="keyword">then</span></span><br><span class="line">  Usage</span><br><span class="line">  exit <span class="number">2</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">mode=${mode:-mb}</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> $notify <span class="keyword">in</span></span><br><span class="line"><span class="string">'master'</span>)</span><br><span class="line">  <span class="keyword">if</span> <span class="string">\[</span> $serviceflag -eq <span class="number">1</span> <span class="string">\];</span> <span class="keyword">then</span></span><br><span class="line">      RestartService ${services<span class="string">\[*\]}</span></span><br><span class="line">  fi</span><br><span class="line">  Notify master</span><br><span class="line">  ;;</span><br><span class="line"><span class="string">'backup'</span>)</span><br><span class="line">  <span class="keyword">if</span> <span class="string">\[</span> $serviceflag -eq <span class="number">1</span> <span class="string">\];</span> <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">\[</span> <span class="string">"$mode"</span> == <span class="string">'mb'</span> <span class="string">\];</span> <span class="keyword">then</span></span><br><span class="line">      StopService ${services<span class="string">\[*\]}</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      RestartService ${services<span class="string">\[*\]}</span></span><br><span class="line">    fi</span><br><span class="line">  fi</span><br><span class="line">  Notify backup</span><br><span class="line">  ;;</span><br><span class="line"><span class="string">'fault'</span>)</span><br><span class="line">  Notify fault</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  Usage</span><br><span class="line">  exit <span class="number">4</span></span><br><span class="line">  ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">(<span class="number">2</span>) 在keepalived.conf配置文件中，其调用方法如下所示：</span><br><span class="line"></span><br><span class="line">notify<span class="string">\_master</span> <span class="string">"/etc/keepalived/notify.sh -n master -a VIP\_address"</span></span><br><span class="line">notify<span class="string">\_backup</span> <span class="string">"/etc/keepalived/notify.sh -n backup -a VIP\_address"</span></span><br><span class="line">notify<span class="string">\_fault</span> <span class="string">"/etc/keepalived/notify.sh -n fault -a VIP\_address"</span></span><br><span class="line"></span><br><span class="line">(<span class="number">3</span>)修改master/slave 的keepalived配置文件：、 &nbsp;</span><br><span class="line"></span><br><span class="line">master:</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> vim /etc/keepalived/keepalived.conf</span><br><span class="line"></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line">global_defs {</span><br><span class="line">notification_email {</span><br><span class="line"><span class="number">2399447849</span>@qq.com</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">notification<span class="string">\_email\_from</span> root</span><br><span class="line">smtp_server <span class="number">127.0</span>.<span class="number">0.1</span></span><br><span class="line">smtp<span class="string">\_connect\_timeout</span> <span class="number">30</span></span><br><span class="line">router<span class="string">\_id</span> LVS<span class="string">\_DEVEL</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">&nbsp;</span><br><span class="line"></span><br><span class="line">vrrp<span class="string">\_script</span> chk<span class="string">\_schedown</span> {</span><br><span class="line">script <span class="string">"\[ -e /etc/keepalived/down \] &amp;&amp; exit 1 || exit 0"</span></span><br><span class="line">interval <span class="number">1</span></span><br><span class="line">weight -<span class="number">5</span></span><br><span class="line">fall <span class="number">2</span></span><br><span class="line">rise <span class="number">1</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vrrp<span class="string">\_instance</span> VI<span class="string">\_1</span> {</span><br><span class="line">state MASTER</span><br><span class="line">interface eth0</span><br><span class="line">virtual<span class="string">\_router\_id</span> <span class="number">60</span></span><br><span class="line">priority <span class="number">101</span></span><br><span class="line">advert_int <span class="number">1</span></span><br><span class="line">authentication {</span><br><span class="line">auth_type PASS</span><br><span class="line">auth_pass <span class="number">1111</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">virtual_ipaddress {</span><br><span class="line"><span class="number">192.168</span>.<span class="number">254.200</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">track_script {</span><br><span class="line">     chk_schedown</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">#-&gt;增加以下三行(注：在slave上面也一样添加这三行，此处略)</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; notify_master <span class="string">"/etc/keepalived/notify.sh -n master -a 192.168.254.200"</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; notify_backup <span class="string">"/etc/keepalived/notify.sh -n backup -a 192.168.254.200"</span></span><br><span class="line">&nbsp;&nbsp;&nbsp; notify_fault <span class="string">"/etc/keepalived/notify.sh -n fault -a 192.168.254.200"</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">&nbsp;</span><br><span class="line"></span><br><span class="line">virtual_server <span class="number">192.168</span>.<span class="number">254.200</span> <span class="number">80</span> {</span><br><span class="line">delay_loop <span class="number">6</span></span><br><span class="line">lb_algo rr</span><br><span class="line">lb_kind DR</span><br><span class="line">nat_mask <span class="number">255.255</span>.<span class="number">255.0</span></span><br><span class="line"><span class="comment">#persistence_timeout 50</span></span><br><span class="line">protocol TCP</span><br><span class="line">&nbsp;</span><br><span class="line"></span><br><span class="line">real_server <span class="number">192.168</span>.<span class="number">254.45</span> <span class="number">80</span>&nbsp; {</span><br><span class="line">weight <span class="number">1</span></span><br><span class="line">HTTP_GET {</span><br><span class="line">    url {</span><br><span class="line">      path /</span><br><span class="line">      status_code <span class="number">200</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">connect_timeout <span class="number">3</span></span><br><span class="line">nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">real_server <span class="number">192.168</span>.<span class="number">254.46</span> <span class="number">80</span> {</span><br><span class="line">weight <span class="number">1</span></span><br><span class="line">HTTP_GET {</span><br><span class="line">url {</span><br><span class="line">path /</span><br><span class="line">status_code <span class="number">200</span></span><br><span class="line">}</span><br><span class="line">connect_timeout <span class="number">3</span></span><br><span class="line">nb<span class="string">\_get\_retry</span> <span class="number">3</span></span><br><span class="line">delay<span class="string">\_before\_retry</span> <span class="number">3</span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">sorry_server <span class="number">127.0</span>.<span class="number">0.1</span> <span class="number">80</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">&nbsp; (<span class="number">4</span>)添加脚本： 讲上述的脚本添加至master和slave的<span class="regexp">/etc/keepalived/</span>目录下(注意权限)：</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> ll /etc/keepalived</span><br><span class="line">总用量 <span class="number">8</span></span><br><span class="line">-rw-r--r--. <span class="number">1</span> root root <span class="number">1748</span> <span class="number">10</span>月 <span class="number">30</span> <span class="number">17</span>:<span class="number">08</span> keepalived.conf</span><br><span class="line">-rwxr-xr-x. <span class="number">1</span> root root <span class="number">2380</span> <span class="number">10</span>月 <span class="number">30</span> <span class="number">00</span>:<span class="number">57</span> notify.sh</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span></span><br><span class="line">&nbsp;</span><br><span class="line"><span class="comment">#-&gt;复制至slave</span></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> scp <span class="regexp">/etc/keepalived/notify.sh 192.168.254.48:/etc/keepalived/</span></span><br><span class="line"></span><br><span class="line">(<span class="number">5</span>)测试一下脚本可用性：</span><br><span class="line"></span><br><span class="line"><span class="string">\[root@slave</span> keepalived<span class="string">\]#</span> ./notify.sh --help</span><br><span class="line">Usage: notify.sh <span class="string">\[-m|--mode</span> {mm|mb}<span class="string">\]</span> <span class="string">\[-s|--service</span> SERVICE1,...<span class="string">\]</span> &lt;-a|--address VIP&gt;&nbsp; &lt;-n|--notify {master|backup|falut}&gt;</span><br><span class="line">Usage: notify.sh -h|--help</span><br><span class="line"><span class="string">\[root@slave</span> keepalived<span class="string">\]#</span> ./notify.sh -m mb -a <span class="number">2.2</span>.<span class="number">2.2</span> -n master</span><br><span class="line"><span class="string">\[root@slave</span> keepalived<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">查看邮件: [![<span class="number">12</span>](https:<span class="regexp">//qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/12.png)](https://</span>qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/<span class="number">2014</span>/<span class="number">10</span>/<span class="number">12.png</span>) 在模拟故障时重启一下keepalived,以免前面的实验造成影响。 注：现在已经可以成功收到邮件，通知脚本可用； (<span class="number">6</span>)故障模拟： &lt;<span class="number">1</span>&gt;先重启主备keepalived服务</span><br><span class="line"></span><br><span class="line">master:</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> service keepalived restart</span><br><span class="line">停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line">正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span></span><br><span class="line">slave:</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> service keepalived restart</span><br><span class="line">停止 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="string">\[确定\]</span></span><br><span class="line">正在启动 keepalived：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">\[确定\]</span></span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">&lt;<span class="number">2</span>&gt;正常情况下此时VIP在master上面</span><br><span class="line"></span><br><span class="line">master:</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span> ip a</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">inet <span class="number">192.168</span>.<span class="number">254.200</span>/<span class="number">32</span> scope global eth0</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line"><span class="string">\[root@master</span> ~<span class="string">\]#</span></span><br><span class="line"></span><br><span class="line">&lt;<span class="number">3</span>&gt;在master的/etc/keepalived目录下 touch一个文件”down”</span><br><span class="line"></span><br><span class="line">master:</span><br><span class="line"><span class="string">\[root@master</span> keepalived<span class="string">\]#</span> touch down</span><br><span class="line"></span><br><span class="line">&lt;<span class="number">4</span>&gt;观察VIP 转移情况</span><br><span class="line"></span><br><span class="line">slave:</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span> ip a</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">inet <span class="number">192.168</span>.<span class="number">254.200</span>/<span class="number">32</span> scope global eth0</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line"><span class="string">\[root@slave</span> ~<span class="string">\]#</span></span><br></pre></td></tr></tbody></table></figure>
<p>&lt;5&gt;结果查看—&gt;邮件收取 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/13.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/13.png" alt="13"></a> &lt;6&gt;Client访问测试 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/14.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/10/14.png" alt="14"></a> 从上可以看到，在keepalived主备切换时，不仅能够发送邮件，而且访问服务也没有问题； 至此Lvs+Keepalived的基本应用实验演示完毕！</p>
]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>Floating</tag>
        <tag>keepalived</tag>
        <tag>lvs</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB基本操作</title>
    <url>/2016/05/02/mongodb-ji-ben-cao-zuo/</url>
    <content><![CDATA[<h4 id="1-删除用户"><a href="#1-删除用户" class="headerlink" title="1.删除用户"></a>1.删除用户</h4><pre><code>mongo&gt;use db1
mongo&gt;show users;
mongo&gt;db.dropUser("User_name")
</code></pre>
<h4 id="2-备份"><a href="#2-备份" class="headerlink" title="2.备份"></a>2.备份</h4><pre><code>mongodump -h 127.0.0.1:27017 -d health_init -u=health_init -p=health_init -o /tmp/
</code></pre>
<h4 id="3-恢复"><a href="#3-恢复" class="headerlink" title="3.恢复"></a>3.恢复</h4><pre><code>mongorestore -d health_test --drop -u=dachenadm -p=dachen@ad /tmp/health_ini
</code></pre>
<h4 id="4-授权："><a href="#4-授权：" class="headerlink" title="4.授权："></a>4.授权：</h4><pre><code>#创建一个角色：
use amdin;db.createRole({role:'sysadmin',roles:[],privileges:[{resource:{anyResource:true},actions:['anyAction']}]})
#授权一个用户
use health;db.createUser(  {      user: "health",      pwd:  "healthnx",      roles: [{ role: "sysadmin", db: "admin" }]  }
</code></pre>
<h4 id="5-mongo远程执行需要认证的服务器的js文件"><a href="#5-mongo远程执行需要认证的服务器的js文件" class="headerlink" title="5. mongo远程执行需要认证的服务器的js文件"></a>5. mongo远程执行需要认证的服务器的js文件</h4><pre><code>/usr/bin/mongo -h 10.251.225.205:27017/health -u health -p healthnx update.js
</code></pre>
<h4 id="6-查看collection内容："><a href="#6-查看collection内容：" class="headerlink" title="6.查看collection内容："></a>6.查看collection内容：</h4><pre><code>db.d_role.find()
</code></pre>
<h4 id="7-更新操作"><a href="#7-更新操作" class="headerlink" title="7.更新操作"></a>7.更新操作</h4><h5 id="Mongodb数据更新命令"><a href="#Mongodb数据更新命令" class="headerlink" title="Mongodb数据更新命令"></a>Mongodb数据更新命令</h5><h5 id="Mongodb更新有两个命令：update、save。"><a href="#Mongodb更新有两个命令：update、save。" class="headerlink" title="Mongodb更新有两个命令：update、save。"></a>Mongodb更新有两个命令：update、save。</h5><h5 id="update命令格式"><a href="#update命令格式" class="headerlink" title="update命令格式:"></a>update命令格式:</h5><pre><code>db.collection.update(criteria,objNew,upsert,multi)
</code></pre>
<h5 id="参数说明："><a href="#参数说明：" class="headerlink" title="参数说明："></a>参数说明：</h5><h5 id="criteria：查询条件"><a href="#criteria：查询条件" class="headerlink" title="criteria：查询条件"></a>criteria：查询条件</h5><h5 id="objNew：update对象和一些更新操作符"><a href="#objNew：update对象和一些更新操作符" class="headerlink" title="objNew：update对象和一些更新操作符"></a>objNew：update对象和一些更新操作符</h5><h5 id="upsert：如果不存在update的记录，是否插入objNew这个新的文档，true为插入，默认为false，不插入。"><a href="#upsert：如果不存在update的记录，是否插入objNew这个新的文档，true为插入，默认为false，不插入。" class="headerlink" title="upsert：如果不存在update的记录，是否插入objNew这个新的文档，true为插入，默认为false，不插入。"></a>upsert：如果不存在update的记录，是否插入objNew这个新的文档，true为插入，默认为false，不插入。</h5><h5 id="multi：默认是false，只更新找到的第一条记录。如果为true，把按条件查询s出来的记录全部更新。"><a href="#multi：默认是false，只更新找到的第一条记录。如果为true，把按条件查询s出来的记录全部更新。" class="headerlink" title="multi：默认是false，只更新找到的第一条记录。如果为true，把按条件查询s出来的记录全部更新。"></a>multi：默认是false，只更新找到的第一条记录。如果为true，把按条件查询s出来的记录全部更新。</h5><h2 id="8-mongodb的文档导入导出："><a href="#8-mongodb的文档导入导出：" class="headerlink" title="8.mongodb的文档导入导出："></a>8.mongodb的文档导入导出：</h2><pre><code>1.  mongoimport -h 127.0.0.1:27017  -u USENAME -p PASSWORD -c COLLECTIONS b_hospitaldept.json 
2.  mongoexport -h 127.0.0.1:27017 -d DB -u USENAME -p PASSWORD -c COLLECTIONS -o checkin_b_hospitaldept.json
</code></pre>
<h2 id="9-对collections操作"><a href="#9-对collections操作" class="headerlink" title="9.对collections操作"></a>9.对collections操作</h2><pre><code>1.  use demodb &nbsp;//使用demodb，以下假设操作的collection是foo
2.  db.foo.remove({"id":"bar"}) &nbsp;//删除一条数据
3.  db.foo.remove() //删除foo中的所有记录，但是foo还存在，show collection还可以看到foo
4.  db.foo.drop() &nbsp;//删除foo这个collection，（show collection已经看不到foo了）但是查看数据文件发现大小不变，Mongodb不会自动释放文件空间
5.  db.repairDatabase() &nbsp;//执行这个命令后，Mongodb会把不需要的空间释放出来
</code></pre>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL报错ERROR 1030 (HY000): 解决过程</title>
    <url>/2016/03/19/mysql-e6-8a-a5-e9-94-99error-1030-hy000-e8-a7-a3-e5-86-b3-e8-bf-87-e7-a8-8b/</url>
    <content><![CDATA[<p><strong>问题</strong>： 今天开发同事在执行jenkins自动化构建时，由于执行一个mysql更新的操作导致构建失败，报错如下： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/1.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/1.jpg" alt="1"></a> 这个文件主要是开发用于执行构建部署的时候更新数据库的sql文件，里面有大量的插入语句； 登录到测试服务器将jenkins中的语句在命令行执行了一遍还是报同样的错误 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/2.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/2.jpg" alt="2"></a> 看到这个报错貌似与存储引擎有关系，于是登录到mysql中执行了以下操作 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/3.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/3.jpg" alt="3"></a> <strong>解决思路</strong> 1.看了下具体错误代码 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/4.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/4.jpg" alt="4"></a> 在tmp下没有这个文件？但是这个文件时临时生成的，本来就应该没有啊！ 那就是创建的时候出错了?! 2.查看了一下磁盘空间，我滴个乖乖，原来第一块磁盘满了 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/5.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/5.jpg" alt="5"></a> 好吧，清理了一些文件 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/6.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/6.jpg" alt="6"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/66.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/03/66.jpg" alt="66"></a> 再次执行构建，顺利完成！</p>
]]></content>
      <categories>
        <category>故障处理</category>
      </categories>
      <tags>
        <tag>mysql error</tag>
      </tags>
  </entry>
  <entry>
    <title>Nagios-NRPE脚本返回值</title>
    <url>/2014/02/08/nagios-nrpe-e8-84-9a-e6-9c-ac-e8-bf-94-e5-9b-9e-e5-80-bc/</url>
    <content><![CDATA[<p>自定义Nagios NRPE脚本EXIT退出值和nagios状态都应关系：</p>
<p>状态</p>
<p>EXIT退出值</p>
<p>输出</p>
<p>例子</p>
<p>OK</p>
<p>0</p>
<p>echo “OK - it’s ok.”</p>
<p>echo “OK - it’s ok.” exit 0</p>
<p>WARNING</p>
<p>1</p>
<p>echo “WARNING - it’s warning.”</p>
<p>echo “WARNING - it’s warning.” exit 1</p>
<p>CRITICAL</p>
<p>2</p>
<p>echo “CRITICAL - it’s critical.”</p>
<p>echo “CRITICAL - it’s critical.” exit 2</p>
<p>UNKNOWN</p>
<p>3</p>
<p>echo “UNKNOWN - it’s unknown.”</p>
<p>echo “UNKNOWN - it’s unknown.” exit 3</p>
<p>错误的例子： shell脚本中echo和退出值： echo “OK - it’s ok.” exit 1 此时，Nagios会显示： 这条服务对应的状态是”WARNING“，但是输出的信息是”OK - it’s ok.”</p>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>nrpe</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker 部署</title>
    <url>/2018/06/18/nginx-uwsgi-flask-supervisord-redis-mysql-docker-e9-83-a8-e7-bd-b2-2/</url>
    <content><![CDATA[<p>之前使用Flask开发了两三个公司或个人使用的平台；在搭建过程当中如果换了环境的话比较麻烦；这次尝试放到docker里面去跑；下面是搭建的一个过程以及对于学习的一个记录，此次web框架还是使用的之前用Flask写的一个基础后台。</p>
<h2 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构:"></a>部署架构:</h2><pre><code>.
├── README.md
├── docker-compose.yaml              # 使用docker-compose来编排部署
├── flask_app                        # 用于跑Flask应用的容器
│&nbsp;&nbsp; ├── Dockerfile
│&nbsp;&nbsp; └── wait_for_db_complete.sh
├── flask_app_code                   # 后端项目应用代码目
│&nbsp;&nbsp; ├── LICENSE
│&nbsp;&nbsp; ├── README.md
│&nbsp;&nbsp; ├── app
│&nbsp;&nbsp; ├── config.py
│&nbsp;&nbsp; ├── manage.py
│&nbsp;&nbsp; ├── requirements.txt
│&nbsp;&nbsp; ├── screenshots
│&nbsp;&nbsp; └── tests
├── nginx                            # Nginx用于前端接收用户请求的容器
│&nbsp;&nbsp; └── nginx.conf
├── python27_baseenv                 # 基础Python环境镜像
│&nbsp;&nbsp; ├── Dockerfile
│&nbsp;&nbsp; └── README.md
├── supervisor                       # 用于管理uwsgi服务进程
│&nbsp;&nbsp; └── supervisord.conf
└── uwsgi                            # 通过uWsgi来为Nginx-Flask牵线搭桥
    └── flask_uwsgi.ini
</code></pre>
<h2 id="访问流程"><a href="#访问流程" class="headerlink" title="访问流程:"></a>访问流程:</h2><p><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/261529307197_.pic_hd.jpg"></p>
<ul>
<li>Nginx Web服务器层作为前端接收用户请求；</li>
<li>uWSGI层作为Web服务器层与Web框架层Flask的一条纽带，将Web服务器层与Web框架连接起来</li>
<li>后端Web框架与数据层MySQL或Redis交互</li>
</ul>
<h3 id="简单理解起来就是酱紫的"><a href="#简单理解起来就是酱紫的" class="headerlink" title="简单理解起来就是酱紫的:"></a>简单理解起来就是酱紫的:</h3><ol>
<li>Nginx：Hey，WSGI，我刚收到了一个请求，我需要你作些准备，然后由Flask来处理这个请求。</li>
<li>WSGI：OK，Nginx。我会设置好环境变量，然后将这个请求传递给Flask处理。</li>
<li>Flask：Thanks WSGI！给我一些时间，我将会把请求的响应返回给你。</li>
<li>WSGI：Alright，那我等你。</li>
<li>Flask：Okay，我完成了，这里是请求的响应结果，请求把结果传递给Nginx。 WSGI：Good job！</li>
<li>Nginx，这里是响应结果，已经按照要求给你传递回来了。</li>
<li>Nginx：Cool，我收到了，我把响应结果返回给客户端。大家合作愉快~</li>
</ol>
<h2 id="搭建思路"><a href="#搭建思路" class="headerlink" title="搭建思路:"></a>搭建思路:</h2><ul>
<li>Nginx 单独一个容器</li>
<li>uWSGI+Flask 单独一个容器，其中uWSGI进程由Supervisor来管理</li>
<li>MySQL 单独一个容器，数据目录挂载到宿主机</li>
<li>Redis 单独一个容器</li>
</ul>
<p>各个容器之间的关联通过docker-compose编排来实现</p>
<h2 id="部署步骤："><a href="#部署步骤：" class="headerlink" title="部署步骤："></a>部署步骤：</h2><p>主要还是通过编写Dockerfile来定制特定的运行环境镜像</p>
<h5 id="0-安装docker环境"><a href="#0-安装docker环境" class="headerlink" title="0.安装docker环境"></a>0.安装docker环境</h5><pre><code>cd /etc/yum.repos.d/
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum install -y docker-ce docker-compose
systemctl start docker
</code></pre>
<h5 id="1-构建python基础运行环境镜像，基于alpine镜像"><a href="#1-构建python基础运行环境镜像，基于alpine镜像" class="headerlink" title="1.构建python基础运行环境镜像，基于alpine镜像"></a>1.构建python基础运行环境镜像，基于alpine镜像</h5><pre><code>cd Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker
docker build -f python27_baseenv/Dockerfile . -t python27_baseenv
</code></pre>
<h5 id="2-构建Flask应用框架运行所需依赖包镜像"><a href="#2-构建Flask应用框架运行所需依赖包镜像" class="headerlink" title="2.构建Flask应用框架运行所需依赖包镜像"></a>2.构建Flask应用框架运行所需依赖包镜像</h5><pre><code>cd Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker
docker build -f flask_app/Dockerfile . -t flask_app
</code></pre>
<h5 id="4-Nginx镜像使用默认，配置文件需要修改，这里通过挂载方式"><a href="#4-Nginx镜像使用默认，配置文件需要修改，这里通过挂载方式" class="headerlink" title="4.Nginx镜像使用默认，配置文件需要修改，这里通过挂载方式"></a>4.Nginx镜像使用默认，配置文件需要修改，这里通过挂载方式</h5><h5 id="5-Redis镜像使用默认的"><a href="#5-Redis镜像使用默认的" class="headerlink" title="5.Redis镜像使用默认的"></a>5.Redis镜像使用默认的</h5><h5 id="6-执行docker-compose"><a href="#6-执行docker-compose" class="headerlink" title="6.执行docker-compose"></a>6.执行docker-compose</h5><pre><code>cd Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker
docker-compose up
</code></pre>
<h4 id="运行状态"><a href="#运行状态" class="headerlink" title="运行状态"></a>运行状态</h4><p><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/status.jpeg"></p>
<h4 id="登录"><a href="#登录" class="headerlink" title="登录"></a>登录</h4><p><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/login.jpeg"></p>
<h4 id="用户注册"><a href="#用户注册" class="headerlink" title="用户注册"></a>用户注册</h4><p><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/login_unconfiremd.jpeg"><br><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/email.jpeg"><br><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/login_ok.jpeg"><br><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/db.jpeg"></p>
<h4 id="Flask应用的访问、登录、注册过程日志"><a href="#Flask应用的访问、登录、注册过程日志" class="headerlink" title="Flask应用的访问、登录、注册过程日志"></a>Flask应用的访问、登录、注册过程日志</h4><h5 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h5><p><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/nginxlog.jpeg"></p>
<h5 id="uWSGI"><a href="#uWSGI" class="headerlink" title="uWSGI"></a>uWSGI</h5><p><img src="https://raw.githubusercontent.com/guomaoqiu/Nginx-uWsgi-Flask-Supervisord-Redis-MySQL-Docker/master/flask_app_code/screenshots/uwsgilog.jpeg"></p>
<h2 id="部署总结"><a href="#部署总结" class="headerlink" title="部署总结:"></a>部署总结:</h2><p>部署过程中，感觉在宿主机中部署还是没多大的区别，差别可能是在效率上面。宿主机中不能影响系统自带的一些东西，比如python的版本，这时候可能就需要用到virtualenv, 如果服务器迁移了那整个环境就需要重新搭建，还是不太方便。</p>
<p>此次部署呢主要目的还是以这个为一个实践目标去学习docker的compose文件编写，再把各个工具结合在一起跑在docker中实现之前在宿主机中的东西；其实把整个流程梳理清楚后编写yaml文件也很快的。后续尝试放到k8s集群中跑🍺🍺🍺</p>
<p>在我的VPS上面跑起来了… <a href="http://blog.sctux.com:8090/">http://blog.sctux.com:8090</a></p>
]]></content>
      <categories>
        <category>Docker</category>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Perl: Warning: Falling Back to the Standard Locale (“C”)</title>
    <url>/2014/08/21/perl-warning-falling-back-to-the-standard-locale-c/</url>
    <content><![CDATA[<p>今天在安装memcache的php的扩展时遇到的错误： 1.将该扩展包解压 2.使用/usr/bin/phpize(rpm包安装后的位置)命令来准备 PHP 外挂模块的编译环境(如果找不到该命令则需要安装，这个命令有php-devel这个包生成，并且该包位于DVD2中) 上面1、2步成功 3.使用/usr/bin/phpize时报以下错误:</p>
<p>[root@node1 memcache-2.2.5]# /usr/bin/phpize<br>Configuring for:<br>PHP Api Version:         20090626<br>Zend Module Api No:      20090626<br>Zend Extension Api No:   220090626<br>perl: warning: Setting locale failed.<br>perl: warning: Please check that your locale settings:<br>    LANGUAGE = (unset),<br>    LC_ALL = (unset),<br>    LANG = “en”<br>    are supported and installed on your system.<br>perl: warning: Falling back to the standard locale (“C”).<br>perl: warning: Setting locale failed.<br>perl: warning: Please check that your locale settings:<br>    LANGUAGE = (unset),<br>    LC_ALL = (unset),<br>    LANG = “en”<br>    are supported and installed on your system.<br>perl: warning: Falling back to the standard locale (“C”).</p>
<p>在使用其它的某些命令时(如：mysqlhotcopy)也会也出现类似的提示。 搜索了好一段时间，并试了几次，找到一个解决此问题的简单方法，如下：</p>
<p>[root@node1 memcache-2.2.5]#vim /root/.bashrc<br>#-&gt;再最底部加上<br>export LC_ALL=C<br>#-&gt;或者直接运行<br>[root@node1 memcache-2.2.5]# echo “export LC_ALL=C” &gt;&gt; /root/.bashrc<br>#-&gt;然后执行一下：<br>[root@node1 memcache-2.2.5]# source /root/.bashrc</p>
]]></content>
      <categories>
        <category>故障处理</category>
      </categories>
      <tags>
        <tag>perl</tag>
        <tag>phpize</tag>
      </tags>
  </entry>
  <entry>
    <title>Prometheus 实践</title>
    <url>/2018/11/16/prometheus-e5-ae-9e-e8-b7-b5/</url>
    <content><![CDATA[<h1 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h1><p>Server端安装 <code>prometheus</code> / <code>alertmanager</code> / <code>node_expoter</code></p>
<h2 id="一、安装prometheus-server"><a href="#一、安装prometheus-server" class="headerlink" title="一、安装prometheus server"></a>一、安装prometheus server</h2><h3 id="0-获取软件包-略-、解压复制二进制文件"><a href="#0-获取软件包-略-、解压复制二进制文件" class="headerlink" title="0.获取软件包(略)、解压复制二进制文件"></a>0.获取软件包(略)、解压复制二进制文件</h3><figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">tar</span> -xf ./packages/alertmanager-<span class="number">0</span>.<span class="number">15</span>.<span class="number">3</span>.linux-amd64.tar.gz</span><br><span class="line"><span class="attribute">mv</span> alertmanager-<span class="number">0</span>.<span class="number">15</span>.<span class="number">3</span>.linux-amd64/alertmanager /usr/local/sbin/</span><br></pre></td></tr></tbody></table></figure>
<h3 id="1-创建运行用户"><a href="#1-创建运行用户" class="headerlink" title="1.创建运行用户"></a>1.创建运行用户</h3><figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line">adduser -M -s <span class="regexp">/sbin/</span>nologin prometheus</span><br></pre></td></tr></tbody></table></figure>
<h3 id="2-创建用户所有者目录"><a href="#2-创建用户所有者目录" class="headerlink" title="2.创建用户所有者目录"></a>2.创建用户所有者目录</h3><figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 创建prometheus server运行的目录主要存放配置文件</span></span><br><span class="line">mdkir <span class="regexp">/usr/</span>local<span class="regexp">/share/</span>prometheus/prometheus_server -p</span><br><span class="line">chown -R prometheus:prometheus  <span class="regexp">/usr/</span>local<span class="regexp">/share/</span>prometheus</span><br><span class="line"><span class="comment"># 创建prometheus server数据存储目录</span></span><br><span class="line">mkdir <span class="regexp">/var/</span>lib<span class="regexp">/prometheus/</span>data</span><br><span class="line">chown -R prometheus:prometheus  <span class="regexp">/var/</span>lib<span class="regexp">/prometheus/</span>data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建prometheus alertmanager运行的目录主要存放配置文件</span></span><br><span class="line">mdkir <span class="regexp">/usr/</span>local<span class="regexp">/share/</span>prometheus/prometheus_alertmanager</span><br><span class="line"><span class="comment"># 创建prometheus alertmanager数据存储目录</span></span><br><span class="line">mkdir <span class="regexp">/var/</span>lib<span class="regexp">/alertmanager/</span>data -p</span><br><span class="line">chown -R prometheus:prometheus  <span class="regexp">/var/</span>lib<span class="regexp">/alertmanager/</span>data</span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-创建prometheus-server-systemd文件"><a href="#3-创建prometheus-server-systemd文件" class="headerlink" title="3.创建prometheus server systemd文件"></a>3.创建prometheus server systemd文件</h3><figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">cp</span> ./<span class="keyword">conf</span>/systemd_conf/prometheus.service /etc/systemd/<span class="built_in">system</span>/</span><br><span class="line"></span><br><span class="line"># 服务操作</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start prometheus</span><br><span class="line">systemctl enable prometheus</span><br><span class="line">systemctl status prometheus</span><br><span class="line"></span><br><span class="line"># 保证端口以及进程</span><br><span class="line"><span class="keyword">ps</span> aux | <span class="keyword">grep</span> prometheus | <span class="keyword">grep</span> -v <span class="keyword">grep</span>  &amp;&amp; ss -tunl | <span class="keyword">grep</span> <span class="number">9090</span> | <span class="keyword">grep</span> -v <span class="keyword">grep</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="4-访问"><a href="#4-访问" class="headerlink" title="4.访问:"></a>4.访问:</h3><figure class="highlight dts"><table><tbody><tr><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//xxxxxxx:9090</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="二、安装prometheus-AlertManager"><a href="#二、安装prometheus-AlertManager" class="headerlink" title="二、安装prometheus AlertManager"></a>二、安装prometheus AlertManager</h2><h3 id="0-获取软件包-略-、解压复制二进制文件-1"><a href="#0-获取软件包-略-、解压复制二进制文件-1" class="headerlink" title="0.获取软件包(略)、解压复制二进制文件"></a>0.获取软件包(略)、解压复制二进制文件</h3><figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">tar</span> -xf ./packages/prometheus-<span class="number">2</span>.<span class="number">4</span>.<span class="number">2</span>.linux-amd64.tar.gz</span><br><span class="line"><span class="attribute">mv</span> prometheus-<span class="number">2</span>.<span class="number">4</span>.<span class="number">2</span>.linux-amd64/prometheus /usr/local/sbin/</span><br></pre></td></tr></tbody></table></figure>
<h3 id="1-创建prometheus-alertmanager-systemd文件"><a href="#1-创建prometheus-alertmanager-systemd文件" class="headerlink" title="1.创建prometheus alertmanager systemd文件"></a>1.创建prometheus alertmanager systemd文件</h3><figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">cp</span> ./<span class="keyword">conf</span>/systemd_conf/alertmanager.service /etc/systemd/<span class="built_in">system</span>/</span><br><span class="line"></span><br><span class="line"># 服务操作</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start alertmanager</span><br><span class="line">systemctl enable alertmanager</span><br><span class="line">systemctl status alertmanager</span><br><span class="line"># 保证端口以及进程</span><br><span class="line"><span class="keyword">ps</span> aux | <span class="keyword">grep</span> alertmanager | <span class="keyword">grep</span> -v <span class="keyword">grep</span>  &amp;&amp; ss -tunl | <span class="keyword">grep</span> <span class="number">9093</span> | <span class="keyword">grep</span> -v <span class="keyword">grep</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="2-将alertmanager跟prometheus-server结合"><a href="#2-将alertmanager跟prometheus-server结合" class="headerlink" title="2.将alertmanager跟prometheus server结合"></a>2.将alertmanager跟prometheus server结合</h3><p>修改prometheus server配置:</p>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line">vim <span class="symbol">/usr/local/share/prometheus/prometheus_server/prometheus.yml</span></span><br><span class="line"><span class="comment"># Alertmanager configuration</span></span><br><span class="line"><span class="params">alerting:</span></span><br><span class="line">  <span class="params">alertmanagers:</span></span><br><span class="line">  <span class="operator">-</span> <span class="params">static_configs:</span></span><br><span class="line">    <span class="operator">-</span> <span class="params">targets:</span> ['localhost:<span class="number">9093</span>']</span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-访问"><a href="#3-访问" class="headerlink" title="3.访问:"></a>3.访问:</h3><figure class="highlight dts"><table><tbody><tr><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//xxxxxxx:9093</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="三、安装Node-Exporter收集主机信息"><a href="#三、安装Node-Exporter收集主机信息" class="headerlink" title="三、安装Node Exporter收集主机信息"></a>三、安装Node Exporter收集主机信息</h2><p>数据收集的任务由不同的 exporter 来完成，如果要收集 linux 主机的信息，可以使用 node exporter。然后由 Prometheus Server 从 node exporter 上拉取信息。</p>
<h3 id="0-获取软件包-略-、解压复制二进制文件-2"><a href="#0-获取软件包-略-、解压复制二进制文件-2" class="headerlink" title="0.获取软件包(略)、解压复制二进制文件"></a>0.获取软件包(略)、解压复制二进制文件</h3><figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">tar</span> -xf ./packages/node_exporter-<span class="number">0</span>.<span class="number">17</span>.<span class="number">0</span>-rc.<span class="number">0</span>.linux-amd64.tar.gz</span><br><span class="line"><span class="attribute">mv</span> node_exporter-<span class="number">0</span>.<span class="number">17</span>.<span class="number">0</span>-rc.<span class="number">0</span>.linux-amd64/node_exporter /usr/local/sbin/</span><br></pre></td></tr></tbody></table></figure>
<h3 id="1-把-node-exporter-也配置成通过-systemd-管理-创建文件-etc-systemd-system-node-exporter-service"><a href="#1-把-node-exporter-也配置成通过-systemd-管理-创建文件-etc-systemd-system-node-exporter-service" class="headerlink" title="1.把 node exporter 也配置成通过 systemd 管理, 创建文件 /etc/systemd/system/node-exporter.service"></a>1.把 node exporter 也配置成通过 systemd 管理, 创建文件 /etc/systemd/system/node-exporter.service</h3><figure class="highlight crmsh"><table><tbody><tr><td class="code"><pre><span class="line">cp ./conf/systemd_conf/node_exporter.service  /etc/systemd/system/<span class="keyword">node</span><span class="title">-exporter</span>.service</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#服务操作</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl enable <span class="keyword">node</span><span class="title">-exporter</span></span><br><span class="line">systemctl <span class="literal">start</span> <span class="keyword">node</span><span class="title">-exporter</span></span><br><span class="line">systemctl status <span class="keyword">node</span><span class="title">-exporter</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保证端口以及进程</span></span><br><span class="line">ps aux | grep node_exporter | grep -v grep  &amp;&amp; ss -tunl | grep <span class="number">9100</span> | grep -v grep</span><br></pre></td></tr></tbody></table></figure>
<p>Prometheus Server 可以从不同的 exporter 上拉取数据，对于上面的 node exporter 我们可以利用 Prometheus 的static_configs 来拉取 node exporter 的数据。<code>conf/prometheus_server_conf/prometheus/prometheus.yml</code>中已经定义好了</p>
<figure class="highlight 1c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="punctuation">-</span> job_name<span class="punctuation">:</span> 'node'</span><br><span class="line">  static_configs<span class="punctuation">:</span></span><br><span class="line">  <span class="punctuation">-</span> targets<span class="punctuation">:</span> ['66.112.211.12:<span class="number">9100</span>']</span><br></pre></td></tr></tbody></table></figure>

<p>重启各服务；重启后 prometheus 服务会每隔 15s 从 node exporter 上拉取一次数据。<br>Prometheus Server 提供了简易的 WebUI 可以进数据查询并展示，它默认监听的端口为 9090。接下来我们进行一次简单的查询来验证本文安装配置的系统。</p>
<p><img src="/2018/11/16/prometheus-e5-ae-9e-e8-b7-b5/media/15423532371020/15423600125042.jpg"></p>
<p>关于各项指标的规则还需要通过编写rule条目来实现；这里简单实现了wechat跟email的报警配置，具体可看规则配置文件<code>conf/prometheus_server_conf/prometheus/rules/hoststas-alert.rules</code>以及报警触发配置文件<code>alertmanager_conf/alertmanager.yml</code>；</p>
]]></content>
      <categories>
        <category>monitor</category>
      </categories>
      <tags>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title>Python发送邮件(邮件内容从文件读入)</title>
    <url>/2015/09/10/python-e5-8f-91-e9-80-81-e9-82-ae-e4-bb-b6-e9-82-ae-e4-bb-b6-e5-86-85-e5-ae-b9-e4-bb-8e-e6-96-87-e4-bb-b6-e8-af-bb-e5-85-a5/</url>
    <content><![CDATA[<p>有个需求，利用python脚本发出来的邮件的内容是从文件读取的。并且保持这个文件原有的格式。</p>
<p>#!/usr/bin/env python<br>#-*- coding: UTF-8 -*-<br>import smtplib,os,sys<br>from email.mime.text import MIMEText</p>
<p>mailto_list=[‘<a href="mailto:guomaoqiu@gmail.com">guomaoqiu@gmail.com</a>‘]<br>mail_host=’smtp.qq.com’<br>mail_port=’25’<br>mail_user=’2399447849’<br>mail_pass=’xxxxxxxxx’   #-&gt;你懂的…<br>mail_postfix=’qq.com’</p>
<p>filename = “/tmp/test.log”   #-&gt;将要读取作为邮件内容的文件</p>
<p>fo = open(filename,”rb”)<br>filecon = fo.read();<br>str1 = “</p><pre>{0}</pre>“.format(filecon)  <p></p>
<p>def send_mail(to_list,sub,content):<br>    me=”Code Sync Notice”+”&lt;”+mail_user+”@”+mail_postfix+”&gt;”<br>    msg = MIMEText(content,_subtype=’html’,_charset=’utf-8’)<br>    msg[‘Subject’] = sub<br>    msg[‘From’]=me<br>    msg[‘to’]=”;”.join(to_list)<br>    try:<br>        s = smtplib.SMTP()<br>        s.connect(mail_host)<br>        s.login(mail_user,mail_pass)<br>        s.sendmail(me,to_list,msg.as_string())<br>        s.close()<br>        return True<br>    except Exception, e:<br>        print str(e)<br>        return False<br>if __name__==’__main__‘:<br>   if send_mail(mailto_list,”New Flies Are Added”,str1):<br>        print “发送成功”<br>   else:<br>        print “发送失败”</p>
<p>执行结果： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-10-222600.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-10-222600.png" alt="Screenshot from 2015-09-10 22:26:00"></a> 之前出现一个问题，就是我在文件读取之后写成这样的，未对读入的字符串格式格式化。</p>
<p>fo = open(filename,”rb”)<br>filecon = fo.read();<br>str1 = filecon</p>
<p>而这样发出来的邮件结果却是这样的 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-10-222819.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-10-222819.png" alt="Screenshot from 2015-09-10 22:28:19"></a> 发送 HTML 形式的邮件，需要 email.mime.text 中的 MIMEText 的 _subtype 设置为 html，并且 _text 的内容应该为 HTML 形式 可参考：<a href="http://m.blog.chinaunix.net/uid-23802873-id-4477364.html">http://m.blog.chinaunix.net/uid-23802873-id-4477364.html</a></p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title>Python链接mysql数据库的一些操作</title>
    <url>/2015/09/10/python-e9-93-be-e6-8e-a5mysql-e6-95-b0-e6-8d-ae-e5-ba-93-e7-9a-84-e4-b8-80-e4-ba-9b-e6-93-8d-e4-bd-9c/</url>
    <content><![CDATA[<p>一、实验规划 1、首先实验之前，我们需要提前创建一个数据库TESTDB，然后授一个用户test管理该数据库,这里我设置的密码是test123; 2、要用python链接数据库需要用到第三方法模块MySQLdb; 3、利用python脚本对数据库做一些简单操作; 二、安装第三方模块MySQLdb. 1、获取模块</p>
<p>wget <a href="https://pypi.python.org/packages/source/M/MySQL-python/MySQL-python-1.2.5.zip">https://pypi.python.org/packages/source/M/MySQL-python/MySQL-python-1.2.5.zip</a></p>
<p>2、解压，安装</p>
<p>unzip MySQL-python-1.2.5.zip<br>cd MySQL-python-1.2.5<br>python setup.py build<br>python setup.py install</p>
<p>3、检查安装情况，进入python命令行执行 import，如果不报出任何信息说明成功安装并且导入成功</p>
<p>demo@demo:~$ python<br>Python 2.7.9 (default, Apr  2 2015, 15:33:21)<br>[GCC 4.9.2] on linux2<br>Type “help”, “copyright”, “credits” or “license” for more information.</p>
<blockquote>
<blockquote>
<p>&gt; import MySQLdb</p>
<blockquote>
</blockquote>
</blockquote>
</blockquote>
<p>三、对mysql数据库的一些操作 1、连接数据库TESTDB并查看数据库的版本</p>
<p>#!/usr/bin/env python<br>#-*- coding: UTF-8 -*-<br>import MySQLdb</p>
<p>db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”)</p>
<p>cursor = db.cursor()</p>
<p>cursor.execute(“SELECT VERSION()”)</p>
<p>data = cursor.fetchone()</p>
<p>print “Database version: %s “ % data</p>
<p>db.close()</p>
<p>执行结果如下：</p>
<p>demo@demo:<del>/python_learn$ python mysql_connect.py<br>Database version: 5.6.25-0ubuntu0.15.04.1<br>demo@demo:</del>/python_learn$ </p>
<p>2、链接到数据库TESTDB并且创建一个张表</p>
<p>#!/usr/bin/env python<br>#-*- coding: UTF-8 -*-<br>import MySQLdb</p>
<p>db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”)</p>
<p>cursor = db.cursor()</p>
<p>cursor.execute(“DROP TABLE IF EXISTS EMPLOYEE”)</p>
<p>sql = “””CREATE TABLE `EMPLOYEE` (<br>  `FIRST_NAME` varchar(20) DEFAULT NULL,<br>  `LAST_NAME` varchar(20) DEFAULT NULL,<br>  `AGE` int(11) DEFAULT NULL,<br>  `SEX` varchar(20) DEFAULT NULL,<br>  `INCOME` varchar(20) DEFAULT NULL<br>) ENGINE=InnoDB DEFAULT CHARSET=latin1 “””</p>
<p>cursor.execute(sql)</p>
<p>db.close()</p>
<p>执行结果我们通过链接到数据库去查看</p>
<p>mysql&gt; use TESTDB;<br>Database changed<br>mysql&gt; SHOW TABLES;<br>+——————+<br>| Tables_in_TESTDB |<br>+——————+<br>| EMPLOYEE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br>+——————+<br>1 row in set (0.00 sec)</p>
<p>mysql&gt;<br>mysql&gt; DESC EMPLOYEE;<br>+————+————-+——+—–+———+——-+<br>| Field&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Null | Key | Default | Extra |<br>+————+————-+——+—–+———+——-+<br>| FIRST_NAME | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br>| LAST_NAME&nbsp; | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br>| AGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | int(11)&nbsp;&nbsp;&nbsp;&nbsp; | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br>| SEX&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br>| INCOME&nbsp;&nbsp;&nbsp;&nbsp; | varchar(20) | YES&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp; | NULL&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<br>+————+————-+——+—–+———+——-+<br>5 rows in set (0.00 sec)</p>
<p>mysql&gt; select * from EMPLOYEE;   #目前该表没有任何数据<br>Empty set (0.00 sec)</p>
<p>mysql&gt;</p>
<p>3、连接到数据库，并在EMPLOYEE表中插入数据</p>
<p>#!/usr/bin/env python<br>#-*- coding: UTF-8 -*-<br>import MySQLdb</p>
<p>db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”)</p>
<p>cursor = db.cursor()</p>
<p>sql = “””INSERT INTO EMPLOYEE(FIRST_NAME,<br>	 LAST_NAME,AGE,SEX,INCOME)<br>	 VALUES (‘Guo’,’Maoqiuo’,’24’,’M’,’1991’)”””</p>
<p>try:<br>    cursor.execute(sql)<br>    db.commit()<br>except:<br>    db.rollback()</p>
<p>db.close()</p>
<p>执行结果如下</p>
<p>mysql&gt; select * from EMPLOYEE;<br>+————+———–+——+——+——–+<br>| FIRST_NAME | LAST_NAME | AGE  | SEX  | INCOME |<br>+————+———–+——+——+——–+<br>| Guo        | Maoqiuo   |   24 | M    | 1991   |<br>+————+———–+——+——+——–+<br>1 row in set (0.00 sec)</p>
<p>mysql&gt; </p>
<p>4、链接数据库使用select语句进行查询，这里实验使用条件查询，那我需要多一点数据才行，那我修改上面的那个脚本然后多执行几遍即可，看到的结果如下</p>
<p>mysql&gt; select * from EMPLOYEE;<br>+————+———–+——+——+——–+<br>| FIRST_NAME | LAST_NAME | AGE  | SEX  | INCOME |<br>+————+———–+——+——+——–+<br>| Guo        | Maoqiuo   |   24 | M    | 1991   |<br>| Bruce      | Li        |   25 | M    | 1989   |<br>| Fan        | bingbing  |   28 | F    | 1988   |<br>| Jack       | cheng     |   45 | M    | 1968   |<br>| joy        | jiang     |   26 | F    | 1977   |<br>+————+———–+——+——+——–+<br>5 rows in set (0.00 sec)</p>
<p>使用条件查询</p>
<p>#!/usr/bin/env python<br>#-*- coding: UTF-8 -*-<br>import MySQLdb</p>
<p>db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”)</p>
<p>cursor = db.cursor()</p>
<p>sql = “SELECT * FROM EMPLOYEE WHERE INCOME &gt; ‘%d’” % (1988)</p>
<p>try:<br>    cursor.execute(sql)<br>    results = cursor.fetchall()</p>
<pre><code>for row in results:
    fname = row\[0\]
    lname = row\[1\]
    age = row\[2\]
    sex = row\[3\]
    income = row\[4\]
    print "Fname=%s, Lanme=%s, Age=%d, Sex=%s, Income=%s\\n"  % (fname,lname,age,sex,income)
</code></pre>
<p>except:<br>    print “Error: unable to fecth data”</p>
<p>db.close()</p>
<p>执行结果如下</p>
<p>deamon@deamon:~/python_learn$ python mysql_select.py<br>Fname=Guo, Lanme=Maoqiuo, Age=24, Sex=M, Income=1991</p>
<p>Fname=Bruce, Lanme=Li, Age=25, Sex=M, Income=1989</p>
<p>4、连接到数据库，并更新在EMPLOYEE表的数据，将性别为M的人员在年龄上加2</p>
<p>#!/usr/bin/env python<br>#-*- coding: UTF-8 -*-<br>import MySQLdb</p>
<p>db = MySQLdb.connect(“localhost”,”testuser”,”test123”,”TESTDB”)</p>
<p>cursor = db.cursor()</p>
<p>sql = “UPDATE EMPLOYEE SET AGE = AGE +2  WHERE  SEX = ‘%c’ “ % (‘M’)</p>
<p>try:<br>    cursor.execute(sql)<br>    db.commit()<br>except:<br>    db.rollback()<br>    print “Error”</p>
<p>db.close()</p>
<p>执行后的结果</p>
<p>mysql&gt; select * from TESTDB.EMPLOYEE;<br>+————+———–+——+——+——–+<br>| FIRST_NAME | LAST_NAME | AGE  | SEX  | INCOME |<br>+————+———–+——+——+——–+<br>| Guo        | Maoqiuo   |   26 | M    | 1991   |<br>| Bruce      | Li        |   27 | M    | 1989   |<br>| Fan        | bingbing  |   28 | F    | 1988   |<br>| Jack       | cheng     |   47 | M    | 1968   |<br>| joy        | jiang     |   26 | F    | 1977   |<br>+————+———–+——+——+——–+<br>5 rows in set (0.00 sec)</p>
<p>mysql&gt;</p>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>mysqldb</tag>
        <tag>python-mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Python os.system的结果不能赋值到变量</title>
    <url>/2015/09/15/python-os-system-e7-9a-84-e7-bb-93-e6-9e-9c-e4-b8-8d-e8-83-bd-e8-b5-8b-e5-80-bc-e5-88-b0-e5-8f-98-e9-87-8f/</url>
    <content><![CDATA[<p>今天在学习python os模块的system方法时，发现不能赋值给变量，具体操作如下 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-15-224523.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-15-224523.png" alt="Screenshot from 2015-09-15 22:45:23"></a> &nbsp; 后来查询得知有更新的模块，如下：</p>
<p>os.system<br>os.spawn*<br>os.popen*<br>popen2.*<br>commands.*</p>
<p>重新使用a = os.popen(‘df -hT’).read()&nbsp; 就能获取到啦。 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-15-225038.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-15-225038.png" alt="Screenshot from 2015-09-15 22:50:38"></a></p>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
  </entry>
  <entry>
    <title>千里马常有，而伯乐不常有</title>
    <url>/2016/05/08/qianlima/</url>
    <content><![CDATA[<p>世有伯乐，然后有千里马。千里马常有，而伯乐不常有；故虽有名马，祗辱于奴隶人之手，骈死于槽枥之间，不以千里称也。 马之千里者，一食或尽粟一石。食马者不知其能千里而食也；是马也，虽有千里之能，食不饱，力不足，才美不外见，且欲与常马等不可得，安求其能千里也？ 策之不以其道，食之不能尽其材，鸣之而不能通其意，执策而临之曰：“天下无马！”呜呼！其真无马邪？其真不知马也！</p>
]]></content>
      <categories>
        <category>心情随笔</category>
      </categories>
  </entry>
  <entry>
    <title>Redis集群详细搭建指南</title>
    <url>/2017/06/04/redis-ji-qun-xiang-xi-da-jian-zhi-nan/</url>
    <content><![CDATA[<h2 id="Redis-集群简介"><a href="#Redis-集群简介" class="headerlink" title="Redis 集群简介"></a>Redis 集群简介</h2><p>Redis 是一个开源的 key-value 存储系统，由于出众的性能，大部分互联网企业都用来做服务器端缓存。Redis 在3.0版本前只支持单实例模式，虽然支持主从模式、哨兵模式部署来解决单点故障，但是现在互联网企业动辄大几百G的数据，可完全是没法满足业务的需求，所以，Redis 在 3.0 版本以后就推出了集群模式。</p>
<p>Redis 从3.0.0正式版开始官方支持集群, Redis 集群采用了P2P的模式，完全去中心化。Redis 把所有的 Key 分成了 16384 个 slot，每个 Redis 实例负责其中一部分 slot 。集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新。</p>
<p>Redis 客户端可以在任意一个 Redis 实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例。</p>
<h2 id="随随便便搭建一个集群"><a href="#随随便便搭建一个集群" class="headerlink" title="随随便便搭建一个集群"></a>随随便便搭建一个集群</h2><p>安装部署任何一个应用其实都很简单，只要安装步骤一步一步来就行了。下面说一下 Redis 集群搭建规划，由于集群至少需要6个节点（3主3从模式），所以，没有这么多机器给我玩，现在计划是在一台机器上模拟一个集群，当然，这和生产环境的集群搭建没本质区别。</p>
<h4 id="1-获取软件包"><a href="#1-获取软件包" class="headerlink" title="1.获取软件包"></a>1.获取软件包</h4><pre><code>cd /tmp/ &amp;&amp; wget http://download.redis.io/releases/redis-4.0.11.tar.gz
mkdir /usr/local/redis &amp;&amp; tar -xr /tmp/redis-4.0.11.tar.gz -C /usr/local/redis
cd /usr/local/redis
./configure
make -j 4
make install
</code></pre>
<h4 id="2-规划创建文件目录"><a href="#2-规划创建文件目录" class="headerlink" title="2.规划创建文件目录"></a>2.规划创建文件目录</h4><p>我们计划集群中 Redis 节点的端口号为 9001-9006 ，端口号即集群下各实例文件夹。数据存放在 端口号/data 文件夹中。</p>
<pre><code>mkdir /usr/local/redis-cluster/
cd /usr/local/redis-cluster/

for i in {1..6};do
    mkdir 900${i}/data
done
</code></pre>
<h4 id="3-复制执行脚本"><a href="#3-复制执行脚本" class="headerlink" title="3. 复制执行脚本"></a>3. 复制执行脚本</h4><p>在 /usr/local/redis-cluster 下创建 bin 文件夹，用来存放集群运行脚本，并把安装好的 Redis 的 src 路径下的运行脚本拷贝过来。看命令：</p>
<pre><code>mkdir /usr/local/redis-cluster/bin
cd /usr/local/redis/src # 下载后解压编译后的src目录
cp mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-dump redis-cli redis-server redis-trib.rb /usr/local/redis-cluster/bin
</code></pre>
<h4 id="4-复制一个新-Redis-实例"><a href="#4-复制一个新-Redis-实例" class="headerlink" title="4.复制一个新 Redis 实例"></a>4.复制一个新 Redis 实例</h4><p>我们现在从已安装好的 Redis 中复制一个新的实例到 9001 文件夹，并修改 redis.conf 配置。</p>
<pre><code>cp /usr/local/redis/* /usr/local/redis-cluster/9001


port 9001（每个节点的端口号）
daemonize yes
bind 192.168.56.127（绑定当前机器 IP）
dir /usr/local/redis-cluster/9001/data/（数据文件存放位置）
pidfile /var/run/redis_9001.pid（pid 9001和port要对应）
cluster-enabled yes（启动集群模式）
cluster-config-file nodes9001.conf（9001和port要对应）
cluster-node-timeout 15000
appendonly yes
</code></pre>
<h4 id="5-再复制出五个新-Redis-实例"><a href="#5-再复制出五个新-Redis-实例" class="headerlink" title="5.再复制出五个新 Redis 实例"></a>5.再复制出五个新 Redis 实例</h4><p>我们已经完成了一个节点了，其实接下来就是机械化的再完成另外五个节点，其实可以这么做：把 9001 实例 复制到另外五个文件夹中，唯一要修改的就是 redis.conf 中的所有和端口的相关的信息即可，其实就那么四个位置。开始操作，看命令：</p>
<pre><code>\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9002
\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9003
\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9004
\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9005
\cp -rf /usr/local/redis-cluster/9001/* /usr/local/redis-cluster/9006
</code></pre>
<p>这里我们把配置复制过去之后需要修改对应文件夹中的redis.conf 用sed或者vim全局替换即可，主要是端口</p>
<p>下面，下面我们来启动它们</p>
<pre><code>for i in {1..6};do
    /usr/local/bin/redis-server /usr/local/redis-cluster/900${i}/redis.conf 
done


[root@localhost redis-cluster]# ps aux | grep redis | grep -v grep
root       3995  0.5  0.6 151404 11864 ?        Ssl  10:51   0:10 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9001 [cluster]
root       4000  0.4  0.6 151404 11856 ?        Ssl  10:51   0:09 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9002 [cluster]
root       4005  0.4  0.6 151404 11856 ?        Ssl  10:51   0:09 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9003 [cluster]
root       4010  0.1  0.6 151404 11872 ?        Ssl  10:51   0:02 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9004 [cluster]
root       4015  0.0  0.6 151404 11876 ?        Ssl  10:51   0:01 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9005 [cluster]
root       4020  0.0  0.6 151404 11872 ?        Ssl  10:51   0:01 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9006 [cluster]
root       4509  0.8  0.6 149356 11880 ?        Ssl  10:55   0:14 /usr/local/redis-cluster/bin/redis-server 
[root@localhost redis-cluster]#
</code></pre>
<p>以上 可以看出我们的6个实例已经启动了<br>测试一下？</p>
<p>随便找个节点测试一下:</p>
<pre><code>/usr/local/redis-cluster/bin/redis-cli -h 192.168.56.127 -p 9001

192.168.56.127:9001&gt; set name guomaoqiu
(error) CLUSTERDOWN Hash slot not served
192.168.56.127:9001&gt;
</code></pre>
<p>连接成功了，但好像报错了</p>
<p>这是因为虽然我们配置并启动了 Redis 集群服务，但是他们暂时还并不在一个集群中，互相直接发现不了，而且还没有可存储的位置，就是所谓的slot（槽）。</p>
<h4 id="6-安装集群所需软件"><a href="#6-安装集群所需软件" class="headerlink" title="6.安装集群所需软件"></a>6.安装集群所需软件</h4><p>由于 Redis 集群需要使用 ruby 命令，所以我们需要安装 ruby 和相关接口。</p>
<pre><code>安装redis需要ruby版本最低是2.2.2，而centos yum库中ruby版本支持到2.0.0。所以，无法满足需求。
解决方案：

1.安装curl
sudo yum install curl
2.安装RVM
curl -L get.rvm.io | bash -s stable
source /usr/local/rvm/scripts/rvm
3.查看rvm库中已知的ruby版本
rvm list known
4.安装一个ruby版本
rvm install 2.3.3
5.设置默认版本
rvm use 2.3.3
6.卸载一个已知版本
rvm remove 2.0.0
7.查看ruby版本
ruby –version
8.再安装redis就可以了
gem install redis
</code></pre>
<h4 id="7-利用官方提供的命令创建集群"><a href="#7-利用官方提供的命令创建集群" class="headerlink" title="7. 利用官方提供的命令创建集群"></a>7. 利用官方提供的命令创建集群</h4><pre><code>/usr/local/redis-cluster/bin/redis-trib.rb create --replicas 1 192.168.56.127:9001 192.168.56.127:9002 192.168.56.127:9003 192.168.56.127:9004 192.168.56.127:9005 192.168.56.127:9006
</code></pre>
<p>简单解释一下这个命令：调用 ruby 命令来进行创建集群，–replicas 1 表示主从复制比例为 1:1，即一个主节点对应一个从节点；然后，默认给我们分配好了每个主节点和对应从节点服务，以及 solt 的大小，因为在 Redis 集群中有且仅有 16383 个 solt ，默认情况会给我们平均分配，当然你可以指定，后续的增减节点也可以重新分配。</p>
<pre><code>[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb create --replicas 1 192.168.56.127:9001 192.168.56.127:9002 192.168.56.127:9003 192.168.56.127:9004 192.168.56.127:9005 192.168.56.127:9006
&gt;&gt;&gt; Creating cluster
&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...
Using 3 masters:
# 三个主节点
192.168.56.127:9001
192.168.56.127:9002
192.168.56.127:9003
# 三个主节点对应三个从节点
Adding replica 192.168.56.127:9005 to 192.168.56.127:9001
Adding replica 192.168.56.127:9006 to 192.168.56.127:9002
Adding replica 192.168.56.127:9004 to 192.168.56.127:9003
&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity
[WARNING] Some slaves are in the same host as their master
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:0-5460 (5461 slots) master
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:5461-10922 (5462 slots) master
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots:10923-16383 (5461 slots) master
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   replicates e070e76789352d2492cfc9800850e943e0c4b72d
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   replicates 0fcbaa301451baf87284546513003568e47b5daa
Can I set the above configuration? (type 'yes' to accept): yes
&gt;&gt;&gt; Nodes configuration updated
&gt;&gt;&gt; Assign a different config epoch to each node
&gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join...
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:5461-10922 (5462 slots) master
   1 additional replica(s)
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots:10923-16383 (5461 slots) master
   1 additional replica(s)
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates e070e76789352d2492cfc9800850e943e0c4b72d
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
</code></pre>
<p>以上执行结果代表集群搭建成功啦！！！</p>
<p>验证：</p>
<pre><code>/usr/local/redis-cluster/bin/redis-cli -c -h 192.168.56.127 -p 9001
192.168.56.127:9001&gt; cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:167
cluster_stats_messages_pong_sent:172
cluster_stats_messages_sent:339
cluster_stats_messages_ping_received:167
cluster_stats_messages_pong_received:167
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:339
192.168.56.127:9001&gt; cluster nodes
474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002@19002 master - 0 1536205294229 2 connected 5461-10922
e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003@19003 master - 0 1536205293000 3 connected 10923-16383
bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006@19006 slave 0fcbaa301451baf87284546513003568e47b5daa 0 1536205295234 6 connected
0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001@19001 myself,master - 0 1536205294000 1 connected 0-5460
950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004@19004 slave 474fd53f7199ad70cce7f18bd68f5445b559509a 0 1536205293217 4 connected
17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005@19005 slave e070e76789352d2492cfc9800850e943e0c4b72d 0 1536205292212 5 connected
192.168.56.127:9001&gt; set name guomaoqiu
-&gt; Redirected to slot [5798] located at 192.168.56.127:9002
OK
192.168.56.127:9002&gt; get name
"guomaoqiu"
192.168.56.127:9002&gt;
</code></pre>
<p>通过命令，可以详细的看出集群信息和各个节点状态，主从信息以及连接数、槽信息等。这么看到，我们已经真的把 Redis 集群搭建部署成功啦！</p>
<h2 id="如何新增节点？"><a href="#如何新增节点？" class="headerlink" title="如何新增节点？"></a>如何新增节点？</h2><p>新增节点无非就是复制配置，修改配置，然后启动；我这里还是创建了一对一主一从的配置，端口分别对应的是9007、9008</p>
<h4 id="1-启动两个实例-假设我这里已经复制配置，修改完配置了"><a href="#1-启动两个实例-假设我这里已经复制配置，修改完配置了" class="headerlink" title="1.启动两个实例(假设我这里已经复制配置，修改完配置了)"></a>1.启动两个实例(假设我这里已经复制配置，修改完配置了)</h4><pre><code>for i in {7..8};do
    /usr/local/redis-cluster/bin/redis-server /usr/local/redis-cluster/900${i}/redis.conf 
done

6002:C 06 Sep 11:50:53.048 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
6002:C 06 Sep 11:50:53.048 # Redis version=4.0.11, bits=64, commit=00000000, modified=0, pid=6002, just started
6002:C 06 Sep 11:50:53.048 # Configuration loaded
6004:C 06 Sep 11:50:53.060 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
6004:C 06 Sep 11:50:53.060 # Redis version=4.0.11, bits=64, commit=00000000, modified=0, pid=6004, just started
6004:C 06 Sep 11:50:53.060 # Configuration loaded

ps aux | grep redis | grep -v grep
redis      1180  0.0  0.3 142900  5828 ?        Ssl  11:28   0:01 /usr/bin/redis-server 0.0.0.0:6379
root       4257  0.0  0.6 149356 11880 ?        Ssl  11:32   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9001 [cluster]
root       4262  0.0  0.6 149356 11880 ?        Ssl  11:32   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9002 [cluster]
root       4267  0.0  0.6 149356 11876 ?        Ssl  11:32   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9003 [cluster]
root       4272  0.0  0.6 149356 11908 ?        Ssl  11:32   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9004 [cluster]
root       4277  0.0  0.6 149356 11920 ?        Ssl  11:32   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9005 [cluster]
root       4282  0.0  0.6 149356 11912 ?        Ssl  11:32   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9006 [cluster]
root       6003  0.1  0.5 147308  9616 ?        Ssl  11:50   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9007 [cluster]
root       6008  0.0  0.5 147308  9616 ?        Ssl  11:50   0:00 /usr/local/redis-cluster/bin/redis-server 192.168.56.127:9008 [cluster]
</code></pre>
<p>以上 可以看到我们启动了后面两个实例，并且进程中标注了是以集群方式启动, 但是此时还并没有将实例加入到集群当中。</p>
<h4 id="2-添加实例到集群"><a href="#2-添加实例到集群" class="headerlink" title="2.添加实例到集群"></a>2.添加实例到集群</h4><pre><code>[root@locahost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb add-node 192.168.56.127:9007 192.168.56.127:9001
&gt;&gt;&gt; Adding node 192.168.56.127:9007 to cluster 192.168.56.127:9001
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:5461-10922 (5462 slots) master
   1 additional replica(s)
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots:10923-16383 (5461 slots) master
   1 additional replica(s)
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates e070e76789352d2492cfc9800850e943e0c4b72d
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
&gt;&gt;&gt; Send CLUSTER MEET to node 192.168.56.127:9007 to make it join the cluster.
[OK] New node added correctly.
[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-cli -c -h 192.168.56.127 -p 9001 cluster nodes
474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002@19002 master - 0 1536215194000 2 connected 5461-10922
e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003@19003 master - 0 1536215194000 3 connected 10923-16383
bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006@19006 slave 0fcbaa301451baf87284546513003568e47b5daa 0 1536215193000 6 connected
0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001@19001 myself,master - 0 1536215192000 1 connected 0-5460
950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004@19004 slave 474fd53f7199ad70cce7f18bd68f5445b559509a 0 1536215195858 4 connected
1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007@19007 master - 0 1536215195000 0 connected
17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005@19005 slave e070e76789352d2492cfc9800850e943e0c4b72d 0 1536215194855 5 connected
</code></pre>
<p>以上可以看到我们通过 add-node 参数添加了 9007这个实例；命令最后一个参数是已经成为集群的集群成员host:port<br>而且实例9007获得了一个集群ID:<code>1714784542b3afc9e2a8b2c93fff49d095e7d72b</code></p>
<h4 id="5-增加实例的从节点到集群"><a href="#5-增加实例的从节点到集群" class="headerlink" title="5. 增加实例的从节点到集群"></a>5. 增加实例的从节点到集群</h4><p>以上我们将实例9007作为master增加到了集群中，但是按照前面的规划，每个master对应一个slave所以我们需要将实例9008加入集群并将其称为9007的slave,命令如下</p>
<pre><code>[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb add-node --slave --master-id 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9008 192.168.56.127:9001
&gt;&gt;&gt; Adding node 192.168.56.127:9008 to cluster 192.168.56.127:9001
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:5461-10922 (5462 slots) master
   1 additional replica(s)
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots:10923-16383 (5461 slots) master
   1 additional replica(s)
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007
   slots: (0 slots) master
   0 additional replica(s)
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates e070e76789352d2492cfc9800850e943e0c4b72d
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
&gt;&gt;&gt; Send CLUSTER MEET to node 192.168.56.127:9008 to make it join the cluster.
Waiting for the cluster to join.
&gt;&gt;&gt; Configure node as replica of 192.168.56.127:9007.
[OK] New node added correctly.

[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb check 192.168.56.127:9001
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:5461-10922 (5462 slots) master
   1 additional replica(s)
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots:10923-16383 (5461 slots) master
   1 additional replica(s)
S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008
   slots: (0 slots) slave
   replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007
   slots: (0 slots) master
   1 additional replica(s)
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates e070e76789352d2492cfc9800850e943e0c4b72d
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
</code></pre>
<p>以上我们将两个节点加入到了集群中，但是细心的同学会发现我们的实例9007所拥有的slot数量为0，那此时就需要将重新调整哈希槽啦~<br>再次之前我们三个master 平均将16384 分成了三等分，那我们新增加了一个master ，为了平衡我们可以计算出这个新的master需要多少哈希槽，即 16384/4 = 4096 我们需要移动4096个槽点到9007上。</p>
<pre><code>[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb reshard 192.168.56.127:9001
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008
   slots: (0 slots) slave
   replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b
M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007
   slots:5461-5798 (0 slots) master
   1 additional replica(s)
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates e070e76789352d2492cfc9800850e943e0c4b72d
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:5799-10922 (5462 slots) master
   1 additional replica(s)
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots:10923-16383 (5461 slots) master
   1 additional replica(s)
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
# 这里需要输入我们所需要的哈希槽
How many slots do you want to move (from 1 to 16384)? 4097
# 这里需要输入我们实例9007的ID
What is the receiving node ID? 1714784542b3afc9e2a8b2c93fff49d095e7d72b
Please enter all the source node IDs.
  Type 'all' to use all the nodes as source nodes for the hash slots.
  Type 'done' once you entered all the source nodes IDs.
Source node #1:all
#输入all 表示从所有的主节点中随机转移，凑够1000个哈希槽
#然后再输入yes，redis集群就开始分配哈希槽了。
</code></pre>
<p>以上:<br>redis-trib 会向你询问重新分片的源节点（source node），即，要从特点的哪个节点中取出 4096 个哈希槽，还是从全部节点提取4096个哈希槽， 并将这些槽移动到9007节点上面。</p>
<p>如果我们不打算从特定的节点上取出指定数量的哈希槽，那么可以向redis-trib输入 all，这样的话， 集群中的所有主节点都会成为源节点，redis-trib从各个源节点中各取出一部分哈希槽，凑够4096个，然后移动到9007节点上：</p>
<p>验证：</p>
<pre><code>[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb check 192.168.56.127:9001
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:1280-5460 (4096 slots) master
   1 additional replica(s)
S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008
   slots: (0 slots) slave
   replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b
M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007
   slots:0-1279,5461-6998,10923-12201 (4096 slots) master
   1 additional replica(s)
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates e070e76789352d2492cfc9800850e943e0c4b72d
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:6999-10922 (4096 slots) master
   1 additional replica(s)
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots:12202-16383 (4096 slots) master
   1 additional replica(s)
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
</code></pre>
<p>以上我们的集群搭建以及新增节点就完成了；</p>
<h4 id="6-从集群中删除节点"><a href="#6-从集群中删除节点" class="headerlink" title="6. 从集群中删除节点:"></a>6. 从集群中删除节点:</h4><p>和节点添加一样，移除节点也有移除主节点，从节点</p>
<h5 id="1、移除主节点"><a href="#1、移除主节点" class="headerlink" title="1、移除主节点"></a>1、移除主节点</h5><p>移除节点使用redis-trib的del-node命令</p>
<pre><code>/usr/local/redis-cluster/bin/redis-trib.rb del-node 192.168.56.127:9001 ${node-id}
</code></pre>
<p>192.168.56.127:9001其中一个集群节点，node-id为要删除的主节点,这里和添加节点不同,移除节点node-id是必需的，测试删除9003主节点</p>
<pre><code>[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb del-node 192.168.56.127:9001 e070e76789352d2492cfc9800850e943e0c4b72d
&gt;&gt;&gt; Removing node e070e76789352d2492cfc9800850e943e0c4b72d from cluster 192.168.56.127:9001
[ERR] Node 192.168.56.127:9003 is not empty! Reshard data away and try again.
</code></pre>
<p>redis cluster提示9003已经有数据了，不能够被删除，需要将他的数据转移出去，也就是和新增主节点一样需重新分片。</p>
<pre><code>/usr/local/redis-cluster/bin/redis-trib.rb reshard 192.168.56.127:9001
</code></pre>
<p>执行以后会提示我们移除的大小，因为9003占用了4096个槽点</p>
<pre><code>&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
How many slots do you want to move (from 1 to 16384)?
</code></pre>
<p>输入4096<br>提示移动的node id，填写9007的node id。</p>
<pre><code>&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
How many slots do you want to move (from 1 to 16384)? 4182
What is the receiving node ID? 1714784542b3afc9e2a8b2c93fff49d095e7d72b
</code></pre>
<p>需要移动到全部主节点上还是单个主节点</p>
<pre><code>Please enter all the source node IDs.
  Type 'all' to use all the nodes as source nodes for the hash slots.
  Type 'done' once you entered all the source nodes IDs.
Source node #1:
</code></pre>
<p>将4096个槽点移动到9007上，填写9003的node id ：e070e76789352d2492cfc9800850e943e0c4b72d</p>
<pre><code>Source node #1:e070e76789352d2492cfc9800850e943e0c4b72d
Source node #2:done
</code></pre>
<p>确认之后会一个一个将9003的卡槽移到到9007上。</p>
<pre><code>[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb check 192.168.56.127:9001
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:1280-5460 (4096 slots) master
   1 additional replica(s)
S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008
   slots: (0 slots) slave
   replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b
M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007
   slots:0-1279,5461-6998,10923-16383 (8192 slots) master
   2 additional replica(s)
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:6999-10922 (4096 slots) master
   1 additional replica(s)
M: e070e76789352d2492cfc9800850e943e0c4b72d 192.168.56.127:9003
   slots: (0 slots) master
   0 additional replica(s)
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
</code></pre>
<p>以上我们将实例9003的哈希槽全部转移到了9007，已经为0 ,此时再来删除我们的节点就行了~</p>
<pre><code>[root@localhost redis-cluster]# /usr/local/redis-cluster/bin/redis-trib.rb del-node 192.168.56.127:9001 e070e76789352d2492cfc9800850e943e0c4b72d
&gt;&gt;&gt; Removing node e070e76789352d2492cfc9800850e943e0c4b72d from cluster 192.168.56.127:9001
&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...
&gt;&gt;&gt; SHUTDOWN the node.

# 看下面结果 执行完del-node 9003节点就已经从集群中剔除啦~😁
&gt;&gt;&gt; Performing Cluster Check (using node 192.168.56.127:9001)
M: 0fcbaa301451baf87284546513003568e47b5daa 192.168.56.127:9001
   slots:1280-5460 (4096 slots) master
   1 additional replica(s)
S: 5ed978b695e6a222e67d13c68e55f6c4e74b7ba6 192.168.56.127:9008
   slots: (0 slots) slave
   replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b
M: 1714784542b3afc9e2a8b2c93fff49d095e7d72b 192.168.56.127:9007
   slots:0-1279,5461-6998,10923-16383 (8192 slots) master
   2 additional replica(s)
S: 17db098db9fd222df002e649506f88efcdc83633 192.168.56.127:9005
   slots: (0 slots) slave
   replicates 1714784542b3afc9e2a8b2c93fff49d095e7d72b
M: 474fd53f7199ad70cce7f18bd68f5445b559509a 192.168.56.127:9002
   slots:6999-10922 (4096 slots) master
   1 additional replica(s)
S: bd278fcea63862199e964a6aab2e786710cf5575 192.168.56.127:9006
   slots: (0 slots) slave
   replicates 0fcbaa301451baf87284546513003568e47b5daa
S: 950c399165439660efb6ff2f907f938e213de530 192.168.56.127:9004
   slots: (0 slots) slave
   replicates 474fd53f7199ad70cce7f18bd68f5445b559509a
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
</code></pre>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>当然我们集群搭建已经完成，那集群的作用是为了解决分片的问题，如果需要搭建高可用集群；也就是主的失败后从的能够切换角色,<br>所以可以使用哨兵机制来解决HA.<br>sentinel和cluster主要区别，sentinel用来解决HA（高可用）问题，而cluster主要解决sharding（分片）问题。两个经常结合使用搭建高可用集群。</p>
<h2 id="关于哈希槽的概念："><a href="#关于哈希槽的概念：" class="headerlink" title="关于哈希槽的概念："></a>关于哈希槽的概念：</h2><p>Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。</p>
<p>Redis 集群没有使用一致性hash, 而是引入了哈希槽的概念。</p>
<p>Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽。这种结构很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。</p>
<p>使用哈希槽的好处就在于可以方便的添加或移除节点。</p>
<p>当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了；</p>
<p>当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了；</p>
<p>在这一点上，我们以后新增或移除节点的时候不用先停掉所有的 redis 服务。</p>
<p>“用了哈希槽的概念，而没有用一致性哈希算法，不都是哈希么？这样做的原因是为什么呢？”<br>Redis Cluster是自己做的crc16的简单hash算法，没有用一致性hash。Redis的作者认为它的crc16(key) mod 16384的效果已经不错了，虽然没有一致性hash灵活，但实现很简单，节点增删时处理起来也很方便。</p>
<p>“为了动态增删节点的时候，不至于丢失数据么？”<br>节点增删时不丢失数据和hash算法没什么关系，不丢失数据要求的是一份数据有多个副本。</p>
<p>“还有集群总共有2的14次方，16384个哈希槽，那么每一个哈希槽中存的key 和 value是什么？”<br>当你往Redis Cluster中加入一个Key时，会根据crc16(key) mod 16384计算这个key应该分布到哪个hash slot中，一个hash slot中会有很多key和value。你可以理解成表的分区，使用单节点时的redis时只有一个表，所有的key都放在这个表里；改用Redis Cluster以后会自动为你生成16384个分区表，你insert数据时会根据上面的简单算法来决定你的key应该存在哪个分区，每个分区里有很多key。</p>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>redis-cluster</tag>
      </tags>
  </entry>
  <entry>
    <title>如何将pod中的container时区更改为同一时区的城市或UTC时区偏移</title>
    <url>/2018/07/14/ru-he-jiangpod-zhong-decontainer-shi-qu-geng-gai-w/</url>
    <content><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题:"></a>问题:</h2><p>在创建pod container后发现里面的时区是UTC,对于国内习惯还是CST时区比较易读；那如何解决这种问题嘛？暂时想到的两种办法:</p>
<pre><code>[root@linux-node1 ~]# kubectl exec flask-app-nginx-66b56f556c-zb84s date -n flask-app-extions-stage
Mon Jul 14 07:32:52 UTC 2018
[root@linux-node1 ~]# date
Mon Jul 14 15:32:52 CST 2018
</code></pre>
<ul>
<li>直接修改镜像的时间设置，好处是应用部署时无需特殊设置，但是需要手动从新构建Docker镜像</li>
<li>部署应用时，单独读取主机的”/etc/localtime”,无需修改镜像，但是每个应用都要单独设置。</li>
</ul>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决:"></a>解决:</h2><p>为了快速，简单的解决此问题，先使用第二种方法；yaml文件中映射主机的”/etc/localtime”文件, 添加yaml配置:</p>
<pre><code>......
......
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        volumeMounts:
          - name: nginx-conf
            mountPath: "/etc/nginx/nginx.conf"
            subPath: nginx.conf
          - name: host-time
            mountPath: /etc/localtime
      volumes:
      - name: nginx-conf
        configMap:
          name: nginx-conf
      - name: host-time
        hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
......
......   

[root@linux-node1 flask_app_nginx]# kubectl apply -f flask_app_nginx_deploy.yaml 

[root@linux-node1 flask_app_nginx]# kubectl exec flask-app-nginx-f4d9759b4-xq4rk date -n flask-app-extions-stage
Mon Jul 14 15:50:29 CST 2018
[root@linux-node1 flask_app_nginx]# date
Mon Jul 14 15:50:29 CST 2018      

# 以上，就完成了pod container的时区修改问题...
</code></pre>
<p>其实还有一种方法就是将 <code>/usr/share/zoneinfo/Asia/Shanghai</code> 这个文件做成之前我挂载nginx配置文件那样通过ConfigMap的形式挂载.</p>
]]></content>
      <categories>
        <category>虚拟化&amp;amp;云计算&amp;amp;大数据</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>如何实现Mysql数据库差异化对比</title>
    <url>/2018/03/18/ru-he-shi-xianmysql-shu-ju-ku-cha-yi-hua-dui-bi/</url>
    <content><![CDATA[<p>在团队开发中，一般都会存在测试、预发布、正式环境或多版本进行开发；代码的管理一般也有git/svn等等工具；</p>
<p>但是在mysql的管理就有些麻烦了，对于一些正规化的大厂团队，对数据库的每一次表结构都有详细的记录，这样在执行变更/升级的时候只需要执行直接执行变更过的SQL即可，但是有时候也会出现记录不完整或者遗漏造成测试/预发布/正式环境的不一致。</p>
<p>这时候就需要人工去查找两个数据库数据表中的不同；看哪里少什么，哪里多了什么，但是如果人工去每次desc/select是很费时费力的事情；那么这时候我们就需要用到mysql的相关工具；例如<a href="https://dev.mysql.com/doc/mysql-utilities/1.6/en/mysqldiff.html">mysqldiff</a></p>
<h5 id="例如"><a href="#例如" class="headerlink" title="例如:"></a>例如:</h5><p>这里有个<code>db1</code>跟<code>db2</code> 数据库，各自里面有两张表<code>student_1</code>,<code>student2</code>,这里只是举个例子，下面的结构用肉眼是可以看出来的；</p>
<pre><code>MariaDB [(none)]&gt; use db1;
Database changed
MariaDB [db1]&gt; show tables;
+---------------+
| Tables_in_db1 |
+---------------+
| student_1     |
+---------------+
1 row in set (0.00 sec)

MariaDB [db1]&gt; desc student_1;
+-------------+-------------+------+-----+---------+-------+
| Field       | Type        | Null | Key | Default | Extra |
+-------------+-------------+------+-----+---------+-------+
| studentNo   | char(10)    | NO   | PRI | NULL    |       |
| studentName | varchar(20) | NO   |     | NULL    |       |
| sex         | char(2)     | YES  |     | NULL    |       |
| birthday    | date        | YES  |     | NULL    |       |
| native      | varchar(20) | YES  |     | NULL    |       |
| nation      | varchar(20) | YES  |     | NULL    |       |
| classNo     | char(6)     | YES  |     | NULL    |       |
+-------------+-------------+------+-----+---------+-------+
7 rows in set (0.01 sec)

MariaDB [db1]&gt; use db2;
Database changed
MariaDB [db2]&gt; desc student_2;
+-------------+-------------+------+-----+---------+-------+
| Field       | Type        | Null | Key | Default | Extra |
+-------------+-------------+------+-----+---------+-------+
| studentNo   | char(10)    | NO   | PRI | NULL    |       |
| studentName | varchar(20) | NO   |     | NULL    |       |
| sex         | char(2)     | YES  |     | NULL    |       |
| birthday    | date        | YES  |     | NULL    |       |
| native      | varchar(40) | YES  |     | NULL    |       |
| nation      | varchar(20) | YES  |     | NULL    |       |
+-------------+-------------+------+-----+---------+-------+
6 rows in set (0.00 sec)

MariaDB [db2]&gt;
</code></pre>
<h5 id="如果使用mysqldiff工具输出将会是这样的"><a href="#如果使用mysqldiff工具输出将会是这样的" class="headerlink" title="如果使用mysqldiff工具输出将会是这样的:"></a>如果使用mysqldiff工具输出将会是这样的:</h5><pre><code>mysqldiff --server1=root:123.com@127.0.0.1:3306 --server2=root:123.com@127.0.0.1:3306   db1.student_1:db2.student_2;
# WARNING: Using a password on the command line interface can be insecure.
# server1 on 127.0.0.1: ... connected.
# server2 on 127.0.0.1: ... connected.
# Comparing db1.student_1 to db2.student_2                         [FAIL]
# Object definitions differ. (--changes-for=server1)
#

--- db1.student_1
+++ db2.student_2
@@ -1,10 +1,9 @@
-CREATE TABLE `student_1` (
+CREATE TABLE `student_2` (
   `studentNo` char(10) NOT NULL,
   `studentName` varchar(20) NOT NULL,
   `sex` char(2) DEFAULT NULL,
   `birthday` date DEFAULT NULL,
-  `native` varchar(20) DEFAULT NULL,
+  `native` varchar(40) DEFAULT NULL,
   `nation` varchar(20) DEFAULT NULL,
-  `classNo` char(6) DEFAULT NULL,
   UNIQUE KEY `studentNo` (`studentNo`)
 ) ENGINE=InnoDB DEFAULT CHARSET=utf8
# Compare failed. One or more differences found.
</code></pre>
<p>从以上输出可以看出来, <code>db2</code>的<code>student_2</code>相对于<code>db1</code>的<code>student_1</code>结构<br>1.字段<code>native</code>的vachar类型限制不同；表<code>student_2</code>是40,表<code>student_1</code>是40；<br>2.表<code>student_2</code>缺少字段<code>classNo</code></p>
<p>这样一来就能很快速的输出两个库中表结构的差异；然后看以那个库为标杆进行Alert操作就行了<br>下面是我改正后再次执行mysqldiff工具命令的结果输出:</p>
<pre><code>mysqldiff --server1=root:123.com@127.0.0.1:3306 --server2=root:123.com@127.0.0.1:3306   db1.student_1:db2.student_2;
# WARNING: Using a password on the command line interface can be insecure.
# server1 on 127.0.0.1: ... connected.
# server2 on 127.0.0.1: ... connected.
# Comparing db1.student_1 to db2.student_2                         [PASS]
# WARNING: The tables structure is the same, but the columns order is different. Use --change-for to take the order into account.
# Success. All objects are the same.
</code></pre>
<p>意思就是说检测通过，看起来所有的对象都是一样的。</p>
<p>So, 简单使用方法就是酱紫的了；其实还有更多的选项就行操作的；但是目前暂时没有用到；需要的话找man就好了。</p>
]]></content>
      <categories>
        <category>必备知识</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>系统运维</tag>
      </tags>
  </entry>
  <entry>
    <title>CKA(Certified Kubernetes Adminsitrator)认证获取历程</title>
    <url>/2019/01/07/road-to-cka/</url>
    <content><![CDATA[<p><em>my experience with cka exam preparation</em></p>
<hr>
<h1 id="What-is-CKA"><a href="#What-is-CKA" class="headerlink" title="What is CKA?"></a>What is CKA?</h1><p>CKA全称就是<code>Certified Kubernetes Adminsitrator</code>，是由CNCF(Cloud Native Computing Foundation 云原生计算基金会)提供的认证项目，考试费用为<code>300</code>美金，必须要双币行用卡，考试过程为<code>3</code>个小时。<br>我办理的是招行的MasterCard，给了20块加急；缴考试费到考完整个流程方面还是非常简单的。</p>
<div style="width: 40%; margin: auto">![](road-to-cka/WechatIMG319.jpeg)</div>
如果考试第一次考试不通过，账号内会生成一个`Free Retake`，在一年之内有一次免费重考的机会。 

<h1 id="Why-should-obtain-certification"><a href="#Why-should-obtain-certification" class="headerlink" title="Why should obtain certification"></a>Why should obtain certification</h1><p>很有幸在春节前(2019/01/06)完成了2018下半年既定的目标，将CKA顺利拿下。<br>很巧合的是在6年前也就是2013年1月5号我通过了<a href="https://blog.sctux.com/Guo_Maoqiu_RHCE.pdf">RHCE</a>认证考试(虽然我的RHCE认证早已过期，但是我更想说的是: <em><strong>万变不离其宗</strong></em> )<br><img src="/2019/01/07/road-to-cka/CKA.png"></p>
<p><img src="/2019/01/07/road-to-cka/verification.png"><br>当K8S已经进入到各行各业，那么一套公正权威的管理员测评体系必然就呼之欲出了。<br>也许你会说互联网企业不相信认证，确实一张纸并不能说明什么。这些认证的意义何在，或者怎么才能向别人证明你会用呢？答案自然就是考证了。RHCE也好、CKA也好对我而言其实更多的是一种鞭策，让自己心里有个小目标，同时能够以运维人员的身份去系统的学习、使用他们。</p>
<p>经过我个人的学习，参考过程，我认为这个认证的意义和价值如下：</p>
<ol>
<li>从考试大纲可以看出考试内容涵盖的知识点很全面，也就是说你需要学习或者了解的知识点也就很多;</li>
<li>对于计划构建自己k8s集群的个人和企业，有CKA在你的团队里能让你更快更稳的开始你的实践。毕竟这帮人是从徒手搭集群开始的。</li>
<li>参加这个考试的时间非常紧张，参加人必须对k8s大部分资源的yaml和命令烂熟于心。你随便找个cka来，让他徒手给你写一个应用的编排出来，从deploy 到pv到service，也许他个别细节和词法可能会写错，大体写出来是一点问题都没有的，期间完全不用参考文档。我要说的是cka的基本技能非常过硬。</li>
<li>截至到现在，我没有见过任何组织和个人提供cka必过手册（也就是传说中的bible）。这个认证诞生也就一、两年的时间， 因此现在通过的人都是货真价实考出来的（相对于其他公司的某cm，某某ca等等），这个价值会体现在证书编号上。再过几年，以我们中国人的考试能力来说，你懂的…</li>
<li>本考试从报名考试到接洽参考，全程英文交流，能考过的人英文水平还算说得过去吧。</li>
</ol>
<h1 id="我的学习过程、途径"><a href="#我的学习过程、途径" class="headerlink" title="我的学习过程、途径:"></a>我的学习过程、途径:</h1><ul>
<li>其实早在2015年就在接触容器技术，从LXC到Docker再到现在的kubernetes, 日常工作中的也尽量把一些应用做成docker去跑；省下了不少精力时间去做一些重复复杂性的工作；比如以前跑个gitLab的环境，亦或是想要个Python应用运行的环境，除了要把服务器初始化完成之后，还需要一些繁琐的配置，有了docker之后，通过<code>dockerfile/docker-compose</code>就可以很轻松的实现一些应用跑在容器里面了。2018年5月份开始研究学习kubernetes相关东西，虽然学习得不是很系统，但是还是完成了一个小小的项目，感兴趣的点<a href="https://github.com/guomaoqiu/flask_kubernetes">这里</a> 可以作为学习练习的项目;既然学了就要用上；不然也是徒劳。</li>
<li>K8s.io里面tasks相关的东西都需要自己实践下，还是有些日常没怎么用到的,最好把整个文档都看一遍。</li>
<li>宋大神的博客: <a href="https://jimmysong.io/kubernetes-handbook/">https://jimmysong.io/kubernetes-handbook/</a></li>
<li>学习实操环境做最好是通过二进制的方式来搭建，毕竟如果你使用kubeadm来搭建的话很多的事情它已经帮你完成设定了，也就没理解为什么要这么做。每个组件结合起来之后整个架构在脑子里面就会非常的清楚，排错起来也更加容易一些。考试不难，只要经过大量的实践练习就可以啦~</li>
</ul>
<p><em><strong>注:</strong></em> 这次CKA考试，我没有参加任何国内培训，主要是根据考试大纲、官方文档以及CNCF推出的一套<a href="https://www.edx.org/course/introduction-to-kubernetes">免费课程</a>进行自学。</p>
<h1 id="考前注意事项"><a href="#考前注意事项" class="headerlink" title="考前注意事项"></a>考前注意事项</h1><ul>
<li>考试平台是由CNCF委托的一个计算机方面非常专业的服务商<a href="https://www.examslocal.com/">PSI</a>来进行监督考试；</li>
<li>官方认证证件：需要有照片和Latin字母写的全名，我用的是护照;</li>
<li>考试中不能喝水、吃东西，可以申请休息，但是不清楚能不能离开摄像头监视范围内，没请过，反正我全程无尿点。</li>
<li>考试的网页一半是试题、一半是GateOne的终端界面，最好能有一台mac电脑然后外接显示器、键盘，鼠标，mac系统比windows有优势，再一个就是终端就显得不是很小了，操作也顺手; (另外为了提前熟悉使用gateone终端我这里也做了一个<a href="https://github.com/guomaoqiu/kubectl-terminal">镜像</a>，具体食用方法参见README.md)</li>
<li>一个没有其他人的空房间，空桌子，不能带手机，不能有书本，监考官会让你拿着电脑展示你周围的所有环境，我的考试是预约在周末下午在公司一个会议室完成的;</li>
<li>按照官方考试流程指导还需要做一下硬件要求性的测试: 使用Chrome浏览器访问<a href="https://www.examslocal.com/ScheduleExam/Home/CompatibilityCheck">https://www.examslocal.com/ScheduleExam/Home/CompatibilityCheck</a> 选择”Linux Foundation” as the Exam Sponsor and “CKA” as the Exam根据提示安装一个chrome插件（实测有项网速的检查，需要有个稳定的科学上网工具）</li>
<li>180分钟，24道题(涵盖所有k8s基础知识点),平均一道题7.5分钟，时间是非常紧的；一道题做完要去验证，有把握没问题的就果断下一题，千万不要犹豫；打脑壳的那种题记住题号，做完简单的在回过头来整。</li>
</ul>
<h1 id="考试结束注意事项"><a href="#考试结束注意事项" class="headerlink" title="考试结束注意事项"></a>考试结束注意事项</h1><p>在考试结束之后，36个小时之内，CNCF就会通过邮件告诉你考试成绩了，如果你的分数大于74%，那么恭喜你通过了！并且附件就直接会有你的通过的证书。如果考试不通过，你的账号上就会直接有一次<code>Free Retake</code>考试的机会。<br><img src="/2019/01/07/road-to-cka/score.png"></p>
<p>😢丢了3分，官方也不会告诉你到底错在哪里了；就只有这么一个成绩跟证书发过来。</p>
<hr>
<p>以上，希望可以帮到你。</p>
<p>ps: <em><del>_</del>该考试签订了保密协议，So 考试原题无法分享~~~_~~</em></p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>CKA</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用Celery(芹菜)异步神器执行后台任务</title>
    <url>/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/</url>
    <content><![CDATA[<p>关于异步的知识网上很多，这里就直接上代码，目前结合Flask这个Python框架实现后台任务的执行操作；</p>
<h3 id="1-需要了解的知识点"><a href="#1-需要了解的知识点" class="headerlink" title="1.需要了解的知识点:"></a>1.需要了解的知识点:</h3><ul>
<li>了解生产消费模型或者发布订阅模式来实现消息队列</li>
<li>了解异步、同步之间的差别</li>
</ul>
<h3 id="2-实现过程"><a href="#2-实现过程" class="headerlink" title="2.实现过程"></a>2.实现过程</h3><h4 id="1-目录结构"><a href="#1-目录结构" class="headerlink" title="(1)目录结构"></a>(1)目录结构</h4><pre><code>├── LICENSE
├── README.md
├── app                     // Flask APP应用
│&nbsp;&nbsp; ├── __init__.py 
│&nbsp;&nbsp; ├── auth
│&nbsp;&nbsp; ├── email.py
│&nbsp;&nbsp; ├── main
│&nbsp;&nbsp; ├── models.py
│&nbsp;&nbsp; ├── salt
│&nbsp;&nbsp; ├── static
│&nbsp;&nbsp; └── templates
├── config.py               // 通用配置
├── config.pyc
├── manager.py
├── migrations
│&nbsp;&nbsp; ├── README
│&nbsp;&nbsp; ├── alembic.ini
│&nbsp;&nbsp; ├── env.py
│&nbsp;&nbsp; └── versions
└── requirements.txt
</code></pre>
<h4 id="2-指定broker-backend-此处使用redis"><a href="#2-指定broker-backend-此处使用redis" class="headerlink" title="(2)指定broker/backend(此处使用redis)"></a>(2)指定broker/backend(此处使用redis)</h4><p>vim app/__init__.py</p>
<pre><code>// 部分代码
from celery import Celery

app = Flask(__name__)

broker = 'redis://127.0.0.1:6379'
backend = 'redis://127.0.0.1:6379/0'

celery = Celery(app.name, broker=broker, backend=backend)

// 部分代码
</code></pre>
<p>以上引入了Celery,指定了Celery的生产消费端都为我们的redis</p>
<h4 id="3-这里我将任务写到了视图函数中"><a href="#3-这里我将任务写到了视图函数中" class="headerlink" title="(3)这里我将任务写到了视图函数中"></a>(3)这里我将任务写到了视图函数中</h4><p>vim app/main/views.py</p>
<ul>
<li><p>(1 创建一个任务函数, 并且将这个任务绑定上了celery的标记</p>
<h3 id="部分代码"><a href="#部分代码" class="headerlink" title="部分代码"></a>部分代码</h3><p>@celery.task(bind=True)<br>def update_cbt_resource(self):<br>‘’’<br>@summary: 创建一个需要后台去执行的任务，我这里只是举个栗子<br>‘’’<br>result=commands.getoutput(“sleep 20 &amp;&amp; echo ‘ok’”)<br>return result</p>
</li>
<li><p>(2 创建一个执行任务请求函数</p>
<h3 id="部分代码-1"><a href="#部分代码-1" class="headerlink" title="部分代码"></a>部分代码</h3><p>@main.route(‘/execute_task/<update_env>‘, methods=[‘GET’, ‘POST’])<br>def execute_task(update_env):<br>“””<br>@summary: 请求函数入口<br>“””<br># 这里用最笨的办法执行了在我们web中点击执行任务之前去检查celery这个服务是否存在，如果不存在会提示用户<br>result = commands.getoutput(“ps -ef | grep celery | grep -v grep”)<br>if not result:<br>    return jsonify({“result”:False,”message”:u’未发现Celery进程，请检查该服务是否正常启动’})<br><br> # 将前面写的任务函数直接调用apply_async属性<br>task = update_cbt_resource.apply_async()<br><br> # 这里就可以获取到这个任务一开始执行就返回的一些属性<br> # 比如任务ID, 任务状态<br>data = {<br>    “task_id”: task.id,<br>    “task_status”: task.status<br>}<br><br># 将以上的信息作为这请求函数的返回<br>result = {“result”:True,”data”:data,”message”:u’执行开始’}<br>return jsonify(result)</update_env></p>
</li>
<li><p>(3 创建一个任务状态查询的函数</p>
</li>
</ul>
<p>一般任务触发之后我们想要知道这个任务的执行状态，是处于哪个阶段的，各个状态已经在官网详细说明了；有兴趣的直接看<a href="http://docs.celeryproject.org/en/master/userguide/tasks.html?highlight=built-in-states">官网</a></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">### 部分的代码</span></span><br><span class="line"><span class="meta">@main.route(<span class="params"><span class="string">'/task_result'</span>, methods=[<span class="string">'GET'</span>]</span>)</span></span><br><span class="line"><span class="meta">@login_required</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">task_result</span>():</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    @summary: 任务的状态是通过task_id来获取,在触发了任务之后,通过前端js轮询请求这个函数，就可以得到该任务的当前执行状态</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 点击执行按钮前端会传一个task_id到后端,这里使用form的方式获取到task_id</span></span><br><span class="line">    task_id = json.loads(request.form.get(<span class="string">'data'</span>))[<span class="string">'task_id'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># AsyncResult，它的作用是被用来检查任务状态，等待任务执行完毕或获取任务结果，如果任务失败，它会返回异常信息或者调用栈。</span></span><br><span class="line">    the_task = update_cbt_resource.AsyncResult(task_id)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"任务：{0} 当前的 state 为：{1}"</span>.<span class="built_in">format</span>(task_id, the_task.state))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 执the_task.state</span></span><br><span class="line">    <span class="keyword">if</span>  the_task.state  == <span class="string">'PROGRESS'</span>:</span><br><span class="line">        <span class="built_in">print</span> the_task.info.get(<span class="string">'i'</span>, <span class="number">0</span>)</span><br><span class="line">        result = {<span class="string">'state'</span>: <span class="string">'progress'</span>,<span class="string">"result_data"</span>:the_task.result}</span><br><span class="line">    <span class="keyword">elif</span>  the_task.state  == <span class="string">'SUCCESS'</span>:</span><br><span class="line">        result = {<span class="string">'state'</span>: <span class="string">"success"</span>, <span class="string">"result_data"</span>:the_task.result}</span><br><span class="line">    <span class="keyword">elif</span>  the_task.state  == <span class="string">'PENDING'</span>:</span><br><span class="line">        result = {<span class="string">'state'</span>: <span class="string">'waitting'</span>,<span class="string">"result_data"</span>:the_task.result}</span><br><span class="line">    <span class="keyword">elif</span>  the_task.state  == <span class="string">'REVOKED'</span>:</span><br><span class="line">        result = { <span class="string">'state'</span>: <span class="string">'revoke'</span>, <span class="string">"result_data"</span>:the_task.result}</span><br><span class="line">        <span class="built_in">print</span> the_task.result</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = {<span class="string">'state'</span>: the_task.state,<span class="string">'progress'</span>:<span class="number">0</span>,<span class="string">"result_data"</span>:the_task.result}</span><br><span class="line">    <span class="keyword">return</span> jsonify(result)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p>(4 尝试在前端页面点击执行任务<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115172368997.jpg">￼<br>Oops! 这就是上面进行celery进程判断之后得到的提示信息，那下面就把celery启动起来吧~~~</p>
</li>
<li><p>(5 启动Celey服务</p>
<p>celery worker -A manager.celery -l debug</p>
<h3 id="我这里是本地使用的virtualenv环境"><a href="#我这里是本地使用的virtualenv环境" class="headerlink" title="我这里是本地使用的virtualenv环境~~~"></a>我这里是本地使用的virtualenv环境~~~</h3></li>
</ul>
<p><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115170661909.jpg">￼<br>ok, 现在启动了celery服务之后我们在前台执行一下:<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115174358505.jpg">￼<br>通过点击执行后我们看日志：<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115175046963.jpg">￼</p>
<p>任务执行完毕了；<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115178118958.jpg">￼</p>
<p>再看看Flask的日志,这就是通过js轮询请求这个task_resul函数的结果<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115175573963.jpg">￼<br>再来看看redis中的内容,这就将结果保存到了backend中<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115176602486.jpg">￼</p>
<h4 id="4-如何撤销一个任务呢"><a href="#4-如何撤销一个任务呢" class="headerlink" title="(4)如何撤销一个任务呢?"></a>(4)如何撤销一个任务呢?</h4><p>还是通过task_id来实现。视图函数中编写一个任务的函数</p>
<pre><code>### 部分代码
@main.route('/cancel_task/', methods=['GET', 'POST'])
@login_required
def cancel_task():
     # 通过前端的取消按钮来获取到这个任务的id
    task_id = json.loads(request.form.get('data'))['task_id']
    try:
        celery.control.revoke(task_id, terminate=True, signal='SIGKILL')
        return Response(json.dumps({'result':True,"message": "取消任务完成" }))
    except Exception,e:
        return Response(json.dumps({'result': True, "message": u'取消任务失败.{0}'.format(e)}))
</code></pre>
<p>执行过程如下:<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115182402670.jpg">￼<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115182685942.jpg">￼<br><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115213514662.jpg">￼</p>
<p><img src="/2017/11/24/ru-he-shi-yongcelery-qin-cai-yi-bu-shen-qi-zhi-xin/15115182945362.jpg">￼<br>需要注意的是,在上面视图函数中的这段代码其实有也可以撤销</p>
<pre><code>task = update_cbt_resource.apply_async()
task.revoke() 
</code></pre>
<p>但是这种方式只是撤销，如果任务已经在执行撤销则无效;所以我这里可以使用下面的方法来撤销</p>
<pre><code># 通过task_id撤销
celery.control.revoke(task_id)
# 撤销正在执行的任务，默认使用TERM信号
celery.control.revoke(task_id, terminate=True)
# 撤销正在执行的任务，默认使用KILL信号
celery.control.revoke(task_id, terminate=True, signal='SIGKILL')
#在官网文档中也可以将多个task_id组成列表形式然后同时撤销多个任务
celery.control.revoke([task_id1,task_id2,task_id3,task_id4.......])
</code></pre>
<h4 id="ok，以上就是Celery-的基本使用。"><a href="#ok，以上就是Celery-的基本使用。" class="headerlink" title="ok，以上就是Celery 的基本使用。"></a>ok，以上就是Celery 的基本使用。</h4>]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>Celery</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Saltstack利用Returners程序保存执行结果到mysql</title>
    <url>/2015/11/19/saltstack-e5-88-a9-e7-94-a8returners-e7-a8-8b-e5-ba-8f-e4-bf-9d-e5-ad-98-e6-89-a7-e8-a1-8c-e7-bb-93-e6-9e-9c-e5-88-b0mysql/</url>
    <content><![CDATA[<p>在我们执行saltstack的时候，minion端会返回一大堆的执行结果显示在master端，那如何将每一次slat执行的结果这些结果保存起来便于日后查询，这里就用到了saltstack的返回程序Returners，可以保存在Redis，Mongodb，MySQL等这些程序当中； 注意这的返回并不是将执行结果返回给master，master再写入到MySQL或者Redis中，而是salt-minion端直接向MySQL或者Redis中写， 下面是操作步骤： 1.安装软件包</p>
<p>master端：yum install -y MySQL-python mysql mysql-server<br>minion端：yum install -y MySQL-python</p>
<p>2.建立数据库，创建salt所需要的数据库及表结构</p>
<p>CREATE DATABASE  `salt`<br>  DEFAULT CHARACTER SET utf8<br>  DEFAULT COLLATE utf8_general_ci;</p>
<p>USE `salt`;</p>
<h2 id="Table-structure-for-table-jids"><a href="#Table-structure-for-table-jids" class="headerlink" title="---- Table structure for table `jids`"></a>--<br>-- Table structure for table `jids`</h2><p>DROP TABLE IF EXISTS `jids`;<br>CREATE TABLE `jids` (<br>  `jid` varchar(255) NOT NULL,<br>  `load` mediumtext NOT NULL,<br>  UNIQUE KEY `jid` (`jid`)<br>) ENGINE=InnoDB DEFAULT CHARSET=utf8;</p>
<h2 id="Table-structure-for-table-salt-returns"><a href="#Table-structure-for-table-salt-returns" class="headerlink" title="---- Table structure for table `salt_returns`"></a>--<br>-- Table structure for table `salt_returns`</h2><p>DROP TABLE IF EXISTS `salt_returns`;<br>CREATE TABLE `salt_returns` (<br>  `fun` varchar(50) NOT NULL,<br>  `jid` varchar(255) NOT NULL,<br>  `return` mediumtext NOT NULL,<br>  `id` varchar(255) NOT NULL,<br>  `success` varchar(10) NOT NULL,<br>  `full_ret` mediumtext NOT NULL,<br>  `alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,<br>  KEY `id` (`id`),<br>  KEY `jid` (`jid`),<br>  KEY `fun` (`fun`)<br>) ENGINE=InnoDB DEFAULT CHARSET=utf8;</p>
<h2 id="Table-structure-for-table-salt-events"><a href="#Table-structure-for-table-salt-events" class="headerlink" title="---- Table structure for table `salt_events`"></a>--<br>-- Table structure for table `salt_events`</h2><p>DROP TABLE IF EXISTS `salt_events`;<br>CREATE TABLE `salt_events` (<br>`id` BIGINT NOT NULL AUTO_INCREMENT,<br>`tag` varchar(255) NOT NULL,<br>`data` varchar(1024) NOT NULL,<br>`alter_time` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,<br>PRIMARY KEY (`id`),<br>KEY `tag` (`tag`)<br>) ENGINE=InnoDB DEFAULT CHARSET=utf8;</p>
<p>#检查一下是否创建成功：<br>mysql&gt; use salt<br>Database changed<br>mysql&gt; show tables;<br>+—————-+<br>| Tables_in_salt |<br>+—————-+<br>| jids           |<br>| salt_events    |<br>| salt_returns   |<br>+—————-+<br>3 rows in set (0.00 sec)</p>
<p>3.授权数据库，修改master配置</p>
<p>mysql &gt; grant all on salt.* to salt@’192.168.2.0/255.255.255.0’ identified by ‘salt’;<br>mysql &gt; flush privileges;</p>
<p>在master端或者每个minion端都写如以下配置内容。当然如果写在master端是比较简单的做法，因为只需要写一次就行啦。我这里写在了master端。</p>
<p>[root@saltstack-node1 ~]# vim /etc/salt/master<br>return: mysql<br>mysql.host: ‘192.168.2.21’#mysql 数据库服务器地址<br>mysql.user: ‘salt’<br>mysql.pass: ‘salt’<br>mysql.db: ‘salt’<br>mysql.port: 3306</p>
<p>4.执行语句测试： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/salt-returners_meitu_1.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/salt-returners_meitu_1.jpg" alt="salt-returners_meitu_1"></a></p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
  </entry>
  <entry>
    <title>学习SaltStack小记—第三章《Pillar使用方法》</title>
    <url>/2015/08/18/saltstack-pillar-de-shi-xian-fang-shi/</url>
    <content><![CDATA[<h4 id="1-要启用pillar，首先要修改master中的配置"><a href="#1-要启用pillar，首先要修改master中的配置" class="headerlink" title="1.要启用pillar，首先要修改master中的配置"></a>1.要启用pillar，首先要修改master中的配置</h4><pre><code>[root@salt-master pillar]# vim /etc/salt/master
pillar_roots:
 &nbsp;base：
 &nbsp; &nbsp;- /salt/pillar
</code></pre>
<h4 id="2-重启master"><a href="#2-重启master" class="headerlink" title="2.重启master"></a>2.重启master</h4><pre><code>[root@salt-master pillar]# systemctl restar salt-master
</code></pre>
<h4 id="3-是存放在master端，默认位置-srv-pillar，需要新建目录。和salt-sls类似，也是需要top-sls"><a href="#3-是存放在master端，默认位置-srv-pillar，需要新建目录。和salt-sls类似，也是需要top-sls" class="headerlink" title="3.是存放在master端，默认位置/srv/pillar，需要新建目录。和salt sls类似，也是需要top.sls"></a>3.是存放在master端，默认位置/srv/pillar，需要新建目录。和salt sls类似，也是需要top.sls</h4><pre><code>mkdir /srv/pillar
</code></pre>
<h4 id="4-在-srv-pillar目录中创建一个top-sls文件"><a href="#4-在-srv-pillar目录中创建一个top-sls文件" class="headerlink" title="4.在/srv/pillar目录中创建一个top.sls文件"></a>4.在/srv/pillar目录中创建一个top.sls文件</h4><pre><code>[root@salt-master pillar]# vim top.sls #内容如下：
base:
 &nbsp;'node2.example.com': &nbsp;#指定的主机
 &nbsp; &nbsp;- sc #调用sc模版中的值 
</code></pre>
<h4 id="5-在-srv-pillar中创建一个sc-sls文件"><a href="#5-在-srv-pillar中创建一个sc-sls文件" class="headerlink" title="5.在/srv/pillar中创建一个sc.sls文件"></a>5.在/srv/pillar中创建一个sc.sls文件</h4><pre><code>[root@salt-master pillar]# vim sc.sls #内容为键值对,k, v格式的
name: guomaoqiu
age: 22
language: chineses
</code></pre>
<p>好啦，此时pillar定义好了</p>
<h4 id="6-执行saltutil-sync-grains-刷新pillar的值"><a href="#6-执行saltutil-sync-grains-刷新pillar的值" class="headerlink" title="6.执行saltutil.sync_grains #刷新pillar的值"></a>6.执行saltutil.sync_grains #刷新pillar的值</h4><pre><code>[root@salt-master pillar]# salt 'node2.exmaple.com' saltutil.refresh_pillar

#看下结果：
[root@salt-master pillar]# salt "node2.example.com" pillar.data
node2.example.com:
 &nbsp; &nbsp;----------
 &nbsp; &nbsp;age:
 &nbsp; &nbsp; &nbsp; &nbsp;22
 &nbsp; &nbsp;language:
 &nbsp; &nbsp; &nbsp; &nbsp;chineses
 &nbsp; &nbsp;name:
 &nbsp; &nbsp; &nbsp; &nbsp;guomaoqiu
</code></pre>
<h4 id="7-由上可见在node2这台主机上已经有pillar值啦，只是这个值是保存在master端的；那问题来了-如何使用jinja模板来调用grains或者是pillar的值呢？看下面："><a href="#7-由上可见在node2这台主机上已经有pillar值啦，只是这个值是保存在master端的；那问题来了-如何使用jinja模板来调用grains或者是pillar的值呢？看下面：" class="headerlink" title="7.由上可见在node2这台主机上已经有pillar值啦，只是这个值是保存在master端的；那问题来了,如何使用jinja模板来调用grains或者是pillar的值呢？看下面："></a>7.由上可见在node2这台主机上已经有pillar值啦，只是这个值是保存在master端的；那问题来了,如何使用jinja模板来调用grains或者是pillar的值呢？看下面：</h4><pre><code>[root@salt-master salt]# pwd
/srv/salt
[root@salt-master salt]# cat top.sls 
base:
 &nbsp;'node2.example.com':
 &nbsp; &nbsp;- test.test
[root@salt-master salt]# cat test/test.sls 
/tmp/test1.conf:
 &nbsp;file.managed:
 &nbsp; &nbsp;- source: salt://test/test1.conf.jinja
 &nbsp; &nbsp;- template: jinja &nbsp;#调用jinja模板
</code></pre>
<p>看一下模板文件：test1.conf.jinja <img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/05/14621012702761.jpg">￼ 该模板文件都调用了grains及pillar的值</p>
<h4 id="8-推到node2上面去："><a href="#8-推到node2上面去：" class="headerlink" title="8.推到node2上面去："></a>8.推到node2上面去：</h4><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/05/14620997738947.jpg">￼</p>
<h4 id="9-在minion端查看"><a href="#9-在minion端查看" class="headerlink" title="9.在minion端查看"></a>9.在minion端查看</h4><p><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2016/05/14620997971326.jpg">￼ 看到了红色框住的就是通过调用grains,pillar值生成的。</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
  </entry>
  <entry>
    <title>如何利用Git Webhook 进行部署</title>
    <url>/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/</url>
    <content><![CDATA[<p>作为一名”伪码农”运维工程师,在接触了开发方面的知识后；也在写项目时一直使用git,可是开发、调试、部署都是在本地进行的；在部署到服务器时也是通过手工去获取仓库的代码；<br>1.开发完代码提交到远程仓库;<br>2.登录远程服务器,并切到代码目录进行git pull;<br>3.重启supervisor应用(我这边开发的python web应用是supervisor进行管理);</p>
<p>当然如果只是一次性部署上去就不再修改的话并没啥问题，但是要是项目持续性修改迭代的话，就比较麻烦了，就在不断的重复着上面的步骤。作为一个”伪码农”，怎么允许不断的重复同样的工作，于是git webhooks闪亮登场；大家对于钩子并不陌生，同样的版本控制SVN也有钩子这个功能；只是个人更加的倾向于使用git; 然后git也催生出来了很多；比如使用最广泛的是GitHub，再其次就是Gitlab,最后就是我这边是用的Gogs;他们都有同样的功能；只是说看需求跟习惯; gogs有个docker版本；能在2分钟之内就可以跑起来非常轻巧方便；感兴趣的请点击这里<a href="https://gogs.io/">Gogs</a></p>
<h2 id="git-webhook进行自动部署"><a href="#git-webhook进行自动部署" class="headerlink" title="git webhook进行自动部署"></a>git webhook进行自动部署</h2><p>如何实现 Git webhook进行自动部署，其实原理很简单，如图所示:<br><img src="/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/15204907260564.jpg">￼</p>
<p>1.本地代码开发完毕提交到远程仓库;<br><img src="/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/15204935021288.jpg">￼</p>
<p>2.通过 POST 请求将订阅事件信息发送至向指定 URL 地址;<br><img src="/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/15204937008543.jpg">￼</p>
<p><img src="/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/15204935532769.jpg">￼</p>
<p>3.指定的url是使用Flask写的一个应用,触发这个URL就会进行远程仓库的代码clone/pull<br><img src="/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/15204936075814.jpg">￼<br>4.拉取代码完毕后,重启服务</p>
<h2 id="Flask-web应用代码"><a href="#Flask-web应用代码" class="headerlink" title="Flask web应用代码:"></a>Flask web应用代码:</h2><pre><code># -*- coding:utf-8 -*-
# 依赖包: pip install flask gitpython

from flask import Flask, request, jsonify,abort
import git, os

# 远程服务器代码地址
code_dir = "./code"

# 远程仓库地址
git_url = "git@192.168.1.105:guomaoqiu/devopscode.git"

#白名单
allow_ip=["192.168.1.105"]

app = Flask(__name__)

#重启服务
restart_services = os.system("supervisorctl -c /etc/supervisord.conf restart devops &amp;&amp; supervisorctl -c /etc/supervisord.conf restart celery")

@app.route('/pullcode', methods=['POST'])
def pullcode():
    # 只允许指定服务器向Flask应用发起POST请求，否则直接返回403
    if request.headers.get('X-Forwarded-For', request.remote_addr) not in allow_ip:
        return abort(403)

    if request.method == 'POST':
        if os.path.isdir(code_dir):
            local_repo = git.Repo(code_dir)
            try:
                print local_repo.git.pull()
                # 重新加载代码、重启服务
                restart_services
                return jsonify({"result":True,"message":"pull success"})
            except Exception,e:
                return jsonify({"result":False,"message": "pull faild".format(e)})
        else:
            try:
                print git.Repo.clone_from(url=git_url, to_path=code_dir)
                # 重新加载代码、重启服务
                restart_services
                return jsonify({"result":True,"message":"clone success"})
            except Exception, e:
                return jsonify({"result":False,"message": "clone faild".format(e)})
if __name__ == '__main__':
    app.run(host='192.168.1.29', port=5003)
</code></pre>
<p>如果其他地址发起POST请求那么就直接返回403,这样就避免了其他人乱来的情况:<br><img src="/2018/03/08/ru-he-li-yonggit-webhook-jin-xing-bu-shu/15204939019299.jpg">￼</p>
<p>总结起来也就是:<br>本地仓库推送代码到远程仓库后，一旦本地仓库变更提交就会触发webhook发送post请求，驱动自动部署、重启服务等。<br>这样一来只需要本地开发好代码提交后直接访问服务器就可以验证我们的应用程序了;</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>自动化运维</tag>
      </tags>
  </entry>
  <entry>
    <title>Snmp协议详解</title>
    <url>/2014/08/27/snmp-e5-8d-8f-e8-ae-ae-e8-af-a6-e8-a7-a3/</url>
    <content><![CDATA[<p>监控主机的方式： 1.基于通行的snmp 2.基于专用agent 3.基于ssh(shell) 对于不同的监控指标，最后实现的功能也有所不同.下面是本人通过学习总结关于与snmp协议的一些解释以及运用 SNMP<simple network="" management="" protocol=""> 主要几个简单的操作就可以实现对远程设备上的服务、资源等各种系统状态信息的获取,不仅仅是获取信息这么简单,还能在达到我们设定值或状态状态发生转换时能向对方发送控制指令; SNMP的工作机制： 监控端/一个或多个被监控端 监控端:NMS,这个平台会为管理员提供命令行接口,可以发送snmp指令到任何一个被监控主机,而在被监控端安装了一个服务进程,它用于接收来自监控端的查询请求, 并且能够解析对方的查询请求,并将查询数据返回给对方,所以这个服务进程可以称之为是一个agent,它只是接收一些查询或者是管理指令而存在的； 监控端发送请求—&gt;agent接受请求(根据查询请求内容)—&gt;agent返回查询请求到监控端. 但是上面有个弊端,也就是说任何主机连接到我们的agent就可以发送指令,这样一来安全方面就有了很大的隐患,于是在此基础上面就引入了communities在监控端和agent之间建立通信的;相当于两个端之间发送认证信息,但是这个认证信息是明文的; SNMP的协议 v1: 所有的安全机制都是基于communities机制来实现,snmp的所有操作在服务器端、客户端也是基于communities机制来实现,在此版本中communities有三种: read-only:只读,监控端只能到agent端获取信息; read-write:读写,可发送管理指令;有权限操作agent端 trap:agent端主动联系监控端；引起监控端注意的一种机制； v2: 对于v2来说只是对v1做了强化,此版本是基于community-string-based,这个版本也叫做v2c； v3: 由于前面两个版本过于简化,认证机制较为薄弱,故此增强了其认证机制,数据传输方面也有了安全性.但是在三个版本中用得最多的还是v1版本,原因是简单; snmp(NMS)只是提供了命令接口去完成采集数据而已,对于采集到的数据保存在哪里它不会关心； snmp这种机制虽然实现了网络管理功能，但是它只是一种协议,把这种协议予以实现的软件的功能也是各不相同的; MIBs<management information="" base="">管理信息库. OID&lt;对象标识符&gt; 将大多数监控对象它的名称和ID号之间的对应关系; 比如在agent端,它不仅仅要收集主机的信息外还需要维持一个MIB,请求达到agent端后它要知道请求的内容是什么,那么在agent端就会记录下监控对象和ID之间的映射关系;所以一个标准的agent要提供至少一个基本的MIB,以维持监控对象;也叫做MIB-II,根据主机不同的需要,MIB的内容也有所不同； 大多数能够基于snmp的管理功能都属于mgmt节点,而在mgmt下面的这个mib下面则有众多的监控对象，1.3.6.1.2.1是每一个主机或设备默认提供的MIB的标识：</management></simple></p>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
  </entry>
  <entry>
    <title>Seafile-个人/团队/公司专属私有文件同步服务 (云存储网盘)</title>
    <url>/2015/09/08/seafile-e4-b8-aa-e4-ba-ba-e5-9b-a2-e9-98-9f-e5-85-ac-e5-8f-b8-e4-b8-93-e5-b1-9e-e7-a7-81-e6-9c-89-e6-96-87-e4-bb-b6-e5-90-8c-e6-ad-a5-e6-9c-8d-e5-8a-a1-e4-ba-91-e5-ad-98-e5-82-a8-e7-bd-91/</url>
    <content><![CDATA[<p>一、简介： <strong>seafile</strong> 是由国内团队开发的一个国际化的<strong>开源云存储软件</strong>项目，目前据说已有10万左右的用户，典型的机构用户包括比利时的皇家自然科学博物馆、德国的 Wuppertal 气候、能源研究所等等。Seafile 同时提供了客户端和服务器端软件免费下载，任何个人或公司都能搭建属于自己的私有文件同步服务。Seafile 的服务器端支持 Linux&nbsp;、Windows 以及树莓派平台，客户端除了网页版之外，还支持 Mac、Linux、Windows 三个桌面平台以及 Android 和 iOS 两个移动平台。你可以利用局域网里的一台电脑作为服务器，搭建一个仅局域网内部能访问的专有云存储服务，也能将 Seafile 部署到互联网上的诸如阿里云、Linode 或任何 VPS、独立服务器上，实现一个私人的在线云存储服务。 同时，Seafile 支持用户同时使用多个同步服务器，而且能够在不同服务器之间切换。比如，用户可以用公司服务器来同步工作文件，用个人服务器与朋友共享私人文件，两者互不干扰，私密性也可保证。而且，由于 Seafile 是开源的项目，因此相对来说数据的私密性还是有保障的，起码不必担心有什么看不见的后门。具体介绍可以参见seafile官方文档介绍。 下面我们就开始在局域网内搭建一台私有的云存储。 二、安装seafile服务器 1、安装前准备： 请确保服务器 上面安装了以下模块(这款软件是用Django+Python2.7所开发的，所以要保证服务器上面的python版本) python 2.7 python-setuptools python-imaging (这个是python的一个库，网上不好找到，下载地址<a href="http://www.pythonware.com/products/pil/">http://www.pythonware.com/products/pil/</a>) python-mysqldb 2、获取服务端软件包</p>
<p>wget <a href="https://bintray.com/artifact/download/seafile-org/seafile/seafile-server/_4.3.2/_x86-64.tar.gz">https://bintray.com/artifact/download/seafile-org/seafile/seafile-server\_4.3.2\_x86-64.tar.gz</a></p>
<p>3、解压安装</p>
<p>tar -xf seafile-server_4.3.2_x86-64.tar.gz<br>mkdir /home/seafile<br>mv seafile-server-4.3.2 /home/seafile/seafile-server<br>cd /home/seafile/seafile-server</p>
<p>./setup-seafile-mysql.sh  #运行安装脚本并回答预设问题</p>
<p>如果你的系统中没有安装上面的某个软件，那么 Seafile初始化脚本会提醒你安装相应的软件包.</p>
<p>./setup-seafile-mysql.sh<br>Checking python on this machine …  #-&gt;执行这个脚本之后会去检查之前说的那些依赖包，如果安装包不完整将会提示你某个软件包未安装<br>  Checking python module: setuptools … Done.<br>  Checking python module: python-imaging … Done.<br>  Checking python module: python-mysqldb … Done.</p>
<p>-----------------------------------------------------------------<br>This script will guide you to setup your seafile server using MySQL.<br>Make sure you have read seafile server manual at</p>
<pre><code>    https://github.com/haiwen/seafile/wiki
</code></pre>
<p>Press ENTER to continue              #-&gt;这里我们需要一下回车，再继续<br>-----------------------------------------------------------------</p>
<p>What is the name of the server? It will be displayed on the client.<br>3 - 15 letters or digits<br>[ server name ] seafile-server       #-&gt;设置我们的服务器名称</p>
<p>What is the ip or domain of the server?<br>For example: <a href="http://www.mycompany.com/">www.mycompany.com</a>, 192.168.1.101         #-&gt;服务器的IP<br>[ This server’s ip or domain ] 192.168.2.108</p>
<p>Where do you want to put your seafile data?<br>Please use a volume with enough free space<br>[ default “/home/seafile/seafile-data” ] /data/seafile   #-&gt;存储的位置我这里选择的是/data/seafile</p>
<p>Which port do you want to use for the seafile fileserver?<br>[ default “8082” ]     #-&gt;默认的工作端口</p>
<p>-------------------------------------------------------<br>Please choose a way to initialize seafile databases:<br>-------------------------------------------------------<br>#-&gt;如果选择1, 你需要提供根密码. 脚本程序会创建数据库和用户。<br>#-&gt;如果选择2, ccnet/seafile/seahub 数据库应该已经被你（或者其他人）提前创建。<br>[1] Create new ccnet/seafile/seahub databases<br>[2] Use existing ccnet/seafile/seahub databases</p>
<p>[ 1 or 2 ] 1</p>
<p>What is the host of mysql server?<br>[ default “localhost” ] </p>
<p>What is the port of mysql server?<br>[ default “3306” ] </p>
<p>What is the password of the mysql root user?<br>[ root password ]    #-&gt;这里需要mysql的root权限进行创建库的操作</p>
<p>verifying password of user root …  done</p>
<p>Enter the name for mysql user of seafile. It would be created if not exists.<br>[ default “root” ] </p>
<p>Enter the database name for ccnet-server:<br>[ default “ccnet-db” ] </p>
<p>Enter the database name for seafile-server:<br>[ default “seafile-db” ] </p>
<p>Enter the database name for seahub:<br>[ default “seahub-db” ] </p>
<p>#-&gt;以上三个库名都用默认的 </p>
<p>以上步骤完成后将会出现一下提示信息，说明我们安装就成功啦 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/seaf.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/seaf.png" alt="seaf"></a> 三、启动seafile服务和seahub网站 1、在/home/seafile/seafile-server目录下执行</p>
<p>#-&gt;启动seafile<br>./seafile.sh start # 启动 Seafile 服务</p>
<p>#-&gt;启动seahub<br>./seahub.sh start <port>  # 启动 Seahub 网站 （默认运行在8000端口上）</port></p>
<p>注意：你第一次启动 seahub 时，<code>seahub.sh</code> 脚本会提示你创建一个 seafile 管理员帐号，就像下面这样 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/seahub.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/seahub.png" alt="seahub"></a> 这个管理账号必须是你自己取注册的任意邮箱地址，登陆管理使用这个地址，我这里用的是gmail. 服务启动后, 打开浏览器并输入这个地址 <a href="http://192.168.2.108:8000/">http://192.168.2.108:8000</a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/web.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/web.png" alt="web"></a> 输入账号密码就会被重定向到登陆页面. 输入你在安装 Seafile 时提供的用户名和密码后，你会进入 Myhome 页面，新建资料库. <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/seafile.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/seafile.png" alt="seafile"></a> &nbsp; 至此，seafile私有与存储共享平台已经部署完毕了。 下面我们可以去下载一个客户端安装上 我这里使用的Ubuntu下的客户端 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/account.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/account.png" alt="account"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-220859.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-220859.png" alt="Screenshot from 2015-09-08 22:08:59"></a> &nbsp; 因为之前就把我的库下载到了本地，你可以在本地库添加文件，然后点击同步就会同步到服务器端啦，就像这样(如果你不使用客户端的话，可以使用网页版) <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-221208.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-221208.png" alt="Screenshot from 2015-09-08 22:12:08"></a> 当然他的客户端不只是在linux(ubuntu) 上面才有哦， 客户端在： 移动端有：Android，Ios 桌面端有：windows、Linux、Mac 服务端在： Windows、Linux、树莓派 任何平台的浏览器。 下面是我手机端的截图，服务器跟手机的wifi是在一个局域网内的 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/1-281x500.png" alt="1"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/2-281x500.png" alt="2"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/3.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/3-281x500.png" alt="3"></a> &nbsp; 既然说了是团队、企业或者个人使用，那每个人都要有个账号，那我们需要新建一个账号才行，因为我使用的seafile server是社区版，有许多的功能都不能使用，但是我觉得作为一个小团队，或者个人使用再适合不过了。 下面我们创建一个账号，在win上面登陆，必须要让你都服务器连接网络，并且注册使用seafile的用户必须使用email地址作为登陆账号，这个账号必须是存在可登录的email <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-224030.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-224030.png" alt="Screenshot from 2015-09-08 22:40:30"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-224241.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-224241.png" alt="Screenshot from 2015-09-08 22:42:41"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-224530.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-224530.png" alt="Screenshot from 2015-09-08 22:45:30"></a> 我们只需要将这个资料库下载到本地就可了，添加文件，然后右击“我的资料库”就可以看到同步到服务器了。 然后我们可以在本地资料库中指定某个文件生成一个下载链接，可以分享给我的小伙伴下载。 我看了一下服务器上面的数据存储目录，数据传入到服务器端就类似于Map Reduce这种软件架构将数据切成了很多很多份，然后创建索引保存到数据库，获取数据时拿到索引，最后根据索引重组数据，再返回结果给客户端。 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-230020.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/Screenshot-from-2015-09-08-230020.png" alt="Screenshot from 2015-09-08 23:00:20"></a> 上图中0211e*********这个目录是用户seafileshare的资料库，文件上传到服务器是它就将一个完整的文件拆散了，在服务端我们找不到一个完整的软件包。这种存储方式跟mogilefs有些类似。 感觉社区版还是有些局限性，但是对于个人，一个团队我个人觉得完全足够啦。 好了如果需要了解更多，可以到官网了解使用：<a href="https://www.seafile.com/en/home/">http://www.seafile.com</a></p>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
  </entry>
  <entry>
    <title>通过Consul-Template实现动态配置服务</title>
    <url>/2018/08/06/tong-guoconsultemplate-shi-xian-dong-tai-pei-zhi-f/</url>
    <content><![CDATA[<h4 id="背景"><a href="#背景" class="headerlink" title="背景:"></a>背景:</h4><p>公司的测试、预发布环境的配置修改在前期都是通过手工登录到服务器上去vim配置文件的，这样一来就会产生一定的安全或者误操作以及频繁的操作真的是有些恶心的；去年在此基础上也为运营/测试使用Flask 写了一个平台让他们自己用；但是由于一些不定因素，不能够满足这方面的需求；但是本人还是坚持以自动化的理念来操作；所以学习了解了一下自动配置的一些工具，比如Consul，当然他的原理功能网上有很多；也没有必要在这里再一次说了；主要记录一下对于这个需求的一个想法到实现的过程。”妈妈再也不担心我vim错配置啦~~~😁”</p>
<hr>
<p>这是前期为运营人员使用Flask开发的一个平台,主要日常涉及到的操作(测试/预发布环境)<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/08/15335443893508.jpg">￼</p>
<p>下面是针对以上一些操作规划的一个拓扑图:<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/08/15337809989270.jpg">￼</p>
<p>这里我主要使用到了consul的k/v存储以及consul-template动态的配置系统</p>
<ol>
<li>操作人员通过Opsplatform操作后，通过调用consul-http-api将最新的key/value put到consul-datacenter;</li>
<li>客户端服务器添加对应的模板文件，通过consul-template命令启动监控模板文件</li>
<li>当有新的key/value put到consul-datacenter时，consul-template根据模板文件替换掉里面的kv</li>
<li>在启动监控模板文件时也可以增加后续的操作，例如: <img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/08/15335448934622.jpg">￼ 客户端连接到consul-http-api，指定模板文件以及输出文件，然后指定模板文件替换成功后执行其他命令例如上图中的<code>date</code>命令… 整个过程无需人员操作。</li>
</ol>
<hr>
<p>当然这里只是用到了consul的冰山一角，也是作为一个方向去实现各种需求~</p>
<p>通过reqeusts来操作key/value<br><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2018/08/15335452141233.jpg">￼</p>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>consul</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat环境搭建及jsp站点实现</title>
    <url>/2014/08/28/tomcat-e7-8e-af-e5-a2-83-e6-90-ad-e5-bb-ba-e5-8f-8ajsp-e7-ab-99-e7-82-b9-e5-ae-9e-e7-8e-b0/</url>
    <content><![CDATA[<p>&nbsp; 安装JDK(配置java环境)： 环境：CentOS6.5_x86-64bit 一、先安装JVM 1.使用7版本的jdk(到官网下载相应的软件包到本地<a href="http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jre-7u9-oth-JPR">http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jre-7u9-oth-JPR</a>)</p>
<p>[root@localhost ~]# rpm -ivh jdk-7u9-linux-x64.rpm<br>Preparing… ########################################### [100%]<br>1:jdk ########################################### [100%]<br>Unpacking JAR files…<br>rt.jar…<br>Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/rt.pack<br>jsse.jar..<br>Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/jsse.pack<br>charsets.jar…<br>Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/charsets.pack<br>tools.jar…<br>Error: Could not open input file: /usr/java/jdk1.7.0_09/lib/tools.pack<br>localedata.jar…<br>Error: Could not open input file: /usr/java/jdk1.7.0_09/jre/lib/ext/localedata.pack<br>#-&gt;以上那些错误可以忽略，不影响jdk到安装和使用</p>
<p>2.安装后生成的文件：</p>
<p>[root@localhost ~]# cd /usr/java/<br>[root@localhost java]# ll<br>total 4<br>lrwxrwxrwx 1 root root 16 May 6 10:58 default -&gt; /usr/java/latest<br>drwxr-xr-x 10 root root 4096 May 6 10:58 jdk1.7.0_09<br>lrwxrwxrwx 1 root root 21 May 6 10:58 latest -&gt; /usr/java/jdk1.7.0_09</p>
<p>3.修改java环境变量：</p>
<p>[root@localhost ~]# vim /etc/profile.d/java.sh<br>export JAVA_HOME=/usr/java/latest<br>export PATH=$JAVA_HOME/bin:$PATH<br>[root@localhost ~]# . /etc/profile.d/java.sh</p>
<p>4.测试java环境是否可用.</p>
<p>[root@localhost java]# java -version<br>java version “1.7.0_09”<br>Java(TM) SE Runtime Environment (build 1. 7.0_09-b05)<br>Java HotSpot(TM) 64-Bit Server VM (build 23.5-b02, mixed mode)<br>[root@localhost java]#</p>
<p>此时只是部署好了java环境,而并非运行任何的JAVA程序,而Tomcat就是运行在这个环境上面的JAVA程序 二、安装配置tomcat 1.获取软件包并解压安装：</p>
<p>[root@localhost ~]# wget <a href="http://apache.fayea.com/apache-mirror/tomcat/tomcat-7/v7.0.55/bin/apache-tomcat-7.0.55.tar.gz">http://apache.fayea.com/apache-mirror/tomcat/tomcat-7/v7.0.55/bin/apache-tomcat-7.0.55.tar.gz</a><br>[root@localhost ~]# tar -xf apache-tomcat-7.0.55.tar.gz -C /usr/local/<br>[root@localhost ~]# cd /usr/local/<br>[root@localhost local]# ln -sv apache-tomcat-7.0.5 tomcat<br>`tomcat’ -&gt; `apache-tomcat-7.0.55’<br>[root@localhost local]# cd tomcat/<br>[root@localhost tomcat]#</p>
<p>2.更改tomcat环境变量：</p>
<p>[root@localhost ~]# vim /etc/profile.d/tomcat.sh<br>export CATALINA_HOME=/usr/local/tomcat<br>export PATH=$CATALINA_HOME/bin/:$PATH<br>[root@localhost ~]# . /etc/profile.d/tomcat</p>
<p>3.启动tomcat</p>
<p>[root@localhost tomcat]# catalina.sh start</p>
<p>4.验证是否启动</p>
<p>[root@localhost tomcat]# ss -tunlp | grep java<br>tcp LISTEN 0 100 :::8080 :::* users:((“java”,25953,42))<br>tcp LISTEN 0 1 ::ffff:127.0.0.1:8005 :::* users:((“java”,25953,46))<br>tcp LISTEN 0 100 :::8009 :::* users:((“java”,25953,43))<br>#—&gt;注意三个端口，每一个单独的java程序都是由一个jvm来运行的.<br>[root@localhost tomcat]# jps<br>25953 Bootstrap #—&gt;这个进程是tomcat启动时为了实现其加速的一个程序.有这个进程则表示启动成功.<br>25995 Jps<br>[root@localhost tomcat]#</p>
<p>5.网页访问测试：<a href="http://ip:8080/">http://IP:8080</a> 现在tomcat已经启动了,注意上图中的三个按钮,后续说. <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/wpid-d698a595a631e5b1f4c10780198b6dc9_c8b5d927-ed11-4825-ac80-c5f047b7c4a0.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/wpid-d698a595a631e5b1f4c10780198b6dc9_c8b5d927-ed11-4825-ac80-c5f047b7c4a0-600x348.jpg" alt="wpid-d698a595a631e5b1f4c10780198b6dc9_c8b5d927-ed11-4825-ac80-c5f047b7c4a0.jpg"></a> 6.为了方便管理tomcat，为其提供SysV管理脚本.</p>
<p>[root@localhost ~]# vim /etc/rc.d/init.d/tomcat<br>#!/bin/sh<br># Tomcat init script for Linux.</p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p># chkconfig: 2345 96 14<br># description: The Apache Tomcat servlet/JSP container.<br>JAVA_HOME=/usr/java/latest<br>CATALINA_HOME=/usr/local/tomcat<br>export JAVA_HOME CATALINA_HOME<br>. /etc/init.d/functions</p>
<p>case $1 in<br>start)<br>  $CATALINA_HOME/bin/catalina.sh start &amp;&gt;/dev/null;;<br>stop)<br>  $CATALINA_HOME/bin/catalina.sh stop &amp;&gt;/dev/null;;<br>  restart)<br>$CATALINA_HOME/bin/catalina.sh stop &amp;&gt;/dev/null<br>  sleep 2<br>  $CATALINA_HOME/bin/catalina.sh start &amp;&gt;/dev/null;;<br>*)<br>  echo “Usage:`basename $0` {start|stop|restart}”<br>exit 1<br>;;<br>esac<br>[root@localhost ~]# chmod +X /etc/rc.d/init.d/tomcat<br>[root@localhost ~]# chkconfig –add tomcat<br>#-&gt;通过上面这个服务控制脚本就可以控制tomcat的启动、关闭等操作了;</p>
<p>7.Tomcat 配置文件配置层次： Tomcat是一个基于组件的服务器，它的构成组件都是可配置的，其中最外层的给件是CATALINA SERVLET容器，其他的组件按照一定的格式要求配置在这个顶层容器中。Tomcat的各个组件是server.xml文件中配置的，Tomcat服务器默认情况下对各种组件都有默认的实现，下面通过分析server.xml文件来理解Tomcat的各个组件是如何组织的。</p>
<p><server>    #—&gt;顶层组件，代表一个服务器<br>     <service>    #—&gt;顶层组件，是Connector的集合，将连接器关联至engine；因此一个service内部可以有多个connector，但只能有一个engine<br>          <connector>    #—&gt;连接器类组件，代表通信接口<br>          <engine>    #—&gt;容器类组件，为特定的Service组件处理所有客户请求，可包含多个Host<br>               <host>    #—&gt;为特定的虚拟主机处理所有客户请求<br>                    <context>    #—&gt;为特定的WEB应用处理所有客户请求<br>                    </context><br>               </host><br>               <host><br>               </host><br>          </engine><br>     </connector></service><br></server></p>
<p>详述这些组件： **顶级组件：**位于整个配置的顶层； **容器类：**可以包含其它组件的组件；<br>&nbsp; &nbsp; &nbsp;engine: 核心容器，catalina引擎，负责通过connector接收用户请求,在一个engin内部可以有多个host<br>&nbsp; &nbsp; &nbsp;host: 类似于httpd中的虚拟主机；支持基于FQDN的虚拟主机<br>&nbsp;&nbsp;&nbsp;&nbsp; context: 最内层的容器类组件，一个context代表一个web应用程序；配置context的主要目的，指定对应的webapp的根目录；还能为webapp指定额外的属性，如部署方式等； **连接器组件：**连接用户请求至tomcat； 两类连接器： 1.基于HTTP 协议 2.基于AJP 二进制协议,使用httpd反向代理用户请求至tomcat时,在httpd和tomcat之间使用.被嵌套类的组件：位于一个容器当中，不能包含其它组件； **被嵌套类的组件：**位于一个容器当中，不能包含其它组件； valve: 拦截请求并在将其转至对应的webapp之前进行某种处理操作；可以用于任何容器中；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; access log valve:&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; remote address filter value: 基于IP做访问控制<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger: 日志记录器，用于记录组件 内部的状态信息(可用于除context之外的任何容器中)<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; realm: 可以用于任何容器类的组件中，关联一个用户认证库，实现认证和授权；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; UserDatabaseRealm: 使用JNDI自定义的用户认证库；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MemoryRealm: tomcat-users.xml中&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; JDBCRealm: 基于JDBC连接至数据库中查找用户； **部署:**使用类加载器,为webapp准备好其依赖所有类. 8.服务端口说明： 在我们启动之后有3个端口：8005，8009，8080 <strong>8005</strong> &nbsp;==&gt;位于server顶级组件当中,它监听于本地&lt;127.0.0.1&gt; 通过这个port我们可以直接在本地关闭tomcat应用程序,以及java进程. 如下： (1)在本机安装telnet ;&nbsp;</p>
<p>[root@localhost ~]# yum install -y telnet</p>
<p>(2)连接本机8005port,发送”SHUTDOWN”指令</p>
<p>[root@localhost ~]# telnet localhost 8005  #–&gt;telnet本地8005port<br>Trying ::1…<br>telnet: connect to address ::1: Connection refused<br>Trying 127.0.0.1…<br>Connected to localhost.<br>Escape character is ‘^]‘.<br>SHUTDOWN     #–&gt;发送”SHUTDOWN”指令！<br>Connection closed by foreign host.<br>[root@node1 ~]# ss -tunlp | grep java  #–&gt;此时再去检查端口,显示没有,说明我们通过telnet已经发送指令将其关闭成功啦,危险系数较高,配置服务安全性时需注意！<br>[root@node1 ~]# service tomcat start   #–&gt;再次启动,检查端口正常否.<br>[root@node1 ~]# ss -tunlp | grep java<br>tcp    LISTEN     0      100                   :::8080                 :::*      users:((“java”,6430,42))<br>tcp    LISTEN     0      1       ::ffff:127.0.0.1:8005                 :::*      users:((“java”,6430,46))<br>tcp    LISTEN     0      100                   :::8009                 :::*      users:((“java”,6430,43))<br>[root@node1 ~]#</p>
<p>**&nbsp;8009，8080 &nbsp;==&gt;**前面已说到了在连接器组件当中有个<connector>,他们在这个组件当中分别对应了两种协议. 8009 &nbsp;&nbsp;基于AJD协议 8080 &nbsp;基于http协议,当然在<connector>中可以添加各种参数,例如”address”等,默认配置文件中没有指定ip地址,默认就是监听所有.这个端口一般修改为80. 三、虚拟主机定义： (1)创建网页目录：</connector></connector></p>
<p>[root@node1 ~]# mkdir /www/webapps/ROOT</p>
<p>(2)创建网页测试文件:</p>
<p>[root@localhost ~]# vim /www/webapps/ROOT/index.jsp<br>&lt;%@ page language=”java” %&gt;<br>&lt;%@ page import=”java.util.*” %&gt;</p>

  
    <title>JSP test page.</title>
  
  
    &lt;% out.println("hello,world!"); %&gt;
  <!--%-->


<p>(3)修改配置文件如下：</p>
<p>[root@node1 ~]# vim /usr/local/tomcat/conf/server.xml<br>……<br>……<br>  <connector port="80" protocol="HTTP/1.1" connectiontimeout="20000" redirectport="8443"><br>…..<br>…..<br><host name="www.aaa.com" appbase="/www/webapps" unpackwars="true" autodeploy="true"><br>             <context path="" docbase="ROOT" reloadable="true"><br>         &lt;Valve className=”org.apache.catalina.valves.AccessLogValve” directory=”logs”   #-&gt;日志保存格式<br>                        prefix=”<a href="http://www.aaa.www_log/">www.aaa.www_log</a>.” suffix=”.txt”<br>                        pattern=”%h %l %u %t "%r" %s %b” /&gt;<br></context></host><br>……<br>……</connector></p>
<p>(4)重启服务测试：</p>
<p>[root@localhost ~]# service tomcat restart</p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat.png" alt="tomcat"></a> 注意:此处我已经对该服务器做了解析. (5)虚拟主机的别名机制：</p>
<p>[root@node1 ~]# vim /usr/local/tomcat/conf/server.xml<br>……<br>……<br>  <connector port="80" protocol="HTTP/1.1" connectiontimeout="20000" redirectport="8443"><br>…..<br>…..<br><host name="www.aaa.com" appbase="/www/webapps" unpackwars="true" autodeploy="true"><br>             <context path="" docbase="ROOT" reloadable="true"><br>             <context path="/bbs" docbase="/www/webapps/bbsapp" reloadable="true">  #—&gt;在原有虚拟主机中添加一个Context<br>         &lt;Valve className=”org.apache.catalina.valves.AccessLogValve” directory=”logs”   #-&gt;日志保存格式<br>                        prefix=”<a href="http://www.aaa.www_log/">www.aaa.www_log</a>.” suffix=”.txt”<br>                        pattern=”%h %l %u %t "%r" %s %b” /&gt;<br></context></context></host><br>……</connector></p>
<p>#—&gt;创建目录bbsapp<br>[root@localhost ~]# mkdir /www/webapps/bbsapp<br>#—&gt;创建测试文件<br>[root@localhost ~]# cp /www/webapps/ROOT/index.jsp /www/webapps/bbsapp/index.jsp<br>#—&gt;编辑测试文件,与上面的不同即可<br>[root@localhost ~]# vim /www/webapps/bbsapp/index.jsp<br>#—&gt;重启服务测试<br>[root@localhost ~]# service tomcat restart<br>[root@localhost ~]#<br>……</p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat1.png" alt="tomcat"></a> 四、服务状态信息&amp;Dashboard: 在最前面的那种图片当中我们看到了tomcat服务的默认界面有三个按钮： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat2.png" alt="tomcat"></a> &nbsp; 当我们点击”Server Status”时,需要身份验证： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat3.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat3.png" alt="tomcat"></a> 点击”取消”,网页会提示我们如何去修改,并能够登录查看状态等信息. <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat4.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat4.png" alt="tomcat"></a> &nbsp;那就去修改这个配置文件吧：</p>
<p>[root@localhost ~]# vim /usr/local/tomcat/conf/tomcat-users.xml<br>……<br>……<br><role rolename="manager-gui"><br><user username="tomcat" password="tomcat" roles="manager-gui"><br>……<br>……</user></role></p>
<p>#—&gt;重启服务<br>[root@varnish ~]# service tomcat restart<br>#—&gt;#测试：(这里只是状态界面,如果想看其他请看帮助信息)</p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat5.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/tomcat5.png" alt="tomcat"></a></p>
]]></content>
      <categories>
        <category>Web相关</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>jdk</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>Vim Nginx配置文件语法高亮</title>
    <url>/2015/03/09/vim-nginx-e9-85-8d-e7-bd-ae-e6-96-87-e4-bb-b6-e8-af-ad-e6-b3-95-e9-ab-98-e4-ba-ae/</url>
    <content><![CDATA[<p>我们在编辑配置nginx的配置文件时，由于他没有高亮的功能，但是nginx官方是支持这个功能的；要想在编辑配置nginx配置文件的时候高亮语法以降低配置的错误发生率可移执行这个小脚本而到达目的：</p>
<p>#!/bin/bash<br>mkdir -p ~/.vim/syntax &amp;&amp; cd ~/.vim/syntax<br>wget <a href="http://www.vim.org/scripts/download/_script.php?src%5C_id=14376">http://www.vim.org/scripts/download\_script.php?src\_id=14376</a> -O nginx.vim &gt;/dev/null<br>echo “au BufRead,BufNewFile /usr/local/webserver/nginx/conf/* set ft=nginx” &gt; ~/.vim/filetype.vim<br>#其中路径/usr/local/webserver/nginx/conf/*为你的nginx.conf文件路径</p>
]]></content>
      <categories>
        <category>Shell</category>
      </categories>
  </entry>
  <entry>
    <title>Vmware Clone陷阱</title>
    <url>/2014/07/19/vmware-clone-e9-99-b7-e9-98-b1/</url>
    <content><![CDATA[<p>在我们平时使用vmware workstation做实验时，会遇到主机不够用的情况，那此时我们的解决办法一般都是要么从新装一台新的，要么就是通过vmware workstation强大的克隆功能克隆出我们需要的虚拟机；显然后者的优势比前者大，毕竟你从新装一台的话占用你的物理硬盘空间，其次就是浪费Your Time. SO,就选择Clone吧. 我现在有一台刚装好的虚拟机(母机)，由于做实验我需要多台主机；我将这台主机命名为node1，目的是通过这台母机克隆一台虚拟机node2； 在Clone Type页中，单击Create a linked clone(创建一个克隆链接)。如果选择第二项Create a full clone，则创建一个完整的克隆。这两个区别在于：第一项创建的虚拟机将依赖于源虚拟机的存在，使用这项创建的虚拟机占用较少的硬盘空间；第二项创建的 虚拟机是一个独立的虚拟机，但占用较多的硬盘空间。我这里选择的是Create a linked clone，具体的步骤我就不再给出； node2通过node1不到10s就克隆好啦，但是待我查看克隆出来的两台机子的网络信息时居然不是默认的eth0网卡，怎么变成eth1了呢，对于我这种学习强迫症的来说，这种事情真不能发生，还有的同学遇到了通过clone这种机制在操作时网卡冲突、无法启动网卡、或者是配置了IP也不济于是；于是通过google了一下： 原因如下： Centos或RedHat使用udev动态管理设备文件，并根据设备的信息对其进行持久化命名。udev会在系统引导的过程中识别网卡，将mac地址和网卡名称对应起来记录在udev的规则脚本中。而对于新的虚拟机，VMware会自动为虚拟机的网卡生成MAC地址，当你克隆或者重装虚拟机软件时，由于你使用的是以前系统虚拟硬盘的信息，而该系统中已经有eth0的信息，对于新增的网卡，udev会自动将其命名为eth1（累加的原则），所以在你的系统启动后，你使用ifconfig看到的网卡名为eth1。这时候在/etc/sysconfig/network-script/下依然是eth0的配置文件，自然他会识别出eth1了噻。 解决办法： 1. 将node2这台主机的/etc/udev/rules.d/70-persistent-net.rules 中 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/1.png" alt="1"></a> 2. 将/etc/sysconfig/network-script/ifcfg-eth0 中关于mac信息删掉； 3. 改完后reboot或者重启网卡：service network restart.完成后你会发现与网卡编号与你的正常逻辑中的一样啦.</p>
]]></content>
      <categories>
        <category>故障处理</category>
      </categories>
      <tags>
        <tag>vmware workstation</tag>
      </tags>
  </entry>
  <entry>
    <title>Vsftp 开启 PASV模式</title>
    <url>/2013/06/19/vsftp-e5-bc-80-e5-90-af-pasv-e6-a8-a1-e5-bc-8f/</url>
    <content><![CDATA[<p>今天应老大要求要在一台在公网的机器配置一个vsftp服务，配置好之后居然不能用，甚是纳闷，具体报错如下图： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/vsftp-info.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/vsftp-info.png" alt="vsftp-info"></a> 1、我在本机 ftp localhost 却一点没有问题？ 2、为什么一出外网就有问题？ 3、这台服务器在公网，在防火墙后面，本机iptables是停了的； 4、以前都是在内网，甚至是同一个网段搭建vsftp来使用； 5、vsftp的配置非常的简单，难道还有些地方有疏漏？ 解决方法： 1、在/etc/vsftpd/vsftpd.conf末尾添加：</p>
<p>#YES，允许数据传输时使用PASV模式。NO，不允许使用PASV模式。默认值为YES。<br>pasv_enable=YES</p>
<p>#设定在PASV模式下，建立数据传输所可以使用port范围的下界和上界，0 表示任意。默认值为0。把端口范围设在比较高的一段范围内，比如50000-60000，将有助于安全性的提高.<br>pasv_min_port=30000<br>pasv_max_port=30005</p>
<p>#此选项为一个数字IP地址，作为PASV命令的响应。默认值为none，即地址是从呼入的连接套接字(incoming connectd socket)中获取。<br>pasv_address=xxxxxxxxx #我这里配置的是这台服务器的外网IP</p>
<p>#此选项激活时，将关闭PASV模式的安全检查。该检查确保数据连接和控制连接是来自同一个IP地址。小心打开此选项。此选项唯一合理的用法是存在于由安全隧道方案构成的组织中。默认值为NO<br>#pasv_promiscuous=NO</p>
<p>2、在防火墙上开启30000-30005端口。 3、重启vsftpd服务 /etc/init.d/vsftpd restart 客户端再次访问就正常啦： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/vsftp-info2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/09/vsftp-info2.png" alt="vsftp-info2"></a></p>
]]></content>
      <categories>
        <category>故障处理</category>
      </categories>
  </entry>
  <entry>
    <title>Zabbix微信报警</title>
    <url>/2015/11/21/zabbix-e5-be-ae-e4-bf-a1-e6-8a-a5-e8-ad-a6/</url>
    <content><![CDATA[<h2 id="一、注册微信公众号"><a href="#一、注册微信公众号" class="headerlink" title="一、注册微信公众号"></a>一、注册微信公众号</h2><p>首先申请微信公众平台<a href="https://mp.weixin.qq.com/">https://mp.weixin.qq.com/</a>一个人最多申请5个公众号，申请完之后就可以根据腾讯的提示使用微信公众号了，然后用你自己的微信扫描关注微信号。 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/%7Fmeitu_1.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/%7Fmeitu_1.jpg" alt="meitu_1"></a> 通过扫描过后就可以看到已经有一个用户关注啦；于是我们这里需要查看用户的ID 点击 “用户管理”，然后点击一下用户的头像，这时候我们可以在浏览器的地址栏就可以看到一个这个，其中红色部分就是用户的微信ID啦，先记下这个ID <a href="https://mp.weixin.qq.com/cgi-bin/singlesendpage?t=message/send&amp;action=index&amp;tofakeid=250995555&amp;token=94167798&amp;lang=zh_CN">https://mp.weixin.qq.com/cgi-bin/singlesendpage?t=message/send&amp;action=index&amp;tofakeid=250995555&amp;token=94167798&amp;lang=zh_CN</a> <img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_2.jpg" alt="meitu_2"> &nbsp;</p>
<h2 id="二、下载并配置微信公众平台私有接口"><a href="#二、下载并配置微信公众平台私有接口" class="headerlink" title="二、下载并配置微信公众平台私有接口"></a>二、下载并配置微信公众平台私有接口</h2><p>1.获取代码</p>
<p>git clone <a href="https://github.com/lealife/WeiXin-Private-API">https://github.com/lealife/WeiXin-Private-API</a></p>
<p>2.修改zabbix配置文件</p>
<p>[root@Control-machine ~]# cp -r WeiXin-Private-API /usr/lib/zabbix/alertscripts/<br>[root@Control-machine ~]# cd /usr/lib/zabbix/alertscripts/<br>[root@Control-machine alertscripts]# chown zabbix:zabbix WeiXin-Private-API<br>[root@Control-machine alertscripts]# </p>
<p>#修改test.php文件<br>[root@Control-machine alertscripts]# vim WeiXin-Private-API/test.php<br>&lt;?php<br>require “config.php”;<br>require “include/WeiXin.php”;<br>$weiXin = new WeiXin($G_CONFIG[‘weiXin’]);<br>$testFakeId = “$argv[1]“;<br>$msg=”$argv[3]“;<br>    print_r($weiXin-&gt;send($testFakeId, “$msg”));</p>
<p>3.修改config.php文件</p>
<p>[root@Control-machine alertscripts]# vim WeiXin-Private-API/config.php<br>&lt;?php<br>// 全局配置<br>$G_ROOT = dirname(__FILE__);<br>$G_CONFIG[“weiXin”] = array(<br>        ‘account’ =&gt; ‘你的微信公众登录号码’,<br>        ‘password’ =&gt; ‘你的微信公众登录密码’,<br>        ‘cookiePath’ =&gt; $G_ROOT. ‘/cache/cookie’, // cookie缓存文件路径<br>        ‘webTokenPath’ =&gt; $G_ROOT. ‘/cache/webToken’, // webToken缓存文件路径<br>);</p>
<p>4.修改test.php文件</p>
<p>[root@Control-machine alertscripts]# vim WeiXin-Private-API/test.php<br>&lt;?php<br>require “config.php”;<br>require “include/WeiXin.php”;<br>$weiXin = new WeiXin($G_CONFIG[‘weiXin’]);<br>$testFakeId = “$argv[1]“;<br>$msg=”$argv[3]“;<br>    print_r($weiXin-&gt;send($testFakeId, “$msg”));</p>
<p>注意这里$msg=”$argv[3]“表示zabbix传入的第三个参数，因为在zabbix报警时会传入三个参数： 一是微信好友ID，二是报警信息的主题，三是报警信息的具体内容，这里跳过了报警信息主题，直接发送报警信息内容 5.创建微信报警脚本</p>
<p>[root@Control-machine alertscripts]# vim weixin<br>/usr/bin/php /usr/lib/zabbix/alertscripts/WeiXin-Private-API/test.php “$1” “$2” “$3”<br>[root@Control-machine alertscripts]# chown zabbix:zabbix weixin<br>[root@Control-machine alertscripts]# chmod +x weixin </p>
<p>6.测试报警</p>
<p>[root@Control-machine alertscripts]# /usr/bin/php /usr/lib/zabbix/alertscripts/WeiXin-Private-API/test.php 250995555 “” “Test”<br>PHP Notice:  Undefined index: HTTP_USER_AGENT in /usr/lib/zabbix/alertscripts/WeiXin-Private-API/include/LeaWeiXinClient.php on line 33<br>PHP Notice:  curl_setopt(): CURLOPT_SSL_VERIFYHOST no longer accepts the value 1, value 2 will be used instead in /usr/lib/zabbix/alertscripts/WeiXin-Private-API/include/LeaWeiXinClient.php on line 32<br>PHP Notice:  Undefined index: HTTP_USER_AGENT in /usr/lib/zabbix/alertscripts/WeiXin-Private-API/include/LeaWeiXinClient.php on line 33<br>stdClass Object<br>(<br>    [base_resp] =&gt; stdClass Object<br>        (<br>            [ret] =&gt; 0<br>            [err_msg] =&gt; ok  #说明已经发送出去啦<br>        )</p>
<p>)<br>[root@Control-machine alertscripts]# </p>
<p>#以上PHP的提示信息可以忽略</p>
<p>查看结果 <img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_3.jpg" alt="meitu_3"></p>
<h2 id="三、配置zabbix"><a href="#三、配置zabbix" class="headerlink" title="三、配置zabbix"></a>三、配置zabbix</h2><p>1.添加报警媒介 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_4.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_4.jpg" alt="meitu_4"></a> 2.添加用户报警媒介，这里使用的是administrator <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_5.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_5.jpg" alt="meitu_5"></a> 3.添加报警动作 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_6.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_6.jpg" alt="meitu_6"></a> 信息如下 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_8.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_8.jpg" alt="meitu_8"></a> 修改操作条件，保持默认的也可以； 保存设置</p>
<h2 id="四、测试"><a href="#四、测试" class="headerlink" title="四、测试"></a>四、测试</h2><p>我这里将监控mysql主从的脚本手动改一下，让其zabbix检测到并报警 手机微信查看报警信息 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_7.jpg"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/meitu_7.jpg" alt="meitu_7"></a> 但是需要注意的是，如果公众号向我的微信发送消息超过48个小时我没有回复，那么公众号将不会主动发送消息。然后我们在微信，也就是说，我们在收到报警通知后在48个小时之内可以简单的回复一个字符，或者一段话即可 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/Screenshot-from-2015-11-30-164029.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/11/Screenshot-from-2015-11-30-164029.png" alt="Screenshot from 2015-11-30 16:40:29"></a> 参考文章链接： <a href="http://blog.chinaunix.net/uid-30236771-id-5037842.html">http://blog.chinaunix.net/uid-30236771-id-5037842.html</a></p>
]]></content>
      <categories>
        <category>Monitor</category>
        <category>自动化运维</category>
      </categories>
  </entry>
  <entry>
    <title>Zabbix 清理过久的历史信息</title>
    <url>/2014/11/04/zabbix-e6-b8-85-e7-90-86-e8-bf-87-e4-b9-85-e7-9a-84-e5-8e-86-e5-8f-b2-e4-bf-a1-e6-81-af/</url>
    <content><![CDATA[<p>当我们的zabbix运行时间久了，监控的节点多了，数据信息会增长的很快，想备份里面的数据库时，要浪费大量的时间，zabbix里面最大的表就是历史记录的表了，网上很多人都是写全部清空这些表的数据，其实我们可以按时间来删除里面的历史记录； 里面最大的表是 “history” 和 “history_uint”两个表； zabbix里面的时间是用的时间戳方式记录，我们可以转换一下，然后根据时间戳来删除； 比如要删除2012年的11月25号以前的数据 1、先将标准时间转换为时间戳</p>
<p># date +%s -d “2012-11-25 00:00:00”<br>1353772800</p>
<p>2、mysql清理数据</p>
<p>mysql&gt; use zabbix;<br>mysql&gt; DELETE FROM `history_uint` WHERE `clock` &lt; 1327939201;<br>mysql&gt; optimize table history_uint;</p>
<p>注：执行过第二行命令之后可能会需要很长的一段时间，中间不要K掉了，否则容易丢失数据的。</p>
]]></content>
      <categories>
        <category>Other</category>
      </categories>
  </entry>
  <entry>
    <title>Vsftp-虚拟用户设置不同权限</title>
    <url>/2015/04/02/vsftp-xu-ni-yong-hu-she-zhi-bu-tong-quan-xian/</url>
    <content><![CDATA[<h4 id="1-system-version"><a href="#1-system-version" class="headerlink" title="1.system version"></a>1.system version</h4><pre><code>CentOS Linux release 7.2.1511 (Core)
</code></pre>
<h4 id="2-install-vsftpd"><a href="#2-install-vsftpd" class="headerlink" title="2.install vsftpd"></a>2.install vsftpd</h4><pre><code>yum install -y vsftpd
</code></pre>
<h4 id="3-create-rootdir"><a href="#3-create-rootdir" class="headerlink" title="3.create rootdir"></a>3.create rootdir</h4><pre><code>mkdir /data/program/ftpdata &amp;&amp; chmod a-w /data/program/ftpdata/
</code></pre>
<h4 id="4-create-local-user-for-map-to-virtualuser"><a href="#4-create-local-user-for-map-to-virtualuser" class="headerlink" title="4.create local user(for map to virtualuser)"></a>4.create local user(for map to virtualuser)</h4><pre><code>useradd -s /sbin/nologin -d /data/program/ftpdata ftpuser
</code></pre>
<h4 id="5-install-db-for-auth"><a href="#5-install-db-for-auth" class="headerlink" title="5.install db(for auth)"></a>5.install db(for auth)</h4><pre><code>yum install -y db*
</code></pre>
<h4 id="6-create-virtual-users"><a href="#6-create-virtual-users" class="headerlink" title="6.create virtual users"></a>6.create virtual users</h4><pre><code>cd /etc/vsftpd/
vim virtualuser.txt 
user1    # this username
1234qwer # this password
user2    # this username
123.com  # this password
</code></pre>
<h4 id="7-create-db"><a href="#7-create-db" class="headerlink" title="7.create db"></a>7.create db</h4><pre><code>db_load -T -t hash -f virtualuser.txt /etc/vsftpd/virtualuser.db
</code></pre>
<h4 id="8-create-pam-auth-file"><a href="#8-create-pam-auth-file" class="headerlink" title="8.create pam auth file"></a>8.create pam auth file</h4><pre><code>vim /etc/pam.d/vsftpd.vu
auth      required  /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser
account   required  /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser
# if your system is centos6.x release so use this configure:
auth      sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser
account   sufficient  /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualuser
</code></pre>
<h4 id="9-configure-vsftpd-conf"><a href="#9-configure-vsftpd-conf" class="headerlink" title="9.configure vsftpd.conf"></a>9.configure vsftpd.conf</h4><pre><code>vim /etc/vsftpd.conf
anonymous_enable=NO
#allow_writeable_chroot # if your vsftpd version is 3.x
local_enable=YES
write_enable=YES
local_umask=022
dirmessage_enable=YES
xferlog_enable=YES
connect_from_port_20=YES
xferlog_std_format=YES
chroot_local_user=YES
chroot_list_enable=YES
chroot_list_file=/etc/vsftpd/chroot_list
listen=NO
listen_ipv6=YES
pam_service_name=ftpuser
userlist_enable=YES
tcp_wrappers=YES
guest_enable=YES
guest_username=ftp01
pam_service_name=vsftpd.vu
user_config_dir=/etc/vsftpd/user_conf
</code></pre>
<h4 id="10-create-each-user-config-dir"><a href="#10-create-each-user-config-dir" class="headerlink" title="10.create each user config dir"></a>10.create each user config dir</h4><pre><code>cd /etc/vsftpd/ &amp;&amp; mkdir user_conf &amp;&amp; cd user_conf
# for user1 config
# this user can upload、wirte、download 
vim user1
anon_world_readable_only=NO
anon_upload_enable=YES
anon_mkdir_write_enable=YES
anon_other_write_enable=YES
/data/program/ftpdata/user1_home/  # virtual user's datadir

# for user2 config
# this user only can read、download, can't upload
vim user2
anon_world_readable_only=NO
anon_upload_enable=NO
anon_mkdir_write_enable=NO
anon_other_write_enable=NO
/data/program/ftpdata/user2_home # virtual user's datadir
</code></pre>
]]></content>
      <categories>
        <category>必备知识</category>
      </categories>
      <tags>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title>Zabbix Server Is Not Running:错误的解决</title>
    <url>/2014/06/16/zabbix-server-is-not-running-e9-94-99-e8-af-af-e7-9a-84-e8-a7-a3-e5-86-b3/</url>
    <content><![CDATA[<p>今天在安装了zabbix-server端后 &nbsp;在zabbix的dashboard中中出现下面的信息:</p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix.png" alt="zabbix"></a></p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix.png">&nbsp;</a><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix1.png" alt="zabbix1">  
</a></p>
<p>随即检查了zabbix-server的运行状态和mysql的运行状态都是正常的；困扰有半刻钟左右，将所有的配置文件检查了一遍 发现在/etc/zabbix/web/zabbix.conf.php 中有这么一行信息：</p>
<p> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix2.png" alt="zabbix2"></a> </p>
<p>解决： 我将”ZABBIX-SERVER” 改成了我的zabbix-server的IP.保存退出、重启zabbix-server、刷新dashboard;恢复正常啦！</p>
<p>问题原因： 我从新安装了一次,发现在dashboard界面安装过程中,我将host的名称给自定义了, 它默认是”localhost”,这里根本就不需要修改,默认就行啦！</p>
<p><a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix3.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2014/08/zabbix3.png" alt="zabbix3">&nbsp;</a></p>
<p>“Name”倒是怎么方便怎么来。 比较重要的文件就是/etc/zabbix/中的那一堆了;日志文件的话都在/var/log/zabbixsrv/下面. 还有一些错误倒是能根据一些提示解决,比如/etc/php.ini中的好几个参数需要修改，按照提示修改完，重启一下apache就ok!</p>
]]></content>
      <categories>
        <category>故障处理</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title>Zabbix 监控Mysql状态以及mysql主从</title>
    <url>/2015/10/23/zabbix-e7-9b-91-e6-8e-a7mysql-e7-8a-b6-e6-80-81-e4-bb-a5-e5-8f-8amysql-e4-b8-bb-e4-bb-8e/</url>
    <content><![CDATA[<p>一，利用zabbix自带模板监控mysql状态： 1，在从的mysql服务器上面创建一个用于zabbix监控的用户 grant replication client on <em>.</em> to zabbix@’localhost’ &nbsp;IDENTIFIED BY ‘PASSWORD’; 2，根据zabbix监控mysql的key改写脚本</p>
<p>#/bin/bash<br>DEF=”–defaults-file=/etc/zabbix/my.conf”<br>MYSQL=’/usr/local/webservers/mysql-5.6.19/bin/mysqladmin’<br>ARGS=1<br>if [ $# -ne “$ARGS” ];then<br>    echo “Please input one arguement:”<br>fi<br>case $1 in<br>    Uptime)<br>        result=<code>${MYSQL} $DEF status|cut -f2 -d":"|cut -f1 -d"T"</code><br>            echo $result<br>            ;;<br>        Com_update)<br>            result=<code>${MYSQL} $DEF extended-status |grep -w "Com_update"|cut -d"|" -f3</code><br>            echo $result<br>            ;;<br>        Slow_queries)<br>        result=<code>${MYSQL} $DEF status |cut -f5 -d":"|cut -f1 -d"O"</code><br>                echo $result<br>                ;;<br>    Com_select)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Com_select"|cut -d"|" -f3</code><br>                echo $result<br>                ;;<br>    Com_rollback)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Com_rollback"|cut -d"|" -f3</code><br>                echo $result<br>                ;;<br>    Questions)<br>        result=<code>${MYSQL} $DEF status|cut -f4 -d":"|cut -f1 -d"S"</code><br>                echo $result<br>                ;;<br>    Com_insert)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Com_insert"|cut -d"|" -f3</code><br>                echo $result<br>                ;;<br>    Com_delete)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Com_delete"|cut -d"|" -f3</code><br>                echo $result<br>                ;;<br>    Com_commit)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Com_commit"|cut -d"|" -f3</code><br>                echo $result<br>                ;;<br>    Bytes_sent)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Bytes_sent" |cut -d"|" -f3</code><br>                echo $result<br>                ;;<br>    Bytes_received)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Bytes_received" |cut -d"|" -f3</code><br>                echo $result<br>                ;;<br>    Com_begin)<br>        result=<code>${MYSQL} $DEF extended-status |grep -w "Com_begin"|cut -d"|" -f3</code><br>                echo $result<br>                ;; </p>
<pre><code>    *) 
    echo "Usage:$0(Uptime|Com\_update|Slow\_queries|Com\_select|Com\_rollback|Questions)" 
    ;; 
</code></pre>
<p>esac</p>
<p>上面的脚本中： /etc/zabbix/my.conf 这个文件定义的是zabbix这个mysql用户的信息，然后直接指定这个文件，如果我们在命令行直接输入zabbix的用户密码的话会一行提示，影响我们zabbix取值，例如： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-131307.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-131307.png" alt="Screenshot from 2015-10-23 13:13:07"></a> 如果我们直接指定了这个文件，那么那行提示就不会出现 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-131457.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-131457.png" alt="Screenshot from 2015-10-23 13:14:57"></a> /etc/zabbix/my.conf的内容：</p>
<p>[client]<br>host=localhost<br>user=zabbix<br>password=’PASSWORD’<br>socket = /data/mysql/mysql.sock</p>
<p>3，设置zabbix_agent端 首先启用自定义的key 去掉zabbix_agent配置文件的259行: Include=/usr/local/etc/zabbix_agentd.conf.d/*.conf 然后自定义一个mysql-status.conf 在这个目录下，内容为：</p>
<p>UserParameter=mysql.version,/usr/local/webservers/mysql-5.6.19/bin/mysql -V<br>UserParameter=mysql.ping,/usr/local/webservers/mysql-5.6.19/bin/mysqladmin –defaults-file=/etc/zabbix/my.conf ping | grep -c alive<br>UserParameter=mysql.status[*],/home/shell/mysql-status.sh $1<br>#注意：这里自定义的key 建议使用命令的绝对路径</p>
<p>ok 上面的内容定义好之后重启zabbix_agent服务，然后就可以在zabbix_server端看一下能不能获取到key值 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-132702.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-132702.png" alt="Screenshot from 2015-10-23 13:27:02"></a> 好啦，以上内容中可以看到取值正常，我们在zabbix dashboard中查看一下吧； <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-132949.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-132949.png" alt="Screenshot from 2015-10-23 13:29:49"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-133031.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-133031.png" alt="Screenshot from 2015-10-23 13:30:31"></a> 至此已经监控到mysql的状态啦；这里一定要注意mysql用户取获取mysql服务的状态时候的权限； 二、zabbix监控mysql主从状态 跟上面的步骤差不多，这里使用的mysql用户我还是使用的zabbix用户 那这里直接就自定义key 啦，大家都知道mysql主从状态我们一般通过在mysql slaves上面的show slave status 然后看</p>
<p>Slave_IO_Running: Yes<br>Slave_SQL_Running: Yes</p>
<p>这两个值，如果其中一个不为Yes那说明主从同步是有问题的，那我们用zabbix这个的身份来获取这个值 编写脚本：mysql_replication.sh</p>
<p>#!/bin/bash<br>/usr/local/webservers/mysql-5.6.19/bin/mysql –defaults-file=/etc/zabbix/my.conf -e ‘show slave status\G’ | grep -E “Slave_IO_Running:|Slave_SQL_Running:”  | awk ‘{print $2}’ | grep -c Yes</p>
<p>这个脚本的用途是，用zabbix用户身份执行”show slave status\G”这个命令，然后截取Yes这个关键字的行数是2 或者非 2 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-134737.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-134737.png" alt="Screenshot from 2015-10-23 13:47:37"></a> 在zabbix_server 端/usr/local/etc/zabbix_agentd.conf.d/目录下新建文件：mysql-replication.conf，内容为：</p>
<p>UserParameter=mysql.replication,/home/shell/mysql-replication.sh</p>
<p>写好之后，重启zabbix_agent 然后在zabbix_server端测试一下看能否获取到这个值 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-140321.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-140321.png" alt="Screenshot from 2015-10-23 14:03:21"></a> zabbix_server端能获取，现在在zabbix dashboard添加这个item吧 Configuration–&gt;Host–&gt;database-node2–&gt;Items–&gt;Create item <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-140754.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-140754.png" alt="Screenshot from 2015-10-23 14:07:54"></a> 这里我们自定义的key 需要手动输入key名称。我这里定义过了，直接点击“Add”就行了 然后我们需要定义一个Triggers Triggers–&gt;Create triggers <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141313.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141313.png" alt="Screenshot from 2015-10-23 14:13:13"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141153.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141153.png" alt="Screenshot from 2015-10-23 14:11:53"></a> 检测它最后一次的取值是不是小于2，定义N的值为2，如果取得的值小于2就说明有问题啦， 定义一下报警： Configuration–&gt;Action–&gt;Create action <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141621.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141621.png" alt="Screenshot from 2015-10-23 14:16:21"></a></p>
<p>邮件通知内容：<br>MySQL.Repliaction<br>ERROR—MySQL master-slave –&gt;{ITEM.VALUE1}<br>MySQL 主从出现问题，请检测主从状态！！！<br>告警主机 : {HOSTNAME1}<br>告警时间 : {EVENT.DATE} {EVENT.TIME}<br>告警等级 : {TRIGGER.SEVERITY}<br>告警信息 : {TRIGGER.NAME}<br>告警项目 : {TRIGGER.KEY1}<br>问题详情 : {ITEM.NAME}:{ITEM.VALUE}<br>当前状态 : {TRIGGER.STATUS}:{ITEM.VALUE1}<br>事件ID : {EVENT.ID}</p>
<p>恢复后的回复:<br>OK—MySQL master-slave –&gt;{ITEM.VALUE1}<br>MySQL 主从问题恢复，请确认主从状态！！！<br>告警主机 : {HOSTNAME1}<br>告警时间 : {EVENT.DATE} {EVENT.TIME}<br>告警等级 : {TRIGGER.SEVERITY}<br>告警信息 : {TRIGGER.NAME}<br>告警项目 : {TRIGGER.KEY1}<br>问题详情 : {ITEM.NAME}:{ITEM.VALUE}<br>当前状态 : {TRIGGER.STATUS}:{ITEM.VALUE1}<br>事件ID   : {EVENT.ID}</p>
<p>条件： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141903.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141903.png" alt="Screenshot from 2015-10-23 14:19:03"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141935.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-141935.png" alt="Screenshot from 2015-10-23 14:19:35"></a> zabbix自定义脚本发送邮件； 首先我们要在zabbix_server端启用自定义脚本发送邮件，在zabbix_server主配置文件中修改为 AlertScriptsPath=/usr/lib/zabbix/alertscripts &nbsp;脚本存放的目录 然后我们自定义一个脚本mail.py</p>
<p>#!/usr/bin/python<br>#coding:utf-8 </p>
<p>import smtplib<br>from email.mime.text import MIMEText<br>import sys </p>
<p>#邮箱服务器地址<br>mail_host = ‘smtp.qq.com’<br>#邮箱用户名<br>mail_user = ‘xxxxxxx’<br>#邮箱密码<br>mail_pass = ‘xxxxxxxx’<br>mail_postfix = ‘qq.com’</p>
<p>def send_mail(to_list,subject,content):<br>    me = mail_user+”&lt;”+mail_user+”@”+mail_postfix+”&gt;”<br>    msg = MIMEText(content,_charset=’utf-8’)<br>    msg[‘Subject’] = subject<br>    msg[‘From’] = me<br>    msg[‘to’] = to_list </p>
<pre><code>try:
    s = smtplib.SMTP()
    s.connect(mail_host)
    s.login(mail\_user,mail\_pass)
    s.sendmail(me,to\_list,msg.as\_string())
    s.close()
    return True
except Exception,e:
    print str(e)
    return False
</code></pre>
<p>if __name__ == “__main__“:<br>    send_mail(sys.argv[1], sys.argv[2], sys.argv[3])</p>
<p>记得加上执行权限； 然后在zabbix dashboard上面定义用户的media加上用户的邮箱即可 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-142528.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-142528.png" alt="Screenshot from 2015-10-23 14:25:28"></a> 以上就可以实现报警啦 测试一下，将mysql-replication.sh这个脚本的值自定义一个输出值为0或者1 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-142832.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-142832.png" alt="Screenshot from 2015-10-23 14:28:32"></a> 查看邮件： <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/zabbix_1.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/zabbix_1.png" alt="zabbix_1"></a> 如果恢复后的回复 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/zabbix_2.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/zabbix_2.png" alt="zabbix_2"></a> ok 已经成功能看到这个监控没问题啦。 下面来看一下zabbix + grafana ,这个是作为页面展示比较华丽的效果 <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-144225.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-144225.png" alt="Screenshot from 2015-10-23 14:42:25"></a> <a href="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-144008.png"><img src="https://qcloud.coding.net/u/guomaoqiu/p/guomaoqiu/git/raw/master/uploads/2015/10/Screenshot-from-2015-10-23-144008.png" alt="Screenshot from 2015-10-23 14:40:08"></a> 只要是在zabbix 中有的 items都可以在grafana中展示。</p>
]]></content>
      <categories>
        <category>Monitor</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>基于容器化部署Nacos集群</title>
    <url>/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1 id="一、搭建架构"><a href="#一、搭建架构" class="headerlink" title="一、搭建架构"></a>一、搭建架构</h1><p><img src="/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/arch-1.png"><br>● 3个或3个以上Nacos节点才能构成集群；<br>● Nacos Nginx Proxy用于代理转发；</p>
<hr>
<h1 id="二、准备工作"><a href="#二、准备工作" class="headerlink" title="二、准备工作"></a>二、准备工作</h1><p><em><strong>Nacos2.0版本相比1.X新增了gRPC的通信方式，因此需要增加2个端口。新增端口是在配置的主端口(server.port)基础上，进行一定偏移量自动生成。</strong></em></p>
<table>
<thead>
<tr>
<th>端口</th>
<th>与主端口的偏移量</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>8848</td>
<td>0</td>
<td>Nacos程序主配置端口</td>
</tr>
<tr>
<td>9848</td>
<td>+1000</td>
<td>客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求</td>
</tr>
<tr>
<td>9849</td>
<td>+1001</td>
<td>服务端gRPC请求服务端端口，用于服务间同步等</td>
</tr>
<tr>
<td>7848</td>
<td>-1000</td>
<td>Nacos 集群通信端口，用于Nacos 集群间进行选举，检测等</td>
</tr>
</tbody></table>
<p><em><strong>使用VIP/nginx请求时，需要配置成TCP转发，不能配置http2转发，否则连接会被nginx断开</strong></em><br><img src="/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/arch-2.png"></p>
<p>按照上述官方的端口分配要求 ，此处部署的使用三台服务器上面创建的Nacos集群端口分配如下：</p>
<table>
<thead>
<tr>
<th>节点</th>
<th>IP</th>
<th>端口（所需暴露）</th>
<th>备注</th>
<th>版本</th>
<th>当前线下环境部署文件路径</th>
</tr>
</thead>
<tbody><tr>
<td>Nacos_1</td>
<td>192168.18.73</td>
<td>宿主机：8858，9858，9859，7858  容器：8858，9858，9859，7858</td>
<td>Nacos 节点一</td>
<td>nacos/nacos-server:2.0.2</td>
<td>/root/nacos-deploy/</td>
</tr>
<tr>
<td>Nacos_2</td>
<td>192168.18.74</td>
<td>宿主机：8858，9858，9859，7858  容器：8858，9858，9859，7858</td>
<td>Nacos 节点二</td>
<td>nacos/nacos-server:2.0.2</td>
<td>/root/nacos-deploy/</td>
</tr>
<tr>
<td>Nacos_3</td>
<td>192168.18.75</td>
<td>宿主机：8858，9858，9859，7858  容器：8858，9858，9859，7858</td>
<td>Nacos 节点三</td>
<td>nacos/nacos-server:2.0.2</td>
<td>/root/nacos-deploy/</td>
</tr>
<tr>
<td>Nacos DB</td>
<td>192168.18.75</td>
<td>宿主机：3306 容器：3306</td>
<td>Nacos所需数据库</td>
<td>mysql:5.7.34</td>
<td>/root/nacos-mysql-deploy</td>
</tr>
<tr>
<td>Nacos  Nginx Proxy</td>
<td>192168.18.75</td>
<td>宿主机：80  容器：80</td>
<td>Nacos代理</td>
<td>nginx:stable-alpine</td>
<td>/root/nacos-proxy-deploy</td>
</tr>
</tbody></table>
<h2 id="2-1-创建Nacos数据库"><a href="#2-1-创建Nacos数据库" class="headerlink" title="2.1  创建Nacos数据库"></a>2.1  创建Nacos数据库</h2><p>这里使用的是容器化运行nacos，需要创建一个数据库，并从官方对应的版本中去导入基本数据库结构的数据文件</p>
<h2 id="2-1-1-获取nacos-2-0-2-包文件中的基础表结构数据"><a href="#2-1-1-获取nacos-2-0-2-包文件中的基础表结构数据" class="headerlink" title="2.1.1 获取nacos 2.0.2 包文件中的基础表结构数据"></a>2.1.1 获取nacos 2.0.2 包文件中的基础表结构数据</h2><figure class="highlight pgsql"><table><tbody><tr><td class="code"><pre><span class="line">https://github.com/alibaba/nacos/releases/download/<span class="number">2.0</span><span class="number">.2</span>/nacos-<span class="keyword">server</span><span class="number">-2.0</span><span class="number">.2</span>.tar.gz</span><br><span class="line"></span><br><span class="line"># tar -xf nacos-<span class="keyword">server</span><span class="number">-2.0</span><span class="number">.2</span>.tar.gz</span><br><span class="line"># cd  nacos</span><br><span class="line"># tree -L <span class="number">2</span></span><br><span class="line">.</span><br><span class="line">├── bin</span><br><span class="line">│   ├── shutdown.cmd</span><br><span class="line">│   ├── shutdown.sh</span><br><span class="line">│   ├── startup.cmd</span><br><span class="line">│   └── startup.sh</span><br><span class="line">├── conf</span><br><span class="line">│   ├── <span class="number">1.4</span><span class="number">.0</span>-ipv6_support-<span class="keyword">update</span>.<span class="keyword">sql</span></span><br><span class="line">│   ├── application.properties</span><br><span class="line">│   ├── application.properties.example</span><br><span class="line">│   ├── <span class="keyword">cluster</span>.conf.example</span><br><span class="line">│   ├── nacos-logback.xml</span><br><span class="line">│   ├── nacos-mysql.<span class="keyword">sql</span></span><br><span class="line">│   └── <span class="keyword">schema</span>.<span class="keyword">sql</span></span><br><span class="line">├── LICENSE</span><br><span class="line">├── <span class="keyword">NOTICE</span></span><br><span class="line">└── target</span><br><span class="line">    └── nacos-<span class="keyword">server</span>.jar</span><br><span class="line">以上是nacos的二进制包文件 ，这里只用到了 nacos-mysql.<span class="keyword">sql</span>  这个文件</span><br></pre></td></tr></tbody></table></figure>
<h2 id="2-1-2-创建mysql服务"><a href="#2-1-2-创建mysql服务" class="headerlink" title="2.1.2 创建mysql服务"></a>2.1.2 创建mysql服务</h2><figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 192.168.18.75</span></span><br><span class="line">sudo <span class="operator">-</span>i </span><br><span class="line">mkdir <span class="symbol">/data/mysql</span></span><br><span class="line">cat <span class="operator">&lt;</span><span class="operator">&lt;</span> EOF <span class="operator">&gt;</span> <span class="symbol">/root/nacos-mysql-deploy/mysql.yaml</span></span><br><span class="line"><span class="params">version:</span> <span class="string">"3"</span></span><br><span class="line"><span class="params">services:</span></span><br><span class="line">  <span class="params">mysql-db:</span></span><br><span class="line">    <span class="params">container_name:</span> nacos-mysql</span><br><span class="line">    <span class="params">image:</span> mysql:<span class="number">5.7</span>.<span class="number">34</span></span><br><span class="line">    <span class="params">ports:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"3306:3306"</span></span><br><span class="line">    <span class="params">environment:</span></span><br><span class="line">      <span class="params">MYSQL_ROOT_PASSWORD:</span> xxxxxxxxxxxx</span><br><span class="line">    <span class="params">volumes:</span></span><br><span class="line">      <span class="operator">-</span> <span class="string">"/data/mysql:/var/lib/mysql"</span></span><br><span class="line">    <span class="params">restart:</span> always</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker-compose <span class="operator">-</span>f  <span class="symbol">/root/nacos-mysql-deploy/mysql.yaml</span> up <span class="operator">-</span>d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入官方表数据</span></span><br><span class="line">docker cp <span class="symbol">/root/nacos/conf/nacos-mysql.sql</span> nacos-mysql:<span class="symbol">/tmp</span></span><br><span class="line"></span><br><span class="line">docker exec <span class="operator">-</span>it nacos-mysql sh</span><br><span class="line"></span><br><span class="line">mysql <span class="operator">-</span>uroot <span class="operator">-</span>pxxxxxxxxxxxx</span><br><span class="line"></span><br><span class="line">create database nacos;</span><br><span class="line"></span><br><span class="line">use nacos;</span><br><span class="line"></span><br><span class="line">source <span class="symbol">/tmp/nacos-mysql.sql</span>;</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>


<h2 id="2-2-Naocs服务"><a href="#2-2-Naocs服务" class="headerlink" title="2.2  Naocs服务"></a>2.2  Naocs服务</h2><h2 id="2-2-1-创建、编写yaml文件"><a href="#2-2-1-创建、编写yaml文件" class="headerlink" title="2.2.1 创建、编写yaml文件"></a>2.2.1 创建、编写yaml文件</h2><figure class="highlight routeros"><table><tbody><tr><td class="code"><pre><span class="line">sudo -i</span><br><span class="line">mkdir /data/nacos2.0.2_1/logs -p</span><br><span class="line">mkdir /root/nacos-deploy/</span><br><span class="line">cat &lt;&lt; EOF &gt; /root/nacos-deploy/nacos1.yaml</span><br><span class="line">version: <span class="string">"3"</span></span><br><span class="line">services:</span><br><span class="line">  nacos1:</span><br><span class="line">    hostname: nacos2.0.2_1</span><br><span class="line">    container_name: nacos2.0.2_1</span><br><span class="line">    image: nacos/nacos-server:2.0.2</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/nacos2.0.2_1/logs:/home/nacos/logs</span><br><span class="line">      - /data/nacos2.0.2_1/custom.properties:/home/nacos/init.d/custom.properties</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"8858:8858"</span></span><br><span class="line">      - <span class="string">"9858:9858"</span></span><br><span class="line">      - <span class="string">"9859:9859"</span></span><br><span class="line">      - <span class="string">"7858:7858"</span></span><br><span class="line">    environment:</span><br><span class="line">      - <span class="attribute">MODE</span>=cluster</span><br><span class="line">      - <span class="attribute">PREFER_HOST_MODE</span>=hostname</span><br><span class="line">      - <span class="attribute">NACOS_SERVER_IP</span>=192.168.18.73</span><br><span class="line">      - <span class="attribute">NACOS_APPLICATION_PORT</span>=8858</span><br><span class="line">      - <span class="attribute">NACOS_SERVERS</span>=192.168.18.73:8858 192.168.18.74:8858 192.168.18.75:8858</span><br><span class="line">      - <span class="attribute">SPRING_DATASOURCE_PLATFORM</span>=mysql</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_HOST</span>=192.168.18.75</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_DB_NAME</span>=nacos</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_PORT</span>=3306</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_USER</span>=root</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_PASSWORD</span>=xxxxxxxxxxx</span><br><span class="line">      - <span class="attribute">NACOS_AUTH_ENABLE</span>=<span class="literal">true</span></span><br><span class="line">      - <span class="attribute">MYSQL_DATABASE_NUM</span>=1</span><br><span class="line">    restart: always</span><br><span class="line">EOF</span><br><span class="line"><span class="comment">######################</span></span><br><span class="line">sudo -i</span><br><span class="line">mkdir /data/nacos2.0.2_2/logs -p</span><br><span class="line">mkdir /root/nacos-deploy/</span><br><span class="line">cat &lt;&lt; EOF &gt; /root/nacos-deploy/nacos2.yaml</span><br><span class="line">version: <span class="string">"3"</span></span><br><span class="line">services:</span><br><span class="line">  nacos2:</span><br><span class="line">    hostname: nacos2.0.2_2</span><br><span class="line">    container_name: nacos2.0.2_2</span><br><span class="line">    image: nacos/nacos-server:2.0.2</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/nacos2.0.2_2/logs:/home/nacos/logs</span><br><span class="line">      - /data/nacos2.0.2_2/custom.properties:/home/nacos/init.d/custom.properties</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"8858:8858"</span></span><br><span class="line">      - <span class="string">"9858:9858"</span></span><br><span class="line">      - <span class="string">"9859:9859"</span></span><br><span class="line">      - <span class="string">"7858:7858"</span></span><br><span class="line">    environment:</span><br><span class="line">      - <span class="attribute">MODE</span>=cluster</span><br><span class="line">      - <span class="attribute">PREFER_HOST_MODE</span>=hostname</span><br><span class="line">      - <span class="attribute">NACOS_SERVER_IP</span>=192.168.18.74</span><br><span class="line">      - <span class="attribute">NACOS_APPLICATION_PORT</span>=8858</span><br><span class="line">      - <span class="attribute">NACOS_SERVERS</span>=192.168.18.73:8858 192.168.18.74:8858 192.168.18.75:8858</span><br><span class="line">      - <span class="attribute">SPRING_DATASOURCE_PLATFORM</span>=mysql</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_HOST</span>=192.168.18.75</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_DB_NAME</span>=nacos</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_PORT</span>=3306</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_USER</span>=root</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_PASSWORD</span>=xxxxxxxxxxx</span><br><span class="line">      - <span class="attribute">NACOS_AUTH_ENABLE</span>=<span class="literal">true</span></span><br><span class="line">      - <span class="attribute">MYSQL_DATABASE_NUM</span>=1</span><br><span class="line">    restart: always</span><br><span class="line">EOF</span><br><span class="line"><span class="comment">######################</span></span><br><span class="line">sudo -i</span><br><span class="line">mkdir /data/nacos2.0.2_3/logs -p</span><br><span class="line">mkdir /root/nacos-deploy/</span><br><span class="line">cat &lt;&lt; EOF &gt; /root/nacos-deploy/nacos3.yaml</span><br><span class="line">version: <span class="string">"3"</span></span><br><span class="line">services:</span><br><span class="line">  nacos3:</span><br><span class="line">    hostname: nacos2.0.2_3</span><br><span class="line">    container_name: nacos2.0.2_3</span><br><span class="line">    image: nacos/nacos-server:2.0.2</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/nacos2.0.2_3/logs:/home/nacos/logs</span><br><span class="line">      - /data/nacos2.0.2_3/custom.properties:/home/nacos/init.d/custom.properties</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"8858:8858"</span></span><br><span class="line">      - <span class="string">"9858:9858"</span></span><br><span class="line">      - <span class="string">"9859:9859"</span></span><br><span class="line">      - <span class="string">"7858:7858"</span></span><br><span class="line">    environment:</span><br><span class="line">      - <span class="attribute">MODE</span>=cluster</span><br><span class="line">      - <span class="attribute">PREFER_HOST_MODE</span>=hostname</span><br><span class="line">      - <span class="attribute">NACOS_SERVER_IP</span>=192.168.18.75</span><br><span class="line">      - <span class="attribute">NACOS_APPLICATION_PORT</span>=8858</span><br><span class="line">      - <span class="attribute">NACOS_SERVERS</span>=192.168.18.73:8858 192.168.18.74:8858 192.168.18.75:8858</span><br><span class="line">      - <span class="attribute">SPRING_DATASOURCE_PLATFORM</span>=mysql</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_HOST</span>=192.168.18.75</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_DB_NAME</span>=nacos</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_PORT</span>=3306	</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_USER</span>=root</span><br><span class="line">      - <span class="attribute">MYSQL_SERVICE_PASSWORD</span>=xxxxxxxxxxx</span><br><span class="line">      - <span class="attribute">NACOS_AUTH_ENABLE</span>=<span class="literal">true</span></span><br><span class="line">      - <span class="attribute">MYSQL_DATABASE_NUM</span>=1</span><br><span class="line">    restart: always</span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>
<p>以上 各配置中的端口非标准端口，需要注意环境变量 NACOS_APPLICATION_PORT  如果不指定为非特定端口，那么缺省在配置中的环境变量则为 8848 ，在同一台机器上运行三个节点会出现问题！</p>
<h2 id="2-2-2-运行Nacos"><a href="#2-2-2-运行Nacos" class="headerlink" title="2.2.2 运行Nacos"></a>2.2.2 运行Nacos</h2><p>各节点运行即可</p>
<figure class="highlight awk"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 192.168.18.73</span></span><br><span class="line">docker-compose -f <span class="regexp">/root/</span>nacos-deploy/nacos1.yaml up -d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 192.168.18.74</span></span><br><span class="line">docker-compose -f <span class="regexp">/root/</span>nacos-deploy/nacos2.yaml up -d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 192.168.18.75</span></span><br><span class="line">docker-compose -f <span class="regexp">/root/</span>nacos-deploy/nacos3.yaml up -d</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/3.png"></p>
<p>上图 三个节点单独访问页面正常；通过代理访问正常；</p>
<p>注意：web页面并没有使用到客户端，服务端gRPC调用服务，但是在程序中需要请求对应的端口，所以务必需要暴露出来，因为使用了非标准端口，那么在nginx做代理时就直接(伪装)映射成标准端口即可。</p>
<h2 id="2-3-Nginx代理配置"><a href="#2-3-Nginx代理配置" class="headerlink" title="2.3 Nginx代理配置"></a>2.3 Nginx代理配置</h2><p>此处仍然使用容器化方式运行。<br>配置nginx代理</p>
<figure class="highlight pgsql"><table><tbody><tr><td class="code"><pre><span class="line">mkdir /root/nacos-proxy-deploy</span><br><span class="line">cd /root/nacos-proxy-deploy</span><br><span class="line">cat &lt;&lt;EOF &gt; nginx.conf  </span><br><span class="line"><span class="keyword">user</span>  nginx;</span><br><span class="line">worker_processes  auto;</span><br><span class="line"> </span><br><span class="line">error_log  /var/<span class="keyword">log</span>/nginx/error.<span class="keyword">log</span> <span class="keyword">notice</span>;</span><br><span class="line">pid        /var/run/nginx.pid;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">events {</span><br><span class="line">    worker_connections  <span class="number">65535</span>;</span><br><span class="line">}</span><br><span class="line"> </span><br><span class="line">stream {</span><br><span class="line">    upstream nacos-<span class="keyword">server</span>-grpc9848 {</span><br><span class="line">      <span class="keyword">server</span> <span class="number">192.168</span><span class="number">.18</span><span class="number">.73</span>:<span class="number">9858</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">      <span class="keyword">server</span> <span class="number">192.168</span><span class="number">.18</span><span class="number">.74</span>:<span class="number">9858</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">      <span class="keyword">server</span> <span class="number">192.168</span><span class="number">.18</span><span class="number">.75</span>:<span class="number">9858</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s</span><br><span class="line">     }</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">server</span> {</span><br><span class="line">        <span class="keyword">listen</span> <span class="number">9848</span>;</span><br><span class="line">        proxy_pass nacos-<span class="keyword">server</span>-grpc9848;</span><br><span class="line">    }</span><br><span class="line"> </span><br><span class="line">    upstream nacos-<span class="keyword">server</span>-grpc9849 {</span><br><span class="line">      <span class="keyword">server</span> <span class="number">192.168</span><span class="number">.18</span><span class="number">.73</span>:<span class="number">9859</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">      <span class="keyword">server</span> <span class="number">192.168</span><span class="number">.18</span><span class="number">.74</span>:<span class="number">9859</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;      </span><br><span class="line">      <span class="keyword">server</span> <span class="number">192.168</span><span class="number">.18</span><span class="number">.75</span>:<span class="number">9859</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">     }</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">server</span> {</span><br><span class="line">        <span class="keyword">listen</span> <span class="number">9849</span>;</span><br><span class="line">        proxy_pass nacos-<span class="keyword">server</span>-grpc9849;</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"> </span><br><span class="line">http {</span><br><span class="line">    <span class="keyword">include</span>       /etc/nginx/mime.<span class="keyword">types</span>;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"> </span><br><span class="line">    log_format  main  <span class="string">'$remote_addr - $remote_user [$time_local] "$request" '</span></span><br><span class="line">                      <span class="string">'$status $body_bytes_sent "$http_referer" '</span></span><br><span class="line">                      <span class="string">'"$http_user_agent" "$http_x_forwarded_for"'</span>;</span><br><span class="line"> </span><br><span class="line">    access_log  /var/<span class="keyword">log</span>/nginx/<span class="keyword">access</span>.<span class="keyword">log</span>  main;</span><br><span class="line"> </span><br><span class="line">    sendfile        <span class="keyword">on</span>;</span><br><span class="line">    #tcp_nopush     <span class="keyword">on</span>;</span><br><span class="line"> </span><br><span class="line">    keepalive_timeout  <span class="number">65</span>;</span><br><span class="line"> </span><br><span class="line">    #gzip  <span class="keyword">on</span>;</span><br><span class="line"> </span><br><span class="line">    upstream NACOS {</span><br><span class="line">      <span class="keyword">server</span>  <span class="number">192.168</span><span class="number">.18</span><span class="number">.73</span>:<span class="number">8858</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">      <span class="keyword">server</span>  <span class="number">192.168</span><span class="number">.18</span><span class="number">.74</span>:<span class="number">8858</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">      <span class="keyword">server</span>  <span class="number">192.168</span><span class="number">.18</span><span class="number">.75</span>:<span class="number">8858</span> max_fails=<span class="number">1</span> fail_timeout=<span class="number">30</span>s;</span><br><span class="line">   }</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">server</span> {</span><br><span class="line">      <span class="keyword">listen</span>       <span class="number">8848</span> default_server;</span><br><span class="line">      server_name  _;</span><br><span class="line"> </span><br><span class="line">      <span class="keyword">location</span> / {</span><br><span class="line">        proxy_pass http://NACOS;</span><br><span class="line">      }</span><br><span class="line"> </span><br><span class="line">    }</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">include</span> /etc/nginx/conf.d<span class="comment">/*.conf;</span></span><br><span class="line"><span class="comment">}</span></span><br><span class="line"><span class="comment">EOF</span></span><br></pre></td></tr></tbody></table></figure>

<p>代理容器yaml文件编写</p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&gt;</span> <span class="string">nacos-nginx-proxy.yaml</span> <span class="string">&lt;&lt;</span> <span class="string">EOF</span>  </span><br><span class="line"><span class="attr">version:</span> <span class="string">"3"</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nacos_proxy:</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">nacos_nginx_proxy</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">"nginx:stable-alpine"</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">80:</span> <span class="number">8848</span>  <span class="comment">#这里定义一个80是为了前端访问，或客户端配置定义时无需再加端口，一个ip或者域名即可</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">8848</span><span class="string">:8848</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">9848</span><span class="string">:9848</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">9849</span><span class="string">:9849</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/root/nacos-proxy-deploy/nginx.conf:/etc/nginx/nginx.conf:ro</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 容器运行</span></span><br><span class="line"><span class="string">docker-compose</span> <span class="string">-f</span> <span class="string">nacos-proxy.yaml</span> <span class="string">up</span> <span class="string">-d</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/4.png"><br>至此搭建完毕，但是容器运行正常不一定服务就是正常，这时需要测试实例服务注册是否正常</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cat &gt; nacos_check_status.py &lt;&lt; EOF</span><br><span class="line"><span class="comment"># author: maoqiu.guo</span></span><br><span class="line"><span class="comment"># desc: Nacos服务注册功能测试、状态监控</span></span><br><span class="line"><span class="comment"># date：2022-06-11</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> logging.handlers <span class="keyword">import</span> RotatingFileHandler</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">logging.basicConfig(level=logging.DEBUG)</span><br><span class="line"><span class="comment"># 创建日志记录器，指明日志保存的路径，每个日志文件的最大值，保存的日志文件个数上限</span></span><br><span class="line">log_handle = RotatingFileHandler(<span class="string">'./log.txt'</span>, maxBytes=<span class="number">1024</span>*<span class="number">1024</span>, backupCount=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 创建日志记录的格式</span></span><br><span class="line">formatter = logging.Formatter(<span class="string">"format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s-%(funcName)s',"</span>)</span><br><span class="line"><span class="comment"># 为创建的日志记录器设置日志记录格式</span></span><br><span class="line">log_handle.setFormatter(formatter)</span><br><span class="line"><span class="comment"># 为全局的日志工具对象添加日志记录器</span></span><br><span class="line">logging.getLogger().addHandler(log_handle)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 进度条</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">progress_bar</span>(<span class="params">timmer</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"\r"</span>, end=<span class="string">""</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Waiting: {}%: "</span>.<span class="built_in">format</span>(i), <span class="string">"▋"</span> * (i // <span class="number">2</span>), end=<span class="string">""</span>)</span><br><span class="line">        sys.stdout.flush()</span><br><span class="line">        time.sleep(timmer)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Nacos</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.webhookurl=<span class="string">"https://oapi.dingtalk.com/robot/send?access_token=1968dd11647dfd686c4e1107cf1ad3d0d21c3813a824ee632728327722d9fa82"</span></span><br><span class="line">        <span class="variable language_">self</span>.login_url = <span class="string">"/nacos/v1/auth/users/login"</span></span><br><span class="line">        <span class="variable language_">self</span>.accessToken=<span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.service_url =  <span class="string">"/nacos/v1/ns/catalog/services"</span></span><br><span class="line">        <span class="variable language_">self</span>.instance_url = <span class="string">"/nacos/v1/ns/instance"</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dingding</span>(<span class="params">self, content</span>):</span><br><span class="line">         </span><br><span class="line">        data = {</span><br><span class="line">            <span class="string">"msgtype"</span>: <span class="string">"text"</span>,</span><br><span class="line">            <span class="string">"text"</span>: {</span><br><span class="line">                <span class="string">"content"</span>: <span class="string">"Nacos-"</span> + content,</span><br><span class="line">            },</span><br><span class="line">        }</span><br><span class="line">        headers = {<span class="string">'Content-Type'</span>: <span class="string">'application/json;charset=utf-8'</span>}</span><br><span class="line">        response = requests.post(<span class="variable language_">self</span>.webhookurl, data=json.dumps(data), headers=headers)</span><br><span class="line">        <span class="built_in">print</span>(response.content)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_access_token</span>(<span class="params">self, nacos_server</span>):</span><br><span class="line">        logging.info(<span class="string">"\n\n************** NacosServer: {0} **************"</span>.<span class="built_in">format</span>(nacos_server))</span><br><span class="line">        <span class="string">"登录获取nacos accessToken"</span></span><br><span class="line">        params= <span class="string">"username=nacos&amp;password=nacos"</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            r = json.loads(requests.post(params=params, url=<span class="string">"http://"</span> + nacos_server + <span class="variable language_">self</span>.login_url, timeout=<span class="number">5</span>).text)</span><br><span class="line">            accessToken = <span class="built_in">str</span>((r[<span class="string">'accessToken'</span>]))</span><br><span class="line">            <span class="keyword">return</span> {<span class="string">"accessToken"</span>: accessToken, <span class="string">"result"</span>: <span class="literal">True</span>, <span class="string">"server"</span>: nacos_server, <span class="string">"msg"</span>: <span class="string">"Login Nacos Success...[{0}]"</span>.<span class="built_in">format</span>(nacos_server)}</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">return</span> {<span class="string">"accessToken"</span>: <span class="literal">None</span>, <span class="string">"result"</span>: <span class="literal">False</span>, <span class="string">"server"</span>: nacos_server, <span class="string">"msg"</span>: <span class="string">"Login Nacos Falied...[{0}] Error: {1}"</span>.<span class="built_in">format</span>(nacos_server, <span class="built_in">str</span>(e))}</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register_test</span>(<span class="params">self, accessToken, nacos_server</span>):</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        1. 创建实例</span></span><br><span class="line"><span class="string">        2. 注册服务</span></span><br><span class="line"><span class="string">        3. 删除实例</span></span><br><span class="line"><span class="string">        4. 删除服务</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        data={</span><br><span class="line">            <span class="string">"serviceName"</span>: <span class="string">"test_service_instance"</span>,</span><br><span class="line">            <span class="string">"namespaceId"</span>: <span class="string">"public"</span>,</span><br><span class="line">            <span class="string">"accessToken"</span>: accessToken,</span><br><span class="line">            <span class="string">"ip"</span>: nacos_server.split(<span class="string">":"</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">"port"</span>: nacos_server.split(<span class="string">":"</span>)[<span class="number">1</span>],</span><br><span class="line">            <span class="string">"ephemeral"</span>: <span class="literal">False</span></span><br><span class="line">        }</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 创建服务(注册实例)</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            r = (requests.post(params=data, url=<span class="string">"http://"</span> + nacos_server + <span class="variable language_">self</span>.instance_url, timeout=<span class="number">5</span>).text)</span><br><span class="line">            <span class="keyword">if</span> r == <span class="string">"ok"</span>:</span><br><span class="line">                msg = <span class="string">"[注册实例&amp;创建服务成功] Service: {0} NameSpace: {1} {2} InstanceIP: {3}"</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server, data[<span class="string">'ip'</span>])</span><br><span class="line">                logging.info(msg)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                msg = <span class="string">"[注册实例&amp;创建服务失败] Service: {0} NameSpace: {1} {2} {3}"</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server, r)</span><br><span class="line">                logging.error(msg)</span><br><span class="line">                <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                msg = <span class="string">"[注册实例&amp;创建服务失败] Service: {0} NameSpace: {1} {2} {3}"</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server, r)</span><br><span class="line">                logging.error(msg)</span><br><span class="line">                <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 删除实例</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delete_instance_service</span>(<span class="params">self, accessToken, nacos_server</span>):</span><br><span class="line">        data={</span><br><span class="line">            <span class="string">"serviceName"</span>: <span class="string">"test_service_instance"</span>,</span><br><span class="line">            <span class="string">"namespaceId"</span>: <span class="string">"public"</span>,</span><br><span class="line">            <span class="string">"accessToken"</span>: accessToken,</span><br><span class="line">            <span class="string">"ip"</span>: nacos_server.split(<span class="string">":"</span>)[<span class="number">0</span>],</span><br><span class="line">            <span class="string">"port"</span>: nacos_server.split(<span class="string">":"</span>)[<span class="number">1</span>],</span><br><span class="line">            <span class="string">"ephemeral"</span>: <span class="literal">False</span></span><br><span class="line">        }</span><br><span class="line">        <span class="comment"># 注销实例</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            r = (requests.delete(params=data, url=<span class="string">"http://"</span> + nacos_server + <span class="variable language_">self</span>.instance_url, timeout=<span class="number">5</span>).text)           </span><br><span class="line">            <span class="keyword">if</span> r == <span class="string">"ok"</span>:</span><br><span class="line">                msg = <span class="string">"[注销实例成功] Service: {0} NameSpace: {1} {2} "</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server)</span><br><span class="line">                logging.info(msg)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                msg = <span class="string">"[注销实例失败] Service: {0} NameSpace: {1} {2} {3}"</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server, r)</span><br><span class="line">                logging.error(msg)</span><br><span class="line">                <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                msg = <span class="string">"[注销实例失败] Service: {0} NameSpace: {1} {2} {3}"</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server, r)</span><br><span class="line">                logging.error(msg)</span><br><span class="line">                <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line"> </span><br><span class="line">        progress_bar(timmer=<span class="number">0.08</span>)</span><br><span class="line">        <span class="comment"># 删除服务</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            r = (requests.delete(params=data, url=<span class="string">"http://"</span> + nacos_server + <span class="string">"/nacos/v1/ns/service"</span>, timeout=<span class="number">5</span>).text)</span><br><span class="line">            <span class="keyword">if</span> r == <span class="string">"ok"</span>:</span><br><span class="line">                msg = <span class="string">"[删除服务成功] Service: {0} NameSpace: {1} {2} "</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server)</span><br><span class="line">                logging.info(msg)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                msg = <span class="string">"[删除服务失败] Service: {0} NameSpace: {1} {2} {3}"</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server, r)</span><br><span class="line">                logging.error(msg)</span><br><span class="line">                <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                msg = <span class="string">"[删除服务失败] Service: {0} NameSpace: {1} {2} {3}"</span>.<span class="built_in">format</span>(data[<span class="string">'serviceName'</span>], data[<span class="string">'namespaceId'</span>], nacos_server, r)</span><br><span class="line">                logging.error(msg)</span><br><span class="line">                <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line">        progress_bar(timmer=<span class="number">0.1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_service_list</span>(<span class="params">self, accessToken, nacos_server</span>):</span><br><span class="line">        <span class="string">"请求访问某个环境的某一个服务列表, 检查返回数据是否为空,为空则为不正常"</span></span><br><span class="line">        payload={</span><br><span class="line">            <span class="string">"accessToken"</span>: accessToken,</span><br><span class="line">            <span class="string">"pageNo"</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">"pageSize"</span>: <span class="number">10</span>,</span><br><span class="line">            <span class="string">"namespaceId"</span>: <span class="string">"stage"</span></span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            r = json.loads(requests.get(params=payload, url=<span class="string">"http://"</span> + nacos_server + <span class="variable language_">self</span>.service_url).text)</span><br><span class="line">            <span class="keyword">if</span> r[<span class="string">'count'</span>] == <span class="number">0</span>:</span><br><span class="line">                msg = <span class="string">"[获取注册服务数据失败] Nacos Server {0} 当前Stage服务列表为空!"</span>.<span class="built_in">format</span>(nacos_server)</span><br><span class="line">                logging.info(msg)</span><br><span class="line">                <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># print("Nacos 注册服务数据正常...[{0}]".format(nacos_server))</span></span><br><span class="line">                logging.info(<span class="string">"[获取注册服务数据成功] ServiceTotal {1} on {0} - SercieName: stage "</span>.<span class="built_in">format</span>(nacos_server,r[<span class="string">'count'</span>]))</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">letsgo</span>(<span class="params">self, nacos_server</span>):</span><br><span class="line">        <span class="comment"># 登录获取token</span></span><br><span class="line">        login_nacos_res = (<span class="variable language_">self</span>.get_access_token(nacos_server))</span><br><span class="line">         </span><br><span class="line">        <span class="keyword">if</span> login_nacos_res[<span class="string">"result"</span>]:</span><br><span class="line">            logging.info(<span class="string">"[登录获取accessToken 成功] Nacos Server {0} "</span>.<span class="built_in">format</span>((login_nacos_res[<span class="string">'server'</span>])))</span><br><span class="line"> </span><br><span class="line">            <span class="variable language_">self</span>.get_service_list(login_nacos_res[<span class="string">'accessToken'</span>], login_nacos_res[<span class="string">'server'</span>] )</span><br><span class="line"> </span><br><span class="line">            <span class="comment"># 创建实例&amp;服务</span></span><br><span class="line">            <span class="variable language_">self</span>.register_test(login_nacos_res[<span class="string">'accessToken'</span>], login_nacos_res[<span class="string">'server'</span>] )</span><br><span class="line"> </span><br><span class="line">            <span class="comment">#time.sleep(60 * 2)</span></span><br><span class="line">            progress_bar(timmer=<span class="number">0.1</span>)</span><br><span class="line">            <span class="variable language_">self</span>.delete_instance_service(login_nacos_res[<span class="string">'accessToken'</span>], login_nacos_res[<span class="string">'server'</span>] )</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            msg = <span class="string">"[登录获取accessToken 失败] {0} "</span>.<span class="built_in">format</span>((login_nacos_res[<span class="string">'msg'</span>]))</span><br><span class="line">            logging.info(msg)</span><br><span class="line">            <span class="variable language_">self</span>.dingding(<span class="variable language_">self</span>, content=msg)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        nacos_server_list=[</span><br><span class="line">        	<span class="comment">#"nacos.axiba.com",</span></span><br><span class="line">            <span class="string">"192.168.18.75:80"</span>,</span><br><span class="line">            <span class="string">"192.168.18.75:8848"</span>,</span><br><span class="line">            <span class="string">"192.168.18.73:8858"</span>,</span><br><span class="line">            <span class="string">"192.168.18.74:8858"</span>,</span><br><span class="line">            <span class="string">"192.168.18.75:8858"</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nacos_server_list:</span><br><span class="line">            Nacos().letsgo(i)</span><br><span class="line">EOF</span><br></pre></td></tr></tbody></table></figure>

<p>运行后：</p>
<p>以上脚本通过Nacos的OpenApi循环式通过从每个节点访问后发起以及通过Nginx代理发起服务实例的查询、注册、删除操作。<br><img src="/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/5.png"></p>
<ol>
<li>登录获取AccessToke(每个请求需要携带)</li>
<li>在 获取stage环境的服务列表数据，这是为了监测返回数据是否正常，因为这里刚搭建起来，如果里面服务数据不正常则会出现上面提示，并发送消息到钉钉；</li>
</ol>
<p> <img src="/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/6.png">  </p>
<p>上图是通过代理访问获取stage中的服务为0，后发送通知，说明此时可能(因为通过负载并不知道该请求是落在了哪一个Nacos节点)集群中某个节点出现故障数据不正常了，但是在检测脚本中会通过每个节点去注册，那么一定会在出现通知某个节点故障。<br>4. 在public环境中通过调用OpenApi方式创建、注册一个实例 test_service_instance 跟服务，然后注销实例，并删除服务，这是为了监测集群是否正常。</p>
<h1 id="三、小记"><a href="#三、小记" class="headerlink" title="三、小记"></a>三、小记</h1><ol>
<li>虽然文档给出的描述是只有当服务下实例数为0时允许删除，但是实际上并没有强制检查校验服务下的实例数是否为0。Nacos的服务创建有多种方式，可以主动创建可以在注册实例的时候创建，可以在实例发送心跳时创建，所以哪怕主动删除了还会自动创建回来。；</li>
<li>所有已注册服务在容器重启后会丢失，但是只要保证集群中有一个节点正常，那么重启后的服务数据会同步其他正常节点的，即需要客户端重新注册；</li>
<li>正常情况在某个节点注册了服务后会同步到其他节点。</li>
<li>各节点无法选举一个leader时 查看日志：/data/nacos2.0.2_1/logs/alipay-jraft.log</li>
<li>实例服务相关日志:  /data/nacos2.0.2_1/naming-raft.log</li>
<li>通过日志发现，暴露8848端口是为了nacos客户端登陆获取Token，后续操作都会携带Token进行资源操作；然后暴露9848端口是客户端用于gRPC的请求，所以如果是设置了代理转发，务必将其暴露，否则连接失败失败。</li>
</ol>
<h1 id="四、吐槽"><a href="#四、吐槽" class="headerlink" title="四、吐槽"></a>四、吐槽</h1><p>Nacos 官方为了推商业版，社区版几乎毛病百出，不管不顾，文档也是写得一团糟~ 无力吐槽</p>
<p> <img src="/2022/06/22/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2Nacos%E9%9B%86%E7%BE%A4/7.jpg">  </p>
]]></content>
      <categories>
        <category>基础组件</category>
        <category>部署</category>
      </categories>
      <tags>
        <tag>nacos</tag>
      </tags>
  </entry>
  <entry>
    <title>如何利用ElastAlert2对ES中的日志创建告警</title>
    <url>/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>线上API服务出现了5xx的请求错误, 未能在第一时间发现导致用户主动向团队报告接口有异常， 由于API的接口比较多，也只是监控了部分，事后通过日志才发现的确用户侧在对某个接口访问时，频繁出现5xx告警，我这边也没有收到什么通知，目前先暂且不说出现5xx是什么造成的，而是在API接口响应状态的监控方面没有做到很到位， 所以需要对Nginx日志中的状态为5xx的请求监控告警，以便能快速响应，解决问题。</p>
<h1 id="ElastAlert2"><a href="#ElastAlert2" class="headerlink" title="ElastAlert2"></a>ElastAlert2</h1><p>ElastAlert 2是一个简单的框架，用于对来自 Elasticsearch 和 OpenSearch 的数据的异常、尖峰或其他感兴趣的模式发出警报。</p>
<p>官方文档：<a href="https://elastalert2.readthedocs.io/en/latest/">https://elastalert2.readthedocs.io/en/latest/</a></p>
<p>Github: <a href="https://github.com/jertel/elastalert2">https://github.com/jertel/elastalert2</a></p>
<h1 id="配置部署"><a href="#配置部署" class="headerlink" title="配置部署"></a>配置部署</h1><p>当然还是选择容器化方式运行，官方也提供得有镜像：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">docker pull docker pull jertel/elastalert2</span><br></pre></td></tr></tbody></table></figure>

<h3 id="配置文件配置"><a href="#配置文件配置" class="headerlink" title="配置文件配置"></a>配置文件配置</h3><p>可参考 <a href="https://github.com/jertel/elastalert2">https://github.com/jertel/elastalert2</a>  目录中的 <code>examples/config.yaml.example</code></p>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">mkdir</span> <span class="string">-p</span> <span class="string">/opt/elastalert2/{data,rules}</span></span><br><span class="line"></span><br><span class="line"><span class="string">vim</span> <span class="string">/opt/elastalert2/config.yaml</span></span><br><span class="line"><span class="attr">rules_folder:</span> <span class="string">rules</span> <span class="comment"># 存放规则文件的目录，这个目录需要挂载到/opt/elastalert/下</span></span><br><span class="line"><span class="attr">run_every:</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">buffer_time:</span></span><br><span class="line">  <span class="attr">minutes:</span> <span class="number">15</span></span><br><span class="line"><span class="attr">es_host:</span> <span class="number">10.10</span><span class="number">.20</span><span class="number">.23</span></span><br><span class="line"><span class="attr">es_port:</span> <span class="number">9200</span></span><br><span class="line"><span class="attr">use_ssl:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">verify_certs:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">ca_certs:</span> <span class="string">/opt/elastalert/ca.crt</span> <span class="comment"># 如果集群启用了ssl记得要把ca证书挂在到容器的这个位置</span></span><br><span class="line"><span class="attr">ssl_show_warn:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">es_send_get_body_as:</span> <span class="string">GET</span></span><br><span class="line"><span class="attr">es_username:</span> <span class="comment"># YOUR ES USERNAME</span></span><br><span class="line"><span class="attr">es_password:</span> <span class="comment"># YOUR ES PASSWORD</span></span><br><span class="line"><span class="attr">writeback_index:</span> <span class="string">elastalert_status</span></span><br><span class="line"><span class="attr">alert_time_limit:</span></span><br><span class="line">  <span class="attr">days:</span> <span class="number">2</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="Rule规则配置"><a href="#Rule规则配置" class="headerlink" title="Rule规则配置"></a>Rule规则配置</h3><p>我们网关服务器的Nginx日志是通过filebeat写到Elasticsearch的，也做好了index，其实在ELK 界面也可以看到这些状态数据，但是仅仅只是展示，没有告警，所以才有了这篇blog。</p>
<p>创建一个针对 某个API域名的日志的状态监控文件： </p>
<figure class="highlight nestedtext"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">vim  /opt/elastalert2/rules/mainnet-api.xxxxx.yaml</span></span><br><span class="line"><span class="attribute">#rule名字,必须唯一</span></span><br><span class="line"><span class="attribute">name</span><span class="punctuation">:</span> <span class="string">The number of times this request responds with a status code of 5xx in the log is greater than 5 times within 1 minute, please pay attention(mainnet-api.xxxxx)!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#类型,官方提供多种类型</span></span><br><span class="line"><span class="attribute">type</span><span class="punctuation">:</span> <span class="string">frequency</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ES索引,支持通配符</span></span><br><span class="line"><span class="attribute">index</span><span class="punctuation">:</span> <span class="string">proxy-mainnet-api.xxxxx.log-*</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在timeframe时间内,匹配到多少个结果便告警</span></span><br><span class="line"><span class="attribute">num_events</span><span class="punctuation">:</span> <span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#监控周期.默认是minutes: 1</span></span><br><span class="line"><span class="attribute">timeframe</span><span class="punctuation">:</span></span><br><span class="line">  <span class="attribute">seconds</span><span class="punctuation">:</span> <span class="string">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#匹配模式.这里我匹配的是500—599的所有可能会出现的状态码</span></span><br><span class="line"><span class="attribute">filter</span><span class="punctuation">:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">range:</span></span><br><span class="line">    <span class="attribute">status</span><span class="punctuation">:</span></span><br><span class="line">      <span class="attribute">from</span><span class="punctuation">:</span> <span class="string">500</span></span><br><span class="line">      <span class="attribute">to</span><span class="punctuation">:</span> <span class="string">599</span></span><br><span class="line"><span class="comment"># 自定义发送格式(start)</span></span><br><span class="line"><span class="comment"># 说明： 以下可以自定义发送的内容，如果什么都不指定，会发送ElastAlert2原生的一大堆json格式的内容，易读性很低，所以这里为了内容简介，我这里就只是定义了这些内容：</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">alert_text_type</span><span class="punctuation">:</span> <span class="string">alert_text_only</span></span><br><span class="line"><span class="attribute">alert_text</span><span class="punctuation">:</span> <span class="string">" </span></span><br><span class="line">  <span class="attribute">响应状态码</span><span class="punctuation">:</span> <span class="string">{} \n</span></span><br><span class="line">  <span class="attribute">发生时间UTC</span><span class="punctuation">:</span> <span class="string">{} \n</span></span><br><span class="line">  <span class="attribute">ES Index</span><span class="punctuation">:</span> <span class="string">{} \n</span></span><br><span class="line">  <span class="attribute">num_hits</span><span class="punctuation">:</span> <span class="string">{} \n</span></span><br><span class="line">  <span class="attribute">请求URL</span><span class="punctuation">:</span> <span class="string">https://mainnet-api.xxxxx{} \n</span></span><br><span class="line">  <span class="attribute">ClientIP</span><span class="punctuation">:</span> <span class="string">{} \n</span></span><br><span class="line">  <span class="attribute">num_matches</span><span class="punctuation">:</span> <span class="string">{} \n</span></span><br><span class="line">  <span class="attribute">http_user_agent</span><span class="punctuation">:</span> <span class="string">{}</span></span><br><span class="line"><span class="attribute">"</span></span><br><span class="line"><span class="attribute">alert_text_args</span><span class="punctuation">:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">status</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"@timestamp"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">_index</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">num_hits</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">request_uri</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">http_x_forwarded_for</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">num_matches</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">http_user_agent</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义发送格式(end)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里选择使用的是Discord来接收通知，ElastAlert2其实很强大，支持很多渠道的告警</span></span><br><span class="line"><span class="comment"># 这里发现并不能同时多个渠道发送告警消息</span></span><br><span class="line"><span class="comment"># 如果需要多个告警渠道，可以写一个类似于AlertCenter的服务，通过这个服务整合后发送到不同的渠道，这里这是个思路，目前先满足需求</span></span><br><span class="line"><span class="attribute">alert</span><span class="punctuation">:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">"discord"</span></span><br><span class="line"><span class="attribute">discord_webhook_url</span><span class="punctuation">:</span> <span class="string">"YOUR DISCORD WEBHOOK URL"</span></span><br><span class="line"><span class="attribute">discord_emoji_title</span><span class="punctuation">:</span> <span class="string">":lock:"</span></span><br><span class="line"><span class="attribute">discord_embed_color</span><span class="punctuation">:</span> <span class="string">0xE24D42</span></span><br><span class="line"><span class="attribute">discord_embed_footer</span><span class="punctuation">:</span> <span class="string">"Message sent by from api status moniotor(for @noardguo)"</span></span><br><span class="line"><span class="attribute">discord_embed_icon_url</span><span class="punctuation">:</span> <span class="string">"https://humancoders-formations.s3.amazonaws.com/uploads/course/logo/38/thumb_bigger_formation-elasticsearch.png"</span></span><br></pre></td></tr></tbody></table></figure>


<h3 id="部署运行"><a href="#部署运行" class="headerlink" title="部署运行"></a>部署运行</h3><p>上面已经准备好了所需要的文件：</p>
<figure class="highlight elixir"><table><tbody><tr><td class="code"><pre><span class="line">root<span class="variable">@ip</span><span class="number">-10</span><span class="number">-10</span><span class="number">-10</span><span class="number">-11</span><span class="symbol">:/opt/elastalert2</span><span class="comment"># tree -L 2</span></span><br><span class="line">.</span><br><span class="line">├── ca.crt</span><br><span class="line">├── config.yaml</span><br><span class="line">├── data</span><br><span class="line">└── rules</span><br><span class="line">    ├── mainnet-api.xxxxx.yml</span><br><span class="line"></span><br><span class="line"><span class="number">2</span> directories, <span class="number">3</span> files</span><br></pre></td></tr></tbody></table></figure>
<p>docker 方式运行：</p>
<figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line">docker run -d --name=elastalert2 \</span><br><span class="line">	--env=TZ=Asia/Shanghai \</span><br><span class="line">	--volume=/<span class="keyword">opt</span>/elastalert2/data:/<span class="keyword">opt</span>/elastalert/data \</span><br><span class="line">	--volume=/<span class="keyword">opt</span>/elastalert2/config.yaml:/<span class="keyword">opt</span>/elastalert/config.yaml \</span><br><span class="line">	--volume=/<span class="keyword">opt</span>/elastalert2/rules:/<span class="keyword">opt</span>/elastalert/rules \</span><br><span class="line">	--volume=/<span class="keyword">opt</span>/elastalert2/<span class="keyword">ca</span>.crt:/<span class="keyword">opt</span>/elastalert/<span class="keyword">ca</span>.crt \</span><br><span class="line">	--restart=always \</span><br><span class="line">	jertel/elastalert2 </span><br><span class="line"></span><br><span class="line">root@ip-<span class="number">10</span>-<span class="number">10</span>-<span class="number">10</span>-<span class="number">11</span>:/<span class="keyword">opt</span>/elastalert2# docker logs -<span class="keyword">f</span> wonderful_tharp</span><br><span class="line">Reading Elastic <span class="number">7</span> <span class="built_in">index</span> mappings:</span><br><span class="line">Reading <span class="built_in">index</span> mapping <span class="string">'es_mappings/7/silence.json'</span></span><br><span class="line">Reading <span class="built_in">index</span> mapping <span class="string">'es_mappings/7/elastalert_status.json'</span></span><br><span class="line">Reading <span class="built_in">index</span> mapping <span class="string">'es_mappings/7/elastalert.json'</span></span><br><span class="line">Reading <span class="built_in">index</span> mapping <span class="string">'es_mappings/7/past_elastalert.json'</span></span><br><span class="line">Reading <span class="built_in">index</span> mapping <span class="string">'es_mappings/7/elastalert_error.json'</span></span><br><span class="line">Index elastalert_status already <span class="built_in">exists</span>. Skipping <span class="built_in">index</span> creation.</span><br><span class="line">WARNING:<span class="keyword">py</span>.warnings:/usr/local/lib/<span class="keyword">python3</span>.<span class="number">12</span>/site-packages/elasticsearch/connection/base.<span class="keyword">py</span>:<span class="number">193</span>: ElasticsearchDeprecationWarning: Camel case format name dateOptionalTime <span class="keyword">is</span> deprecated <span class="built_in">and</span> will <span class="keyword">be</span> removed in <span class="keyword">a</span> future <span class="keyword">version</span>. Use snake case name date_optional_time instead.</span><br><span class="line">  warnings.warn(message, category=ElasticsearchDeprecationWarning)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>如果没有报错就说明运行正常，此时从kibana面板也可以看到ElaltAlert2生成了监控告警所需要的index:<br><img src="/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/1.jpeg"></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>当运行起来后，此时就是是时间监控日志文件的一个状态</p>
<p>在网关服务器上面echo一段带有status为500的message到日志文件中：</p>
<figure class="highlight apache"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attribute">echo</span> '''{<span class="string">"@timestamp"</span>:<span class="string">"2024-07-16T20:28:10+08:00"</span>,<span class="string">"server_addr"</span>:<span class="string">"110.10.30.187"</span>,<span class="string">"remote_addr"</span>:<span class="string">"10.10.10.248"</span>,<span class="string">"http_x_forwarded_for"</span>:<span class="string">"172.104.86.126, 172.68.119.188"</span>,<span class="string">"scheme"</span>:<span class="string">"http"</span>,<span class="string">"request_method"</span>:<span class="string">"POST"</span>,<span class="string">"request_uri"</span>: <span class="string">"Alert_Test"</span>,<span class="string">"request_length"</span>: <span class="string">"953"</span>,<span class="string">"uri"</span>: <span class="string">"/api/v1/alerttest"</span>, <span class="string">"request_time"</span>:<span class="number">0</span>.<span class="number">002</span>,<span class="string">"body_bytes_sent"</span>:<span class="number">0</span>,<span class="string">"bytes_sent"</span>:<span class="number">335</span>,<span class="string">"status"</span>:<span class="string">"500"</span>,<span class="string">"upstream_time"</span>:<span class="string">"0.002"</span>,<span class="string">"upstream_host"</span>:<span class="string">"10.10.10.139:31333"</span>,<span class="string">"upstream_status"</span>:<span class="string">"200"</span>,<span class="string">"host"</span>:<span class="string">"mainnet-api.xxxxxx"</span>,<span class="string">"http_referer"</span>:<span class="string">"https://test.xyz/"</span>,<span class="string">"http_user_agent"</span>:<span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"</span>}''' &gt;&gt; mainnet-api.xxxxxx.log</span><br></pre></td></tr></tbody></table></figure>
<p>此时查看 index已经记录到这条告警消息, 通过查询索引也找到了这个请求</p>
<p><img src="/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/2.jpeg"></p>
<p><img src="/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/4.png"></p>
<p>同时，Discord也收到了告警通知</p>
<p><img src="/2024/07/16/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ElastAlert2%E5%AF%B9ES%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E5%88%9B%E5%BB%BA%E5%91%8A%E8%AD%A6/3.png"></p>
<p>最后，因为我们跑了k8s,下面直接把相关文件用configmap方式挂载进去，运行一个Deployment副本即可。</p>
<ul>
<li>elastalert-config.yaml</li>
</ul>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">data:</span></span><br><span class="line">  config.<span class="params">yaml:</span> |-</span><br><span class="line">    <span class="params">rules_folder:</span> rules</span><br><span class="line">    <span class="params">run_every:</span></span><br><span class="line">      <span class="params">minutes:</span> <span class="number">1</span></span><br><span class="line">    <span class="params">buffer_time:</span></span><br><span class="line">      <span class="params">minutes:</span> <span class="number">15</span></span><br><span class="line">    <span class="params">es_host:</span> <span class="number">10.10</span>.<span class="number">20.23</span></span><br><span class="line">    <span class="params">es_port:</span> <span class="number">9200</span></span><br><span class="line">    <span class="params">use_ssl:</span> True</span><br><span class="line">    <span class="params">verify_certs:</span> True</span><br><span class="line">    <span class="params">ca_certs:</span> <span class="symbol">/opt/elastalert/ca.crt</span></span><br><span class="line">    <span class="params">ssl_show_warn:</span> True</span><br><span class="line">    <span class="params">es_send_get_body_as:</span> GET</span><br><span class="line">    <span class="params">es_username:</span> elastic</span><br><span class="line">    <span class="params">es_password:</span> A2VdKUHHFoUmlyekVFgd</span><br><span class="line">    <span class="params">writeback_index:</span> elastalert_status</span><br><span class="line">    <span class="params">alert_time_limit:</span></span><br><span class="line">      <span class="params">days:</span> <span class="number">2</span></span><br><span class="line"><span class="params">metadata:</span></span><br><span class="line">  <span class="params">name:</span> elastalert-config</span><br><span class="line">  <span class="params">namespace:</span> monitor</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>ealstalert-rules.yaml</li>
</ul>
<figure class="highlight nix"><table><tbody><tr><td class="code"><pre><span class="line"><span class="params">apiVersion:</span> v1</span><br><span class="line"><span class="params">data:</span></span><br><span class="line">  mainnet-api.xxxxxx.<span class="params">yml:</span> <span class="string">"#rule名字,必须唯一<span class="char escape_">\n</span>name: The number of times this</span></span><br><span class="line"><span class="string">    request responds with a status code of 5xx in the log is greater than 5 times</span></span><br><span class="line"><span class="string">    within 1 minute, please pay attention(mainnet-api.xxxxx)!<span class="char escape_">\n</span><span class="char escape_">\n</span>#类型,官方提供多种类型<span class="char escape_">\n</span>type:</span></span><br><span class="line"><span class="string">    frequency<span class="char escape_">\n</span><span class="char escape_">\n</span>#ES索引,支持通配符<span class="char escape_">\n</span>index: proxy-mainnet-api.xxxxx.log-*<span class="char escape_">\n</span><span class="char escape_">\n</span>#在timeframe时间内,匹配到多少个结果便告警<span class="char escape_">\n</span>num_events:</span></span><br><span class="line"><span class="string">    1<span class="char escape_">\n</span><span class="char escape_">\n</span>#监控周期.默认是minutes: 1<span class="char escape_">\n</span>timeframe:<span class="char escape_">\n</span>  seconds: 5  <span class="char escape_">\n</span>  <span class="char escape_">\n</span>#匹配模式.<span class="char escape_">\n</span>filter:<span class="char escape_">\n</span>- range:<span class="char escape_">\n</span></span></span><br><span class="line"><span class="string">    <span class="char escape_">\ </span>  status:<span class="char escape_">\n</span>      from: 500<span class="char escape_">\n</span>      to: 599<span class="char escape_">\n</span>      <span class="char escape_">\n</span>alert_text_type: alert_text_only<span class="char escape_">\n</span>alert_text:</span></span><br><span class="line"><span class="string">    <span class="char escape_">\"</span> <span class="char escape_">\n</span>  响应状态码: {} <span class="char escape_">\\</span>n<span class="char escape_">\n</span>  发生时间UTC: {} <span class="char escape_">\\</span>n<span class="char escape_">\n</span>  ES Index: {} <span class="char escape_">\\</span>n<span class="char escape_">\n</span>  num_hits: {} <span class="char escape_">\\</span>n<span class="char escape_">\n</span></span></span><br><span class="line"><span class="string">    <span class="char escape_">\ </span>请求URL: https://mainnet-api.xxxxx.org{} <span class="char escape_">\\</span>n<span class="char escape_">\n</span>  ClientIP: {} <span class="char escape_">\\</span>n<span class="char escape_">\n</span>  num_matches:</span></span><br><span class="line"><span class="string">    {} <span class="char escape_">\\</span>n<span class="char escape_">\n</span>  http_user_agent: {}<span class="char escape_">\n</span><span class="char escape_">\"</span><span class="char escape_">\n</span>alert_text_args:<span class="char escape_">\n</span>    - status<span class="char escape_">\n</span>    - <span class="char escape_">\"</span>@timestamp<span class="char escape_">\"</span><span class="char escape_">\n</span></span></span><br><span class="line"><span class="string">    <span class="char escape_">\ </span>  - _index<span class="char escape_">\n</span>    - num_hits<span class="char escape_">\n</span>    - request_uri<span class="char escape_">\n</span>    - http_x_forwarded_for<span class="char escape_">\n</span></span></span><br><span class="line"><span class="string">    <span class="char escape_">\ </span>  - num_matches<span class="char escape_">\n</span>    - http_user_agent<span class="char escape_">\n</span>      <span class="char escape_">\n</span>      <span class="char escape_">\n</span>alert:<span class="char escape_">\n</span>- <span class="char escape_">\"</span>discord<span class="char escape_">\"</span><span class="char escape_">\n</span>#discord_webhook_url:</span></span><br><span class="line"><span class="string">    <span class="char escape_">\"</span>DISCORD_WEBHOOK_URL"</span>\<span class="params">ndiscord_webhook_url:</span></span><br><span class="line">    \<span class="string">"https://https://discord.com/api/webhooks/1196752947493752933/VRQ01W1pT0PHpl55z0hayqsyjWzt3bzXUMSA4-_5W56fn9j5Nl1zDQT7ZtU_CQWnnlYH<span class="char escape_">\"</span><span class="char escape_">\n</span>discord_emoji_title:</span></span><br><span class="line"><span class="string">    <span class="char escape_">\"</span>:lock:<span class="char escape_">\"</span><span class="char escape_">\n</span>discord_embed_color: 0xE24D42<span class="char escape_">\n</span>discord_embed_footer: <span class="char escape_">\"</span>Message sent</span></span><br><span class="line"><span class="string">    by from api status moniotor(for @Ops-NoardGuo)<span class="char escape_">\"</span><span class="char escape_">\n</span>discord_embed_icon_url: <span class="char escape_">\"</span>https://humancoders-formations.s3.amazonaws.com/uploads/course/logo/38/thumb_bigger_formation-elasticsearch.png<span class="char escape_">\"</span></span></span><br><span class="line"><span class="string">kind: ConfigMap</span></span><br><span class="line"><span class="string">metadata:</span></span><br><span class="line"><span class="string">  name: ealstalert-rules</span></span><br><span class="line"><span class="string">  namespace: monitor</span></span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p>ca.crt<br>略</p>
</li>
<li><p>elastalert-deployment.yaml</p>
</li>
</ul>
<figure class="highlight yaml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">k8s.kuboard.cn/displayName:</span> <span class="string">elastalert</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s.kuboard.cn/name:</span> <span class="string">elastalert</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">elastalert</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">monitor</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">progressDeadlineSeconds:</span> <span class="number">600</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s.kuboard.cn/name:</span> <span class="string">elastalert</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxSurge:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s.kuboard.cn/name:</span> <span class="string">elastalert</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">'--verbose'</span> <span class="comment"># 这里指定参数是默认镜像里面可以获取的参数，用于定义日志记录 排错或者运行状态查看</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">jertel/elastalert2</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">elastalert</span></span><br><span class="line">          <span class="attr">resources:</span> {}</span><br><span class="line">          <span class="attr">terminationMessagePath:</span> <span class="string">/dev/termination-log</span></span><br><span class="line">          <span class="attr">terminationMessagePolicy:</span> <span class="string">File</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/opt/elastalert/config.yaml</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">volume-ps74r</span></span><br><span class="line">              <span class="attr">subPath:</span> <span class="string">config.yaml</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/opt/elastalert/rules/</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">rules</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/opt/elastalert/ca.crt</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">volume-zmdx7</span></span><br><span class="line">              <span class="attr">subPath:</span> <span class="string">ca.crt</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line">      <span class="attr">securityContext:</span> {}</span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">elastalert-config</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">volume-ps74r</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">            <span class="attr">items:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">mainnet-api.xxxxx.yml</span></span><br><span class="line">                <span class="attr">path:</span> <span class="string">mainnet-api.xxxxx.yml</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">ealstalert-rules</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">rules</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">elasticsearch-ca.crt</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">volume-zmdx7</span></span><br></pre></td></tr></tbody></table></figure>


<p>至此告警功能已经实现，以上只是一个大体的思路，不仅仅监控的可以是状态码，应该是你能想到的，他能提供的都可以监控起来。</p>
<p>好啦，我要去看5xx的报错原因了😀</p>
]]></content>
  </entry>
  <entry>
    <title>如何利用Python获取所有pod容器状态</title>
    <url>/2024/07/09/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8Python%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89pod%E5%AE%B9%E5%99%A8%E7%8A%B6%E6%80%81/</url>
    <content><![CDATA[<p>在日常巡检过程当中，不需要登录服务器去查看，通过调用k8s api的方式获取所有pod的状态</p>
<p>然后在每天9点执行本脚本即可。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> kubernetes <span class="keyword">import</span> client, config</span><br><span class="line"><span class="keyword">from</span> kubernetes.client.rest <span class="keyword">import</span> ApiException</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timezone</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> pytz</span><br><span class="line"><span class="comment"># 加载 Kubernetes 配置</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#默认会在master节点去读取 ~/.kube/config 文件</span></span><br><span class="line">config.load_kube_config()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Kubernetes API 客户端实例</span></span><br><span class="line">api_instance = client.CoreV1Api()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定要获取的命名空间列表</span></span><br><span class="line">target_namespaces = [sys.argv[<span class="number">1</span>]]  <span class="comment"># 替换为你的目标命名空间列表</span></span><br><span class="line"></span><br><span class="line">filtered_keywords = [<span class="string">"mysql"</span>, <span class="string">"redis"</span>, <span class="string">"memcached"</span>, <span class="string">"postgres"</span>, <span class="string">"backend"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Discord Webhook URL</span></span><br><span class="line">discord_webhook_url = <span class="string">""</span> <span class="comment"># FOR TEST</span></span><br><span class="line"><span class="comment"># 替换为你的 Discord Webhook URL</span></span><br><span class="line">containers_without_restart = []  <span class="comment"># 没有重启原因的容器列表</span></span><br><span class="line">containers_with_restart = []  <span class="comment"># 具有重启原因的容器列表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">for</span> target_namespace <span class="keyword">in</span> target_namespaces:</span><br><span class="line">        <span class="comment"># 获取命名空间下的所有 Pod</span></span><br><span class="line">        pods = api_instance.list_namespaced_pod(namespace=target_namespace).items</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> pod <span class="keyword">in</span> pods:</span><br><span class="line">            pod_name = pod.metadata.name</span><br><span class="line">            pod_status = pod.status.phase</span><br><span class="line">            pod_restart_reason = <span class="string">""</span></span><br><span class="line">            pod_start_time = pod.metadata.creation_timestamp</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">any</span>(keyword <span class="keyword">in</span> pod_name <span class="keyword">for</span> keyword <span class="keyword">in</span> filtered_keywords):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获取当前时间</span></span><br><span class="line">            cst_timezone = pytz.timezone(<span class="string">"Asia/Shanghai"</span>)</span><br><span class="line">            current_time = datetime.now(timezone.utc)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算运行时长</span></span><br><span class="line">            <span class="keyword">if</span> pod_start_time <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                pod_duration = current_time - pod_start_time</span><br><span class="line">                pod_duration_str = <span class="built_in">str</span>(pod_duration).split(<span class="string">"."</span>)[<span class="number">0</span>]  <span class="comment"># 格式化为字符串，去掉小数部分</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pod_duration_str = <span class="string">"Unknown"</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 检查 Pod 是否有重启记录</span></span><br><span class="line">            <span class="keyword">if</span> pod.status.container_statuses <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> container_status <span class="keyword">in</span> pod.status.container_statuses:</span><br><span class="line">                    restart_count = container_status.restart_count</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 如果重启次数大于 0，则获取重启原因</span></span><br><span class="line">                    <span class="keyword">if</span> restart_count &gt; <span class="number">0</span>:</span><br><span class="line">                        pod_restart_reason = container_status.last_state.terminated.reason</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> pod_restart_reason:</span><br><span class="line">                containers_with_restart.append(</span><br><span class="line">                    (pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count)</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                containers_without_restart.append(</span><br><span class="line">                    (pod_name, pod_status, pod_duration_str)</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成消息</span></span><br><span class="line">    message = <span class="string">"环境: {0} 获取时间：{1}\n\n"</span>.<span class="built_in">format</span>(target_namespace,datetime.now(cst_timezone).strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>) + <span class="string">" CST"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加没有重启原因的容器信息</span></span><br><span class="line">    <span class="keyword">for</span> container <span class="keyword">in</span> containers_without_restart:</span><br><span class="line">        pod_name, pod_status, pod_duration_str = container</span><br><span class="line">        message += <span class="string">"容器名称: {0}  当前状态: {1}  运行时长: {2}\n"</span>.<span class="built_in">format</span>(</span><br><span class="line">            pod_name, pod_status, pod_duration_str</span><br><span class="line">        )</span><br><span class="line">    message += <span class="string">"------------------------------------------------------\n"</span></span><br><span class="line">    <span class="comment"># 添加具有重启原因的容器信息</span></span><br><span class="line">    <span class="keyword">for</span> container <span class="keyword">in</span> containers_with_restart:</span><br><span class="line">        pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count = container</span><br><span class="line">        message += <span class="string">"容器名称: {0}  当前状态: {1}  运行时长: {2}  重启原因: {3} 重启次数：{4}\n"</span>.<span class="built_in">format</span>(</span><br><span class="line">            pod_name, pod_status, pod_duration_str, pod_restart_reason, restart_count</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    message = <span class="string">"```{0}```"</span>.<span class="built_in">format</span>(message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 发送消息到 Discord</span></span><br><span class="line">    payload = {<span class="string">"content"</span>: message}</span><br><span class="line">    headers = {<span class="string">"Content-Type"</span>: <span class="string">"application/json"</span>}</span><br><span class="line">    response = requests.post(</span><br><span class="line">        discord_webhook_url, data=json.dumps(payload), headers=headers</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(response.content)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">204</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Message sent to Discord successfully"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"Failed to send message to Discord. Status code: <span class="subst">{response.status_code}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> ApiException <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Exception when calling CoreV1Api: <span class="subst">{e}</span>\n"</span>)</span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s,pod</tag>
      </tags>
  </entry>
  <entry>
    <title>求职-系统运维/SRE方向</title>
    <url>/2025/09/04/%E6%B1%82%E8%81%8C%E5%B8%96/</url>
    <content><![CDATA[<p>各位朋友，大家好！</p>
<p>我于今年7月底正式离职，结束了上一段工作旅程。目前正在寻找下一份系统运维或SRE相关的工作，希望能借助大家的力量。</p>
<p>我有10年左右的运维经验，在这期间从事过<code>IDC</code>、<code>游戏</code>、<code>物联网</code>、<code>区块链</code>行业。熟练使用Linux操作系统、Kubernetes集群部署维护、监控系统部署维护以及二次开发以及一些自动化脚本工具开发的能力。在过去的工作中，我负责维护系统的稳定运行，处理过一些故障，也参与从传统架构到云原生架构的迁移和转型。我不是最顶尖的大牛，但我相信自己有很强的责任心、学习能力和团队协作精神，能脚踏实地地完成任务。</p>
<p>我个人非常热爱运维这个事业，作为运维/SRE在为公司创造价值的同时能满足自己的成就感是非常完美的一件事情。非常渴望能加入一个优秀的团队，与大家共同成长，为公司贡献出自己的全部能力。</p>
<p>如果您所在团队有招聘需求，或者有相关的信息，恳请您不吝推荐或联系我。感激不尽！</p>
<p><strong>📞 我的联系方式</strong><br>欢迎通过以下方式与我交流：</p>
<ul>
<li>📧 邮箱：<a href="mailto:guomaoqiu@icloud.com">guomaoqiu@icloud.com</a></li>
<li>🍓 TG: <a href="https://t.me/noardguo_devops">@noardguo_devops</a></li>
<li>💻 GitHub：<a href="https://github.com/guomaoqiu">https://github.com/guomaoqiu</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>运维工作中自动化巡检的必要性以及重要性</title>
    <url>/2022/05/12/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A1%E6%A3%80%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7%E4%BB%A5%E5%8F%8A%E9%87%8D%E8%A6%81%E6%80%A7/</url>
    <content><![CDATA[<h1 id="一、为什么要进行巡检"><a href="#一、为什么要进行巡检" class="headerlink" title="一、为什么要进行巡检"></a>一、为什么要进行巡检</h1><ol>
<li>当前平台架构复杂，中间件繁多，组件之间耦合度高，微服务还未达到故障自愈水平，所以需要通过告警或巡检等手段发现问题来保障平台持续稳定运行。</li>
<li>当前有客户对平台及上层应用使用频率低，例如三四天登录查看一次数据，但是设备是正常运转的，一旦平台出现问题，刚好客户发现问题，运维 才去解决就为时已晚。</li>
<li>定期巡检方案是模拟人工登录各业务页面，而非接口调用，更能真实地发现问题，并通过截图真实保留平台运行状态。</li>
<li>Prometheus平台的监控报警功能还未覆盖到整个业务系统，部分问题还未能实时监控到，导致平台出现异常后而无法感知。</li>
<li>当前并不能保证客户环境的Prometheus平台本身不存在问题，针对这种不确定性，定期巡检是一个保障平台稳定性的方案，实现平台双保障。</li>
<li>部分客户环境不能够连接外网，Prometheus的监控告警信息无法同步到微信、飞书等，但可以通过定期巡检方案来保障平台稳定运行。</li>
</ol>
<p>由于以上原因，为了保证SLA，必须进行定期巡检。</p>
<h1 id="二、巡检检查项"><a href="#二、巡检检查项" class="headerlink" title="二、巡检检查项"></a>二、巡检检查项</h1><h2 id="2-1-服务器基础信息"><a href="#2-1-服务器基础信息" class="headerlink" title="2.1  服务器基础信息"></a>2.1  服务器基础信息</h2><pre><code>	cpu 利用率
	磁盘利用率
	内存利用率
	服务器时间同步
	日常数据备份文件检查
</code></pre>
<h2 id="2-2-k8s集群状态"><a href="#2-2-k8s集群状态" class="headerlink" title="2.2  k8s集群状态"></a>2.2  k8s集群状态</h2><pre><code>	证书过期检查
	API通信是否正常
	各名称空间下的pod运行状态
	ceph共享存储是否正常
</code></pre>
<h2 id="2-3-业务状态"><a href="#2-3-业务状态" class="headerlink" title="2.3  业务状态"></a>2.3  业务状态</h2><pre><code>	业务平台登录是否正常
	kafka是否积压
	kafka消费速率
</code></pre>
<h1 id="三、实现方式"><a href="#三、实现方式" class="headerlink" title="三、实现方式"></a>三、实现方式</h1><p><strong>前期</strong>：前期巡检同事登录各客户环境进行人工手动巡检(登录VPN、连接跳板机、登录业务平台、登录grafana平台等等)一些列操作下来，一轮巡检工作大约在2-3hours。</p>
<p><strong>目前</strong>：通过自动化的方式(shell+python+web框架flask)实现了，人工在windows跳板机上、Linux服务器中的模拟人工操作连接VPN、登录业务平台、登录grafana平台等一系列操作，定期定时将巡检任务结果发送至企业微信群内；从单人单次巡检的2-3小时，直接提效到了5-10min，极大程度上提高了日常巡检的工作效率。<br><img src="/2022/05/12/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A1%E6%A3%80%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7%E4%BB%A5%E5%8F%8A%E9%87%8D%E8%A6%81%E6%80%A7/1.png"><br><img src="/2022/05/12/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A1%E6%A3%80%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7%E4%BB%A5%E5%8F%8A%E9%87%8D%E8%A6%81%E6%80%A7/2.png"></p>
<p><img src="/2022/05/12/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A1%E6%A3%80%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7%E4%BB%A5%E5%8F%8A%E9%87%8D%E8%A6%81%E6%80%A7/3.png"></p>
<p>那么，在客户环境数量达到一定体量时，群消息接收也会造成巡检遗漏的情况，在这种情况下需要一个集中化的平台作为展示，于是将巡检结果发送到群内的同时也会将消息格式化(注：图片是通过Python截图生成后将其转换为base64编码，然后将其他巡检结果内容格式化为json后post到web后端，再在web前端进行展示)</p>
<p><img src="/2022/05/12/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A1%E6%A3%80%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7%E4%BB%A5%E5%8F%8A%E9%87%8D%E8%A6%81%E6%80%A7/4.png"></p>
<p>Detail: 通过点击后弹出整个巡检过程以及结果信息；</p>
<p>Screenshoot: 通过点击后将会弹出base64编码转换为图片的业务平台及grafana截图</p>
<p>最后，巡检人员只需要定期浏览此汇总展示平台即可！</p>
<p>(人工操作是基础，自动化操作才是王道😀 )</p>
<h3 id="技术点"><a href="#技术点" class="headerlink" title="技术点"></a>技术点</h3><ul>
<li>Python自动化</li>
<li>Selenium爬虫技术(网页内容获取)</li>
<li>Windows端UI自动化(用于自动启动VPN程序，并填写账号密码后进行登录操作)</li>
<li>Flask前后端开发(平台展示，内容接收入库)</li>
<li>企业微信机器人Webhook集成</li>
</ul>
]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>自动化运维</tag>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
</search>
